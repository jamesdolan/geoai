{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GeoAI: Artificial Intelligence for Geospatial Data","text":"<p>A powerful Python package for integrating Artificial Intelligence with geospatial data analysis and visualization</p> <p>GeoAI bridges the gap between AI and geospatial analysis, providing tools for processing, analyzing, and visualizing geospatial data using advanced machine learning techniques. Whether you're working with satellite imagery, LiDAR point clouds, or vector data, GeoAI offers intuitive interfaces to apply cutting-edge AI models.</p> <ul> <li>\ud83d\udcd6 Documentation: https://geoai.gishub.org</li> <li>\ud83d\udcac Community: GitHub Discussions</li> <li>\ud83d\udc1b Issue Tracker: GitHub Issues</li> </ul>"},{"location":"#key-features","title":"\ud83d\ude80 Key Features","text":"<p>\u2757 Important notes: The GeoAI package is under active development and new features are being added regularly. Not all features listed below are available in the current release. If you have a feature request or would like to contribute, please let us know!</p>"},{"location":"#advanced-geospatial-data-visualization","title":"\ud83d\udcca Advanced Geospatial Data Visualization","text":"<ul> <li>Interactive multi-layer visualization of vector, raster, and point cloud data</li> <li>Customizable styling and symbology</li> <li>Time-series data visualization capabilities</li> </ul>"},{"location":"#data-preparation-processing","title":"\ud83d\udee0\ufe0f Data Preparation &amp; Processing","text":"<ul> <li>Streamlined access to satellite and aerial imagery from providers like Sentinel, Landsat, NAIP, and other open datasets</li> <li>Tools for downloading, mosaicking, and preprocessing remote sensing data</li> <li>Automated generation of training datasets with image chips and corresponding labels</li> <li>Vector-to-raster and raster-to-vector conversion utilities optimized for AI workflows</li> <li>Data augmentation techniques specific to geospatial data</li> <li>Support for integrating Overture Maps data and other open datasets for training and validation</li> </ul>"},{"location":"#image-segmentation","title":"\ud83d\uddbc\ufe0f Image Segmentation","text":"<ul> <li>Integration with Meta's Segment Anything Model (SAM) for automatic feature extraction</li> <li>Specialized segmentation algorithms optimized for satellite and aerial imagery</li> <li>Streamlined workflows for segmenting buildings, roads, vegetation, and water bodies</li> <li>Export capabilities to standard geospatial formats (GeoJSON, Shapefile, GeoPackage, GeoParquet)</li> </ul>"},{"location":"#image-classification","title":"\ud83d\udd0d Image Classification","text":"<ul> <li>Pre-trained models for land cover and land use classification</li> <li>Transfer learning utilities for fine-tuning models with your own data</li> <li>Multi-temporal classification support for change detection</li> <li>Accuracy assessment and validation tools</li> </ul>"},{"location":"#additional-capabilities","title":"\ud83c\udf0d Additional Capabilities","text":"<ul> <li>Terrain analysis with AI-enhanced feature extraction</li> <li>Point cloud classification and segmentation</li> <li>Object detection in aerial and satellite imagery</li> <li>Georeferencing utilities for AI model outputs</li> </ul>"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":""},{"location":"#using-pip","title":"Using pip","text":"<pre><code>pip install geoai-py\n</code></pre>"},{"location":"#using-conda","title":"Using conda","text":"<pre><code>conda install -c conda-forge geoai\n</code></pre>"},{"location":"#using-mamba","title":"Using mamba","text":"<pre><code>mamba install -c conda-forge geoai\n</code></pre>"},{"location":"#documentation","title":"\ud83d\udccb Documentation","text":"<p>Comprehensive documentation is available at https://geoai.gishub.org, including:</p> <ul> <li>Detailed API reference</li> <li>Tutorials and example notebooks</li> <li>Explanation of algorithms and models</li> <li>Best practices for geospatial AI</li> </ul>"},{"location":"#video-tutorials","title":"\ud83d\udcfa\u00a0Video Tutorials","text":"<p>Check out our YouTube channel for video tutorials on using GeoAI for geospatial data analysis and visualization.</p> <p></p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions of all kinds! See our contributing guide for ways to get started.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>GeoAI is free and open source software, licensed under the MIT License.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-aug-11-2023","title":"v0.0.1 - Aug 11, 2023","text":"<p>Initial release</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/opengeos/geoai/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>geoai could always use more documentation, whether as part of the official geoai docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/opengeos/geoai/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up geoai for local development.</p> <ol> <li> <p>Fork the geoai repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/geoai.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv geoai\n$ cd geoai/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 geoai tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and     for PyPy. Check https://github.com/opengeos/geoai/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"download/","title":"download module","text":"<p>This module provides functions to download data, including NAIP imagery and building data from Overture Maps.</p>"},{"location":"download/#geoai.download.convert_vector_format","title":"<code>convert_vector_format(input_file, output_format='geojson', filter_expression=None)</code>","text":"<p>Convert the downloaded data to a different format or filter it.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>Path to the input file.</p> required <code>output_format</code> <code>str</code> <p>Format to convert to, one of \"geojson\", \"parquet\", \"shapefile\", \"csv\".</p> <code>'geojson'</code> <code>filter_expression</code> <code>Optional[str]</code> <p>Optional GeoDataFrame query expression to filter the data.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the converted file.</p> Source code in <code>geoai/download.py</code> <pre><code>def convert_vector_format(\n    input_file: str,\n    output_format: str = \"geojson\",\n    filter_expression: Optional[str] = None,\n) -&gt; str:\n    \"\"\"Convert the downloaded data to a different format or filter it.\n\n    Args:\n        input_file: Path to the input file.\n        output_format: Format to convert to, one of \"geojson\", \"parquet\", \"shapefile\", \"csv\".\n        filter_expression: Optional GeoDataFrame query expression to filter the data.\n\n    Returns:\n        Path to the converted file.\n    \"\"\"\n    try:\n        # Read the input file\n        logger.info(f\"Reading {input_file}\")\n        gdf = gpd.read_file(input_file)\n\n        # Apply filter if specified\n        if filter_expression:\n            logger.info(f\"Filtering data using expression: {filter_expression}\")\n            gdf = gdf.query(filter_expression)\n            logger.info(f\"After filtering: {len(gdf)} features\")\n\n        # Define output file path\n        base_path = os.path.splitext(input_file)[0]\n\n        if output_format == \"geojson\":\n            output_file = f\"{base_path}.geojson\"\n            logger.info(f\"Converting to GeoJSON: {output_file}\")\n            gdf.to_file(output_file, driver=\"GeoJSON\")\n        elif output_format == \"parquet\":\n            output_file = f\"{base_path}.parquet\"\n            logger.info(f\"Converting to Parquet: {output_file}\")\n            gdf.to_parquet(output_file)\n        elif output_format == \"shapefile\":\n            output_file = f\"{base_path}.shp\"\n            logger.info(f\"Converting to Shapefile: {output_file}\")\n            gdf.to_file(output_file)\n        elif output_format == \"csv\":\n            output_file = f\"{base_path}.csv\"\n            logger.info(f\"Converting to CSV: {output_file}\")\n\n            # For CSV, we need to convert geometry to WKT\n            gdf[\"geometry_wkt\"] = gdf.geometry.apply(lambda g: g.wkt)\n\n            # Save to CSV with geometry as WKT\n            gdf.drop(columns=[\"geometry\"]).to_csv(output_file, index=False)\n        else:\n            raise ValueError(f\"Unsupported output format: {output_format}\")\n\n        return output_file\n\n    except Exception as e:\n        logger.error(f\"Error converting data: {str(e)}\")\n        raise\n</code></pre>"},{"location":"download/#geoai.download.download_naip","title":"<code>download_naip(bbox, output_dir, year=None, max_items=10, overwrite=False, preview=False, **kwargs)</code>","text":"<p>Download NAIP imagery from Planetary Computer based on a bounding box.</p> <p>This function searches for NAIP (National Agriculture Imagery Program) imagery from Microsoft's Planetary Computer that intersects with the specified bounding box. It downloads the imagery and saves it as GeoTIFF files.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save the downloaded imagery.</p> required <code>year</code> <code>Optional[int]</code> <p>Specific year of NAIP imagery to download (e.g., 2020). If None, returns imagery from all available years.</p> <code>None</code> <code>max_items</code> <code>int</code> <p>Maximum number of items to download.</p> <code>10</code> <code>overwrite</code> <code>bool</code> <p>If True, overwrite existing files with the same name.</p> <code>False</code> <code>preview</code> <code>bool</code> <p>If True, display a preview of the downloaded imagery.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of downloaded file paths.</p> <p>Exceptions:</p> Type Description <code>Exception</code> <p>If there is an error downloading or saving the imagery.</p> Source code in <code>geoai/download.py</code> <pre><code>def download_naip(\n    bbox: Tuple[float, float, float, float],\n    output_dir: str,\n    year: Optional[int] = None,\n    max_items: int = 10,\n    overwrite: bool = False,\n    preview: bool = False,\n    **kwargs: Any,\n) -&gt; List[str]:\n    \"\"\"Download NAIP imagery from Planetary Computer based on a bounding box.\n\n    This function searches for NAIP (National Agriculture Imagery Program) imagery\n    from Microsoft's Planetary Computer that intersects with the specified bounding box.\n    It downloads the imagery and saves it as GeoTIFF files.\n\n    Args:\n        bbox: Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.\n        output_dir: Directory to save the downloaded imagery.\n        year: Specific year of NAIP imagery to download (e.g., 2020). If None, returns imagery from all available years.\n        max_items: Maximum number of items to download.\n        overwrite: If True, overwrite existing files with the same name.\n        preview: If True, display a preview of the downloaded imagery.\n\n    Returns:\n        List of downloaded file paths.\n\n    Raises:\n        Exception: If there is an error downloading or saving the imagery.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Create a geometry from the bounding box\n    geometry = box(*bbox)\n\n    # Connect to Planetary Computer STAC API\n    catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n\n    # Build query for NAIP data\n    search_params = {\n        \"collections\": [\"naip\"],\n        \"intersects\": geometry,\n        \"limit\": max_items,\n    }\n\n    # Add year filter if specified\n    if year:\n        search_params[\"query\"] = {\"naip:year\": {\"eq\": year}}\n\n    for key, value in kwargs.items():\n        search_params[key] = value\n\n    # Search for NAIP imagery\n    search_results = catalog.search(**search_params)\n    items = list(search_results.items())\n\n    if len(items) &gt; max_items:\n        items = items[:max_items]\n\n    if not items:\n        print(\"No NAIP imagery found for the specified region and parameters.\")\n        return []\n\n    print(f\"Found {len(items)} NAIP items.\")\n\n    # Download and save each item\n    downloaded_files = []\n    for i, item in enumerate(items):\n        # Sign the assets (required for Planetary Computer)\n        signed_item = pc.sign(item)\n\n        # Get the RGB asset URL\n        rgb_asset = signed_item.assets.get(\"image\")\n        if not rgb_asset:\n            print(f\"No RGB asset found for item {i+1}\")\n            continue\n\n        # Use the original filename from the asset\n        original_filename = os.path.basename(\n            rgb_asset.href.split(\"?\")[0]\n        )  # Remove query parameters\n        output_path = os.path.join(output_dir, original_filename)\n        if not overwrite and os.path.exists(output_path):\n            print(f\"Skipping existing file: {output_path}\")\n            downloaded_files.append(output_path)\n            continue\n\n        print(f\"Downloading item {i+1}/{len(items)}: {original_filename}\")\n\n        try:\n            # Open and save the data with progress bar\n            # For direct file download with progress bar\n            if rgb_asset.href.startswith(\"http\"):\n                download_with_progress(rgb_asset.href, output_path)\n                #\n            else:\n                # Fallback to direct rioxarray opening (less common case)\n                data = rioxarray.open_rasterio(rgb_asset.href)\n                data.rio.to_raster(output_path)\n\n            downloaded_files.append(output_path)\n            print(f\"Successfully saved to {output_path}\")\n\n            # Optional: Display a preview (uncomment if needed)\n            if preview:\n                data = rioxarray.open_rasterio(output_path)\n                preview_raster(data)\n\n        except Exception as e:\n            print(f\"Error downloading item {i+1}: {str(e)}\")\n\n    return downloaded_files\n</code></pre>"},{"location":"download/#geoai.download.download_overture_buildings","title":"<code>download_overture_buildings(bbox, output_file, output_format='geojson', data_type='building', verbose=True)</code>","text":"<p>Download building data from Overture Maps for a given bounding box using the overturemaps CLI tool.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.</p> required <code>output_file</code> <code>str</code> <p>Path to save the output file.</p> required <code>output_format</code> <code>str</code> <p>Format to save the output, one of \"geojson\", \"geojsonseq\", or \"geoparquet\".</p> <code>'geojson'</code> <code>data_type</code> <code>str</code> <p>The Overture Maps data type to download (building, place, etc.).</p> <code>'building'</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the output file.</p> Source code in <code>geoai/download.py</code> <pre><code>def download_overture_buildings(\n    bbox: Tuple[float, float, float, float],\n    output_file: str,\n    output_format: str = \"geojson\",\n    data_type: str = \"building\",\n    verbose: bool = True,\n) -&gt; str:\n    \"\"\"Download building data from Overture Maps for a given bounding box using the overturemaps CLI tool.\n\n    Args:\n        bbox: Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.\n        output_file: Path to save the output file.\n        output_format: Format to save the output, one of \"geojson\", \"geojsonseq\", or \"geoparquet\".\n        data_type: The Overture Maps data type to download (building, place, etc.).\n        verbose: Whether to print verbose output.\n\n    Returns:\n        Path to the output file.\n    \"\"\"\n    # Create output directory if needed\n    output_dir = os.path.dirname(output_file)\n    if output_dir and not os.path.exists(output_dir):\n        os.makedirs(output_dir, exist_ok=True)\n\n    # Format the bounding box string for the command\n    west, south, east, north = bbox\n    bbox_str = f\"{west},{south},{east},{north}\"\n\n    # Build the command\n    cmd = [\n        \"overturemaps\",\n        \"download\",\n        \"--bbox\",\n        bbox_str,\n        \"-f\",\n        output_format,\n        \"--type\",\n        data_type,\n        \"--output\",\n        output_file,\n    ]\n\n    if verbose:\n        logger.info(f\"Running command: {' '.join(cmd)}\")\n        logger.info(\"Downloading %s data for area: %s\", data_type, bbox_str)\n\n    try:\n        # Run the command\n        result = subprocess.run(\n            cmd,\n            check=True,\n            stdout=subprocess.PIPE if not verbose else None,\n            stderr=subprocess.PIPE,\n            text=True,\n        )\n\n        # Check if the file was created\n        if os.path.exists(output_file):\n            file_size = os.path.getsize(output_file) / (1024 * 1024)  # Size in MB\n            logger.info(\n                f\"Successfully downloaded data to {output_file} ({file_size:.2f} MB)\"\n            )\n\n            # Optionally show some stats about the downloaded data\n            if output_format == \"geojson\" and os.path.getsize(output_file) &gt; 0:\n                try:\n                    gdf = gpd.read_file(output_file)\n                    logger.info(f\"Downloaded {len(gdf)} features\")\n\n                    if len(gdf) &gt; 0 and verbose:\n                        # Show a sample of the attribute names\n                        attrs = list(gdf.columns)\n                        attrs.remove(\"geometry\")\n                        logger.info(f\"Available attributes: {', '.join(attrs[:10])}...\")\n                except Exception as e:\n                    logger.warning(f\"Could not read the GeoJSON file: {str(e)}\")\n\n            return output_file\n        else:\n            logger.error(f\"Command completed but file {output_file} was not created\")\n            if result.stderr:\n                logger.error(f\"Command error output: {result.stderr}\")\n            return None\n\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Error running overturemaps command: {str(e)}\")\n        if e.stderr:\n            logger.error(f\"Command error output: {e.stderr}\")\n        raise RuntimeError(f\"Failed to download Overture Maps data: {str(e)}\")\n    except Exception as e:\n        logger.error(f\"Unexpected error: {str(e)}\")\n        raise\n</code></pre>"},{"location":"download/#geoai.download.download_with_progress","title":"<code>download_with_progress(url, output_path)</code>","text":"<p>Download a file with a progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of the file to download.</p> required <code>output_path</code> <code>str</code> <p>Path where the file will be saved.</p> required Source code in <code>geoai/download.py</code> <pre><code>def download_with_progress(url: str, output_path: str) -&gt; None:\n    \"\"\"Download a file with a progress bar.\n\n    Args:\n        url: URL of the file to download.\n        output_path: Path where the file will be saved.\n    \"\"\"\n    response = requests.get(url, stream=True)\n    total_size = int(response.headers.get(\"content-length\", 0))\n    block_size = 1024  # 1 Kibibyte\n\n    with (\n        open(output_path, \"wb\") as file,\n        tqdm(\n            desc=os.path.basename(output_path),\n            total=total_size,\n            unit=\"iB\",\n            unit_scale=True,\n            unit_divisor=1024,\n        ) as bar,\n    ):\n        for data in response.iter_content(block_size):\n            size = file.write(data)\n            bar.update(size)\n</code></pre>"},{"location":"download/#geoai.download.extract_building_stats","title":"<code>extract_building_stats(geojson_file)</code>","text":"<p>Extract statistics from the building data.</p> <p>Parameters:</p> Name Type Description Default <code>geojson_file</code> <code>str</code> <p>Path to the GeoJSON file.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with statistics.</p> Source code in <code>geoai/download.py</code> <pre><code>def extract_building_stats(geojson_file: str) -&gt; Dict[str, Any]:\n    \"\"\"Extract statistics from the building data.\n\n    Args:\n        geojson_file: Path to the GeoJSON file.\n\n    Returns:\n        Dictionary with statistics.\n    \"\"\"\n    try:\n        # Read the GeoJSON file\n        gdf = gpd.read_file(geojson_file)\n\n        # Calculate statistics\n        bbox = gdf.total_bounds.tolist()\n        # Convert numpy values to Python native types\n        bbox = [float(x) for x in bbox]\n\n        stats = {\n            \"total_buildings\": int(len(gdf)),\n            \"has_height\": (\n                int(gdf[\"height\"].notna().sum()) if \"height\" in gdf.columns else 0\n            ),\n            \"has_name\": (\n                int(gdf[\"names.common.value\"].notna().sum())\n                if \"names.common.value\" in gdf.columns\n                else 0\n            ),\n            \"bbox\": bbox,\n        }\n\n        return stats\n\n    except Exception as e:\n        logger.error(f\"Error extracting statistics: {str(e)}\")\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"download/#geoai.download.json_serializable","title":"<code>json_serializable(obj)</code>","text":"<p>Convert NumPy types to native Python types for JSON serialization.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Any object to convert.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>JSON serializable version of the object.</p> Source code in <code>geoai/download.py</code> <pre><code>def json_serializable(obj: Any) -&gt; Any:\n    \"\"\"Convert NumPy types to native Python types for JSON serialization.\n\n    Args:\n        obj: Any object to convert.\n\n    Returns:\n        JSON serializable version of the object.\n    \"\"\"\n    if isinstance(obj, np.integer):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    else:\n        return obj\n</code></pre>"},{"location":"download/#geoai.download.preview_raster","title":"<code>preview_raster(data, title=None)</code>","text":"<p>Display a preview of the downloaded imagery.</p> <p>This function creates a visualization of the downloaded NAIP imagery by converting it to an RGB array and displaying it with matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The raster data as a rioxarray object.</p> required <code>title</code> <code>str</code> <p>The title for the preview plot.</p> <code>None</code> Source code in <code>geoai/download.py</code> <pre><code>def preview_raster(data: Any, title: str = None) -&gt; None:\n    \"\"\"Display a preview of the downloaded imagery.\n\n    This function creates a visualization of the downloaded NAIP imagery\n    by converting it to an RGB array and displaying it with matplotlib.\n\n    Args:\n        data: The raster data as a rioxarray object.\n        title: The title for the preview plot.\n    \"\"\"\n    # Convert to 8-bit RGB for display\n    rgb_data = data.transpose(\"y\", \"x\", \"band\").values[:, :, 0:3]\n    rgb_data = np.where(rgb_data &gt; 255, 255, rgb_data).astype(np.uint8)\n\n    plt.figure(figsize=(10, 10))\n    plt.imshow(rgb_data)\n    if title is not None:\n        plt.title(title)\n    plt.axis(\"off\")\n    plt.show()\n</code></pre>"},{"location":"extract/","title":"extract module","text":""},{"location":"extract/#geoai.extract.BuildingFootprintExtractor","title":"<code> BuildingFootprintExtractor            (ObjectDetector)         </code>","text":"<p>Building footprint extraction using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for building footprint extraction.\"</p> Source code in <code>geoai/extract.py</code> <pre><code>class BuildingFootprintExtractor(ObjectDetector):\n    \"\"\"\n    Building footprint extraction using a pre-trained Mask R-CNN model.\n\n    This class extends the\n    `ObjectDetector` class with additional methods for building footprint extraction.\"\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path=\"building_footprints_usa.pth\",\n        repo_id=None,\n        model=None,\n        device=None,\n    ):\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n\n    def regularize_buildings(\n        self,\n        gdf,\n        min_area=10,\n        angle_threshold=15,\n        orthogonality_threshold=0.3,\n        rectangularity_threshold=0.7,\n    ):\n        \"\"\"\n        Regularize building footprints to enforce right angles and rectangular shapes.\n\n        Args:\n            gdf: GeoDataFrame with building footprints\n            min_area: Minimum area in square units to keep a building\n            angle_threshold: Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)\n            orthogonality_threshold: Percentage of angles that must be orthogonal for a building to be regularized\n            rectangularity_threshold: Minimum area ratio to building's oriented bounding box for rectangular simplification\n\n        Returns:\n            GeoDataFrame with regularized building footprints\n        \"\"\"\n        return self.regularize_objects(\n            gdf,\n            min_area=min_area,\n            angle_threshold=angle_threshold,\n            orthogonality_threshold=orthogonality_threshold,\n            rectangularity_threshold=rectangularity_threshold,\n        )\n</code></pre>"},{"location":"extract/#geoai.extract.BuildingFootprintExtractor.__init__","title":"<code>__init__(self, model_path='building_footprints_usa.pth', repo_id=None, model=None, device=None)</code>  <code>special</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <p>Path to the .pth model file.</p> <code>'building_footprints_usa.pth'</code> <code>repo_id</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path=\"building_footprints_usa.pth\",\n    repo_id=None,\n    model=None,\n    device=None,\n):\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n</code></pre>"},{"location":"extract/#geoai.extract.BuildingFootprintExtractor.regularize_buildings","title":"<code>regularize_buildings(self, gdf, min_area=10, angle_threshold=15, orthogonality_threshold=0.3, rectangularity_threshold=0.7)</code>","text":"<p>Regularize building footprints to enforce right angles and rectangular shapes.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <p>GeoDataFrame with building footprints</p> required <code>min_area</code> <p>Minimum area in square units to keep a building</p> <code>10</code> <code>angle_threshold</code> <p>Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)</p> <code>15</code> <code>orthogonality_threshold</code> <p>Percentage of angles that must be orthogonal for a building to be regularized</p> <code>0.3</code> <code>rectangularity_threshold</code> <p>Minimum area ratio to building's oriented bounding box for rectangular simplification</p> <code>0.7</code> <p>Returns:</p> Type Description <p>GeoDataFrame with regularized building footprints</p> Source code in <code>geoai/extract.py</code> <pre><code>def regularize_buildings(\n    self,\n    gdf,\n    min_area=10,\n    angle_threshold=15,\n    orthogonality_threshold=0.3,\n    rectangularity_threshold=0.7,\n):\n    \"\"\"\n    Regularize building footprints to enforce right angles and rectangular shapes.\n\n    Args:\n        gdf: GeoDataFrame with building footprints\n        min_area: Minimum area in square units to keep a building\n        angle_threshold: Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)\n        orthogonality_threshold: Percentage of angles that must be orthogonal for a building to be regularized\n        rectangularity_threshold: Minimum area ratio to building's oriented bounding box for rectangular simplification\n\n    Returns:\n        GeoDataFrame with regularized building footprints\n    \"\"\"\n    return self.regularize_objects(\n        gdf,\n        min_area=min_area,\n        angle_threshold=angle_threshold,\n        orthogonality_threshold=orthogonality_threshold,\n        rectangularity_threshold=rectangularity_threshold,\n    )\n</code></pre>"},{"location":"extract/#geoai.extract.CarDetector","title":"<code> CarDetector            (ObjectDetector)         </code>","text":"<p>Car detection using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for car detection.\"</p> Source code in <code>geoai/extract.py</code> <pre><code>class CarDetector(ObjectDetector):\n    \"\"\"\n    Car detection using a pre-trained Mask R-CNN model.\n\n    This class extends the\n    `ObjectDetector` class with additional methods for car detection.\"\n    \"\"\"\n\n    def __init__(\n        self, model_path=\"car_detection_usa.pth\", repo_id=None, model=None, device=None\n    ):\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n</code></pre>"},{"location":"extract/#geoai.extract.CarDetector.__init__","title":"<code>__init__(self, model_path='car_detection_usa.pth', repo_id=None, model=None, device=None)</code>  <code>special</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <p>Path to the .pth model file.</p> <code>'car_detection_usa.pth'</code> <code>repo_id</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self, model_path=\"car_detection_usa.pth\", repo_id=None, model=None, device=None\n):\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n</code></pre>"},{"location":"extract/#geoai.extract.CustomDataset","title":"<code> CustomDataset            (NonGeoDataset)         </code>","text":"<p>A TorchGeo dataset for object extraction. Using NonGeoDataset to avoid spatial indexing issues.</p> Source code in <code>geoai/extract.py</code> <pre><code>class CustomDataset(NonGeoDataset):\n    \"\"\"\n    A TorchGeo dataset for object extraction.\n    Using NonGeoDataset to avoid spatial indexing issues.\n    \"\"\"\n\n    def __init__(\n        self, raster_path, chip_size=(512, 512), transforms=None, verbose=False\n    ):\n        \"\"\"\n        Initialize the dataset.\n\n        Args:\n            raster_path: Path to the input raster file\n            chip_size: Size of image chips to extract (height, width)\n            transforms: Transforms to apply to the image\n            verbose: Whether to print detailed processing information\n        \"\"\"\n        super().__init__()\n\n        # Initialize parameters\n        self.raster_path = raster_path\n        self.chip_size = chip_size\n        self.transforms = transforms\n        self.verbose = verbose\n\n        # For tracking warnings about multi-band images\n        self.warned_about_bands = False\n\n        # Open raster and get metadata\n        with rasterio.open(self.raster_path) as src:\n            self.crs = src.crs\n            self.transform = src.transform\n            self.height = src.height\n            self.width = src.width\n            self.count = src.count\n\n            # Define the bounds of the dataset\n            west, south, east, north = src.bounds\n            self.bounds = (west, south, east, north)\n\n            # Define the ROI for the dataset\n            self.roi = box(*self.bounds)\n\n            # Calculate number of chips in each dimension\n            # Use ceil division to ensure we cover the entire image\n            self.rows = (self.height + self.chip_size[0] - 1) // self.chip_size[0]\n            self.cols = (self.width + self.chip_size[1] - 1) // self.chip_size[1]\n\n            print(\n                f\"Dataset initialized with {self.rows} rows and {self.cols} columns of chips\"\n            )\n            print(f\"Image dimensions: {self.width} x {self.height} pixels\")\n            print(f\"Chip size: {self.chip_size[1]} x {self.chip_size[0]} pixels\")\n            if src.crs:\n                print(f\"CRS: {src.crs}\")\n\n        # get raster stats\n        self.raster_stats = get_raster_stats(raster_path, divide_by=255)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Get an image chip from the dataset by index.\n\n        Args:\n            idx: Index of the chip\n\n        Returns:\n            Dict containing image tensor\n        \"\"\"\n        # Convert flat index to grid position\n        row = idx // self.cols\n        col = idx % self.cols\n\n        # Calculate pixel coordinates\n        i = col * self.chip_size[1]\n        j = row * self.chip_size[0]\n\n        # Read window from raster\n        with rasterio.open(self.raster_path) as src:\n            # Make sure we don't read outside the image\n            width = min(self.chip_size[1], self.width - i)\n            height = min(self.chip_size[0], self.height - j)\n\n            window = Window(i, j, width, height)\n            image = src.read(window=window)\n\n            # Handle RGBA or multispectral images - keep only first 3 bands\n            if image.shape[0] &gt; 3:\n                if not self.warned_about_bands and self.verbose:\n                    print(f\"Image has {image.shape[0]} bands, using first 3 bands only\")\n                    self.warned_about_bands = True\n                image = image[:3]\n            elif image.shape[0] &lt; 3:\n                # If image has fewer than 3 bands, duplicate the last band to make 3\n                if not self.warned_about_bands and self.verbose:\n                    print(\n                        f\"Image has {image.shape[0]} bands, duplicating bands to make 3\"\n                    )\n                    self.warned_about_bands = True\n                temp = np.zeros((3, image.shape[1], image.shape[2]), dtype=image.dtype)\n                for c in range(3):\n                    temp[c] = image[min(c, image.shape[0] - 1)]\n                image = temp\n\n            # Handle partial windows at edges by padding\n            if (\n                image.shape[1] != self.chip_size[0]\n                or image.shape[2] != self.chip_size[1]\n            ):\n                temp = np.zeros(\n                    (image.shape[0], self.chip_size[0], self.chip_size[1]),\n                    dtype=image.dtype,\n                )\n                temp[:, : image.shape[1], : image.shape[2]] = image\n                image = temp\n\n        # Convert to format expected by model (C,H,W)\n        image = torch.from_numpy(image).float()\n\n        # Normalize to [0, 1]\n        if image.max() &gt; 1:\n            image = image / 255.0\n\n        # Apply transforms if any\n        if self.transforms is not None:\n            image = self.transforms(image)\n\n        # Create geographic bounding box for the window\n        minx, miny = self.transform * (i, j + height)\n        maxx, maxy = self.transform * (i + width, j)\n        bbox = box(minx, miny, maxx, maxy)\n\n        return {\n            \"image\": image,\n            \"bbox\": bbox,\n            \"coords\": torch.tensor([i, j], dtype=torch.long),  # Consistent format\n            \"window_size\": torch.tensor(\n                [width, height], dtype=torch.long\n            ),  # Consistent format\n        }\n\n    def __len__(self):\n        \"\"\"Return the number of samples in the dataset.\"\"\"\n        return self.rows * self.cols\n</code></pre>"},{"location":"extract/#geoai.extract.CustomDataset.__getitem__","title":"<code>__getitem__(self, idx)</code>  <code>special</code>","text":"<p>Get an image chip from the dataset by index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <p>Index of the chip</p> required <p>Returns:</p> Type Description <p>Dict containing image tensor</p> Source code in <code>geoai/extract.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Get an image chip from the dataset by index.\n\n    Args:\n        idx: Index of the chip\n\n    Returns:\n        Dict containing image tensor\n    \"\"\"\n    # Convert flat index to grid position\n    row = idx // self.cols\n    col = idx % self.cols\n\n    # Calculate pixel coordinates\n    i = col * self.chip_size[1]\n    j = row * self.chip_size[0]\n\n    # Read window from raster\n    with rasterio.open(self.raster_path) as src:\n        # Make sure we don't read outside the image\n        width = min(self.chip_size[1], self.width - i)\n        height = min(self.chip_size[0], self.height - j)\n\n        window = Window(i, j, width, height)\n        image = src.read(window=window)\n\n        # Handle RGBA or multispectral images - keep only first 3 bands\n        if image.shape[0] &gt; 3:\n            if not self.warned_about_bands and self.verbose:\n                print(f\"Image has {image.shape[0]} bands, using first 3 bands only\")\n                self.warned_about_bands = True\n            image = image[:3]\n        elif image.shape[0] &lt; 3:\n            # If image has fewer than 3 bands, duplicate the last band to make 3\n            if not self.warned_about_bands and self.verbose:\n                print(\n                    f\"Image has {image.shape[0]} bands, duplicating bands to make 3\"\n                )\n                self.warned_about_bands = True\n            temp = np.zeros((3, image.shape[1], image.shape[2]), dtype=image.dtype)\n            for c in range(3):\n                temp[c] = image[min(c, image.shape[0] - 1)]\n            image = temp\n\n        # Handle partial windows at edges by padding\n        if (\n            image.shape[1] != self.chip_size[0]\n            or image.shape[2] != self.chip_size[1]\n        ):\n            temp = np.zeros(\n                (image.shape[0], self.chip_size[0], self.chip_size[1]),\n                dtype=image.dtype,\n            )\n            temp[:, : image.shape[1], : image.shape[2]] = image\n            image = temp\n\n    # Convert to format expected by model (C,H,W)\n    image = torch.from_numpy(image).float()\n\n    # Normalize to [0, 1]\n    if image.max() &gt; 1:\n        image = image / 255.0\n\n    # Apply transforms if any\n    if self.transforms is not None:\n        image = self.transforms(image)\n\n    # Create geographic bounding box for the window\n    minx, miny = self.transform * (i, j + height)\n    maxx, maxy = self.transform * (i + width, j)\n    bbox = box(minx, miny, maxx, maxy)\n\n    return {\n        \"image\": image,\n        \"bbox\": bbox,\n        \"coords\": torch.tensor([i, j], dtype=torch.long),  # Consistent format\n        \"window_size\": torch.tensor(\n            [width, height], dtype=torch.long\n        ),  # Consistent format\n    }\n</code></pre>"},{"location":"extract/#geoai.extract.CustomDataset.__init__","title":"<code>__init__(self, raster_path, chip_size=(512, 512), transforms=None, verbose=False)</code>  <code>special</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <p>Path to the input raster file</p> required <code>chip_size</code> <p>Size of image chips to extract (height, width)</p> <code>(512, 512)</code> <code>transforms</code> <p>Transforms to apply to the image</p> <code>None</code> <code>verbose</code> <p>Whether to print detailed processing information</p> <code>False</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self, raster_path, chip_size=(512, 512), transforms=None, verbose=False\n):\n    \"\"\"\n    Initialize the dataset.\n\n    Args:\n        raster_path: Path to the input raster file\n        chip_size: Size of image chips to extract (height, width)\n        transforms: Transforms to apply to the image\n        verbose: Whether to print detailed processing information\n    \"\"\"\n    super().__init__()\n\n    # Initialize parameters\n    self.raster_path = raster_path\n    self.chip_size = chip_size\n    self.transforms = transforms\n    self.verbose = verbose\n\n    # For tracking warnings about multi-band images\n    self.warned_about_bands = False\n\n    # Open raster and get metadata\n    with rasterio.open(self.raster_path) as src:\n        self.crs = src.crs\n        self.transform = src.transform\n        self.height = src.height\n        self.width = src.width\n        self.count = src.count\n\n        # Define the bounds of the dataset\n        west, south, east, north = src.bounds\n        self.bounds = (west, south, east, north)\n\n        # Define the ROI for the dataset\n        self.roi = box(*self.bounds)\n\n        # Calculate number of chips in each dimension\n        # Use ceil division to ensure we cover the entire image\n        self.rows = (self.height + self.chip_size[0] - 1) // self.chip_size[0]\n        self.cols = (self.width + self.chip_size[1] - 1) // self.chip_size[1]\n\n        print(\n            f\"Dataset initialized with {self.rows} rows and {self.cols} columns of chips\"\n        )\n        print(f\"Image dimensions: {self.width} x {self.height} pixels\")\n        print(f\"Chip size: {self.chip_size[1]} x {self.chip_size[0]} pixels\")\n        if src.crs:\n            print(f\"CRS: {src.crs}\")\n\n    # get raster stats\n    self.raster_stats = get_raster_stats(raster_path, divide_by=255)\n</code></pre>"},{"location":"extract/#geoai.extract.CustomDataset.__len__","title":"<code>__len__(self)</code>  <code>special</code>","text":"<p>Return the number of samples in the dataset.</p> Source code in <code>geoai/extract.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the number of samples in the dataset.\"\"\"\n    return self.rows * self.cols\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector","title":"<code> ObjectDetector        </code>","text":"<p>Object extraction using Mask R-CNN with TorchGeo.</p> Source code in <code>geoai/extract.py</code> <pre><code>class ObjectDetector:\n    \"\"\"\n    Object extraction using Mask R-CNN with TorchGeo.\n    \"\"\"\n\n    def __init__(self, model_path=None, repo_id=None, model=None, device=None):\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Hugging Face repository ID for model download.\n            model: Pre-initialized model object (optional).\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        # Set device\n        if device is None:\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = torch.device(device)\n\n        # Default parameters for object detection - these can be overridden in process_raster\n        self.chip_size = (512, 512)  # Size of image chips for processing\n        self.overlap = 0.25  # Default overlap between tiles\n        self.confidence_threshold = 0.5  # Default confidence threshold\n        self.nms_iou_threshold = 0.5  # IoU threshold for non-maximum suppression\n        self.min_object_area = 100  # Minimum area in pixels to keep an object\n        self.max_object_area = None  # Maximum area in pixels to keep an object\n        self.mask_threshold = 0.5  # Threshold for mask binarization\n        self.simplify_tolerance = 1.0  # Tolerance for polygon simplification\n\n        # Initialize model\n        self.model = self.initialize_model(model)\n\n        # Download model if needed\n        if model_path is None or (not os.path.exists(model_path)):\n            model_path = self.download_model_from_hf(model_path, repo_id)\n\n        # Load model weights\n        self.load_weights(model_path)\n\n        # Set model to evaluation mode\n        self.model.eval()\n\n    def download_model_from_hf(self, model_path=None, repo_id=None):\n        \"\"\"\n        Download the object detection model from Hugging Face.\n\n        Args:\n            model_path: Path to the model file.\n            repo_id: Hugging Face repository ID.\n\n        Returns:\n            Path to the downloaded model file\n        \"\"\"\n        try:\n\n            print(\"Model path not specified, downloading from Hugging Face...\")\n\n            # Define the repository ID and model filename\n            if repo_id is None:\n                repo_id = \"giswqs/geoai\"\n\n            if model_path is None:\n                model_path = \"building_footprints_usa.pth\"\n\n            # Download the model\n            model_path = hf_hub_download(repo_id=repo_id, filename=model_path)\n            print(f\"Model downloaded to: {model_path}\")\n\n            return model_path\n\n        except Exception as e:\n            print(f\"Error downloading model from Hugging Face: {e}\")\n            print(\"Please specify a local model path or ensure internet connectivity.\")\n            raise\n\n    def initialize_model(self, model):\n        \"\"\"Initialize a deep learning model for object detection.\n\n        Args:\n            model (torch.nn.Module): A pre-initialized model object.\n\n        Returns:\n            torch.nn.Module: A deep learning model for object detection.\n        \"\"\"\n\n        if model is None:  # Initialize Mask R-CNN model with ResNet50 backbone.\n            # Standard image mean and std for pre-trained models\n            image_mean = [0.485, 0.456, 0.406]\n            image_std = [0.229, 0.224, 0.225]\n\n            # Create model with explicit normalization parameters\n            model = maskrcnn_resnet50_fpn(\n                weights=None,\n                progress=False,\n                num_classes=2,  # Background + object\n                weights_backbone=None,\n                # These parameters ensure consistent normalization\n                image_mean=image_mean,\n                image_std=image_std,\n            )\n\n        model.to(self.device)\n        return model\n\n    def load_weights(self, model_path):\n        \"\"\"\n        Load weights from file with error handling for different formats.\n\n        Args:\n            model_path: Path to model weights\n        \"\"\"\n        if not os.path.exists(model_path):\n            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n\n        try:\n            state_dict = torch.load(model_path, map_location=self.device)\n\n            # Handle different state dict formats\n            if isinstance(state_dict, dict):\n                if \"model\" in state_dict:\n                    state_dict = state_dict[\"model\"]\n                elif \"state_dict\" in state_dict:\n                    state_dict = state_dict[\"state_dict\"]\n\n            # Try to load state dict\n            try:\n                self.model.load_state_dict(state_dict)\n                print(\"Model loaded successfully\")\n            except Exception as e:\n                print(f\"Error loading model: {e}\")\n                print(\"Attempting to fix state_dict keys...\")\n\n                # Try to fix state_dict keys (remove module prefix if needed)\n                new_state_dict = {}\n                for k, v in state_dict.items():\n                    if k.startswith(\"module.\"):\n                        new_state_dict[k[7:]] = v\n                    else:\n                        new_state_dict[k] = v\n\n                self.model.load_state_dict(new_state_dict)\n                print(\"Model loaded successfully after key fixing\")\n\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load model: {e}\")\n\n    def mask_to_polygons(self, mask, **kwargs):\n        \"\"\"\n        Convert binary mask to polygon contours using OpenCV.\n\n        Args:\n            mask: Binary mask as numpy array\n            **kwargs: Optional parameters:\n                simplify_tolerance: Tolerance for polygon simplification\n                mask_threshold: Threshold for mask binarization\n                min_object_area: Minimum area in pixels to keep an object\n                max_object_area: Maximum area in pixels to keep an object\n\n        Returns:\n            List of polygons as lists of (x, y) coordinates\n        \"\"\"\n\n        # Get parameters from kwargs or use instance defaults\n        simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n        mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n        min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n        max_object_area = kwargs.get(\"max_object_area\", self.max_object_area)\n\n        # Ensure binary mask\n        mask = (mask &gt; mask_threshold).astype(np.uint8)\n\n        # Optional: apply morphological operations to improve mask quality\n        kernel = np.ones((3, 3), np.uint8)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Convert to list of [x, y] coordinates\n        polygons = []\n        for contour in contours:\n            # Filter out too small contours\n            if contour.shape[0] &lt; 3 or cv2.contourArea(contour) &lt; min_object_area:\n                continue\n\n            # Filter out too large contours\n            if (\n                max_object_area is not None\n                and cv2.contourArea(contour) &gt; max_object_area\n            ):\n                continue\n\n            # Simplify contour if it has many points\n            if contour.shape[0] &gt; 50:\n                epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                contour = cv2.approxPolyDP(contour, epsilon, True)\n\n            # Convert to list of [x, y] coordinates\n            polygon = contour.reshape(-1, 2).tolist()\n            polygons.append(polygon)\n\n        return polygons\n\n    def filter_overlapping_polygons(self, gdf, **kwargs):\n        \"\"\"\n        Filter overlapping polygons using non-maximum suppression.\n\n        Args:\n            gdf: GeoDataFrame with polygons\n            **kwargs: Optional parameters:\n                nms_iou_threshold: IoU threshold for filtering\n\n        Returns:\n            Filtered GeoDataFrame\n        \"\"\"\n        if len(gdf) &lt;= 1:\n            return gdf\n\n        # Get parameters from kwargs or use instance defaults\n        iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n\n        # Sort by confidence\n        gdf = gdf.sort_values(\"confidence\", ascending=False)\n\n        # Fix any invalid geometries\n        gdf[\"geometry\"] = gdf[\"geometry\"].apply(\n            lambda geom: geom.buffer(0) if not geom.is_valid else geom\n        )\n\n        keep_indices = []\n        polygons = gdf.geometry.values\n\n        for i in range(len(polygons)):\n            if i in keep_indices:\n                continue\n\n            keep = True\n            for j in keep_indices:\n                # Skip invalid geometries\n                if not polygons[i].is_valid or not polygons[j].is_valid:\n                    continue\n\n                # Calculate IoU\n                try:\n                    intersection = polygons[i].intersection(polygons[j]).area\n                    union = polygons[i].area + polygons[j].area - intersection\n                    iou = intersection / union if union &gt; 0 else 0\n\n                    if iou &gt; iou_threshold:\n                        keep = False\n                        break\n                except Exception:\n                    # Skip on topology exceptions\n                    continue\n\n            if keep:\n                keep_indices.append(i)\n\n        return gdf.iloc[keep_indices]\n\n    def filter_edge_objects(self, gdf, raster_path, edge_buffer=10):\n        \"\"\"\n        Filter out object detections that fall in padding/edge areas of the image.\n\n        Args:\n            gdf: GeoDataFrame with object detections\n            raster_path: Path to the original raster file\n            edge_buffer: Buffer in pixels to consider as edge region\n\n        Returns:\n            GeoDataFrame with filtered objects\n        \"\"\"\n        import rasterio\n        from shapely.geometry import box\n\n        # If no objects detected, return empty GeoDataFrame\n        if gdf is None or len(gdf) == 0:\n            return gdf\n\n        print(f\"Objects before filtering: {len(gdf)}\")\n\n        with rasterio.open(raster_path) as src:\n            # Get raster bounds\n            raster_bounds = src.bounds\n            raster_width = src.width\n            raster_height = src.height\n\n            # Convert edge buffer from pixels to geographic units\n            # We need the smallest dimension of a pixel in geographic units\n            pixel_width = (raster_bounds[2] - raster_bounds[0]) / raster_width\n            pixel_height = (raster_bounds[3] - raster_bounds[1]) / raster_height\n            buffer_size = min(pixel_width, pixel_height) * edge_buffer\n\n            # Create a slightly smaller bounding box to exclude edge regions\n            inner_bounds = (\n                raster_bounds[0] + buffer_size,  # min x (west)\n                raster_bounds[1] + buffer_size,  # min y (south)\n                raster_bounds[2] - buffer_size,  # max x (east)\n                raster_bounds[3] - buffer_size,  # max y (north)\n            )\n\n            # Check that inner bounds are valid\n            if inner_bounds[0] &gt;= inner_bounds[2] or inner_bounds[1] &gt;= inner_bounds[3]:\n                print(\"Warning: Edge buffer too large, using original bounds\")\n                inner_box = box(*raster_bounds)\n            else:\n                inner_box = box(*inner_bounds)\n\n            # Filter out objects that intersect with the edge of the image\n            filtered_gdf = gdf[gdf.intersects(inner_box)]\n\n            # Additional check for objects that have &gt;50% of their area outside the valid region\n            valid_objects = []\n            for idx, row in filtered_gdf.iterrows():\n                if row.geometry.intersection(inner_box).area &gt;= 0.5 * row.geometry.area:\n                    valid_objects.append(idx)\n\n            filtered_gdf = filtered_gdf.loc[valid_objects]\n\n            print(f\"Objects after filtering: {len(filtered_gdf)}\")\n\n            return filtered_gdf\n\n    def masks_to_vector(\n        self,\n        mask_path,\n        output_path=None,\n        simplify_tolerance=None,\n        mask_threshold=None,\n        min_object_area=None,\n        max_object_area=None,\n        nms_iou_threshold=None,\n        regularize=True,\n        angle_threshold=15,\n        rectangularity_threshold=0.7,\n    ):\n        \"\"\"\n        Convert an object mask GeoTIFF to vector polygons and save as GeoJSON.\n\n        Args:\n            mask_path: Path to the object masks GeoTIFF\n            output_path: Path to save the output GeoJSON (default: mask_path with .geojson extension)\n            simplify_tolerance: Tolerance for polygon simplification (default: self.simplify_tolerance)\n            mask_threshold: Threshold for mask binarization (default: self.mask_threshold)\n            min_object_area: Minimum area in pixels to keep an object (default: self.min_object_area)\n            max_object_area: Minimum area in pixels to keep an object (default: self.max_object_area)\n            nms_iou_threshold: IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)\n            regularize: Whether to regularize objects to right angles (default: True)\n            angle_threshold: Maximum deviation from 90 degrees for regularization (default: 15)\n            rectangularity_threshold: Threshold for rectangle simplification (default: 0.7)\n\n        Returns:\n            GeoDataFrame with objects\n        \"\"\"\n        # Use class defaults if parameters not provided\n        simplify_tolerance = (\n            simplify_tolerance\n            if simplify_tolerance is not None\n            else self.simplify_tolerance\n        )\n        mask_threshold = (\n            mask_threshold if mask_threshold is not None else self.mask_threshold\n        )\n        min_object_area = (\n            min_object_area if min_object_area is not None else self.min_object_area\n        )\n        max_object_area = (\n            max_object_area if max_object_area is not None else self.max_object_area\n        )\n        nms_iou_threshold = (\n            nms_iou_threshold\n            if nms_iou_threshold is not None\n            else self.nms_iou_threshold\n        )\n\n        # Set default output path if not provided\n        # if output_path is None:\n        #     output_path = os.path.splitext(mask_path)[0] + \".geojson\"\n\n        print(f\"Converting mask to GeoJSON with parameters:\")\n        print(f\"- Mask threshold: {mask_threshold}\")\n        print(f\"- Min object area: {min_object_area}\")\n        print(f\"- Max object area: {max_object_area}\")\n        print(f\"- Simplify tolerance: {simplify_tolerance}\")\n        print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n        print(f\"- Regularize objects: {regularize}\")\n        if regularize:\n            print(f\"- Angle threshold: {angle_threshold}\u00b0 from 90\u00b0\")\n            print(f\"- Rectangularity threshold: {rectangularity_threshold*100}%\")\n\n        # Open the mask raster\n        with rasterio.open(mask_path) as src:\n            # Read the mask data\n            mask_data = src.read(1)\n            transform = src.transform\n            crs = src.crs\n\n            # Print mask statistics\n            print(f\"Mask dimensions: {mask_data.shape}\")\n            print(f\"Mask value range: {mask_data.min()} to {mask_data.max()}\")\n\n            # Prepare for connected component analysis\n            # Binarize the mask based on threshold\n            binary_mask = (mask_data &gt; (mask_threshold * 255)).astype(np.uint8)\n\n            # Apply morphological operations for better results (optional)\n            kernel = np.ones((3, 3), np.uint8)\n            binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n\n            # Find connected components\n            num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n                binary_mask, connectivity=8\n            )\n\n            print(\n                f\"Found {num_labels-1} potential objects\"\n            )  # Subtract 1 for background\n\n            # Create list to store polygons and confidence values\n            all_polygons = []\n            all_confidences = []\n\n            # Process each component (skip the first one which is background)\n            for i in tqdm(range(1, num_labels)):\n                # Extract this object\n                area = stats[i, cv2.CC_STAT_AREA]\n\n                # Skip if too small\n                if area &lt; min_object_area:\n                    continue\n\n                # Skip if too large\n                if max_object_area is not None and area &gt; max_object_area:\n                    continue\n\n                # Create a mask for this object\n                object_mask = (labels == i).astype(np.uint8)\n\n                # Find contours\n                contours, _ = cv2.findContours(\n                    object_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n                )\n\n                # Process each contour\n                for contour in contours:\n                    # Skip if too few points\n                    if contour.shape[0] &lt; 3:\n                        continue\n\n                    # Simplify contour if it has many points\n                    if contour.shape[0] &gt; 50 and simplify_tolerance &gt; 0:\n                        epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                        contour = cv2.approxPolyDP(contour, epsilon, True)\n\n                    # Convert to list of (x, y) coordinates\n                    polygon_points = contour.reshape(-1, 2)\n\n                    # Convert pixel coordinates to geographic coordinates\n                    geo_points = []\n                    for x, y in polygon_points:\n                        gx, gy = transform * (x, y)\n                        geo_points.append((gx, gy))\n\n                    # Create Shapely polygon\n                    if len(geo_points) &gt;= 3:\n                        try:\n                            shapely_poly = Polygon(geo_points)\n                            if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                                all_polygons.append(shapely_poly)\n\n                                # Calculate \"confidence\" as normalized size\n                                # This is a proxy since we don't have model confidence scores\n                                normalized_size = min(1.0, area / 1000)  # Cap at 1.0\n                                all_confidences.append(normalized_size)\n                        except Exception as e:\n                            print(f\"Error creating polygon: {e}\")\n\n            print(f\"Created {len(all_polygons)} valid polygons\")\n\n            # Create GeoDataFrame\n            if not all_polygons:\n                print(\"No valid polygons found\")\n                return None\n\n            gdf = gpd.GeoDataFrame(\n                {\n                    \"geometry\": all_polygons,\n                    \"confidence\": all_confidences,\n                    \"class\": 1,  # Object class\n                },\n                crs=crs,\n            )\n\n            # Apply non-maximum suppression to remove overlapping polygons\n            gdf = self.filter_overlapping_polygons(\n                gdf, nms_iou_threshold=nms_iou_threshold\n            )\n\n            print(f\"Object count after NMS filtering: {len(gdf)}\")\n\n            # Apply regularization if requested\n            if regularize and len(gdf) &gt; 0:\n                # Convert pixel area to geographic units for min_area parameter\n                # Estimate pixel size in geographic units\n                with rasterio.open(mask_path) as src:\n                    pixel_size_x = src.transform[\n                        0\n                    ]  # width of a pixel in geographic units\n                    pixel_size_y = abs(\n                        src.transform[4]\n                    )  # height of a pixel in geographic units\n                    avg_pixel_area = pixel_size_x * pixel_size_y\n\n                # Use 10 pixels as minimum area in geographic units\n                min_geo_area = 10 * avg_pixel_area\n\n                # Regularize objects\n                gdf = self.regularize_objects(\n                    gdf,\n                    min_area=min_geo_area,\n                    angle_threshold=angle_threshold,\n                    rectangularity_threshold=rectangularity_threshold,\n                )\n\n            # Save to file\n            if output_path:\n                gdf.to_file(output_path)\n                print(f\"Saved {len(gdf)} objects to {output_path}\")\n\n            return gdf\n\n    @torch.no_grad()\n    def process_raster(\n        self,\n        raster_path,\n        output_path=None,\n        batch_size=4,\n        filter_edges=True,\n        edge_buffer=20,\n        **kwargs,\n    ):\n        \"\"\"\n        Process a raster file to extract objects with customizable parameters.\n\n        Args:\n            raster_path: Path to input raster file\n            output_path: Path to output GeoJSON file (optional)\n            batch_size: Batch size for processing\n            filter_edges: Whether to filter out objects at the edges of the image\n            edge_buffer: Size of edge buffer in pixels to filter out objects (if filter_edges=True)\n            **kwargs: Additional parameters:\n                confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n                overlap: Overlap between adjacent tiles (0.0-1.0)\n                chip_size: Size of image chips for processing (height, width)\n                nms_iou_threshold: IoU threshold for non-maximum suppression (0.0-1.0)\n                mask_threshold: Threshold for mask binarization (0.0-1.0)\n                min_object_area: Minimum area in pixels to keep an object\n                simplify_tolerance: Tolerance for polygon simplification\n\n        Returns:\n            GeoDataFrame with objects\n        \"\"\"\n        # Get parameters from kwargs or use instance defaults\n        confidence_threshold = kwargs.get(\n            \"confidence_threshold\", self.confidence_threshold\n        )\n        overlap = kwargs.get(\"overlap\", self.overlap)\n        chip_size = kwargs.get(\"chip_size\", self.chip_size)\n        nms_iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n        mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n        min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n        max_object_area = kwargs.get(\"max_object_area\", self.max_object_area)\n        simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n\n        # Print parameters being used\n        print(f\"Processing with parameters:\")\n        print(f\"- Confidence threshold: {confidence_threshold}\")\n        print(f\"- Tile overlap: {overlap}\")\n        print(f\"- Chip size: {chip_size}\")\n        print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n        print(f\"- Mask threshold: {mask_threshold}\")\n        print(f\"- Min object area: {min_object_area}\")\n        print(f\"- Max object area: {max_object_area}\")\n        print(f\"- Simplify tolerance: {simplify_tolerance}\")\n        print(f\"- Filter edge objects: {filter_edges}\")\n        if filter_edges:\n            print(f\"- Edge buffer size: {edge_buffer} pixels\")\n\n        # Create dataset\n        dataset = CustomDataset(raster_path=raster_path, chip_size=chip_size)\n        self.raster_stats = dataset.raster_stats\n\n        # Custom collate function to handle Shapely objects\n        def custom_collate(batch):\n            \"\"\"\n            Custom collate function that handles Shapely geometries\n            by keeping them as Python objects rather than trying to collate them.\n            \"\"\"\n            elem = batch[0]\n            if isinstance(elem, dict):\n                result = {}\n                for key in elem:\n                    if key == \"bbox\":\n                        # Don't collate shapely objects, keep as list\n                        result[key] = [d[key] for d in batch]\n                    else:\n                        # For tensors and other collatable types\n                        try:\n                            result[key] = (\n                                torch.utils.data._utils.collate.default_collate(\n                                    [d[key] for d in batch]\n                                )\n                            )\n                        except TypeError:\n                            # Fall back to list for non-collatable types\n                            result[key] = [d[key] for d in batch]\n                return result\n            else:\n                # Default collate for non-dict types\n                return torch.utils.data._utils.collate.default_collate(batch)\n\n        # Create dataloader with simple indexing and custom collate\n        dataloader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=0,\n            collate_fn=custom_collate,\n        )\n\n        # Process batches\n        all_polygons = []\n        all_scores = []\n\n        print(f\"Processing raster with {len(dataloader)} batches\")\n        for batch in tqdm(dataloader):\n            # Move images to device\n            images = batch[\"image\"].to(self.device)\n            coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n            bboxes = batch[\n                \"bbox\"\n            ]  # Geographic bounding boxes - now a list, not a tensor\n\n            # Run inference\n            predictions = self.model(images)\n\n            # Process predictions\n            for idx, prediction in enumerate(predictions):\n                masks = prediction[\"masks\"].cpu().numpy()\n                scores = prediction[\"scores\"].cpu().numpy()\n                labels = prediction[\"labels\"].cpu().numpy()\n\n                # Skip if no predictions\n                if len(scores) == 0:\n                    continue\n\n                # Filter by confidence threshold\n                valid_indices = scores &gt;= confidence_threshold\n                masks = masks[valid_indices]\n                scores = scores[valid_indices]\n                labels = labels[valid_indices]\n\n                # Skip if no valid predictions\n                if len(scores) == 0:\n                    continue\n\n                # Get window coordinates\n                # The coords might be in different formats depending on batch handling\n                if isinstance(coords, list):\n                    # If coords is a list of tuples\n                    coord_item = coords[idx]\n                    if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                        i, j = coord_item\n                    elif isinstance(coord_item, torch.Tensor):\n                        i, j = coord_item.cpu().numpy().tolist()\n                    else:\n                        print(f\"Unexpected coords format: {type(coord_item)}\")\n                        continue\n                elif isinstance(coords, torch.Tensor):\n                    # If coords is a tensor of shape [batch_size, 2]\n                    i, j = coords[idx].cpu().numpy().tolist()\n                else:\n                    print(f\"Unexpected coords type: {type(coords)}\")\n                    continue\n\n                # Get window size\n                if isinstance(batch[\"window_size\"], list):\n                    window_item = batch[\"window_size\"][idx]\n                    if isinstance(window_item, tuple) and len(window_item) == 2:\n                        window_width, window_height = window_item\n                    elif isinstance(window_item, torch.Tensor):\n                        window_width, window_height = window_item.cpu().numpy().tolist()\n                    else:\n                        print(f\"Unexpected window_size format: {type(window_item)}\")\n                        continue\n                elif isinstance(batch[\"window_size\"], torch.Tensor):\n                    window_width, window_height = (\n                        batch[\"window_size\"][idx].cpu().numpy().tolist()\n                    )\n                else:\n                    print(f\"Unexpected window_size type: {type(batch['window_size'])}\")\n                    continue\n\n                # Process masks to polygons\n                for mask_idx, mask in enumerate(masks):\n                    # Get binary mask\n                    binary_mask = mask[0]  # Get binary mask\n\n                    # Convert mask to polygon with custom parameters\n                    contours = self.mask_to_polygons(\n                        binary_mask,\n                        simplify_tolerance=simplify_tolerance,\n                        mask_threshold=mask_threshold,\n                        min_object_area=min_object_area,\n                        max_object_area=max_object_area,\n                    )\n\n                    # Skip if no valid polygons\n                    if not contours:\n                        continue\n\n                    # Transform polygons to geographic coordinates\n                    with rasterio.open(raster_path) as src:\n                        transform = src.transform\n\n                        for contour in contours:\n                            # Convert polygon to global coordinates\n                            global_polygon = []\n                            for x, y in contour:\n                                # Adjust coordinates based on window position\n                                gx, gy = transform * (i + x, j + y)\n                                global_polygon.append((gx, gy))\n\n                            # Create Shapely polygon\n                            if len(global_polygon) &gt;= 3:\n                                try:\n                                    shapely_poly = Polygon(global_polygon)\n                                    if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                                        all_polygons.append(shapely_poly)\n                                        all_scores.append(float(scores[mask_idx]))\n                                except Exception as e:\n                                    print(f\"Error creating polygon: {e}\")\n\n        # Create GeoDataFrame\n        if not all_polygons:\n            print(\"No valid polygons found\")\n            return None\n\n        gdf = gpd.GeoDataFrame(\n            {\n                \"geometry\": all_polygons,\n                \"confidence\": all_scores,\n                \"class\": 1,  # Object class\n            },\n            crs=dataset.crs,\n        )\n\n        # Remove overlapping polygons with custom threshold\n        gdf = self.filter_overlapping_polygons(gdf, nms_iou_threshold=nms_iou_threshold)\n\n        # Filter edge objects if requested\n        if filter_edges:\n            gdf = self.filter_edge_objects(gdf, raster_path, edge_buffer=edge_buffer)\n\n        # Save to file if requested\n        if output_path:\n            gdf.to_file(output_path, driver=\"GeoJSON\")\n            print(f\"Saved {len(gdf)} objects to {output_path}\")\n\n        return gdf\n\n    def save_masks_as_geotiff(\n        self, raster_path, output_path=None, batch_size=4, verbose=False, **kwargs\n    ):\n        \"\"\"\n        Process a raster file to extract object masks and save as GeoTIFF.\n\n        Args:\n            raster_path: Path to input raster file\n            output_path: Path to output GeoTIFF file (optional, default: input_masks.tif)\n            batch_size: Batch size for processing\n            verbose: Whether to print detailed processing information\n            **kwargs: Additional parameters:\n                confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n                chip_size: Size of image chips for processing (height, width)\n                mask_threshold: Threshold for mask binarization (0.0-1.0)\n\n        Returns:\n            Path to the saved GeoTIFF file\n        \"\"\"\n\n        # Get parameters from kwargs or use instance defaults\n        confidence_threshold = kwargs.get(\n            \"confidence_threshold\", self.confidence_threshold\n        )\n        chip_size = kwargs.get(\"chip_size\", self.chip_size)\n        mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n\n        # Set default output path if not provided\n        if output_path is None:\n            output_path = os.path.splitext(raster_path)[0] + \"_masks.tif\"\n\n        # Print parameters being used\n        print(f\"Processing masks with parameters:\")\n        print(f\"- Confidence threshold: {confidence_threshold}\")\n        print(f\"- Chip size: {chip_size}\")\n        print(f\"- Mask threshold: {mask_threshold}\")\n\n        # Create dataset\n        dataset = CustomDataset(\n            raster_path=raster_path, chip_size=chip_size, verbose=verbose\n        )\n\n        # Store a flag to avoid repetitive messages\n        self.raster_stats = dataset.raster_stats\n        seen_warnings = {\n            \"bands\": False,\n            \"resize\": {},  # Dictionary to track resize warnings by shape\n        }\n\n        # Open original raster to get metadata\n        with rasterio.open(raster_path) as src:\n            # Create output binary mask raster with same dimensions as input\n            output_profile = src.profile.copy()\n            output_profile.update(\n                dtype=rasterio.uint8,\n                count=1,  # Single band for object mask\n                compress=\"lzw\",\n                nodata=0,\n            )\n\n            # Create output mask raster\n            with rasterio.open(output_path, \"w\", **output_profile) as dst:\n                # Initialize mask with zeros\n                mask_array = np.zeros((src.height, src.width), dtype=np.uint8)\n\n                # Custom collate function to handle Shapely objects\n                def custom_collate(batch):\n                    \"\"\"Custom collate function for DataLoader\"\"\"\n                    elem = batch[0]\n                    if isinstance(elem, dict):\n                        result = {}\n                        for key in elem:\n                            if key == \"bbox\":\n                                # Don't collate shapely objects, keep as list\n                                result[key] = [d[key] for d in batch]\n                            else:\n                                # For tensors and other collatable types\n                                try:\n                                    result[key] = (\n                                        torch.utils.data._utils.collate.default_collate(\n                                            [d[key] for d in batch]\n                                        )\n                                    )\n                                except TypeError:\n                                    # Fall back to list for non-collatable types\n                                    result[key] = [d[key] for d in batch]\n                        return result\n                    else:\n                        # Default collate for non-dict types\n                        return torch.utils.data._utils.collate.default_collate(batch)\n\n                # Create dataloader\n                dataloader = torch.utils.data.DataLoader(\n                    dataset,\n                    batch_size=batch_size,\n                    shuffle=False,\n                    num_workers=0,\n                    collate_fn=custom_collate,\n                )\n\n                # Process batches\n                print(f\"Processing raster with {len(dataloader)} batches\")\n                for batch in tqdm(dataloader):\n                    # Move images to device\n                    images = batch[\"image\"].to(self.device)\n                    coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n\n                    # Run inference\n                    with torch.no_grad():\n                        predictions = self.model(images)\n\n                    # Process predictions\n                    for idx, prediction in enumerate(predictions):\n                        masks = prediction[\"masks\"].cpu().numpy()\n                        scores = prediction[\"scores\"].cpu().numpy()\n\n                        # Skip if no predictions\n                        if len(scores) == 0:\n                            continue\n\n                        # Filter by confidence threshold\n                        valid_indices = scores &gt;= confidence_threshold\n                        masks = masks[valid_indices]\n                        scores = scores[valid_indices]\n\n                        # Skip if no valid predictions\n                        if len(scores) == 0:\n                            continue\n\n                        # Get window coordinates\n                        if isinstance(coords, list):\n                            coord_item = coords[idx]\n                            if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                                i, j = coord_item\n                            elif isinstance(coord_item, torch.Tensor):\n                                i, j = coord_item.cpu().numpy().tolist()\n                            else:\n                                print(f\"Unexpected coords format: {type(coord_item)}\")\n                                continue\n                        elif isinstance(coords, torch.Tensor):\n                            i, j = coords[idx].cpu().numpy().tolist()\n                        else:\n                            print(f\"Unexpected coords type: {type(coords)}\")\n                            continue\n\n                        # Get window size\n                        if isinstance(batch[\"window_size\"], list):\n                            window_item = batch[\"window_size\"][idx]\n                            if isinstance(window_item, tuple) and len(window_item) == 2:\n                                window_width, window_height = window_item\n                            elif isinstance(window_item, torch.Tensor):\n                                window_width, window_height = (\n                                    window_item.cpu().numpy().tolist()\n                                )\n                            else:\n                                print(\n                                    f\"Unexpected window_size format: {type(window_item)}\"\n                                )\n                                continue\n                        elif isinstance(batch[\"window_size\"], torch.Tensor):\n                            window_width, window_height = (\n                                batch[\"window_size\"][idx].cpu().numpy().tolist()\n                            )\n                        else:\n                            print(\n                                f\"Unexpected window_size type: {type(batch['window_size'])}\"\n                            )\n                            continue\n\n                        # Combine all masks for this window\n                        combined_mask = np.zeros(\n                            (window_height, window_width), dtype=np.uint8\n                        )\n\n                        for mask in masks:\n                            # Get the binary mask\n                            binary_mask = (mask[0] &gt; mask_threshold).astype(\n                                np.uint8\n                            ) * 255\n\n                            # Handle size mismatch - resize binary_mask if needed\n                            mask_h, mask_w = binary_mask.shape\n                            if mask_h != window_height or mask_w != window_width:\n                                resize_key = f\"{(mask_h, mask_w)}-&gt;{(window_height, window_width)}\"\n                                if resize_key not in seen_warnings[\"resize\"]:\n                                    if verbose:\n                                        print(\n                                            f\"Resizing mask from {binary_mask.shape} to {(window_height, window_width)}\"\n                                        )\n                                    else:\n                                        if not seen_warnings[\n                                            \"resize\"\n                                        ]:  # If this is the first resize warning\n                                            print(\n                                                f\"Resizing masks at image edges (set verbose=True for details)\"\n                                            )\n                                    seen_warnings[\"resize\"][resize_key] = True\n\n                                # Crop or pad the binary mask to match window size\n                                resized_mask = np.zeros(\n                                    (window_height, window_width), dtype=np.uint8\n                                )\n                                copy_h = min(mask_h, window_height)\n                                copy_w = min(mask_w, window_width)\n                                resized_mask[:copy_h, :copy_w] = binary_mask[\n                                    :copy_h, :copy_w\n                                ]\n                                binary_mask = resized_mask\n\n                            # Update combined mask (taking maximum where masks overlap)\n                            combined_mask = np.maximum(combined_mask, binary_mask)\n\n                        # Write combined mask to output array\n                        # Handle edge cases where window might be smaller than chip size\n                        h, w = combined_mask.shape\n                        valid_h = min(h, src.height - j)\n                        valid_w = min(w, src.width - i)\n\n                        if valid_h &gt; 0 and valid_w &gt; 0:\n                            mask_array[j : j + valid_h, i : i + valid_w] = np.maximum(\n                                mask_array[j : j + valid_h, i : i + valid_w],\n                                combined_mask[:valid_h, :valid_w],\n                            )\n\n                # Write the final mask to the output file\n                dst.write(mask_array, 1)\n\n        print(f\"Object masks saved to {output_path}\")\n        return output_path\n\n    def regularize_objects(\n        self,\n        gdf,\n        min_area=10,\n        angle_threshold=15,\n        orthogonality_threshold=0.3,\n        rectangularity_threshold=0.7,\n    ):\n        \"\"\"\n        Regularize objects to enforce right angles and rectangular shapes.\n\n        Args:\n            gdf: GeoDataFrame with objects\n            min_area: Minimum area in square units to keep an object\n            angle_threshold: Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)\n            orthogonality_threshold: Percentage of angles that must be orthogonal for an object to be regularized\n            rectangularity_threshold: Minimum area ratio to Object's oriented bounding box for rectangular simplification\n\n        Returns:\n            GeoDataFrame with regularized objects\n        \"\"\"\n        import numpy as np\n        from shapely.geometry import Polygon, MultiPolygon, box\n        from shapely.affinity import rotate, translate\n        import geopandas as gpd\n        import math\n        from tqdm import tqdm\n        import cv2\n\n        def get_angle(p1, p2, p3):\n            \"\"\"Calculate angle between three points in degrees (0-180)\"\"\"\n            a = np.array(p1)\n            b = np.array(p2)\n            c = np.array(p3)\n\n            ba = a - b\n            bc = c - b\n\n            cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n            # Handle numerical errors that could push cosine outside [-1, 1]\n            cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n            angle = np.degrees(np.arccos(cosine_angle))\n\n            return angle\n\n        def is_orthogonal(angle, threshold=angle_threshold):\n            \"\"\"Check if angle is close to 90 degrees\"\"\"\n            return abs(angle - 90) &lt;= threshold\n\n        def calculate_dominant_direction(polygon):\n            \"\"\"Find the dominant direction of a polygon using PCA\"\"\"\n            # Extract coordinates\n            coords = np.array(polygon.exterior.coords)\n\n            # Mean center the coordinates\n            mean = np.mean(coords, axis=0)\n            centered_coords = coords - mean\n\n            # Calculate covariance matrix and its eigenvalues/eigenvectors\n            cov_matrix = np.cov(centered_coords.T)\n            eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n            # Get the index of the largest eigenvalue\n            largest_idx = np.argmax(eigenvalues)\n\n            # Get the corresponding eigenvector (principal axis)\n            principal_axis = eigenvectors[:, largest_idx]\n\n            # Calculate the angle in degrees\n            angle_rad = np.arctan2(principal_axis[1], principal_axis[0])\n            angle_deg = np.degrees(angle_rad)\n\n            # Normalize to range 0-180\n            if angle_deg &lt; 0:\n                angle_deg += 180\n\n            return angle_deg\n\n        def create_oriented_envelope(polygon, angle_deg):\n            \"\"\"Create an oriented minimum area rectangle for the polygon\"\"\"\n            # Create a rotated rectangle using OpenCV method (more robust than Shapely methods)\n            coords = np.array(polygon.exterior.coords)[:-1].astype(\n                np.float32\n            )  # Skip the last point (same as first)\n\n            # Use OpenCV's minAreaRect\n            rect = cv2.minAreaRect(coords)\n            box_points = cv2.boxPoints(rect)\n\n            # Convert to shapely polygon\n            oriented_box = Polygon(box_points)\n\n            return oriented_box\n\n        def get_rectangularity(polygon, oriented_box):\n            \"\"\"Calculate the rectangularity (area ratio to its oriented bounding box)\"\"\"\n            if oriented_box.area == 0:\n                return 0\n            return polygon.area / oriented_box.area\n\n        def check_orthogonality(polygon):\n            \"\"\"Check what percentage of angles in the polygon are orthogonal\"\"\"\n            coords = list(polygon.exterior.coords)\n            if len(coords) &lt;= 4:  # Triangle or point\n                return 0\n\n            # Remove last point (same as first)\n            coords = coords[:-1]\n\n            orthogonal_count = 0\n            total_angles = len(coords)\n\n            for i in range(total_angles):\n                p1 = coords[i]\n                p2 = coords[(i + 1) % total_angles]\n                p3 = coords[(i + 2) % total_angles]\n\n                angle = get_angle(p1, p2, p3)\n                if is_orthogonal(angle):\n                    orthogonal_count += 1\n\n            return orthogonal_count / total_angles\n\n        def simplify_to_rectangle(polygon):\n            \"\"\"Simplify a polygon to a rectangle using its oriented bounding box\"\"\"\n            # Get dominant direction\n            angle = calculate_dominant_direction(polygon)\n\n            # Create oriented envelope\n            rect = create_oriented_envelope(polygon, angle)\n\n            return rect\n\n        if gdf is None or len(gdf) == 0:\n            print(\"No Objects to regularize\")\n            return gdf\n\n        print(f\"Regularizing {len(gdf)} objects...\")\n        print(f\"- Angle threshold: {angle_threshold}\u00b0 from 90\u00b0\")\n        print(f\"- Min orthogonality: {orthogonality_threshold*100}% of angles\")\n        print(\n            f\"- Min rectangularity: {rectangularity_threshold*100}% of bounding box area\"\n        )\n\n        # Create a copy to avoid modifying the original\n        result_gdf = gdf.copy()\n\n        # Track statistics\n        total_objects = len(gdf)\n        regularized_count = 0\n        rectangularized_count = 0\n\n        # Process each Object\n        for idx, row in tqdm(gdf.iterrows(), total=len(gdf)):\n            geom = row.geometry\n\n            # Skip invalid or empty geometries\n            if geom is None or geom.is_empty:\n                continue\n\n            # Handle MultiPolygons by processing the largest part\n            if isinstance(geom, MultiPolygon):\n                areas = [p.area for p in geom.geoms]\n                if not areas:\n                    continue\n                geom = list(geom.geoms)[np.argmax(areas)]\n\n            # Filter out tiny Objects\n            if geom.area &lt; min_area:\n                continue\n\n            # Check orthogonality\n            orthogonality = check_orthogonality(geom)\n\n            # Create oriented envelope\n            oriented_box = create_oriented_envelope(\n                geom, calculate_dominant_direction(geom)\n            )\n\n            # Check rectangularity\n            rectangularity = get_rectangularity(geom, oriented_box)\n\n            # Decide how to regularize\n            if rectangularity &gt;= rectangularity_threshold:\n                # Object is already quite rectangular, simplify to a rectangle\n                result_gdf.at[idx, \"geometry\"] = oriented_box\n                result_gdf.at[idx, \"regularized\"] = \"rectangle\"\n                rectangularized_count += 1\n            elif orthogonality &gt;= orthogonality_threshold:\n                # Object has many orthogonal angles but isn't rectangular\n                # Could implement more sophisticated regularization here\n                # For now, we'll still use the oriented rectangle\n                result_gdf.at[idx, \"geometry\"] = oriented_box\n                result_gdf.at[idx, \"regularized\"] = \"orthogonal\"\n                regularized_count += 1\n            else:\n                # Object doesn't have clear orthogonal structure\n                # Keep original but flag as unmodified\n                result_gdf.at[idx, \"regularized\"] = \"original\"\n\n        # Report statistics\n        print(f\"Regularization completed:\")\n        print(f\"- Total objects: {total_objects}\")\n        print(\n            f\"- Rectangular objects: {rectangularized_count} ({rectangularized_count/total_objects*100:.1f}%)\"\n        )\n        print(\n            f\"- Other regularized objects: {regularized_count} ({regularized_count/total_objects*100:.1f}%)\"\n        )\n        print(\n            f\"- Unmodified objects: {total_objects-rectangularized_count-regularized_count} ({(total_objects-rectangularized_count-regularized_count)/total_objects*100:.1f}%)\"\n        )\n\n        return result_gdf\n\n    def visualize_results(\n        self, raster_path, gdf=None, output_path=None, figsize=(12, 12)\n    ):\n        \"\"\"\n        Visualize object detection results with proper coordinate transformation.\n\n        This function displays objects on top of the raster image,\n        ensuring proper alignment between the GeoDataFrame polygons and the image.\n\n        Args:\n            raster_path: Path to input raster\n            gdf: GeoDataFrame with object polygons (optional)\n            output_path: Path to save visualization (optional)\n            figsize: Figure size (width, height) in inches\n\n        Returns:\n            bool: True if visualization was successful\n        \"\"\"\n        # Check if raster file exists\n        if not os.path.exists(raster_path):\n            print(f\"Error: Raster file '{raster_path}' not found.\")\n            return False\n\n        # Process raster if GeoDataFrame not provided\n        if gdf is None:\n            gdf = self.process_raster(raster_path)\n\n        if gdf is None or len(gdf) == 0:\n            print(\"No objects to visualize\")\n            return False\n\n        # Check if confidence column exists in the GeoDataFrame\n        has_confidence = False\n        if hasattr(gdf, \"columns\") and \"confidence\" in gdf.columns:\n            # Try to access a confidence value to confirm it works\n            try:\n                if len(gdf) &gt; 0:\n                    # Try getitem access\n                    conf_val = gdf[\"confidence\"].iloc[0]\n                    has_confidence = True\n                    print(\n                        f\"Using confidence values (range: {gdf['confidence'].min():.2f} - {gdf['confidence'].max():.2f})\"\n                    )\n            except Exception as e:\n                print(f\"Confidence column exists but couldn't access values: {e}\")\n                has_confidence = False\n        else:\n            print(\"No confidence column found in GeoDataFrame\")\n            has_confidence = False\n\n        # Read raster for visualization\n        with rasterio.open(raster_path) as src:\n            # Read the entire image or a subset if it's very large\n            if src.height &gt; 2000 or src.width &gt; 2000:\n                # Calculate scale factor to reduce size\n                scale = min(2000 / src.height, 2000 / src.width)\n                out_shape = (\n                    int(src.count),\n                    int(src.height * scale),\n                    int(src.width * scale),\n                )\n\n                # Read and resample\n                image = src.read(\n                    out_shape=out_shape, resampling=rasterio.enums.Resampling.bilinear\n                )\n\n                # Create a scaled transform for the resampled image\n                # Calculate scaling factors\n                x_scale = src.width / out_shape[2]\n                y_scale = src.height / out_shape[1]\n\n                # Get the original transform\n                orig_transform = src.transform\n\n                # Create a scaled transform\n                scaled_transform = rasterio.transform.Affine(\n                    orig_transform.a * x_scale,\n                    orig_transform.b,\n                    orig_transform.c,\n                    orig_transform.d,\n                    orig_transform.e * y_scale,\n                    orig_transform.f,\n                )\n            else:\n                image = src.read()\n                scaled_transform = src.transform\n\n            # Convert to RGB for display\n            if image.shape[0] &gt; 3:\n                image = image[:3]\n            elif image.shape[0] == 1:\n                image = np.repeat(image, 3, axis=0)\n\n            # Normalize image for display\n            image = image.transpose(1, 2, 0)  # CHW to HWC\n            image = image.astype(np.float32)\n\n            if image.max() &gt; 10:  # Likely 0-255 range\n                image = image / 255.0\n\n            image = np.clip(image, 0, 1)\n\n            # Get image bounds\n            bounds = src.bounds\n            crs = src.crs\n\n        # Create figure with appropriate aspect ratio\n        aspect_ratio = image.shape[1] / image.shape[0]  # width / height\n        plt.figure(figsize=(figsize[0], figsize[0] / aspect_ratio))\n        ax = plt.gca()\n\n        # Display image\n        ax.imshow(image)\n\n        # Make sure the GeoDataFrame has the same CRS as the raster\n        if gdf.crs != crs:\n            print(f\"Reprojecting GeoDataFrame from {gdf.crs} to {crs}\")\n            gdf = gdf.to_crs(crs)\n\n        # Set up colors for confidence visualization\n        if has_confidence:\n            try:\n                import matplotlib.cm as cm\n                from matplotlib.colors import Normalize\n\n                # Get min/max confidence values\n                min_conf = gdf[\"confidence\"].min()\n                max_conf = gdf[\"confidence\"].max()\n\n                # Set up normalization and colormap\n                norm = Normalize(vmin=min_conf, vmax=max_conf)\n                cmap = cm.viridis\n\n                # Create scalar mappable for colorbar\n                sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n                sm.set_array([])\n\n                # Add colorbar\n                cbar = plt.colorbar(\n                    sm, ax=ax, orientation=\"vertical\", shrink=0.7, pad=0.01\n                )\n                cbar.set_label(\"Confidence Score\")\n            except Exception as e:\n                print(f\"Error setting up confidence visualization: {e}\")\n                has_confidence = False\n\n        # Function to convert coordinates\n        def geo_to_pixel(geometry, transform):\n            \"\"\"Convert geometry to pixel coordinates using the provided transform.\"\"\"\n            if geometry.is_empty:\n                return None\n\n            if geometry.geom_type == \"Polygon\":\n                # Get exterior coordinates\n                exterior_coords = list(geometry.exterior.coords)\n\n                # Convert to pixel coordinates\n                pixel_coords = [~transform * (x, y) for x, y in exterior_coords]\n\n                # Split into x and y lists\n                pixel_x = [coord[0] for coord in pixel_coords]\n                pixel_y = [coord[1] for coord in pixel_coords]\n\n                return pixel_x, pixel_y\n            else:\n                print(f\"Unsupported geometry type: {geometry.geom_type}\")\n                return None\n\n        # Plot each object\n        for idx, row in gdf.iterrows():\n            try:\n                # Convert polygon to pixel coordinates\n                coords = geo_to_pixel(row.geometry, scaled_transform)\n\n                if coords:\n                    pixel_x, pixel_y = coords\n\n                    if has_confidence:\n                        try:\n                            # Get confidence value using different methods\n                            # Method 1: Try direct attribute access\n                            confidence = None\n                            try:\n                                confidence = row.confidence\n                            except:\n                                pass\n\n                            # Method 2: Try dictionary-style access\n                            if confidence is None:\n                                try:\n                                    confidence = row[\"confidence\"]\n                                except:\n                                    pass\n\n                            # Method 3: Try accessing by index from the GeoDataFrame\n                            if confidence is None:\n                                try:\n                                    confidence = gdf.iloc[idx][\"confidence\"]\n                                except:\n                                    pass\n\n                            if confidence is not None:\n                                color = cmap(norm(confidence))\n                                # Fill polygon with semi-transparent color\n                                ax.fill(pixel_x, pixel_y, color=color, alpha=0.5)\n                                # Draw border\n                                ax.plot(\n                                    pixel_x,\n                                    pixel_y,\n                                    color=color,\n                                    linewidth=1,\n                                    alpha=0.8,\n                                )\n                            else:\n                                # Fall back to red if confidence value couldn't be accessed\n                                ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n                        except Exception as e:\n                            print(\n                                f\"Error using confidence value for polygon {idx}: {e}\"\n                            )\n                            ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n                    else:\n                        # No confidence data, just plot outlines in red\n                        ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n            except Exception as e:\n                print(f\"Error plotting polygon {idx}: {e}\")\n\n        # Remove axes\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"objects (Found: {len(gdf)})\")\n\n        # Save if requested\n        if output_path:\n            plt.tight_layout()\n            plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n            print(f\"Visualization saved to {output_path}\")\n\n        plt.close()\n\n        # Create a simpler visualization focused just on a subset of objects\n        if len(gdf) &gt; 0:\n            plt.figure(figsize=figsize)\n            ax = plt.gca()\n\n            # Choose a subset of the image to show\n            with rasterio.open(raster_path) as src:\n                # Get centroid of first object\n                sample_geom = gdf.iloc[0].geometry\n                centroid = sample_geom.centroid\n\n                # Convert to pixel coordinates\n                center_x, center_y = ~src.transform * (centroid.x, centroid.y)\n\n                # Define a window around this object\n                window_size = 500  # pixels\n                window = rasterio.windows.Window(\n                    max(0, int(center_x - window_size / 2)),\n                    max(0, int(center_y - window_size / 2)),\n                    min(window_size, src.width - int(center_x - window_size / 2)),\n                    min(window_size, src.height - int(center_y - window_size / 2)),\n                )\n\n                # Read this window\n                sample_image = src.read(window=window)\n\n                # Convert to RGB for display\n                if sample_image.shape[0] &gt; 3:\n                    sample_image = sample_image[:3]\n                elif sample_image.shape[0] == 1:\n                    sample_image = np.repeat(sample_image, 3, axis=0)\n\n                # Normalize image for display\n                sample_image = sample_image.transpose(1, 2, 0)  # CHW to HWC\n                sample_image = sample_image.astype(np.float32)\n\n                if sample_image.max() &gt; 10:  # Likely 0-255 range\n                    sample_image = sample_image / 255.0\n\n                sample_image = np.clip(sample_image, 0, 1)\n\n                # Display sample image\n                ax.imshow(sample_image, extent=[0, window.width, window.height, 0])\n\n                # Get the correct transform for this window\n                window_transform = src.window_transform(window)\n\n                # Calculate bounds of the window\n                window_bounds = rasterio.windows.bounds(window, src.transform)\n                window_box = box(*window_bounds)\n\n                # Filter objects that intersect with this window\n                visible_gdf = gdf[gdf.intersects(window_box)]\n\n                # Set up colors for sample view if confidence data exists\n                if has_confidence:\n                    try:\n                        # Reuse the same normalization and colormap from main view\n                        sample_sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n                        sample_sm.set_array([])\n\n                        # Add colorbar to sample view\n                        sample_cbar = plt.colorbar(\n                            sample_sm,\n                            ax=ax,\n                            orientation=\"vertical\",\n                            shrink=0.7,\n                            pad=0.01,\n                        )\n                        sample_cbar.set_label(\"Confidence Score\")\n                    except Exception as e:\n                        print(f\"Error setting up sample confidence visualization: {e}\")\n\n                # Plot objects in sample view\n                for idx, row in visible_gdf.iterrows():\n                    try:\n                        # Get window-relative pixel coordinates\n                        geom = row.geometry\n\n                        # Skip empty geometries\n                        if geom.is_empty:\n                            continue\n\n                        # Get exterior coordinates\n                        exterior_coords = list(geom.exterior.coords)\n\n                        # Convert to pixel coordinates relative to window origin\n                        pixel_coords = []\n                        for x, y in exterior_coords:\n                            px, py = ~src.transform * (x, y)  # Convert to image pixels\n                            # Make coordinates relative to window\n                            px = px - window.col_off\n                            py = py - window.row_off\n                            pixel_coords.append((px, py))\n\n                        # Extract x and y coordinates\n                        pixel_x = [coord[0] for coord in pixel_coords]\n                        pixel_y = [coord[1] for coord in pixel_coords]\n\n                        # Use confidence colors if available\n                        if has_confidence:\n                            try:\n                                # Try different methods to access confidence\n                                confidence = None\n                                try:\n                                    confidence = row.confidence\n                                except:\n                                    pass\n\n                                if confidence is None:\n                                    try:\n                                        confidence = row[\"confidence\"]\n                                    except:\n                                        pass\n\n                                if confidence is None:\n                                    try:\n                                        confidence = visible_gdf.iloc[idx][\"confidence\"]\n                                    except:\n                                        pass\n\n                                if confidence is not None:\n                                    color = cmap(norm(confidence))\n                                    # Fill polygon with semi-transparent color\n                                    ax.fill(pixel_x, pixel_y, color=color, alpha=0.5)\n                                    # Draw border\n                                    ax.plot(\n                                        pixel_x,\n                                        pixel_y,\n                                        color=color,\n                                        linewidth=1.5,\n                                        alpha=0.8,\n                                    )\n                                else:\n                                    ax.plot(\n                                        pixel_x, pixel_y, color=\"red\", linewidth=1.5\n                                    )\n                            except Exception as e:\n                                print(\n                                    f\"Error using confidence in sample view for polygon {idx}: {e}\"\n                                )\n                                ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                        else:\n                            ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                    except Exception as e:\n                        print(f\"Error plotting polygon in sample view: {e}\")\n\n                # Set title\n                ax.set_title(f\"Sample Area - objects (Showing: {len(visible_gdf)})\")\n\n                # Remove axes\n                ax.set_xticks([])\n                ax.set_yticks([])\n\n                # Save if requested\n                if output_path:\n                    sample_output = (\n                        os.path.splitext(output_path)[0]\n                        + \"_sample\"\n                        + os.path.splitext(output_path)[1]\n                    )\n                    plt.tight_layout()\n                    plt.savefig(sample_output, dpi=300, bbox_inches=\"tight\")\n                    print(f\"Sample visualization saved to {sample_output}\")\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.__init__","title":"<code>__init__(self, model_path=None, repo_id=None, model=None, device=None)</code>  <code>special</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <p>Path to the .pth model file.</p> <code>None</code> <code>repo_id</code> <p>Hugging Face repository ID for model download.</p> <code>None</code> <code>model</code> <p>Pre-initialized model object (optional).</p> <code>None</code> <code>device</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(self, model_path=None, repo_id=None, model=None, device=None):\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Hugging Face repository ID for model download.\n        model: Pre-initialized model object (optional).\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    # Set device\n    if device is None:\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    else:\n        self.device = torch.device(device)\n\n    # Default parameters for object detection - these can be overridden in process_raster\n    self.chip_size = (512, 512)  # Size of image chips for processing\n    self.overlap = 0.25  # Default overlap between tiles\n    self.confidence_threshold = 0.5  # Default confidence threshold\n    self.nms_iou_threshold = 0.5  # IoU threshold for non-maximum suppression\n    self.min_object_area = 100  # Minimum area in pixels to keep an object\n    self.max_object_area = None  # Maximum area in pixels to keep an object\n    self.mask_threshold = 0.5  # Threshold for mask binarization\n    self.simplify_tolerance = 1.0  # Tolerance for polygon simplification\n\n    # Initialize model\n    self.model = self.initialize_model(model)\n\n    # Download model if needed\n    if model_path is None or (not os.path.exists(model_path)):\n        model_path = self.download_model_from_hf(model_path, repo_id)\n\n    # Load model weights\n    self.load_weights(model_path)\n\n    # Set model to evaluation mode\n    self.model.eval()\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.download_model_from_hf","title":"<code>download_model_from_hf(self, model_path=None, repo_id=None)</code>","text":"<p>Download the object detection model from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <p>Path to the model file.</p> <code>None</code> <code>repo_id</code> <p>Hugging Face repository ID.</p> <code>None</code> <p>Returns:</p> Type Description <p>Path to the downloaded model file</p> Source code in <code>geoai/extract.py</code> <pre><code>def download_model_from_hf(self, model_path=None, repo_id=None):\n    \"\"\"\n    Download the object detection model from Hugging Face.\n\n    Args:\n        model_path: Path to the model file.\n        repo_id: Hugging Face repository ID.\n\n    Returns:\n        Path to the downloaded model file\n    \"\"\"\n    try:\n\n        print(\"Model path not specified, downloading from Hugging Face...\")\n\n        # Define the repository ID and model filename\n        if repo_id is None:\n            repo_id = \"giswqs/geoai\"\n\n        if model_path is None:\n            model_path = \"building_footprints_usa.pth\"\n\n        # Download the model\n        model_path = hf_hub_download(repo_id=repo_id, filename=model_path)\n        print(f\"Model downloaded to: {model_path}\")\n\n        return model_path\n\n    except Exception as e:\n        print(f\"Error downloading model from Hugging Face: {e}\")\n        print(\"Please specify a local model path or ensure internet connectivity.\")\n        raise\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.filter_edge_objects","title":"<code>filter_edge_objects(self, gdf, raster_path, edge_buffer=10)</code>","text":"<p>Filter out object detections that fall in padding/edge areas of the image.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <p>GeoDataFrame with object detections</p> required <code>raster_path</code> <p>Path to the original raster file</p> required <code>edge_buffer</code> <p>Buffer in pixels to consider as edge region</p> <code>10</code> <p>Returns:</p> Type Description <p>GeoDataFrame with filtered objects</p> Source code in <code>geoai/extract.py</code> <pre><code>def filter_edge_objects(self, gdf, raster_path, edge_buffer=10):\n    \"\"\"\n    Filter out object detections that fall in padding/edge areas of the image.\n\n    Args:\n        gdf: GeoDataFrame with object detections\n        raster_path: Path to the original raster file\n        edge_buffer: Buffer in pixels to consider as edge region\n\n    Returns:\n        GeoDataFrame with filtered objects\n    \"\"\"\n    import rasterio\n    from shapely.geometry import box\n\n    # If no objects detected, return empty GeoDataFrame\n    if gdf is None or len(gdf) == 0:\n        return gdf\n\n    print(f\"Objects before filtering: {len(gdf)}\")\n\n    with rasterio.open(raster_path) as src:\n        # Get raster bounds\n        raster_bounds = src.bounds\n        raster_width = src.width\n        raster_height = src.height\n\n        # Convert edge buffer from pixels to geographic units\n        # We need the smallest dimension of a pixel in geographic units\n        pixel_width = (raster_bounds[2] - raster_bounds[0]) / raster_width\n        pixel_height = (raster_bounds[3] - raster_bounds[1]) / raster_height\n        buffer_size = min(pixel_width, pixel_height) * edge_buffer\n\n        # Create a slightly smaller bounding box to exclude edge regions\n        inner_bounds = (\n            raster_bounds[0] + buffer_size,  # min x (west)\n            raster_bounds[1] + buffer_size,  # min y (south)\n            raster_bounds[2] - buffer_size,  # max x (east)\n            raster_bounds[3] - buffer_size,  # max y (north)\n        )\n\n        # Check that inner bounds are valid\n        if inner_bounds[0] &gt;= inner_bounds[2] or inner_bounds[1] &gt;= inner_bounds[3]:\n            print(\"Warning: Edge buffer too large, using original bounds\")\n            inner_box = box(*raster_bounds)\n        else:\n            inner_box = box(*inner_bounds)\n\n        # Filter out objects that intersect with the edge of the image\n        filtered_gdf = gdf[gdf.intersects(inner_box)]\n\n        # Additional check for objects that have &gt;50% of their area outside the valid region\n        valid_objects = []\n        for idx, row in filtered_gdf.iterrows():\n            if row.geometry.intersection(inner_box).area &gt;= 0.5 * row.geometry.area:\n                valid_objects.append(idx)\n\n        filtered_gdf = filtered_gdf.loc[valid_objects]\n\n        print(f\"Objects after filtering: {len(filtered_gdf)}\")\n\n        return filtered_gdf\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.filter_overlapping_polygons","title":"<code>filter_overlapping_polygons(self, gdf, **kwargs)</code>","text":"<p>Filter overlapping polygons using non-maximum suppression.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <p>GeoDataFrame with polygons</p> required <code>**kwargs</code> <p>Optional parameters: nms_iou_threshold: IoU threshold for filtering</p> <code>{}</code> <p>Returns:</p> Type Description <p>Filtered GeoDataFrame</p> Source code in <code>geoai/extract.py</code> <pre><code>def filter_overlapping_polygons(self, gdf, **kwargs):\n    \"\"\"\n    Filter overlapping polygons using non-maximum suppression.\n\n    Args:\n        gdf: GeoDataFrame with polygons\n        **kwargs: Optional parameters:\n            nms_iou_threshold: IoU threshold for filtering\n\n    Returns:\n        Filtered GeoDataFrame\n    \"\"\"\n    if len(gdf) &lt;= 1:\n        return gdf\n\n    # Get parameters from kwargs or use instance defaults\n    iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n\n    # Sort by confidence\n    gdf = gdf.sort_values(\"confidence\", ascending=False)\n\n    # Fix any invalid geometries\n    gdf[\"geometry\"] = gdf[\"geometry\"].apply(\n        lambda geom: geom.buffer(0) if not geom.is_valid else geom\n    )\n\n    keep_indices = []\n    polygons = gdf.geometry.values\n\n    for i in range(len(polygons)):\n        if i in keep_indices:\n            continue\n\n        keep = True\n        for j in keep_indices:\n            # Skip invalid geometries\n            if not polygons[i].is_valid or not polygons[j].is_valid:\n                continue\n\n            # Calculate IoU\n            try:\n                intersection = polygons[i].intersection(polygons[j]).area\n                union = polygons[i].area + polygons[j].area - intersection\n                iou = intersection / union if union &gt; 0 else 0\n\n                if iou &gt; iou_threshold:\n                    keep = False\n                    break\n            except Exception:\n                # Skip on topology exceptions\n                continue\n\n        if keep:\n            keep_indices.append(i)\n\n    return gdf.iloc[keep_indices]\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.initialize_model","title":"<code>initialize_model(self, model)</code>","text":"<p>Initialize a deep learning model for object detection.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>A pre-initialized model object.</p> required <p>Returns:</p> Type Description <code>torch.nn.Module</code> <p>A deep learning model for object detection.</p> Source code in <code>geoai/extract.py</code> <pre><code>def initialize_model(self, model):\n    \"\"\"Initialize a deep learning model for object detection.\n\n    Args:\n        model (torch.nn.Module): A pre-initialized model object.\n\n    Returns:\n        torch.nn.Module: A deep learning model for object detection.\n    \"\"\"\n\n    if model is None:  # Initialize Mask R-CNN model with ResNet50 backbone.\n        # Standard image mean and std for pre-trained models\n        image_mean = [0.485, 0.456, 0.406]\n        image_std = [0.229, 0.224, 0.225]\n\n        # Create model with explicit normalization parameters\n        model = maskrcnn_resnet50_fpn(\n            weights=None,\n            progress=False,\n            num_classes=2,  # Background + object\n            weights_backbone=None,\n            # These parameters ensure consistent normalization\n            image_mean=image_mean,\n            image_std=image_std,\n        )\n\n    model.to(self.device)\n    return model\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.load_weights","title":"<code>load_weights(self, model_path)</code>","text":"<p>Load weights from file with error handling for different formats.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <p>Path to model weights</p> required Source code in <code>geoai/extract.py</code> <pre><code>def load_weights(self, model_path):\n    \"\"\"\n    Load weights from file with error handling for different formats.\n\n    Args:\n        model_path: Path to model weights\n    \"\"\"\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n\n    try:\n        state_dict = torch.load(model_path, map_location=self.device)\n\n        # Handle different state dict formats\n        if isinstance(state_dict, dict):\n            if \"model\" in state_dict:\n                state_dict = state_dict[\"model\"]\n            elif \"state_dict\" in state_dict:\n                state_dict = state_dict[\"state_dict\"]\n\n        # Try to load state dict\n        try:\n            self.model.load_state_dict(state_dict)\n            print(\"Model loaded successfully\")\n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n            print(\"Attempting to fix state_dict keys...\")\n\n            # Try to fix state_dict keys (remove module prefix if needed)\n            new_state_dict = {}\n            for k, v in state_dict.items():\n                if k.startswith(\"module.\"):\n                    new_state_dict[k[7:]] = v\n                else:\n                    new_state_dict[k] = v\n\n            self.model.load_state_dict(new_state_dict)\n            print(\"Model loaded successfully after key fixing\")\n\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load model: {e}\")\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.mask_to_polygons","title":"<code>mask_to_polygons(self, mask, **kwargs)</code>","text":"<p>Convert binary mask to polygon contours using OpenCV.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <p>Binary mask as numpy array</p> required <code>**kwargs</code> <p>Optional parameters: simplify_tolerance: Tolerance for polygon simplification mask_threshold: Threshold for mask binarization min_object_area: Minimum area in pixels to keep an object max_object_area: Maximum area in pixels to keep an object</p> <code>{}</code> <p>Returns:</p> Type Description <p>List of polygons as lists of (x, y) coordinates</p> Source code in <code>geoai/extract.py</code> <pre><code>def mask_to_polygons(self, mask, **kwargs):\n    \"\"\"\n    Convert binary mask to polygon contours using OpenCV.\n\n    Args:\n        mask: Binary mask as numpy array\n        **kwargs: Optional parameters:\n            simplify_tolerance: Tolerance for polygon simplification\n            mask_threshold: Threshold for mask binarization\n            min_object_area: Minimum area in pixels to keep an object\n            max_object_area: Maximum area in pixels to keep an object\n\n    Returns:\n        List of polygons as lists of (x, y) coordinates\n    \"\"\"\n\n    # Get parameters from kwargs or use instance defaults\n    simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n    mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n    min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n    max_object_area = kwargs.get(\"max_object_area\", self.max_object_area)\n\n    # Ensure binary mask\n    mask = (mask &gt; mask_threshold).astype(np.uint8)\n\n    # Optional: apply morphological operations to improve mask quality\n    kernel = np.ones((3, 3), np.uint8)\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n    # Find contours\n    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Convert to list of [x, y] coordinates\n    polygons = []\n    for contour in contours:\n        # Filter out too small contours\n        if contour.shape[0] &lt; 3 or cv2.contourArea(contour) &lt; min_object_area:\n            continue\n\n        # Filter out too large contours\n        if (\n            max_object_area is not None\n            and cv2.contourArea(contour) &gt; max_object_area\n        ):\n            continue\n\n        # Simplify contour if it has many points\n        if contour.shape[0] &gt; 50:\n            epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n            contour = cv2.approxPolyDP(contour, epsilon, True)\n\n        # Convert to list of [x, y] coordinates\n        polygon = contour.reshape(-1, 2).tolist()\n        polygons.append(polygon)\n\n    return polygons\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.masks_to_vector","title":"<code>masks_to_vector(self, mask_path, output_path=None, simplify_tolerance=None, mask_threshold=None, min_object_area=None, max_object_area=None, nms_iou_threshold=None, regularize=True, angle_threshold=15, rectangularity_threshold=0.7)</code>","text":"<p>Convert an object mask GeoTIFF to vector polygons and save as GeoJSON.</p> <p>Parameters:</p> Name Type Description Default <code>mask_path</code> <p>Path to the object masks GeoTIFF</p> required <code>output_path</code> <p>Path to save the output GeoJSON (default: mask_path with .geojson extension)</p> <code>None</code> <code>simplify_tolerance</code> <p>Tolerance for polygon simplification (default: self.simplify_tolerance)</p> <code>None</code> <code>mask_threshold</code> <p>Threshold for mask binarization (default: self.mask_threshold)</p> <code>None</code> <code>min_object_area</code> <p>Minimum area in pixels to keep an object (default: self.min_object_area)</p> <code>None</code> <code>max_object_area</code> <p>Minimum area in pixels to keep an object (default: self.max_object_area)</p> <code>None</code> <code>nms_iou_threshold</code> <p>IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)</p> <code>None</code> <code>regularize</code> <p>Whether to regularize objects to right angles (default: True)</p> <code>True</code> <code>angle_threshold</code> <p>Maximum deviation from 90 degrees for regularization (default: 15)</p> <code>15</code> <code>rectangularity_threshold</code> <p>Threshold for rectangle simplification (default: 0.7)</p> <code>0.7</code> <p>Returns:</p> Type Description <p>GeoDataFrame with objects</p> Source code in <code>geoai/extract.py</code> <pre><code>def masks_to_vector(\n    self,\n    mask_path,\n    output_path=None,\n    simplify_tolerance=None,\n    mask_threshold=None,\n    min_object_area=None,\n    max_object_area=None,\n    nms_iou_threshold=None,\n    regularize=True,\n    angle_threshold=15,\n    rectangularity_threshold=0.7,\n):\n    \"\"\"\n    Convert an object mask GeoTIFF to vector polygons and save as GeoJSON.\n\n    Args:\n        mask_path: Path to the object masks GeoTIFF\n        output_path: Path to save the output GeoJSON (default: mask_path with .geojson extension)\n        simplify_tolerance: Tolerance for polygon simplification (default: self.simplify_tolerance)\n        mask_threshold: Threshold for mask binarization (default: self.mask_threshold)\n        min_object_area: Minimum area in pixels to keep an object (default: self.min_object_area)\n        max_object_area: Minimum area in pixels to keep an object (default: self.max_object_area)\n        nms_iou_threshold: IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)\n        regularize: Whether to regularize objects to right angles (default: True)\n        angle_threshold: Maximum deviation from 90 degrees for regularization (default: 15)\n        rectangularity_threshold: Threshold for rectangle simplification (default: 0.7)\n\n    Returns:\n        GeoDataFrame with objects\n    \"\"\"\n    # Use class defaults if parameters not provided\n    simplify_tolerance = (\n        simplify_tolerance\n        if simplify_tolerance is not None\n        else self.simplify_tolerance\n    )\n    mask_threshold = (\n        mask_threshold if mask_threshold is not None else self.mask_threshold\n    )\n    min_object_area = (\n        min_object_area if min_object_area is not None else self.min_object_area\n    )\n    max_object_area = (\n        max_object_area if max_object_area is not None else self.max_object_area\n    )\n    nms_iou_threshold = (\n        nms_iou_threshold\n        if nms_iou_threshold is not None\n        else self.nms_iou_threshold\n    )\n\n    # Set default output path if not provided\n    # if output_path is None:\n    #     output_path = os.path.splitext(mask_path)[0] + \".geojson\"\n\n    print(f\"Converting mask to GeoJSON with parameters:\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n    print(f\"- Min object area: {min_object_area}\")\n    print(f\"- Max object area: {max_object_area}\")\n    print(f\"- Simplify tolerance: {simplify_tolerance}\")\n    print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n    print(f\"- Regularize objects: {regularize}\")\n    if regularize:\n        print(f\"- Angle threshold: {angle_threshold}\u00b0 from 90\u00b0\")\n        print(f\"- Rectangularity threshold: {rectangularity_threshold*100}%\")\n\n    # Open the mask raster\n    with rasterio.open(mask_path) as src:\n        # Read the mask data\n        mask_data = src.read(1)\n        transform = src.transform\n        crs = src.crs\n\n        # Print mask statistics\n        print(f\"Mask dimensions: {mask_data.shape}\")\n        print(f\"Mask value range: {mask_data.min()} to {mask_data.max()}\")\n\n        # Prepare for connected component analysis\n        # Binarize the mask based on threshold\n        binary_mask = (mask_data &gt; (mask_threshold * 255)).astype(np.uint8)\n\n        # Apply morphological operations for better results (optional)\n        kernel = np.ones((3, 3), np.uint8)\n        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find connected components\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n            binary_mask, connectivity=8\n        )\n\n        print(\n            f\"Found {num_labels-1} potential objects\"\n        )  # Subtract 1 for background\n\n        # Create list to store polygons and confidence values\n        all_polygons = []\n        all_confidences = []\n\n        # Process each component (skip the first one which is background)\n        for i in tqdm(range(1, num_labels)):\n            # Extract this object\n            area = stats[i, cv2.CC_STAT_AREA]\n\n            # Skip if too small\n            if area &lt; min_object_area:\n                continue\n\n            # Skip if too large\n            if max_object_area is not None and area &gt; max_object_area:\n                continue\n\n            # Create a mask for this object\n            object_mask = (labels == i).astype(np.uint8)\n\n            # Find contours\n            contours, _ = cv2.findContours(\n                object_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n            )\n\n            # Process each contour\n            for contour in contours:\n                # Skip if too few points\n                if contour.shape[0] &lt; 3:\n                    continue\n\n                # Simplify contour if it has many points\n                if contour.shape[0] &gt; 50 and simplify_tolerance &gt; 0:\n                    epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                    contour = cv2.approxPolyDP(contour, epsilon, True)\n\n                # Convert to list of (x, y) coordinates\n                polygon_points = contour.reshape(-1, 2)\n\n                # Convert pixel coordinates to geographic coordinates\n                geo_points = []\n                for x, y in polygon_points:\n                    gx, gy = transform * (x, y)\n                    geo_points.append((gx, gy))\n\n                # Create Shapely polygon\n                if len(geo_points) &gt;= 3:\n                    try:\n                        shapely_poly = Polygon(geo_points)\n                        if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                            all_polygons.append(shapely_poly)\n\n                            # Calculate \"confidence\" as normalized size\n                            # This is a proxy since we don't have model confidence scores\n                            normalized_size = min(1.0, area / 1000)  # Cap at 1.0\n                            all_confidences.append(normalized_size)\n                    except Exception as e:\n                        print(f\"Error creating polygon: {e}\")\n\n        print(f\"Created {len(all_polygons)} valid polygons\")\n\n        # Create GeoDataFrame\n        if not all_polygons:\n            print(\"No valid polygons found\")\n            return None\n\n        gdf = gpd.GeoDataFrame(\n            {\n                \"geometry\": all_polygons,\n                \"confidence\": all_confidences,\n                \"class\": 1,  # Object class\n            },\n            crs=crs,\n        )\n\n        # Apply non-maximum suppression to remove overlapping polygons\n        gdf = self.filter_overlapping_polygons(\n            gdf, nms_iou_threshold=nms_iou_threshold\n        )\n\n        print(f\"Object count after NMS filtering: {len(gdf)}\")\n\n        # Apply regularization if requested\n        if regularize and len(gdf) &gt; 0:\n            # Convert pixel area to geographic units for min_area parameter\n            # Estimate pixel size in geographic units\n            with rasterio.open(mask_path) as src:\n                pixel_size_x = src.transform[\n                    0\n                ]  # width of a pixel in geographic units\n                pixel_size_y = abs(\n                    src.transform[4]\n                )  # height of a pixel in geographic units\n                avg_pixel_area = pixel_size_x * pixel_size_y\n\n            # Use 10 pixels as minimum area in geographic units\n            min_geo_area = 10 * avg_pixel_area\n\n            # Regularize objects\n            gdf = self.regularize_objects(\n                gdf,\n                min_area=min_geo_area,\n                angle_threshold=angle_threshold,\n                rectangularity_threshold=rectangularity_threshold,\n            )\n\n        # Save to file\n        if output_path:\n            gdf.to_file(output_path)\n            print(f\"Saved {len(gdf)} objects to {output_path}\")\n\n        return gdf\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.process_raster","title":"<code>process_raster(self, raster_path, output_path=None, batch_size=4, filter_edges=True, edge_buffer=20, **kwargs)</code>","text":"<p>Process a raster file to extract objects with customizable parameters.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <p>Path to input raster file</p> required <code>output_path</code> <p>Path to output GeoJSON file (optional)</p> <code>None</code> <code>batch_size</code> <p>Batch size for processing</p> <code>4</code> <code>filter_edges</code> <p>Whether to filter out objects at the edges of the image</p> <code>True</code> <code>edge_buffer</code> <p>Size of edge buffer in pixels to filter out objects (if filter_edges=True)</p> <code>20</code> <code>**kwargs</code> <p>Additional parameters: confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0) overlap: Overlap between adjacent tiles (0.0-1.0) chip_size: Size of image chips for processing (height, width) nms_iou_threshold: IoU threshold for non-maximum suppression (0.0-1.0) mask_threshold: Threshold for mask binarization (0.0-1.0) min_object_area: Minimum area in pixels to keep an object simplify_tolerance: Tolerance for polygon simplification</p> <code>{}</code> <p>Returns:</p> Type Description <p>GeoDataFrame with objects</p> Source code in <code>geoai/extract.py</code> <pre><code>@torch.no_grad()\ndef process_raster(\n    self,\n    raster_path,\n    output_path=None,\n    batch_size=4,\n    filter_edges=True,\n    edge_buffer=20,\n    **kwargs,\n):\n    \"\"\"\n    Process a raster file to extract objects with customizable parameters.\n\n    Args:\n        raster_path: Path to input raster file\n        output_path: Path to output GeoJSON file (optional)\n        batch_size: Batch size for processing\n        filter_edges: Whether to filter out objects at the edges of the image\n        edge_buffer: Size of edge buffer in pixels to filter out objects (if filter_edges=True)\n        **kwargs: Additional parameters:\n            confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n            overlap: Overlap between adjacent tiles (0.0-1.0)\n            chip_size: Size of image chips for processing (height, width)\n            nms_iou_threshold: IoU threshold for non-maximum suppression (0.0-1.0)\n            mask_threshold: Threshold for mask binarization (0.0-1.0)\n            min_object_area: Minimum area in pixels to keep an object\n            simplify_tolerance: Tolerance for polygon simplification\n\n    Returns:\n        GeoDataFrame with objects\n    \"\"\"\n    # Get parameters from kwargs or use instance defaults\n    confidence_threshold = kwargs.get(\n        \"confidence_threshold\", self.confidence_threshold\n    )\n    overlap = kwargs.get(\"overlap\", self.overlap)\n    chip_size = kwargs.get(\"chip_size\", self.chip_size)\n    nms_iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n    mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n    min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n    max_object_area = kwargs.get(\"max_object_area\", self.max_object_area)\n    simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n\n    # Print parameters being used\n    print(f\"Processing with parameters:\")\n    print(f\"- Confidence threshold: {confidence_threshold}\")\n    print(f\"- Tile overlap: {overlap}\")\n    print(f\"- Chip size: {chip_size}\")\n    print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n    print(f\"- Min object area: {min_object_area}\")\n    print(f\"- Max object area: {max_object_area}\")\n    print(f\"- Simplify tolerance: {simplify_tolerance}\")\n    print(f\"- Filter edge objects: {filter_edges}\")\n    if filter_edges:\n        print(f\"- Edge buffer size: {edge_buffer} pixels\")\n\n    # Create dataset\n    dataset = CustomDataset(raster_path=raster_path, chip_size=chip_size)\n    self.raster_stats = dataset.raster_stats\n\n    # Custom collate function to handle Shapely objects\n    def custom_collate(batch):\n        \"\"\"\n        Custom collate function that handles Shapely geometries\n        by keeping them as Python objects rather than trying to collate them.\n        \"\"\"\n        elem = batch[0]\n        if isinstance(elem, dict):\n            result = {}\n            for key in elem:\n                if key == \"bbox\":\n                    # Don't collate shapely objects, keep as list\n                    result[key] = [d[key] for d in batch]\n                else:\n                    # For tensors and other collatable types\n                    try:\n                        result[key] = (\n                            torch.utils.data._utils.collate.default_collate(\n                                [d[key] for d in batch]\n                            )\n                        )\n                    except TypeError:\n                        # Fall back to list for non-collatable types\n                        result[key] = [d[key] for d in batch]\n            return result\n        else:\n            # Default collate for non-dict types\n            return torch.utils.data._utils.collate.default_collate(batch)\n\n    # Create dataloader with simple indexing and custom collate\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=0,\n        collate_fn=custom_collate,\n    )\n\n    # Process batches\n    all_polygons = []\n    all_scores = []\n\n    print(f\"Processing raster with {len(dataloader)} batches\")\n    for batch in tqdm(dataloader):\n        # Move images to device\n        images = batch[\"image\"].to(self.device)\n        coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n        bboxes = batch[\n            \"bbox\"\n        ]  # Geographic bounding boxes - now a list, not a tensor\n\n        # Run inference\n        predictions = self.model(images)\n\n        # Process predictions\n        for idx, prediction in enumerate(predictions):\n            masks = prediction[\"masks\"].cpu().numpy()\n            scores = prediction[\"scores\"].cpu().numpy()\n            labels = prediction[\"labels\"].cpu().numpy()\n\n            # Skip if no predictions\n            if len(scores) == 0:\n                continue\n\n            # Filter by confidence threshold\n            valid_indices = scores &gt;= confidence_threshold\n            masks = masks[valid_indices]\n            scores = scores[valid_indices]\n            labels = labels[valid_indices]\n\n            # Skip if no valid predictions\n            if len(scores) == 0:\n                continue\n\n            # Get window coordinates\n            # The coords might be in different formats depending on batch handling\n            if isinstance(coords, list):\n                # If coords is a list of tuples\n                coord_item = coords[idx]\n                if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                    i, j = coord_item\n                elif isinstance(coord_item, torch.Tensor):\n                    i, j = coord_item.cpu().numpy().tolist()\n                else:\n                    print(f\"Unexpected coords format: {type(coord_item)}\")\n                    continue\n            elif isinstance(coords, torch.Tensor):\n                # If coords is a tensor of shape [batch_size, 2]\n                i, j = coords[idx].cpu().numpy().tolist()\n            else:\n                print(f\"Unexpected coords type: {type(coords)}\")\n                continue\n\n            # Get window size\n            if isinstance(batch[\"window_size\"], list):\n                window_item = batch[\"window_size\"][idx]\n                if isinstance(window_item, tuple) and len(window_item) == 2:\n                    window_width, window_height = window_item\n                elif isinstance(window_item, torch.Tensor):\n                    window_width, window_height = window_item.cpu().numpy().tolist()\n                else:\n                    print(f\"Unexpected window_size format: {type(window_item)}\")\n                    continue\n            elif isinstance(batch[\"window_size\"], torch.Tensor):\n                window_width, window_height = (\n                    batch[\"window_size\"][idx].cpu().numpy().tolist()\n                )\n            else:\n                print(f\"Unexpected window_size type: {type(batch['window_size'])}\")\n                continue\n\n            # Process masks to polygons\n            for mask_idx, mask in enumerate(masks):\n                # Get binary mask\n                binary_mask = mask[0]  # Get binary mask\n\n                # Convert mask to polygon with custom parameters\n                contours = self.mask_to_polygons(\n                    binary_mask,\n                    simplify_tolerance=simplify_tolerance,\n                    mask_threshold=mask_threshold,\n                    min_object_area=min_object_area,\n                    max_object_area=max_object_area,\n                )\n\n                # Skip if no valid polygons\n                if not contours:\n                    continue\n\n                # Transform polygons to geographic coordinates\n                with rasterio.open(raster_path) as src:\n                    transform = src.transform\n\n                    for contour in contours:\n                        # Convert polygon to global coordinates\n                        global_polygon = []\n                        for x, y in contour:\n                            # Adjust coordinates based on window position\n                            gx, gy = transform * (i + x, j + y)\n                            global_polygon.append((gx, gy))\n\n                        # Create Shapely polygon\n                        if len(global_polygon) &gt;= 3:\n                            try:\n                                shapely_poly = Polygon(global_polygon)\n                                if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                                    all_polygons.append(shapely_poly)\n                                    all_scores.append(float(scores[mask_idx]))\n                            except Exception as e:\n                                print(f\"Error creating polygon: {e}\")\n\n    # Create GeoDataFrame\n    if not all_polygons:\n        print(\"No valid polygons found\")\n        return None\n\n    gdf = gpd.GeoDataFrame(\n        {\n            \"geometry\": all_polygons,\n            \"confidence\": all_scores,\n            \"class\": 1,  # Object class\n        },\n        crs=dataset.crs,\n    )\n\n    # Remove overlapping polygons with custom threshold\n    gdf = self.filter_overlapping_polygons(gdf, nms_iou_threshold=nms_iou_threshold)\n\n    # Filter edge objects if requested\n    if filter_edges:\n        gdf = self.filter_edge_objects(gdf, raster_path, edge_buffer=edge_buffer)\n\n    # Save to file if requested\n    if output_path:\n        gdf.to_file(output_path, driver=\"GeoJSON\")\n        print(f\"Saved {len(gdf)} objects to {output_path}\")\n\n    return gdf\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.regularize_objects","title":"<code>regularize_objects(self, gdf, min_area=10, angle_threshold=15, orthogonality_threshold=0.3, rectangularity_threshold=0.7)</code>","text":"<p>Regularize objects to enforce right angles and rectangular shapes.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <p>GeoDataFrame with objects</p> required <code>min_area</code> <p>Minimum area in square units to keep an object</p> <code>10</code> <code>angle_threshold</code> <p>Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)</p> <code>15</code> <code>orthogonality_threshold</code> <p>Percentage of angles that must be orthogonal for an object to be regularized</p> <code>0.3</code> <code>rectangularity_threshold</code> <p>Minimum area ratio to Object's oriented bounding box for rectangular simplification</p> <code>0.7</code> <p>Returns:</p> Type Description <p>GeoDataFrame with regularized objects</p> Source code in <code>geoai/extract.py</code> <pre><code>def regularize_objects(\n    self,\n    gdf,\n    min_area=10,\n    angle_threshold=15,\n    orthogonality_threshold=0.3,\n    rectangularity_threshold=0.7,\n):\n    \"\"\"\n    Regularize objects to enforce right angles and rectangular shapes.\n\n    Args:\n        gdf: GeoDataFrame with objects\n        min_area: Minimum area in square units to keep an object\n        angle_threshold: Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)\n        orthogonality_threshold: Percentage of angles that must be orthogonal for an object to be regularized\n        rectangularity_threshold: Minimum area ratio to Object's oriented bounding box for rectangular simplification\n\n    Returns:\n        GeoDataFrame with regularized objects\n    \"\"\"\n    import numpy as np\n    from shapely.geometry import Polygon, MultiPolygon, box\n    from shapely.affinity import rotate, translate\n    import geopandas as gpd\n    import math\n    from tqdm import tqdm\n    import cv2\n\n    def get_angle(p1, p2, p3):\n        \"\"\"Calculate angle between three points in degrees (0-180)\"\"\"\n        a = np.array(p1)\n        b = np.array(p2)\n        c = np.array(p3)\n\n        ba = a - b\n        bc = c - b\n\n        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n        # Handle numerical errors that could push cosine outside [-1, 1]\n        cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n        angle = np.degrees(np.arccos(cosine_angle))\n\n        return angle\n\n    def is_orthogonal(angle, threshold=angle_threshold):\n        \"\"\"Check if angle is close to 90 degrees\"\"\"\n        return abs(angle - 90) &lt;= threshold\n\n    def calculate_dominant_direction(polygon):\n        \"\"\"Find the dominant direction of a polygon using PCA\"\"\"\n        # Extract coordinates\n        coords = np.array(polygon.exterior.coords)\n\n        # Mean center the coordinates\n        mean = np.mean(coords, axis=0)\n        centered_coords = coords - mean\n\n        # Calculate covariance matrix and its eigenvalues/eigenvectors\n        cov_matrix = np.cov(centered_coords.T)\n        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n        # Get the index of the largest eigenvalue\n        largest_idx = np.argmax(eigenvalues)\n\n        # Get the corresponding eigenvector (principal axis)\n        principal_axis = eigenvectors[:, largest_idx]\n\n        # Calculate the angle in degrees\n        angle_rad = np.arctan2(principal_axis[1], principal_axis[0])\n        angle_deg = np.degrees(angle_rad)\n\n        # Normalize to range 0-180\n        if angle_deg &lt; 0:\n            angle_deg += 180\n\n        return angle_deg\n\n    def create_oriented_envelope(polygon, angle_deg):\n        \"\"\"Create an oriented minimum area rectangle for the polygon\"\"\"\n        # Create a rotated rectangle using OpenCV method (more robust than Shapely methods)\n        coords = np.array(polygon.exterior.coords)[:-1].astype(\n            np.float32\n        )  # Skip the last point (same as first)\n\n        # Use OpenCV's minAreaRect\n        rect = cv2.minAreaRect(coords)\n        box_points = cv2.boxPoints(rect)\n\n        # Convert to shapely polygon\n        oriented_box = Polygon(box_points)\n\n        return oriented_box\n\n    def get_rectangularity(polygon, oriented_box):\n        \"\"\"Calculate the rectangularity (area ratio to its oriented bounding box)\"\"\"\n        if oriented_box.area == 0:\n            return 0\n        return polygon.area / oriented_box.area\n\n    def check_orthogonality(polygon):\n        \"\"\"Check what percentage of angles in the polygon are orthogonal\"\"\"\n        coords = list(polygon.exterior.coords)\n        if len(coords) &lt;= 4:  # Triangle or point\n            return 0\n\n        # Remove last point (same as first)\n        coords = coords[:-1]\n\n        orthogonal_count = 0\n        total_angles = len(coords)\n\n        for i in range(total_angles):\n            p1 = coords[i]\n            p2 = coords[(i + 1) % total_angles]\n            p3 = coords[(i + 2) % total_angles]\n\n            angle = get_angle(p1, p2, p3)\n            if is_orthogonal(angle):\n                orthogonal_count += 1\n\n        return orthogonal_count / total_angles\n\n    def simplify_to_rectangle(polygon):\n        \"\"\"Simplify a polygon to a rectangle using its oriented bounding box\"\"\"\n        # Get dominant direction\n        angle = calculate_dominant_direction(polygon)\n\n        # Create oriented envelope\n        rect = create_oriented_envelope(polygon, angle)\n\n        return rect\n\n    if gdf is None or len(gdf) == 0:\n        print(\"No Objects to regularize\")\n        return gdf\n\n    print(f\"Regularizing {len(gdf)} objects...\")\n    print(f\"- Angle threshold: {angle_threshold}\u00b0 from 90\u00b0\")\n    print(f\"- Min orthogonality: {orthogonality_threshold*100}% of angles\")\n    print(\n        f\"- Min rectangularity: {rectangularity_threshold*100}% of bounding box area\"\n    )\n\n    # Create a copy to avoid modifying the original\n    result_gdf = gdf.copy()\n\n    # Track statistics\n    total_objects = len(gdf)\n    regularized_count = 0\n    rectangularized_count = 0\n\n    # Process each Object\n    for idx, row in tqdm(gdf.iterrows(), total=len(gdf)):\n        geom = row.geometry\n\n        # Skip invalid or empty geometries\n        if geom is None or geom.is_empty:\n            continue\n\n        # Handle MultiPolygons by processing the largest part\n        if isinstance(geom, MultiPolygon):\n            areas = [p.area for p in geom.geoms]\n            if not areas:\n                continue\n            geom = list(geom.geoms)[np.argmax(areas)]\n\n        # Filter out tiny Objects\n        if geom.area &lt; min_area:\n            continue\n\n        # Check orthogonality\n        orthogonality = check_orthogonality(geom)\n\n        # Create oriented envelope\n        oriented_box = create_oriented_envelope(\n            geom, calculate_dominant_direction(geom)\n        )\n\n        # Check rectangularity\n        rectangularity = get_rectangularity(geom, oriented_box)\n\n        # Decide how to regularize\n        if rectangularity &gt;= rectangularity_threshold:\n            # Object is already quite rectangular, simplify to a rectangle\n            result_gdf.at[idx, \"geometry\"] = oriented_box\n            result_gdf.at[idx, \"regularized\"] = \"rectangle\"\n            rectangularized_count += 1\n        elif orthogonality &gt;= orthogonality_threshold:\n            # Object has many orthogonal angles but isn't rectangular\n            # Could implement more sophisticated regularization here\n            # For now, we'll still use the oriented rectangle\n            result_gdf.at[idx, \"geometry\"] = oriented_box\n            result_gdf.at[idx, \"regularized\"] = \"orthogonal\"\n            regularized_count += 1\n        else:\n            # Object doesn't have clear orthogonal structure\n            # Keep original but flag as unmodified\n            result_gdf.at[idx, \"regularized\"] = \"original\"\n\n    # Report statistics\n    print(f\"Regularization completed:\")\n    print(f\"- Total objects: {total_objects}\")\n    print(\n        f\"- Rectangular objects: {rectangularized_count} ({rectangularized_count/total_objects*100:.1f}%)\"\n    )\n    print(\n        f\"- Other regularized objects: {regularized_count} ({regularized_count/total_objects*100:.1f}%)\"\n    )\n    print(\n        f\"- Unmodified objects: {total_objects-rectangularized_count-regularized_count} ({(total_objects-rectangularized_count-regularized_count)/total_objects*100:.1f}%)\"\n    )\n\n    return result_gdf\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.save_masks_as_geotiff","title":"<code>save_masks_as_geotiff(self, raster_path, output_path=None, batch_size=4, verbose=False, **kwargs)</code>","text":"<p>Process a raster file to extract object masks and save as GeoTIFF.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <p>Path to input raster file</p> required <code>output_path</code> <p>Path to output GeoTIFF file (optional, default: input_masks.tif)</p> <code>None</code> <code>batch_size</code> <p>Batch size for processing</p> <code>4</code> <code>verbose</code> <p>Whether to print detailed processing information</p> <code>False</code> <code>**kwargs</code> <p>Additional parameters: confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0) chip_size: Size of image chips for processing (height, width) mask_threshold: Threshold for mask binarization (0.0-1.0)</p> <code>{}</code> <p>Returns:</p> Type Description <p>Path to the saved GeoTIFF file</p> Source code in <code>geoai/extract.py</code> <pre><code>def save_masks_as_geotiff(\n    self, raster_path, output_path=None, batch_size=4, verbose=False, **kwargs\n):\n    \"\"\"\n    Process a raster file to extract object masks and save as GeoTIFF.\n\n    Args:\n        raster_path: Path to input raster file\n        output_path: Path to output GeoTIFF file (optional, default: input_masks.tif)\n        batch_size: Batch size for processing\n        verbose: Whether to print detailed processing information\n        **kwargs: Additional parameters:\n            confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n            chip_size: Size of image chips for processing (height, width)\n            mask_threshold: Threshold for mask binarization (0.0-1.0)\n\n    Returns:\n        Path to the saved GeoTIFF file\n    \"\"\"\n\n    # Get parameters from kwargs or use instance defaults\n    confidence_threshold = kwargs.get(\n        \"confidence_threshold\", self.confidence_threshold\n    )\n    chip_size = kwargs.get(\"chip_size\", self.chip_size)\n    mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n\n    # Set default output path if not provided\n    if output_path is None:\n        output_path = os.path.splitext(raster_path)[0] + \"_masks.tif\"\n\n    # Print parameters being used\n    print(f\"Processing masks with parameters:\")\n    print(f\"- Confidence threshold: {confidence_threshold}\")\n    print(f\"- Chip size: {chip_size}\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n\n    # Create dataset\n    dataset = CustomDataset(\n        raster_path=raster_path, chip_size=chip_size, verbose=verbose\n    )\n\n    # Store a flag to avoid repetitive messages\n    self.raster_stats = dataset.raster_stats\n    seen_warnings = {\n        \"bands\": False,\n        \"resize\": {},  # Dictionary to track resize warnings by shape\n    }\n\n    # Open original raster to get metadata\n    with rasterio.open(raster_path) as src:\n        # Create output binary mask raster with same dimensions as input\n        output_profile = src.profile.copy()\n        output_profile.update(\n            dtype=rasterio.uint8,\n            count=1,  # Single band for object mask\n            compress=\"lzw\",\n            nodata=0,\n        )\n\n        # Create output mask raster\n        with rasterio.open(output_path, \"w\", **output_profile) as dst:\n            # Initialize mask with zeros\n            mask_array = np.zeros((src.height, src.width), dtype=np.uint8)\n\n            # Custom collate function to handle Shapely objects\n            def custom_collate(batch):\n                \"\"\"Custom collate function for DataLoader\"\"\"\n                elem = batch[0]\n                if isinstance(elem, dict):\n                    result = {}\n                    for key in elem:\n                        if key == \"bbox\":\n                            # Don't collate shapely objects, keep as list\n                            result[key] = [d[key] for d in batch]\n                        else:\n                            # For tensors and other collatable types\n                            try:\n                                result[key] = (\n                                    torch.utils.data._utils.collate.default_collate(\n                                        [d[key] for d in batch]\n                                    )\n                                )\n                            except TypeError:\n                                # Fall back to list for non-collatable types\n                                result[key] = [d[key] for d in batch]\n                    return result\n                else:\n                    # Default collate for non-dict types\n                    return torch.utils.data._utils.collate.default_collate(batch)\n\n            # Create dataloader\n            dataloader = torch.utils.data.DataLoader(\n                dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=0,\n                collate_fn=custom_collate,\n            )\n\n            # Process batches\n            print(f\"Processing raster with {len(dataloader)} batches\")\n            for batch in tqdm(dataloader):\n                # Move images to device\n                images = batch[\"image\"].to(self.device)\n                coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n\n                # Run inference\n                with torch.no_grad():\n                    predictions = self.model(images)\n\n                # Process predictions\n                for idx, prediction in enumerate(predictions):\n                    masks = prediction[\"masks\"].cpu().numpy()\n                    scores = prediction[\"scores\"].cpu().numpy()\n\n                    # Skip if no predictions\n                    if len(scores) == 0:\n                        continue\n\n                    # Filter by confidence threshold\n                    valid_indices = scores &gt;= confidence_threshold\n                    masks = masks[valid_indices]\n                    scores = scores[valid_indices]\n\n                    # Skip if no valid predictions\n                    if len(scores) == 0:\n                        continue\n\n                    # Get window coordinates\n                    if isinstance(coords, list):\n                        coord_item = coords[idx]\n                        if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                            i, j = coord_item\n                        elif isinstance(coord_item, torch.Tensor):\n                            i, j = coord_item.cpu().numpy().tolist()\n                        else:\n                            print(f\"Unexpected coords format: {type(coord_item)}\")\n                            continue\n                    elif isinstance(coords, torch.Tensor):\n                        i, j = coords[idx].cpu().numpy().tolist()\n                    else:\n                        print(f\"Unexpected coords type: {type(coords)}\")\n                        continue\n\n                    # Get window size\n                    if isinstance(batch[\"window_size\"], list):\n                        window_item = batch[\"window_size\"][idx]\n                        if isinstance(window_item, tuple) and len(window_item) == 2:\n                            window_width, window_height = window_item\n                        elif isinstance(window_item, torch.Tensor):\n                            window_width, window_height = (\n                                window_item.cpu().numpy().tolist()\n                            )\n                        else:\n                            print(\n                                f\"Unexpected window_size format: {type(window_item)}\"\n                            )\n                            continue\n                    elif isinstance(batch[\"window_size\"], torch.Tensor):\n                        window_width, window_height = (\n                            batch[\"window_size\"][idx].cpu().numpy().tolist()\n                        )\n                    else:\n                        print(\n                            f\"Unexpected window_size type: {type(batch['window_size'])}\"\n                        )\n                        continue\n\n                    # Combine all masks for this window\n                    combined_mask = np.zeros(\n                        (window_height, window_width), dtype=np.uint8\n                    )\n\n                    for mask in masks:\n                        # Get the binary mask\n                        binary_mask = (mask[0] &gt; mask_threshold).astype(\n                            np.uint8\n                        ) * 255\n\n                        # Handle size mismatch - resize binary_mask if needed\n                        mask_h, mask_w = binary_mask.shape\n                        if mask_h != window_height or mask_w != window_width:\n                            resize_key = f\"{(mask_h, mask_w)}-&gt;{(window_height, window_width)}\"\n                            if resize_key not in seen_warnings[\"resize\"]:\n                                if verbose:\n                                    print(\n                                        f\"Resizing mask from {binary_mask.shape} to {(window_height, window_width)}\"\n                                    )\n                                else:\n                                    if not seen_warnings[\n                                        \"resize\"\n                                    ]:  # If this is the first resize warning\n                                        print(\n                                            f\"Resizing masks at image edges (set verbose=True for details)\"\n                                        )\n                                seen_warnings[\"resize\"][resize_key] = True\n\n                            # Crop or pad the binary mask to match window size\n                            resized_mask = np.zeros(\n                                (window_height, window_width), dtype=np.uint8\n                            )\n                            copy_h = min(mask_h, window_height)\n                            copy_w = min(mask_w, window_width)\n                            resized_mask[:copy_h, :copy_w] = binary_mask[\n                                :copy_h, :copy_w\n                            ]\n                            binary_mask = resized_mask\n\n                        # Update combined mask (taking maximum where masks overlap)\n                        combined_mask = np.maximum(combined_mask, binary_mask)\n\n                    # Write combined mask to output array\n                    # Handle edge cases where window might be smaller than chip size\n                    h, w = combined_mask.shape\n                    valid_h = min(h, src.height - j)\n                    valid_w = min(w, src.width - i)\n\n                    if valid_h &gt; 0 and valid_w &gt; 0:\n                        mask_array[j : j + valid_h, i : i + valid_w] = np.maximum(\n                            mask_array[j : j + valid_h, i : i + valid_w],\n                            combined_mask[:valid_h, :valid_w],\n                        )\n\n            # Write the final mask to the output file\n            dst.write(mask_array, 1)\n\n    print(f\"Object masks saved to {output_path}\")\n    return output_path\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.visualize_results","title":"<code>visualize_results(self, raster_path, gdf=None, output_path=None, figsize=(12, 12))</code>","text":"<p>Visualize object detection results with proper coordinate transformation.</p> <p>This function displays objects on top of the raster image, ensuring proper alignment between the GeoDataFrame polygons and the image.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <p>Path to input raster</p> required <code>gdf</code> <p>GeoDataFrame with object polygons (optional)</p> <code>None</code> <code>output_path</code> <p>Path to save visualization (optional)</p> <code>None</code> <code>figsize</code> <p>Figure size (width, height) in inches</p> <code>(12, 12)</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if visualization was successful</p> Source code in <code>geoai/extract.py</code> <pre><code>def visualize_results(\n    self, raster_path, gdf=None, output_path=None, figsize=(12, 12)\n):\n    \"\"\"\n    Visualize object detection results with proper coordinate transformation.\n\n    This function displays objects on top of the raster image,\n    ensuring proper alignment between the GeoDataFrame polygons and the image.\n\n    Args:\n        raster_path: Path to input raster\n        gdf: GeoDataFrame with object polygons (optional)\n        output_path: Path to save visualization (optional)\n        figsize: Figure size (width, height) in inches\n\n    Returns:\n        bool: True if visualization was successful\n    \"\"\"\n    # Check if raster file exists\n    if not os.path.exists(raster_path):\n        print(f\"Error: Raster file '{raster_path}' not found.\")\n        return False\n\n    # Process raster if GeoDataFrame not provided\n    if gdf is None:\n        gdf = self.process_raster(raster_path)\n\n    if gdf is None or len(gdf) == 0:\n        print(\"No objects to visualize\")\n        return False\n\n    # Check if confidence column exists in the GeoDataFrame\n    has_confidence = False\n    if hasattr(gdf, \"columns\") and \"confidence\" in gdf.columns:\n        # Try to access a confidence value to confirm it works\n        try:\n            if len(gdf) &gt; 0:\n                # Try getitem access\n                conf_val = gdf[\"confidence\"].iloc[0]\n                has_confidence = True\n                print(\n                    f\"Using confidence values (range: {gdf['confidence'].min():.2f} - {gdf['confidence'].max():.2f})\"\n                )\n        except Exception as e:\n            print(f\"Confidence column exists but couldn't access values: {e}\")\n            has_confidence = False\n    else:\n        print(\"No confidence column found in GeoDataFrame\")\n        has_confidence = False\n\n    # Read raster for visualization\n    with rasterio.open(raster_path) as src:\n        # Read the entire image or a subset if it's very large\n        if src.height &gt; 2000 or src.width &gt; 2000:\n            # Calculate scale factor to reduce size\n            scale = min(2000 / src.height, 2000 / src.width)\n            out_shape = (\n                int(src.count),\n                int(src.height * scale),\n                int(src.width * scale),\n            )\n\n            # Read and resample\n            image = src.read(\n                out_shape=out_shape, resampling=rasterio.enums.Resampling.bilinear\n            )\n\n            # Create a scaled transform for the resampled image\n            # Calculate scaling factors\n            x_scale = src.width / out_shape[2]\n            y_scale = src.height / out_shape[1]\n\n            # Get the original transform\n            orig_transform = src.transform\n\n            # Create a scaled transform\n            scaled_transform = rasterio.transform.Affine(\n                orig_transform.a * x_scale,\n                orig_transform.b,\n                orig_transform.c,\n                orig_transform.d,\n                orig_transform.e * y_scale,\n                orig_transform.f,\n            )\n        else:\n            image = src.read()\n            scaled_transform = src.transform\n\n        # Convert to RGB for display\n        if image.shape[0] &gt; 3:\n            image = image[:3]\n        elif image.shape[0] == 1:\n            image = np.repeat(image, 3, axis=0)\n\n        # Normalize image for display\n        image = image.transpose(1, 2, 0)  # CHW to HWC\n        image = image.astype(np.float32)\n\n        if image.max() &gt; 10:  # Likely 0-255 range\n            image = image / 255.0\n\n        image = np.clip(image, 0, 1)\n\n        # Get image bounds\n        bounds = src.bounds\n        crs = src.crs\n\n    # Create figure with appropriate aspect ratio\n    aspect_ratio = image.shape[1] / image.shape[0]  # width / height\n    plt.figure(figsize=(figsize[0], figsize[0] / aspect_ratio))\n    ax = plt.gca()\n\n    # Display image\n    ax.imshow(image)\n\n    # Make sure the GeoDataFrame has the same CRS as the raster\n    if gdf.crs != crs:\n        print(f\"Reprojecting GeoDataFrame from {gdf.crs} to {crs}\")\n        gdf = gdf.to_crs(crs)\n\n    # Set up colors for confidence visualization\n    if has_confidence:\n        try:\n            import matplotlib.cm as cm\n            from matplotlib.colors import Normalize\n\n            # Get min/max confidence values\n            min_conf = gdf[\"confidence\"].min()\n            max_conf = gdf[\"confidence\"].max()\n\n            # Set up normalization and colormap\n            norm = Normalize(vmin=min_conf, vmax=max_conf)\n            cmap = cm.viridis\n\n            # Create scalar mappable for colorbar\n            sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n            sm.set_array([])\n\n            # Add colorbar\n            cbar = plt.colorbar(\n                sm, ax=ax, orientation=\"vertical\", shrink=0.7, pad=0.01\n            )\n            cbar.set_label(\"Confidence Score\")\n        except Exception as e:\n            print(f\"Error setting up confidence visualization: {e}\")\n            has_confidence = False\n\n    # Function to convert coordinates\n    def geo_to_pixel(geometry, transform):\n        \"\"\"Convert geometry to pixel coordinates using the provided transform.\"\"\"\n        if geometry.is_empty:\n            return None\n\n        if geometry.geom_type == \"Polygon\":\n            # Get exterior coordinates\n            exterior_coords = list(geometry.exterior.coords)\n\n            # Convert to pixel coordinates\n            pixel_coords = [~transform * (x, y) for x, y in exterior_coords]\n\n            # Split into x and y lists\n            pixel_x = [coord[0] for coord in pixel_coords]\n            pixel_y = [coord[1] for coord in pixel_coords]\n\n            return pixel_x, pixel_y\n        else:\n            print(f\"Unsupported geometry type: {geometry.geom_type}\")\n            return None\n\n    # Plot each object\n    for idx, row in gdf.iterrows():\n        try:\n            # Convert polygon to pixel coordinates\n            coords = geo_to_pixel(row.geometry, scaled_transform)\n\n            if coords:\n                pixel_x, pixel_y = coords\n\n                if has_confidence:\n                    try:\n                        # Get confidence value using different methods\n                        # Method 1: Try direct attribute access\n                        confidence = None\n                        try:\n                            confidence = row.confidence\n                        except:\n                            pass\n\n                        # Method 2: Try dictionary-style access\n                        if confidence is None:\n                            try:\n                                confidence = row[\"confidence\"]\n                            except:\n                                pass\n\n                        # Method 3: Try accessing by index from the GeoDataFrame\n                        if confidence is None:\n                            try:\n                                confidence = gdf.iloc[idx][\"confidence\"]\n                            except:\n                                pass\n\n                        if confidence is not None:\n                            color = cmap(norm(confidence))\n                            # Fill polygon with semi-transparent color\n                            ax.fill(pixel_x, pixel_y, color=color, alpha=0.5)\n                            # Draw border\n                            ax.plot(\n                                pixel_x,\n                                pixel_y,\n                                color=color,\n                                linewidth=1,\n                                alpha=0.8,\n                            )\n                        else:\n                            # Fall back to red if confidence value couldn't be accessed\n                            ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n                    except Exception as e:\n                        print(\n                            f\"Error using confidence value for polygon {idx}: {e}\"\n                        )\n                        ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n                else:\n                    # No confidence data, just plot outlines in red\n                    ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n        except Exception as e:\n            print(f\"Error plotting polygon {idx}: {e}\")\n\n    # Remove axes\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(f\"objects (Found: {len(gdf)})\")\n\n    # Save if requested\n    if output_path:\n        plt.tight_layout()\n        plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n        print(f\"Visualization saved to {output_path}\")\n\n    plt.close()\n\n    # Create a simpler visualization focused just on a subset of objects\n    if len(gdf) &gt; 0:\n        plt.figure(figsize=figsize)\n        ax = plt.gca()\n\n        # Choose a subset of the image to show\n        with rasterio.open(raster_path) as src:\n            # Get centroid of first object\n            sample_geom = gdf.iloc[0].geometry\n            centroid = sample_geom.centroid\n\n            # Convert to pixel coordinates\n            center_x, center_y = ~src.transform * (centroid.x, centroid.y)\n\n            # Define a window around this object\n            window_size = 500  # pixels\n            window = rasterio.windows.Window(\n                max(0, int(center_x - window_size / 2)),\n                max(0, int(center_y - window_size / 2)),\n                min(window_size, src.width - int(center_x - window_size / 2)),\n                min(window_size, src.height - int(center_y - window_size / 2)),\n            )\n\n            # Read this window\n            sample_image = src.read(window=window)\n\n            # Convert to RGB for display\n            if sample_image.shape[0] &gt; 3:\n                sample_image = sample_image[:3]\n            elif sample_image.shape[0] == 1:\n                sample_image = np.repeat(sample_image, 3, axis=0)\n\n            # Normalize image for display\n            sample_image = sample_image.transpose(1, 2, 0)  # CHW to HWC\n            sample_image = sample_image.astype(np.float32)\n\n            if sample_image.max() &gt; 10:  # Likely 0-255 range\n                sample_image = sample_image / 255.0\n\n            sample_image = np.clip(sample_image, 0, 1)\n\n            # Display sample image\n            ax.imshow(sample_image, extent=[0, window.width, window.height, 0])\n\n            # Get the correct transform for this window\n            window_transform = src.window_transform(window)\n\n            # Calculate bounds of the window\n            window_bounds = rasterio.windows.bounds(window, src.transform)\n            window_box = box(*window_bounds)\n\n            # Filter objects that intersect with this window\n            visible_gdf = gdf[gdf.intersects(window_box)]\n\n            # Set up colors for sample view if confidence data exists\n            if has_confidence:\n                try:\n                    # Reuse the same normalization and colormap from main view\n                    sample_sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n                    sample_sm.set_array([])\n\n                    # Add colorbar to sample view\n                    sample_cbar = plt.colorbar(\n                        sample_sm,\n                        ax=ax,\n                        orientation=\"vertical\",\n                        shrink=0.7,\n                        pad=0.01,\n                    )\n                    sample_cbar.set_label(\"Confidence Score\")\n                except Exception as e:\n                    print(f\"Error setting up sample confidence visualization: {e}\")\n\n            # Plot objects in sample view\n            for idx, row in visible_gdf.iterrows():\n                try:\n                    # Get window-relative pixel coordinates\n                    geom = row.geometry\n\n                    # Skip empty geometries\n                    if geom.is_empty:\n                        continue\n\n                    # Get exterior coordinates\n                    exterior_coords = list(geom.exterior.coords)\n\n                    # Convert to pixel coordinates relative to window origin\n                    pixel_coords = []\n                    for x, y in exterior_coords:\n                        px, py = ~src.transform * (x, y)  # Convert to image pixels\n                        # Make coordinates relative to window\n                        px = px - window.col_off\n                        py = py - window.row_off\n                        pixel_coords.append((px, py))\n\n                    # Extract x and y coordinates\n                    pixel_x = [coord[0] for coord in pixel_coords]\n                    pixel_y = [coord[1] for coord in pixel_coords]\n\n                    # Use confidence colors if available\n                    if has_confidence:\n                        try:\n                            # Try different methods to access confidence\n                            confidence = None\n                            try:\n                                confidence = row.confidence\n                            except:\n                                pass\n\n                            if confidence is None:\n                                try:\n                                    confidence = row[\"confidence\"]\n                                except:\n                                    pass\n\n                            if confidence is None:\n                                try:\n                                    confidence = visible_gdf.iloc[idx][\"confidence\"]\n                                except:\n                                    pass\n\n                            if confidence is not None:\n                                color = cmap(norm(confidence))\n                                # Fill polygon with semi-transparent color\n                                ax.fill(pixel_x, pixel_y, color=color, alpha=0.5)\n                                # Draw border\n                                ax.plot(\n                                    pixel_x,\n                                    pixel_y,\n                                    color=color,\n                                    linewidth=1.5,\n                                    alpha=0.8,\n                                )\n                            else:\n                                ax.plot(\n                                    pixel_x, pixel_y, color=\"red\", linewidth=1.5\n                                )\n                        except Exception as e:\n                            print(\n                                f\"Error using confidence in sample view for polygon {idx}: {e}\"\n                            )\n                            ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                    else:\n                        ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                except Exception as e:\n                    print(f\"Error plotting polygon in sample view: {e}\")\n\n            # Set title\n            ax.set_title(f\"Sample Area - objects (Showing: {len(visible_gdf)})\")\n\n            # Remove axes\n            ax.set_xticks([])\n            ax.set_yticks([])\n\n            # Save if requested\n            if output_path:\n                sample_output = (\n                    os.path.splitext(output_path)[0]\n                    + \"_sample\"\n                    + os.path.splitext(output_path)[1]\n                )\n                plt.tight_layout()\n                plt.savefig(sample_output, dpi=300, bbox_inches=\"tight\")\n                print(f\"Sample visualization saved to {sample_output}\")\n</code></pre>"},{"location":"extract/#geoai.extract.ShipDetector","title":"<code> ShipDetector            (ObjectDetector)         </code>","text":"<p>Ship detection using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for ship detection.\"</p> Source code in <code>geoai/extract.py</code> <pre><code>class ShipDetector(ObjectDetector):\n    \"\"\"\n    Ship detection using a pre-trained Mask R-CNN model.\n\n    This class extends the\n    `ObjectDetector` class with additional methods for ship detection.\"\n    \"\"\"\n\n    def __init__(\n        self, model_path=\"ship_detection.pth\", repo_id=None, model=None, device=None\n    ):\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n</code></pre>"},{"location":"extract/#geoai.extract.ShipDetector.__init__","title":"<code>__init__(self, model_path='ship_detection.pth', repo_id=None, model=None, device=None)</code>  <code>special</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <p>Path to the .pth model file.</p> <code>'ship_detection.pth'</code> <code>repo_id</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self, model_path=\"ship_detection.pth\", repo_id=None, model=None, device=None\n):\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n</code></pre>"},{"location":"faq/","title":"FAQ","text":""},{"location":"geoai/","title":"geoai module","text":"<p>Main module.</p>"},{"location":"installation/","title":"Installation","text":"<p>This guide covers various methods for installing GeoAI on different platforms with different package managers.</p>"},{"location":"installation/#prerequisites","title":"\u2705 Prerequisites","text":"<p>GeoAI requires:</p> <ul> <li>Python 3.9 or above</li> <li>The required dependencies will be installed automatically</li> </ul>"},{"location":"installation/#recommended-installation-methods","title":"\ud83d\ude80 Recommended Installation Methods","text":""},{"location":"installation/#using-pip","title":"\ud83d\udc0d Using pip","text":"<p>The simplest way to install the latest stable release of GeoAI is via pip:</p> <pre><code>pip install geoai-py\n</code></pre> <p>To install GeoAI with all optional dependencies for additional features:</p> <pre><code>pip install \"geoai-py[all]\"\n</code></pre>"},{"location":"installation/#using-uv","title":"\ud83d\udc0d Using uv","text":"<p>To install the latest stable release of GeoAI with uv, a faster alternative to pip:</p> <pre><code>uv pip install geoai-py\n</code></pre>"},{"location":"installation/#using-conda","title":"\ud83d\udc3c Using conda","text":"<p>For Anaconda/Miniconda users, we recommend installing GeoAI via conda-forge, which handles dependencies like GDAL more elegantly:</p> <pre><code>conda install -c conda-forge geoai\n</code></pre>"},{"location":"installation/#using-mamba","title":"\ud83e\udda1 Using mamba","text":"<p>Mamba provides faster dependency resolution compared to conda. This is especially useful for large packages like GeoAI:</p> <pre><code>conda create -n geo python=3.12\nconda activate geo\nconda install -c conda-forge mamba\nmamba install -c conda-forge geoai\n</code></pre>"},{"location":"installation/#advanced-installation-options","title":"\ud83d\udd27 Advanced Installation Options","text":""},{"location":"installation/#gpu-support","title":"\ud83d\udda5\ufe0f GPU Support","text":"<p>To enable GPU acceleration for deep learning models (requires NVIDIA GPU):</p> <pre><code>mamba install -c conda-forge geoai \"pytorch=*=cuda*\"\n</code></pre> <p>This will install the appropriate PyTorch version with CUDA support.</p> <p>If you run into issues with the ipympl package, you can install it using the following command:</p> <pre><code>mamba install -c conda-forge geoai \"pytorch=*=cuda*\" jupyterlab ipympl\n</code></pre> <p>If you encounter issues with the sqlite package, you can update it using the following command:</p> <pre><code>mamba update -c conda-forge sqlite\n</code></pre>"},{"location":"installation/#notes-for-windows-users","title":"Notes for Windows Users","text":"<p>If you use mamba to install geoai, you may not have the latest version of torchgeo, which may cause issues when importing geoai. To fix this, you can install the latest version of torchgeo using the following command:</p> <pre><code>pip install -U torchgeo\n</code></pre>"},{"location":"installation/#development-installation","title":"\ud83d\udc69\u200d\ud83d\udcbb Development Installation","text":"<p>For contributing to GeoAI development, install directly from the source repository:</p> <pre><code>git clone https://github.com/opengeos/geoai.git\ncd geoai\npip install -e .\n</code></pre> <p>The <code>-e</code> flag installs the package in development mode, allowing you to modify the code and immediately see the effects.</p>"},{"location":"installation/#installing-from-github","title":"\ud83d\udce6 Installing from GitHub","text":"<p>To install the latest development version directly from GitHub:</p> <pre><code>pip install git+https://github.com/opengeos/geoai.git\n</code></pre> <p>For a specific branch:</p> <pre><code>pip install git+https://github.com/opengeos/geoai.git@branch-name\n</code></pre>"},{"location":"installation/#verifying-installation","title":"\u2713 Verifying Installation","text":"<p>To verify your installation, run:</p> <pre><code>import geoai\nprint(geoai.__version__)\n</code></pre>"},{"location":"installation/#troubleshooting","title":"\u26a0\ufe0f Troubleshooting","text":"<p>If you encounter installation problems:</p> <ol> <li>Check the FAQ section of our documentation</li> <li>Search for similar issues in our GitHub Issues</li> <li>Ask for help in our GitHub Discussions</li> </ol>"},{"location":"installation/#upgrading","title":"\ud83d\udd04 Upgrading","text":"<p>To upgrade GeoAI to the latest version:</p> <pre><code>pip install -U geoai-py\n</code></pre> <p>Or with conda:</p> <pre><code>conda update -c conda-forge geoai\n</code></pre>"},{"location":"segmentation/","title":"segmentation module","text":""},{"location":"segmentation/#geoai.segmentation.CustomDataset","title":"<code> CustomDataset            (Dataset)         </code>","text":"<p>Custom Dataset for loading images and masks.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>class CustomDataset(Dataset):\n    \"\"\"Custom Dataset for loading images and masks.\"\"\"\n\n    def __init__(\n        self,\n        images_dir: str,\n        masks_dir: str,\n        transform: A.Compose = None,\n        target_size: tuple = (256, 256),\n        num_classes: int = 2,\n    ):\n        \"\"\"\n        Args:\n            images_dir (str): Directory containing images.\n            masks_dir (str): Directory containing masks.\n            transform (A.Compose, optional): Transformations to be applied on the images and masks.\n            target_size (tuple, optional): Target size for resizing images and masks.\n            num_classes (int, optional): Number of classes in the masks.\n        \"\"\"\n        self.images_dir = images_dir\n        self.masks_dir = masks_dir\n        self.transform = transform\n        self.target_size = target_size\n        self.num_classes = num_classes\n        self.images = sorted(os.listdir(images_dir))\n        self.masks = sorted(os.listdir(masks_dir))\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the total number of samples.\"\"\"\n        return len(self.images)\n\n    def __getitem__(self, idx: int) -&gt; dict:\n        \"\"\"\n        Args:\n            idx (int): Index of the sample to fetch.\n\n        Returns:\n            dict: A dictionary with 'pixel_values' and 'labels'.\n        \"\"\"\n        img_path = os.path.join(self.images_dir, self.images[idx])\n        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n        image = Image.open(img_path).convert(\"RGB\")\n        mask = Image.open(mask_path).convert(\"L\")\n\n        image = image.resize(self.target_size)\n        mask = mask.resize(self.target_size)\n\n        image = np.array(image)\n        mask = np.array(mask)\n\n        mask = (mask &gt; 127).astype(np.uint8)\n\n        if self.transform:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed[\"image\"]\n            mask = transformed[\"mask\"]\n\n        assert (\n            mask.max() &lt; self.num_classes\n        ), f\"Mask values should be less than {self.num_classes}, but found {mask.max()}\"\n        assert (\n            mask.min() &gt;= 0\n        ), f\"Mask values should be greater than or equal to 0, but found {mask.min()}\"\n\n        mask = mask.clone().detach().long()\n\n        return {\"pixel_values\": image, \"labels\": mask}\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.CustomDataset.__getitem__","title":"<code>__getitem__(self, idx)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the sample to fetch.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with 'pixel_values' and 'labels'.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; dict:\n    \"\"\"\n    Args:\n        idx (int): Index of the sample to fetch.\n\n    Returns:\n        dict: A dictionary with 'pixel_values' and 'labels'.\n    \"\"\"\n    img_path = os.path.join(self.images_dir, self.images[idx])\n    mask_path = os.path.join(self.masks_dir, self.masks[idx])\n    image = Image.open(img_path).convert(\"RGB\")\n    mask = Image.open(mask_path).convert(\"L\")\n\n    image = image.resize(self.target_size)\n    mask = mask.resize(self.target_size)\n\n    image = np.array(image)\n    mask = np.array(mask)\n\n    mask = (mask &gt; 127).astype(np.uint8)\n\n    if self.transform:\n        transformed = self.transform(image=image, mask=mask)\n        image = transformed[\"image\"]\n        mask = transformed[\"mask\"]\n\n    assert (\n        mask.max() &lt; self.num_classes\n    ), f\"Mask values should be less than {self.num_classes}, but found {mask.max()}\"\n    assert (\n        mask.min() &gt;= 0\n    ), f\"Mask values should be greater than or equal to 0, but found {mask.min()}\"\n\n    mask = mask.clone().detach().long()\n\n    return {\"pixel_values\": image, \"labels\": mask}\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.CustomDataset.__init__","title":"<code>__init__(self, images_dir, masks_dir, transform=None, target_size=(256, 256), num_classes=2)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Directory containing images.</p> required <code>masks_dir</code> <code>str</code> <p>Directory containing masks.</p> required <code>transform</code> <code>A.Compose</code> <p>Transformations to be applied on the images and masks.</p> <code>None</code> <code>target_size</code> <code>tuple</code> <p>Target size for resizing images and masks.</p> <code>(256, 256)</code> <code>num_classes</code> <code>int</code> <p>Number of classes in the masks.</p> <code>2</code> Source code in <code>geoai/segmentation.py</code> <pre><code>def __init__(\n    self,\n    images_dir: str,\n    masks_dir: str,\n    transform: A.Compose = None,\n    target_size: tuple = (256, 256),\n    num_classes: int = 2,\n):\n    \"\"\"\n    Args:\n        images_dir (str): Directory containing images.\n        masks_dir (str): Directory containing masks.\n        transform (A.Compose, optional): Transformations to be applied on the images and masks.\n        target_size (tuple, optional): Target size for resizing images and masks.\n        num_classes (int, optional): Number of classes in the masks.\n    \"\"\"\n    self.images_dir = images_dir\n    self.masks_dir = masks_dir\n    self.transform = transform\n    self.target_size = target_size\n    self.num_classes = num_classes\n    self.images = sorted(os.listdir(images_dir))\n    self.masks = sorted(os.listdir(masks_dir))\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.CustomDataset.__len__","title":"<code>__len__(self)</code>  <code>special</code>","text":"<p>Returns the total number of samples.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the total number of samples.\"\"\"\n    return len(self.images)\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.get_transform","title":"<code>get_transform()</code>","text":"<p>Returns:</p> Type Description <code>A.Compose</code> <p>A composition of image transformations.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def get_transform() -&gt; A.Compose:\n    \"\"\"\n    Returns:\n        A.Compose: A composition of image transformations.\n    \"\"\"\n    return A.Compose(\n        [\n            A.Resize(256, 256),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.RandomRotate90(p=0.5),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            ToTensorV2(),\n        ]\n    )\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.load_model","title":"<code>load_model(model_path, device)</code>","text":"<p>Loads the fine-tuned model from the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model.</p> required <code>device</code> <code>torch.device</code> <p>Device to load the model on.</p> required <p>Returns:</p> Type Description <code>SegformerForSemanticSegmentation</code> <p>Loaded model.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def load_model(\n    model_path: str, device: torch.device\n) -&gt; SegformerForSemanticSegmentation:\n    \"\"\"\n    Loads the fine-tuned model from the specified path.\n\n    Args:\n        model_path (str): Path to the model.\n        device (torch.device): Device to load the model on.\n\n    Returns:\n        SegformerForSemanticSegmentation: Loaded model.\n    \"\"\"\n    model = SegformerForSemanticSegmentation.from_pretrained(model_path)\n    model.to(device)\n    model.eval()\n    return model\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.predict_image","title":"<code>predict_image(model, image_tensor, original_size, device)</code>","text":"<p>Predicts the segmentation mask for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>SegformerForSemanticSegmentation</code> <p>Fine-tuned model.</p> required <code>image_tensor</code> <code>torch.Tensor</code> <p>Preprocessed image tensor.</p> required <code>original_size</code> <code>tuple</code> <p>Original size of the image (width, height).</p> required <code>device</code> <code>torch.device</code> <p>Device to perform inference on.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Predicted segmentation mask.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def predict_image(\n    model: SegformerForSemanticSegmentation,\n    image_tensor: torch.Tensor,\n    original_size: tuple,\n    device: torch.device,\n) -&gt; np.ndarray:\n    \"\"\"\n    Predicts the segmentation mask for the input image.\n\n    Args:\n        model (SegformerForSemanticSegmentation): Fine-tuned model.\n        image_tensor (torch.Tensor): Preprocessed image tensor.\n        original_size (tuple): Original size of the image (width, height).\n        device (torch.device): Device to perform inference on.\n\n    Returns:\n        np.ndarray: Predicted segmentation mask.\n    \"\"\"\n    with torch.no_grad():\n        image_tensor = image_tensor.to(device)\n        outputs = model(pixel_values=image_tensor)\n        logits = outputs.logits\n        upsampled_logits = F.interpolate(\n            logits, size=original_size[::-1], mode=\"bilinear\", align_corners=False\n        )\n        predictions = torch.argmax(upsampled_logits, dim=1).cpu().numpy()\n    return predictions[0]\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.prepare_datasets","title":"<code>prepare_datasets(images_dir, masks_dir, transform, test_size=0.2, random_state=42)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Directory containing images.</p> required <code>masks_dir</code> <code>str</code> <p>Directory containing masks.</p> required <code>transform</code> <code>A.Compose</code> <p>Transformations to be applied.</p> required <code>test_size</code> <code>float</code> <p>Proportion of the dataset to include in the validation split.</p> <code>0.2</code> <code>random_state</code> <code>int</code> <p>Random seed for shuffling the dataset.</p> <code>42</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Training and validation datasets.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def prepare_datasets(\n    images_dir: str,\n    masks_dir: str,\n    transform: A.Compose,\n    test_size: float = 0.2,\n    random_state: int = 42,\n) -&gt; tuple:\n    \"\"\"\n    Args:\n        images_dir (str): Directory containing images.\n        masks_dir (str): Directory containing masks.\n        transform (A.Compose): Transformations to be applied.\n        test_size (float, optional): Proportion of the dataset to include in the validation split.\n        random_state (int, optional): Random seed for shuffling the dataset.\n\n    Returns:\n        tuple: Training and validation datasets.\n    \"\"\"\n    dataset = CustomDataset(images_dir, masks_dir, transform)\n    train_indices, val_indices = train_test_split(\n        list(range(len(dataset))), test_size=test_size, random_state=random_state\n    )\n    train_dataset = Subset(dataset, train_indices)\n    val_dataset = Subset(dataset, val_indices)\n    return train_dataset, val_dataset\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.preprocess_image","title":"<code>preprocess_image(image_path, target_size=(256, 256))</code>","text":"<p>Preprocesses the input image for prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the input image.</p> required <code>target_size</code> <code>tuple</code> <p>Target size for resizing the image.</p> <code>(256, 256)</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Preprocessed image tensor.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def preprocess_image(image_path: str, target_size: tuple = (256, 256)) -&gt; torch.Tensor:\n    \"\"\"\n    Preprocesses the input image for prediction.\n\n    Args:\n        image_path (str): Path to the input image.\n        target_size (tuple, optional): Target size for resizing the image.\n\n    Returns:\n        torch.Tensor: Preprocessed image tensor.\n    \"\"\"\n    image = Image.open(image_path).convert(\"RGB\")\n    transform = A.Compose(\n        [\n            A.Resize(target_size[0], target_size[1]),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            ToTensorV2(),\n        ]\n    )\n    image = np.array(image)\n    transformed = transform(image=image)\n    return transformed[\"image\"].unsqueeze(0)\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.segment_image","title":"<code>segment_image(image_path, model_path, target_size=(256, 256), device=device(type='cpu'))</code>","text":"<p>Segments the input image using the fine-tuned model.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the input image.</p> required <code>model_path</code> <code>str</code> <p>Path to the fine-tuned model.</p> required <code>target_size</code> <code>tuple</code> <p>Target size for resizing the image.</p> <code>(256, 256)</code> <code>device</code> <code>torch.device</code> <p>Device to perform inference on.</p> <code>device(type='cpu')</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Predicted segmentation mask.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def segment_image(\n    image_path: str,\n    model_path: str,\n    target_size: tuple = (256, 256),\n    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n) -&gt; np.ndarray:\n    \"\"\"\n    Segments the input image using the fine-tuned model.\n\n    Args:\n        image_path (str): Path to the input image.\n        model_path (str): Path to the fine-tuned model.\n        target_size (tuple, optional): Target size for resizing the image.\n        device (torch.device, optional): Device to perform inference on.\n\n    Returns:\n        np.ndarray: Predicted segmentation mask.\n    \"\"\"\n    model = load_model(model_path, device)\n    image = Image.open(image_path).convert(\"RGB\")\n    original_size = image.size\n    image_tensor = preprocess_image(image_path, target_size)\n    predictions = predict_image(model, image_tensor, original_size, device)\n    return predictions\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.train_model","title":"<code>train_model(train_dataset, val_dataset, pretrained_model='nvidia/segformer-b0-finetuned-ade-512-512', model_save_path='./model', output_dir='./results', num_epochs=10, batch_size=8, learning_rate=5e-05)</code>","text":"<p>Trains the model and saves the fine-tuned model to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataset</code> <code>Dataset</code> <p>Training dataset.</p> required <code>val_dataset</code> <code>Dataset</code> <p>Validation dataset.</p> required <code>pretrained_model</code> <code>str</code> <p>Pretrained model to fine-tune.</p> <code>'nvidia/segformer-b0-finetuned-ade-512-512'</code> <code>model_save_path</code> <code>str</code> <p>Path to save the fine-tuned model. Defaults to './model'.</p> <code>'./model'</code> <code>output_dir</code> <code>str</code> <p>Directory to save training outputs.</p> <code>'./results'</code> <code>num_epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>Batch size for training and evaluation.</p> <code>8</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for training.</p> <code>5e-05</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the saved fine-tuned model.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def train_model(\n    train_dataset: Dataset,\n    val_dataset: Dataset,\n    pretrained_model: str = \"nvidia/segformer-b0-finetuned-ade-512-512\",\n    model_save_path: str = \"./model\",\n    output_dir: str = \"./results\",\n    num_epochs: int = 10,\n    batch_size: int = 8,\n    learning_rate: float = 5e-5,\n) -&gt; str:\n    \"\"\"\n    Trains the model and saves the fine-tuned model to the specified path.\n\n    Args:\n        train_dataset (Dataset): Training dataset.\n        val_dataset (Dataset): Validation dataset.\n        pretrained_model (str, optional): Pretrained model to fine-tune.\n        model_save_path (str): Path to save the fine-tuned model. Defaults to './model'.\n        output_dir (str, optional): Directory to save training outputs.\n        num_epochs (int, optional): Number of training epochs.\n        batch_size (int, optional): Batch size for training and evaluation.\n        learning_rate (float, optional): Learning rate for training.\n\n    Returns:\n        str: Path to the saved fine-tuned model.\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = SegformerForSemanticSegmentation.from_pretrained(pretrained_model).to(\n        device\n    )\n    data_collator = DefaultDataCollator(return_tensors=\"pt\")\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=num_epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_dir=\"./logs\",\n        learning_rate=learning_rate,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n    )\n\n    trainer.train()\n    model.save_pretrained(model_save_path)\n    print(f\"Model saved to {model_save_path}\")\n    return model_save_path\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.visualize_predictions","title":"<code>visualize_predictions(image_path, segmented_mask, target_size=(256, 256), reference_image_path=None)</code>","text":"<p>Visualizes the original image, segmented mask, and optionally the reference image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the original image.</p> required <code>segmented_mask</code> <code>np.ndarray</code> <p>Predicted segmentation mask.</p> required <code>target_size</code> <code>tuple</code> <p>Target size for resizing images.</p> <code>(256, 256)</code> <code>reference_image_path</code> <code>str</code> <p>Path to the reference image.</p> <code>None</code> Source code in <code>geoai/segmentation.py</code> <pre><code>def visualize_predictions(\n    image_path: str,\n    segmented_mask: np.ndarray,\n    target_size: tuple = (256, 256),\n    reference_image_path: str = None,\n) -&gt; None:\n    \"\"\"\n    Visualizes the original image, segmented mask, and optionally the reference image.\n\n    Args:\n        image_path (str): Path to the original image.\n        segmented_mask (np.ndarray): Predicted segmentation mask.\n        target_size (tuple, optional): Target size for resizing images.\n        reference_image_path (str, optional): Path to the reference image.\n    \"\"\"\n    original_image = Image.open(image_path).convert(\"RGB\")\n    original_image = original_image.resize(target_size)\n    segmented_image = Image.fromarray((segmented_mask * 255).astype(np.uint8))\n\n    if reference_image_path:\n        reference_image = Image.open(reference_image_path).convert(\"RGB\")\n        reference_image = reference_image.resize(target_size)\n        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n        axes[1].imshow(reference_image)\n        axes[1].set_title(\"Reference Image\")\n        axes[1].axis(\"off\")\n    else:\n        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n    axes[0].imshow(original_image)\n    axes[0].set_title(\"Original Image\")\n    axes[0].axis(\"off\")\n\n    if reference_image_path:\n        axes[2].imshow(segmented_image, cmap=\"gray\")\n        axes[2].set_title(\"Segmented Image\")\n        axes[2].axis(\"off\")\n    else:\n        axes[1].imshow(segmented_image, cmap=\"gray\")\n        axes[1].set_title(\"Segmented Image\")\n        axes[1].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use geoai in a project:</p> <pre><code>import geoai\n</code></pre>"},{"location":"utils/","title":"utils module","text":"<p>The utils module contains common functions and classes used by the other modules.</p>"},{"location":"utils/#geoai.utils.adaptive_regularization","title":"<code>adaptive_regularization(building_polygons, simplify_tolerance=0.5, area_threshold=0.9, preserve_shape=True)</code>","text":"<p>Adaptively regularizes building footprints based on their characteristics.</p> <p>This approach determines the best regularization method for each building.</p> <p>Parameters:</p> Name Type Description Default <code>building_polygons</code> <p>GeoDataFrame or list of shapely Polygons</p> required <code>simplify_tolerance</code> <p>Distance tolerance for simplification</p> <code>0.5</code> <code>area_threshold</code> <p>Minimum acceptable area ratio</p> <code>0.9</code> <code>preserve_shape</code> <p>Whether to preserve overall shape for complex buildings</p> <code>True</code> <p>Returns:</p> Type Description <p>GeoDataFrame or list of shapely Polygons with regularized building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def adaptive_regularization(\n    building_polygons, simplify_tolerance=0.5, area_threshold=0.9, preserve_shape=True\n):\n    \"\"\"\n    Adaptively regularizes building footprints based on their characteristics.\n\n    This approach determines the best regularization method for each building.\n\n    Args:\n        building_polygons: GeoDataFrame or list of shapely Polygons\n        simplify_tolerance: Distance tolerance for simplification\n        area_threshold: Minimum acceptable area ratio\n        preserve_shape: Whether to preserve overall shape for complex buildings\n\n    Returns:\n        GeoDataFrame or list of shapely Polygons with regularized building footprints\n    \"\"\"\n    from shapely.geometry import Polygon\n    from shapely.affinity import rotate\n\n    # Analyze the overall dataset to set appropriate parameters\n    if is_gdf := isinstance(building_polygons, gpd.GeoDataFrame):\n        geom_objects = building_polygons.geometry\n    else:\n        geom_objects = building_polygons\n\n    results = []\n\n    for building in geom_objects:\n        # Skip invalid geometries\n        if not hasattr(building, \"exterior\") or building.is_empty:\n            results.append(building)\n            continue\n\n        # Measure building complexity\n        complexity = building.length / (4 * np.sqrt(building.area))\n\n        # Determine if the building has a clear principal direction\n        coords = np.array(building.exterior.coords)[:-1]\n        segments = np.diff(np.vstack([coords, coords[0]]), axis=0)\n        segment_lengths = np.sqrt(segments[:, 0] ** 2 + segments[:, 1] ** 2)\n        angles = np.arctan2(segments[:, 1], segments[:, 0]) * 180 / np.pi\n\n        # Normalize angles to 0-180 range and get histogram\n        norm_angles = angles % 180\n        hist, bins = np.histogram(\n            norm_angles, bins=18, range=(0, 180), weights=segment_lengths\n        )\n\n        # Calculate direction clarity (ratio of longest direction to total)\n        direction_clarity = np.max(hist) / np.sum(hist) if np.sum(hist) &gt; 0 else 0\n\n        # Choose regularization method based on building characteristics\n        if complexity &lt; 1.2 and direction_clarity &gt; 0.5:\n            # Simple building with clear direction: use rotated rectangle\n            bin_max = np.argmax(hist)\n            bin_centers = (bins[:-1] + bins[1:]) / 2\n            dominant_angle = bin_centers[bin_max]\n\n            # Rotate to align with coordinate system\n            rotated = rotate(building, -dominant_angle, origin=\"centroid\")\n\n            # Create bounding box in rotated space\n            bounds = rotated.bounds\n            rect = Polygon(\n                [\n                    (bounds[0], bounds[1]),\n                    (bounds[2], bounds[1]),\n                    (bounds[2], bounds[3]),\n                    (bounds[0], bounds[3]),\n                ]\n            )\n\n            # Rotate back\n            result = rotate(rect, dominant_angle, origin=\"centroid\")\n\n            # Quality check\n            if (\n                result.area / building.area &lt; area_threshold\n                or result.area / building.area &gt; (1.0 / area_threshold)\n            ):\n                # Too much area change, use simplified original\n                result = building.simplify(simplify_tolerance, preserve_topology=True)\n\n        else:\n            # Complex building or no clear direction: preserve shape\n            if preserve_shape:\n                # Simplify with topology preservation\n                result = building.simplify(simplify_tolerance, preserve_topology=True)\n            else:\n                # Fall back to convex hull for very complex shapes\n                result = building.convex_hull\n\n        results.append(result)\n\n    # Return in same format as input\n    if is_gdf:\n        return gpd.GeoDataFrame(geometry=results, crs=building_polygons.crs)\n    else:\n        return results\n</code></pre>"},{"location":"utils/#geoai.utils.add_geometric_properties","title":"<code>add_geometric_properties(data, properties=None, area_unit='m2', length_unit='m')</code>","text":"<p>Calculates geometric properties and adds them to the GeoDataFrame.</p> <p>This function calculates various geometric properties of features in a GeoDataFrame and adds them as new columns without modifying existing attributes.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>GeoDataFrame containing vector features.</p> required <code>properties</code> <p>List of geometric properties to calculate. Options include: 'area', 'length', 'perimeter', 'centroid_x', 'centroid_y', 'bounds', 'convex_hull_area', 'orientation', 'complexity', 'area_bbox', 'area_convex', 'area_filled', 'major_length', 'minor_length', 'eccentricity', 'diameter_areagth', 'extent', 'solidity', 'elongation'. Defaults to ['area', 'length'] if None.</p> <code>None</code> <code>area_unit</code> <p>String specifying the unit for area calculation ('m2', 'km2', 'ha'). Defaults to 'm2'.</p> <code>'m2'</code> <code>length_unit</code> <p>String specifying the unit for length calculation ('m', 'km'). Defaults to 'm'.</p> <code>'m'</code> <p>Returns:</p> Type Description <code>geopandas.GeoDataFrame</code> <p>A copy of the input GeoDataFrame with added geometric property columns.</p> Source code in <code>geoai/utils.py</code> <pre><code>def add_geometric_properties(data, properties=None, area_unit=\"m2\", length_unit=\"m\"):\n    \"\"\"Calculates geometric properties and adds them to the GeoDataFrame.\n\n    This function calculates various geometric properties of features in a\n    GeoDataFrame and adds them as new columns without modifying existing attributes.\n\n    Args:\n        data: GeoDataFrame containing vector features.\n        properties: List of geometric properties to calculate. Options include:\n            'area', 'length', 'perimeter', 'centroid_x', 'centroid_y', 'bounds',\n            'convex_hull_area', 'orientation', 'complexity', 'area_bbox',\n            'area_convex', 'area_filled', 'major_length', 'minor_length',\n            'eccentricity', 'diameter_areagth', 'extent', 'solidity',\n            'elongation'.\n            Defaults to ['area', 'length'] if None.\n        area_unit: String specifying the unit for area calculation ('m2', 'km2',\n            'ha'). Defaults to 'm2'.\n        length_unit: String specifying the unit for length calculation ('m', 'km').\n            Defaults to 'm'.\n\n    Returns:\n        geopandas.GeoDataFrame: A copy of the input GeoDataFrame with added\n        geometric property columns.\n    \"\"\"\n    from shapely.ops import unary_union\n\n    if isinstance(data, str):\n        data = read_vector(data)\n\n    # Make a copy to avoid modifying the original\n    result = data.copy()\n\n    # Default properties to calculate\n    if properties is None:\n        properties = [\n            \"area\",\n            \"length\",\n            \"perimeter\",\n            \"convex_hull_area\",\n            \"orientation\",\n            \"complexity\",\n            \"area_bbox\",\n            \"area_convex\",\n            \"area_filled\",\n            \"major_length\",\n            \"minor_length\",\n            \"eccentricity\",\n            \"diameter_area\",\n            \"extent\",\n            \"solidity\",\n            \"elongation\",\n        ]\n\n    # Make sure we're working with a GeoDataFrame with a valid CRS\n\n    if not isinstance(result, gpd.GeoDataFrame):\n        raise ValueError(\"Input must be a GeoDataFrame\")\n\n    if result.crs is None:\n        raise ValueError(\n            \"GeoDataFrame must have a defined coordinate reference system (CRS)\"\n        )\n\n    # Ensure we're working with a projected CRS for accurate measurements\n    if result.crs.is_geographic:\n        # Reproject to a suitable projected CRS for accurate measurements\n        result = result.to_crs(result.estimate_utm_crs())\n\n    # Basic area calculation with unit conversion\n    if \"area\" in properties:\n        # Calculate area (only for polygons)\n        result[\"area\"] = result.geometry.apply(\n            lambda geom: geom.area if isinstance(geom, (Polygon, MultiPolygon)) else 0\n        )\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area\"] = result[\"area\"] / 1_000_000  # m\u00b2 to km\u00b2\n            result.rename(columns={\"area\": \"area_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area\"] = result[\"area\"] / 10_000  # m\u00b2 to hectares\n            result.rename(columns={\"area\": \"area_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area\": \"area_m2\"}, inplace=True)\n\n    # Length calculation with unit conversion\n    if \"length\" in properties:\n        # Calculate length (works for lines and polygon boundaries)\n        result[\"length\"] = result.geometry.length\n\n        # Convert to requested units\n        if length_unit == \"km\":\n            result[\"length\"] = result[\"length\"] / 1_000  # m to km\n            result.rename(columns={\"length\": \"length_km\"}, inplace=True)\n        else:  # Default is m\n            result.rename(columns={\"length\": \"length_m\"}, inplace=True)\n\n    # Perimeter calculation (for polygons)\n    if \"perimeter\" in properties:\n        result[\"perimeter\"] = result.geometry.apply(\n            lambda geom: (\n                geom.boundary.length if isinstance(geom, (Polygon, MultiPolygon)) else 0\n            )\n        )\n\n        # Convert to requested units\n        if length_unit == \"km\":\n            result[\"perimeter\"] = result[\"perimeter\"] / 1_000  # m to km\n            result.rename(columns={\"perimeter\": \"perimeter_km\"}, inplace=True)\n        else:  # Default is m\n            result.rename(columns={\"perimeter\": \"perimeter_m\"}, inplace=True)\n\n    # Centroid coordinates\n    if \"centroid_x\" in properties or \"centroid_y\" in properties:\n        centroids = result.geometry.centroid\n\n        if \"centroid_x\" in properties:\n            result[\"centroid_x\"] = centroids.x\n\n        if \"centroid_y\" in properties:\n            result[\"centroid_y\"] = centroids.y\n\n    # Bounding box properties\n    if \"bounds\" in properties:\n        bounds = result.geometry.bounds\n        result[\"minx\"] = bounds.minx\n        result[\"miny\"] = bounds.miny\n        result[\"maxx\"] = bounds.maxx\n        result[\"maxy\"] = bounds.maxy\n\n    # Area of bounding box\n    if \"area_bbox\" in properties:\n        bounds = result.geometry.bounds\n        result[\"area_bbox\"] = (bounds.maxx - bounds.minx) * (bounds.maxy - bounds.miny)\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area_bbox\"] = result[\"area_bbox\"] / 1_000_000\n            result.rename(columns={\"area_bbox\": \"area_bbox_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area_bbox\"] = result[\"area_bbox\"] / 10_000\n            result.rename(columns={\"area_bbox\": \"area_bbox_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area_bbox\": \"area_bbox_m2\"}, inplace=True)\n\n    # Area of convex hull\n    if \"area_convex\" in properties or \"convex_hull_area\" in properties:\n        result[\"area_convex\"] = result.geometry.convex_hull.area\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area_convex\"] = result[\"area_convex\"] / 1_000_000\n            result.rename(columns={\"area_convex\": \"area_convex_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area_convex\"] = result[\"area_convex\"] / 10_000\n            result.rename(columns={\"area_convex\": \"area_convex_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area_convex\": \"area_convex_m2\"}, inplace=True)\n\n        # For backward compatibility\n        if \"convex_hull_area\" in properties and \"area_convex\" not in properties:\n            result[\"convex_hull_area\"] = result[\"area_convex\"]\n            if area_unit == \"km2\":\n                result.rename(\n                    columns={\"convex_hull_area\": \"convex_hull_area_km2\"}, inplace=True\n                )\n            elif area_unit == \"ha\":\n                result.rename(\n                    columns={\"convex_hull_area\": \"convex_hull_area_ha\"}, inplace=True\n                )\n            else:\n                result.rename(\n                    columns={\"convex_hull_area\": \"convex_hull_area_m2\"}, inplace=True\n                )\n\n    # Area of filled geometry (no holes)\n    if \"area_filled\" in properties:\n\n        def get_filled_area(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)):\n                return 0\n\n            if isinstance(geom, MultiPolygon):\n                # For MultiPolygon, fill all constituent polygons\n                filled_polys = [Polygon(p.exterior) for p in geom.geoms]\n                return unary_union(filled_polys).area\n            else:\n                # For single Polygon, create a new one with just the exterior ring\n                return Polygon(geom.exterior).area\n\n        result[\"area_filled\"] = result.geometry.apply(get_filled_area)\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area_filled\"] = result[\"area_filled\"] / 1_000_000\n            result.rename(columns={\"area_filled\": \"area_filled_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area_filled\"] = result[\"area_filled\"] / 10_000\n            result.rename(columns={\"area_filled\": \"area_filled_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area_filled\": \"area_filled_m2\"}, inplace=True)\n\n    # Axes lengths, eccentricity, orientation, and elongation\n    if any(\n        p in properties\n        for p in [\n            \"major_length\",\n            \"minor_length\",\n            \"eccentricity\",\n            \"orientation\",\n            \"elongation\",\n        ]\n    ):\n\n        def get_axes_properties(geom):\n            # Skip non-polygons\n            if not isinstance(geom, (Polygon, MultiPolygon)):\n                return None, None, None, None, None\n\n            # Handle multipolygons by using the largest polygon\n            if isinstance(geom, MultiPolygon):\n                # Get the polygon with the largest area\n                geom = sorted(list(geom.geoms), key=lambda p: p.area, reverse=True)[0]\n\n            try:\n                # Get the minimum rotated rectangle\n                rect = geom.minimum_rotated_rectangle\n\n                # Extract coordinates\n                coords = list(rect.exterior.coords)[\n                    :-1\n                ]  # Remove the duplicated last point\n\n                if len(coords) &lt; 4:\n                    return None, None, None, None, None\n\n                # Calculate lengths of all four sides\n                sides = []\n                for i in range(len(coords)):\n                    p1 = coords[i]\n                    p2 = coords[(i + 1) % len(coords)]\n                    dx = p2[0] - p1[0]\n                    dy = p2[1] - p1[1]\n                    length = np.sqrt(dx**2 + dy**2)\n                    angle = np.degrees(np.arctan2(dy, dx)) % 180\n                    sides.append((length, angle, p1, p2))\n\n                # Group sides by length (allowing for small differences due to floating point precision)\n                # This ensures we correctly identify the rectangle's dimensions\n                sides_grouped = {}\n                tolerance = 1e-6  # Tolerance for length comparison\n\n                for s in sides:\n                    length, angle = s[0], s[1]\n                    matched = False\n\n                    for key in sides_grouped:\n                        if abs(length - key) &lt; tolerance:\n                            sides_grouped[key].append(s)\n                            matched = True\n                            break\n\n                    if not matched:\n                        sides_grouped[length] = [s]\n\n                # Get unique lengths (should be 2 for a rectangle, parallel sides have equal length)\n                unique_lengths = sorted(sides_grouped.keys(), reverse=True)\n\n                if len(unique_lengths) != 2:\n                    # If we don't get exactly 2 unique lengths, something is wrong with the rectangle\n                    # Fall back to simpler method using bounds\n                    bounds = rect.bounds\n                    width = bounds[2] - bounds[0]\n                    height = bounds[3] - bounds[1]\n                    major_length = max(width, height)\n                    minor_length = min(width, height)\n                    orientation = 0 if width &gt; height else 90\n                else:\n                    major_length = unique_lengths[0]\n                    minor_length = unique_lengths[1]\n                    # Get orientation from the major axis\n                    orientation = sides_grouped[major_length][0][1]\n\n                # Calculate eccentricity\n                if major_length &gt; 0:\n                    # Eccentricity for an ellipse: e = sqrt(1 - (b\u00b2/a\u00b2))\n                    # where a is the semi-major axis and b is the semi-minor axis\n                    eccentricity = np.sqrt(\n                        1 - ((minor_length / 2) ** 2 / (major_length / 2) ** 2)\n                    )\n                else:\n                    eccentricity = 0\n\n                # Calculate elongation (ratio of minor to major axis)\n                elongation = major_length / minor_length if major_length &gt; 0 else 1\n\n                return major_length, minor_length, eccentricity, orientation, elongation\n\n            except Exception as e:\n                # For debugging\n                # print(f\"Error calculating axes: {e}\")\n                return None, None, None, None, None\n\n        # Apply the function and split the results\n        axes_data = result.geometry.apply(get_axes_properties)\n\n        if \"major_length\" in properties:\n            result[\"major_length\"] = axes_data.apply(lambda x: x[0] if x else None)\n            # Convert to requested units\n            if length_unit == \"km\":\n                result[\"major_length\"] = result[\"major_length\"] / 1_000\n                result.rename(columns={\"major_length\": \"major_length_km\"}, inplace=True)\n            else:\n                result.rename(columns={\"major_length\": \"major_length_m\"}, inplace=True)\n\n        if \"minor_length\" in properties:\n            result[\"minor_length\"] = axes_data.apply(lambda x: x[1] if x else None)\n            # Convert to requested units\n            if length_unit == \"km\":\n                result[\"minor_length\"] = result[\"minor_length\"] / 1_000\n                result.rename(columns={\"minor_length\": \"minor_length_km\"}, inplace=True)\n            else:\n                result.rename(columns={\"minor_length\": \"minor_length_m\"}, inplace=True)\n\n        if \"eccentricity\" in properties:\n            result[\"eccentricity\"] = axes_data.apply(lambda x: x[2] if x else None)\n\n        if \"orientation\" in properties:\n            result[\"orientation\"] = axes_data.apply(lambda x: x[3] if x else None)\n\n        if \"elongation\" in properties:\n            result[\"elongation\"] = axes_data.apply(lambda x: x[4] if x else None)\n\n    # Equivalent diameter based on area\n    if \"diameter_areagth\" in properties:\n\n        def get_equivalent_diameter(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)) or geom.area &lt;= 0:\n                return None\n            # Diameter of a circle with the same area: d = 2 * sqrt(A / \u03c0)\n            return 2 * np.sqrt(geom.area / np.pi)\n\n        result[\"diameter_areagth\"] = result.geometry.apply(get_equivalent_diameter)\n\n        # Convert to requested units\n        if length_unit == \"km\":\n            result[\"diameter_areagth\"] = result[\"diameter_areagth\"] / 1_000\n            result.rename(\n                columns={\"diameter_areagth\": \"equivalent_diameter_area_km\"},\n                inplace=True,\n            )\n        else:\n            result.rename(\n                columns={\"diameter_areagth\": \"equivalent_diameter_area_m\"},\n                inplace=True,\n            )\n\n    # Extent (ratio of shape area to bounding box area)\n    if \"extent\" in properties:\n\n        def get_extent(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)) or geom.area &lt;= 0:\n                return None\n\n            bounds = geom.bounds\n            bbox_area = (bounds[2] - bounds[0]) * (bounds[3] - bounds[1])\n\n            if bbox_area &gt; 0:\n                return geom.area / bbox_area\n            return None\n\n        result[\"extent\"] = result.geometry.apply(get_extent)\n\n    # Solidity (ratio of shape area to convex hull area)\n    if \"solidity\" in properties:\n\n        def get_solidity(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)) or geom.area &lt;= 0:\n                return None\n\n            convex_hull_area = geom.convex_hull.area\n\n            if convex_hull_area &gt; 0:\n                return geom.area / convex_hull_area\n            return None\n\n        result[\"solidity\"] = result.geometry.apply(get_solidity)\n\n    # Complexity (ratio of perimeter to area)\n    if \"complexity\" in properties:\n\n        def calc_complexity(geom):\n            if isinstance(geom, (Polygon, MultiPolygon)) and geom.area &gt; 0:\n                # Shape index: P / (2 * sqrt(\u03c0 * A))\n                # Normalized to 1 for a circle, higher for more complex shapes\n                return geom.boundary.length / (2 * np.sqrt(np.pi * geom.area))\n            return None\n\n        result[\"complexity\"] = result.geometry.apply(calc_complexity)\n\n    return result\n</code></pre>"},{"location":"utils/#geoai.utils.analyze_vector_attributes","title":"<code>analyze_vector_attributes(vector_path, attribute_name)</code>","text":"<p>Analyze a specific attribute in a vector dataset and create a histogram.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <code>attribute_name</code> <code>str</code> <p>Name of the attribute to analyze</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing analysis results for the attribute</p> Source code in <code>geoai/utils.py</code> <pre><code>def analyze_vector_attributes(vector_path, attribute_name):\n    \"\"\"Analyze a specific attribute in a vector dataset and create a histogram.\n\n    Args:\n        vector_path (str): Path to the vector file\n        attribute_name (str): Name of the attribute to analyze\n\n    Returns:\n        dict: Dictionary containing analysis results for the attribute\n    \"\"\"\n    try:\n        gdf = gpd.read_file(vector_path)\n\n        # Check if attribute exists\n        if attribute_name not in gdf.columns:\n            print(f\"Attribute '{attribute_name}' not found in the dataset\")\n            return None\n\n        # Get the attribute series\n        attr = gdf[attribute_name]\n\n        # Perform different analyses based on data type\n        if pd.api.types.is_numeric_dtype(attr):\n            # Numeric attribute\n            analysis = {\n                \"attribute\": attribute_name,\n                \"type\": \"numeric\",\n                \"count\": attr.count(),\n                \"null_count\": attr.isna().sum(),\n                \"min\": attr.min(),\n                \"max\": attr.max(),\n                \"mean\": attr.mean(),\n                \"median\": attr.median(),\n                \"std\": attr.std(),\n                \"unique_values\": attr.nunique(),\n            }\n\n            # Create histogram\n            plt.figure(figsize=(10, 6))\n            plt.hist(attr.dropna(), bins=20, alpha=0.7, color=\"blue\")\n            plt.title(f\"Histogram of {attribute_name}\")\n            plt.xlabel(attribute_name)\n            plt.ylabel(\"Frequency\")\n            plt.grid(True, alpha=0.3)\n            plt.show()\n\n        else:\n            # Categorical attribute\n            analysis = {\n                \"attribute\": attribute_name,\n                \"type\": \"categorical\",\n                \"count\": attr.count(),\n                \"null_count\": attr.isna().sum(),\n                \"unique_values\": attr.nunique(),\n                \"value_counts\": attr.value_counts().to_dict(),\n            }\n\n            # Create bar plot for top categories\n            top_n = min(10, attr.nunique())\n            plt.figure(figsize=(10, 6))\n            attr.value_counts().head(top_n).plot(kind=\"bar\", color=\"skyblue\")\n            plt.title(f\"Top {top_n} values for {attribute_name}\")\n            plt.xlabel(attribute_name)\n            plt.ylabel(\"Count\")\n            plt.xticks(rotation=45)\n            plt.grid(True, alpha=0.3)\n            plt.tight_layout()\n            plt.show()\n\n        return analysis\n\n    except Exception as e:\n        print(f\"Error analyzing attribute: {str(e)}\")\n        return None\n</code></pre>"},{"location":"utils/#geoai.utils.batch_raster_to_vector","title":"<code>batch_raster_to_vector(input_dir, output_dir, pattern='*.tif', threshold=0, min_area=10, simplify_tolerance=None, class_values=None, attribute_name='class', output_format='geojson', merge_output=False, merge_filename='merged_vectors')</code>","text":"<p>Batch convert multiple raster files to vector polygons.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>Directory containing input raster files.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output vector files.</p> required <code>pattern</code> <code>str</code> <p>Pattern to match raster files (e.g., '*.tif').</p> <code>'*.tif'</code> <code>threshold</code> <code>int/float</code> <p>Pixel values greater than this threshold will be vectorized.</p> <code>0</code> <code>min_area</code> <code>float</code> <p>Minimum polygon area in square map units to keep.</p> <code>10</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for geometry simplification. None for no simplification.</p> <code>None</code> <code>class_values</code> <code>list</code> <p>Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.</p> <code>None</code> <code>attribute_name</code> <code>str</code> <p>Name of the attribute field for the class values.</p> <code>'class'</code> <code>output_format</code> <code>str</code> <p>Format for output files - 'geojson', 'shapefile', 'gpkg'.</p> <code>'geojson'</code> <code>merge_output</code> <code>bool</code> <p>Whether to merge all output vectors into a single file.</p> <code>False</code> <code>merge_filename</code> <code>str</code> <p>Filename for the merged output (without extension).</p> <code>'merged_vectors'</code> <p>Returns:</p> Type Description <code>geopandas.GeoDataFrame or None</code> <p>If merge_output is True, returns the merged GeoDataFrame.</p> Source code in <code>geoai/utils.py</code> <pre><code>def batch_raster_to_vector(\n    input_dir,\n    output_dir,\n    pattern=\"*.tif\",\n    threshold=0,\n    min_area=10,\n    simplify_tolerance=None,\n    class_values=None,\n    attribute_name=\"class\",\n    output_format=\"geojson\",\n    merge_output=False,\n    merge_filename=\"merged_vectors\",\n):\n    \"\"\"\n    Batch convert multiple raster files to vector polygons.\n\n    Args:\n        input_dir (str): Directory containing input raster files.\n        output_dir (str): Directory to save output vector files.\n        pattern (str): Pattern to match raster files (e.g., '*.tif').\n        threshold (int/float): Pixel values greater than this threshold will be vectorized.\n        min_area (float): Minimum polygon area in square map units to keep.\n        simplify_tolerance (float): Tolerance for geometry simplification. None for no simplification.\n        class_values (list): Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.\n        attribute_name (str): Name of the attribute field for the class values.\n        output_format (str): Format for output files - 'geojson', 'shapefile', 'gpkg'.\n        merge_output (bool): Whether to merge all output vectors into a single file.\n        merge_filename (str): Filename for the merged output (without extension).\n\n    Returns:\n        geopandas.GeoDataFrame or None: If merge_output is True, returns the merged GeoDataFrame.\n    \"\"\"\n    import glob\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Get list of raster files\n    raster_files = glob.glob(os.path.join(input_dir, pattern))\n\n    if not raster_files:\n        print(f\"No files matching pattern '{pattern}' found in {input_dir}\")\n        return None\n\n    print(f\"Found {len(raster_files)} raster files to process\")\n\n    # Process each raster file\n    gdfs = []\n    for raster_file in tqdm(raster_files, desc=\"Processing rasters\"):\n        # Get output filename\n        base_name = os.path.splitext(os.path.basename(raster_file))[0]\n        if output_format.lower() == \"geojson\":\n            out_file = os.path.join(output_dir, f\"{base_name}.geojson\")\n        elif output_format.lower() == \"shapefile\":\n            out_file = os.path.join(output_dir, f\"{base_name}.shp\")\n        elif output_format.lower() == \"gpkg\":\n            out_file = os.path.join(output_dir, f\"{base_name}.gpkg\")\n        else:\n            raise ValueError(f\"Unsupported output format: {output_format}\")\n\n        # Convert raster to vector\n        if merge_output:\n            # Don't save individual files if merging\n            gdf = raster_to_vector(\n                raster_file,\n                output_path=None,\n                threshold=threshold,\n                min_area=min_area,\n                simplify_tolerance=simplify_tolerance,\n                class_values=class_values,\n                attribute_name=attribute_name,\n            )\n\n            # Add filename as attribute\n            if not gdf.empty:\n                gdf[\"source_file\"] = base_name\n                gdfs.append(gdf)\n        else:\n            # Save individual files\n            raster_to_vector(\n                raster_file,\n                output_path=out_file,\n                threshold=threshold,\n                min_area=min_area,\n                simplify_tolerance=simplify_tolerance,\n                class_values=class_values,\n                attribute_name=attribute_name,\n                output_format=output_format,\n            )\n\n    # Merge output if requested\n    if merge_output and gdfs:\n        merged_gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True))\n\n        # Set CRS to the CRS of the first GeoDataFrame\n        if merged_gdf.crs is None and gdfs:\n            merged_gdf.crs = gdfs[0].crs\n\n        # Save merged output\n        if output_format.lower() == \"geojson\":\n            merged_file = os.path.join(output_dir, f\"{merge_filename}.geojson\")\n            merged_gdf.to_file(merged_file, driver=\"GeoJSON\")\n        elif output_format.lower() == \"shapefile\":\n            merged_file = os.path.join(output_dir, f\"{merge_filename}.shp\")\n            merged_gdf.to_file(merged_file)\n        elif output_format.lower() == \"gpkg\":\n            merged_file = os.path.join(output_dir, f\"{merge_filename}.gpkg\")\n            merged_gdf.to_file(merged_file, driver=\"GPKG\")\n\n        print(f\"Merged vector data saved to {merged_file}\")\n        return merged_gdf\n\n    return None\n</code></pre>"},{"location":"utils/#geoai.utils.batch_vector_to_raster","title":"<code>batch_vector_to_raster(vector_path, output_dir, attribute_field=None, reference_rasters=None, bounds_list=None, output_filename_pattern='{vector_name}_{index}', pixel_size=1.0, all_touched=False, fill_value=0, dtype=&lt;class 'numpy.uint8'&gt;, nodata=None)</code>","text":"<p>Batch convert vector data to multiple rasters based on different extents or reference rasters.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str or GeoDataFrame</code> <p>Path to the input vector file or a GeoDataFrame.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output raster files.</p> required <code>attribute_field</code> <code>str</code> <p>Field name in the vector data to use for pixel values.</p> <code>None</code> <code>reference_rasters</code> <code>list</code> <p>List of paths to reference rasters for dimensions, transform and CRS.</p> <code>None</code> <code>bounds_list</code> <code>list</code> <p>List of bounds tuples (left, bottom, right, top) to use if reference_rasters not provided.</p> <code>None</code> <code>output_filename_pattern</code> <code>str</code> <p>Pattern for output filenames. Can include {vector_name} and {index} placeholders.</p> <code>'{vector_name}_{index}'</code> <code>pixel_size</code> <code>float or tuple</code> <p>Pixel size to use if reference_rasters not provided.</p> <code>1.0</code> <code>all_touched</code> <code>bool</code> <p>If True, all pixels touched by geometries will be burned in.</p> <code>False</code> <code>fill_value</code> <code>int</code> <p>Value to fill the raster with before burning in features.</p> <code>0</code> <code>dtype</code> <code>numpy.dtype</code> <p>Data type of the output raster.</p> <code>&lt;class 'numpy.uint8'&gt;</code> <code>nodata</code> <code>int</code> <p>No data value for the output raster.</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>List of paths to the created raster files.</p> Source code in <code>geoai/utils.py</code> <pre><code>def batch_vector_to_raster(\n    vector_path,\n    output_dir,\n    attribute_field=None,\n    reference_rasters=None,\n    bounds_list=None,\n    output_filename_pattern=\"{vector_name}_{index}\",\n    pixel_size=1.0,\n    all_touched=False,\n    fill_value=0,\n    dtype=np.uint8,\n    nodata=None,\n):\n    \"\"\"\n    Batch convert vector data to multiple rasters based on different extents or reference rasters.\n\n    Args:\n        vector_path (str or GeoDataFrame): Path to the input vector file or a GeoDataFrame.\n        output_dir (str): Directory to save output raster files.\n        attribute_field (str): Field name in the vector data to use for pixel values.\n        reference_rasters (list): List of paths to reference rasters for dimensions, transform and CRS.\n        bounds_list (list): List of bounds tuples (left, bottom, right, top) to use if reference_rasters not provided.\n        output_filename_pattern (str): Pattern for output filenames.\n            Can include {vector_name} and {index} placeholders.\n        pixel_size (float or tuple): Pixel size to use if reference_rasters not provided.\n        all_touched (bool): If True, all pixels touched by geometries will be burned in.\n        fill_value (int): Value to fill the raster with before burning in features.\n        dtype (numpy.dtype): Data type of the output raster.\n        nodata (int): No data value for the output raster.\n\n    Returns:\n        list: List of paths to the created raster files.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Load vector data if it's a path\n    if isinstance(vector_path, str):\n        gdf = gpd.read_file(vector_path)\n        vector_name = os.path.splitext(os.path.basename(vector_path))[0]\n    else:\n        gdf = vector_path\n        vector_name = \"vector\"\n\n    # Check input parameters\n    if reference_rasters is None and bounds_list is None:\n        raise ValueError(\"Either reference_rasters or bounds_list must be provided.\")\n\n    # Use reference_rasters if provided, otherwise use bounds_list\n    if reference_rasters is not None:\n        sources = reference_rasters\n        is_raster_reference = True\n    else:\n        sources = bounds_list\n        is_raster_reference = False\n\n    # Create output filenames\n    output_files = []\n\n    # Process each source (reference raster or bounds)\n    for i, source in enumerate(tqdm(sources, desc=\"Processing\")):\n        # Generate output filename\n        output_filename = output_filename_pattern.format(\n            vector_name=vector_name, index=i\n        )\n        if not output_filename.endswith(\".tif\"):\n            output_filename += \".tif\"\n        output_path = os.path.join(output_dir, output_filename)\n\n        if is_raster_reference:\n            # Use reference raster\n            vector_to_raster(\n                vector_path=gdf,\n                output_path=output_path,\n                reference_raster=source,\n                attribute_field=attribute_field,\n                all_touched=all_touched,\n                fill_value=fill_value,\n                dtype=dtype,\n                nodata=nodata,\n            )\n        else:\n            # Use bounds\n            vector_to_raster(\n                vector_path=gdf,\n                output_path=output_path,\n                bounds=source,\n                pixel_size=pixel_size,\n                attribute_field=attribute_field,\n                all_touched=all_touched,\n                fill_value=fill_value,\n                dtype=dtype,\n                nodata=nodata,\n            )\n\n        output_files.append(output_path)\n\n    return output_files\n</code></pre>"},{"location":"utils/#geoai.utils.calc_stats","title":"<code>calc_stats(dataset, divide_by=1.0)</code>","text":"<p>Calculate the statistics (mean and std) for the entire dataset.</p> <p>This function is adapted from the plot_batch() function in the torchgeo library at https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html. Credit to the torchgeo developers for the original implementation.</p> <p>Warning: This is an approximation. The correct value should take into account the mean for the whole dataset for computing individual stds.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>RasterDataset</code> <p>The dataset to calculate statistics for.</p> required <code>divide_by</code> <code>float</code> <p>The value to divide the image data by. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tuple[np.ndarray, np.ndarray]</code> <p>The mean and standard deviation for each band.</p> Source code in <code>geoai/utils.py</code> <pre><code>def calc_stats(\n    dataset: RasterDataset, divide_by: float = 1.0\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate the statistics (mean and std) for the entire dataset.\n\n    This function is adapted from the plot_batch() function in the torchgeo library at\n    https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html.\n    Credit to the torchgeo developers for the original implementation.\n\n    Warning: This is an approximation. The correct value should take into account the\n    mean for the whole dataset for computing individual stds.\n\n    Args:\n        dataset (RasterDataset): The dataset to calculate statistics for.\n        divide_by (float, optional): The value to divide the image data by. Defaults to 1.0.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: The mean and standard deviation for each band.\n    \"\"\"\n\n    # To avoid loading the entire dataset in memory, we will loop through each img\n    # The filenames will be retrieved from the dataset's rtree index\n    files = [\n        item.object\n        for item in dataset.index.intersection(dataset.index.bounds, objects=True)\n    ]\n\n    # Resetting statistics\n    accum_mean = 0\n    accum_std = 0\n\n    for file in files:\n        img = rasterio.open(file).read() / divide_by  # type: ignore\n        accum_mean += img.reshape((img.shape[0], -1)).mean(axis=1)\n        accum_std += img.reshape((img.shape[0], -1)).std(axis=1)\n\n    # at the end, we shall have 2 vectors with length n=chnls\n    # we will average them considering the number of images\n    return accum_mean / len(files), accum_std / len(files)\n</code></pre>"},{"location":"utils/#geoai.utils.clip_raster_by_bbox","title":"<code>clip_raster_by_bbox(input_raster, output_raster, bbox, bands=None, bbox_type='geo', bbox_crs=None)</code>","text":"<p>Clip a raster dataset using a bounding box and optionally select specific bands.</p> <p>Parameters:</p> Name Type Description Default <code>input_raster</code> <code>str</code> <p>Path to the input raster file.</p> required <code>output_raster</code> <code>str</code> <p>Path where the clipped raster will be saved.</p> required <code>bbox</code> <code>tuple</code> <p>Bounding box coordinates either as:          - Geographic coordinates (minx, miny, maxx, maxy) if bbox_type=\"geo\"          - Pixel indices (min_row, min_col, max_row, max_col) if bbox_type=\"pixel\"</p> required <code>bands</code> <code>list</code> <p>List of band indices to keep (1-based indexing).                    If None, all bands will be kept.</p> <code>None</code> <code>bbox_type</code> <code>str</code> <p>Type of bounding box coordinates. Either \"geo\" for                       geographic coordinates or \"pixel\" for row/column indices.                       Default is \"geo\".</p> <code>'geo'</code> <code>bbox_crs</code> <code>str or dict</code> <p>CRS of the bbox if different from the raster CRS.                              Can be provided as EPSG code (e.g., \"EPSG:4326\") or                              as a proj4 string. Only applies when bbox_type=\"geo\".                              If None, assumes bbox is in the same CRS as the raster.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the clipped output raster.</p> <p>Exceptions:</p> Type Description <code>ImportError</code> <p>If required dependencies are not installed.</p> <code>ValueError</code> <p>If the bbox is invalid, bands are out of range, or bbox_type is invalid.</p> <code>RuntimeError</code> <p>If the clipping operation fails.</p> <p>Examples:</p> <p>Clip using geographic coordinates in the same CRS as the raster</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_geo.tif', (100, 200, 300, 400))\n'clipped_geo.tif'\n</code></pre> <p>Clip using WGS84 coordinates when the raster is in a different CRS</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_wgs84.tif', (-122.5, 37.7, -122.4, 37.8),\n...                     bbox_crs=\"EPSG:4326\")\n'clipped_wgs84.tif'\n</code></pre> <p>Clip using row/column indices</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_pixel.tif', (50, 100, 150, 200),\n...                     bbox_type=\"pixel\")\n'clipped_pixel.tif'\n</code></pre> <p>Clip with band selection</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_bands.tif', (100, 200, 300, 400),\n...                     bands=[1, 3])\n'clipped_bands.tif'\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def clip_raster_by_bbox(\n    input_raster, output_raster, bbox, bands=None, bbox_type=\"geo\", bbox_crs=None\n):\n    \"\"\"\n    Clip a raster dataset using a bounding box and optionally select specific bands.\n\n    Args:\n        input_raster (str): Path to the input raster file.\n        output_raster (str): Path where the clipped raster will be saved.\n        bbox (tuple): Bounding box coordinates either as:\n                     - Geographic coordinates (minx, miny, maxx, maxy) if bbox_type=\"geo\"\n                     - Pixel indices (min_row, min_col, max_row, max_col) if bbox_type=\"pixel\"\n        bands (list, optional): List of band indices to keep (1-based indexing).\n                               If None, all bands will be kept.\n        bbox_type (str, optional): Type of bounding box coordinates. Either \"geo\" for\n                                  geographic coordinates or \"pixel\" for row/column indices.\n                                  Default is \"geo\".\n        bbox_crs (str or dict, optional): CRS of the bbox if different from the raster CRS.\n                                         Can be provided as EPSG code (e.g., \"EPSG:4326\") or\n                                         as a proj4 string. Only applies when bbox_type=\"geo\".\n                                         If None, assumes bbox is in the same CRS as the raster.\n\n    Returns:\n        str: Path to the clipped output raster.\n\n    Raises:\n        ImportError: If required dependencies are not installed.\n        ValueError: If the bbox is invalid, bands are out of range, or bbox_type is invalid.\n        RuntimeError: If the clipping operation fails.\n\n    Examples:\n        Clip using geographic coordinates in the same CRS as the raster\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_geo.tif', (100, 200, 300, 400))\n        'clipped_geo.tif'\n\n        Clip using WGS84 coordinates when the raster is in a different CRS\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_wgs84.tif', (-122.5, 37.7, -122.4, 37.8),\n        ...                     bbox_crs=\"EPSG:4326\")\n        'clipped_wgs84.tif'\n\n        Clip using row/column indices\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_pixel.tif', (50, 100, 150, 200),\n        ...                     bbox_type=\"pixel\")\n        'clipped_pixel.tif'\n\n        Clip with band selection\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_bands.tif', (100, 200, 300, 400),\n        ...                     bands=[1, 3])\n        'clipped_bands.tif'\n    \"\"\"\n    from rasterio.transform import from_bounds\n    from rasterio.warp import transform_bounds\n\n    # Validate bbox_type\n    if bbox_type not in [\"geo\", \"pixel\"]:\n        raise ValueError(\"bbox_type must be either 'geo' or 'pixel'\")\n\n    # Validate bbox\n    if len(bbox) != 4:\n        raise ValueError(\"bbox must contain exactly 4 values\")\n\n    # Open the source raster\n    with rasterio.open(input_raster) as src:\n        # Get the source CRS\n        src_crs = src.crs\n\n        # Handle different bbox types\n        if bbox_type == \"geo\":\n            minx, miny, maxx, maxy = bbox\n\n            # Validate geographic bbox\n            if minx &gt;= maxx or miny &gt;= maxy:\n                raise ValueError(\n                    \"Invalid geographic bbox. Expected (minx, miny, maxx, maxy) where minx &lt; maxx and miny &lt; maxy\"\n                )\n\n            # If bbox_crs is provided and different from the source CRS, transform the bbox\n            if bbox_crs is not None and bbox_crs != src_crs:\n                try:\n                    # Transform bbox coordinates from bbox_crs to src_crs\n                    minx, miny, maxx, maxy = transform_bounds(\n                        bbox_crs, src_crs, minx, miny, maxx, maxy\n                    )\n                except Exception as e:\n                    raise ValueError(\n                        f\"Failed to transform bbox from {bbox_crs} to {src_crs}: {str(e)}\"\n                    )\n\n            # Calculate the pixel window from geographic coordinates\n            window = src.window(minx, miny, maxx, maxy)\n\n            # Use the same bounds for the output transform\n            output_bounds = (minx, miny, maxx, maxy)\n\n        else:  # bbox_type == \"pixel\"\n            min_row, min_col, max_row, max_col = bbox\n\n            # Validate pixel bbox\n            if min_row &gt;= max_row or min_col &gt;= max_col:\n                raise ValueError(\n                    \"Invalid pixel bbox. Expected (min_row, min_col, max_row, max_col) where min_row &lt; max_row and min_col &lt; max_col\"\n                )\n\n            if (\n                min_row &lt; 0\n                or min_col &lt; 0\n                or max_row &gt; src.height\n                or max_col &gt; src.width\n            ):\n                raise ValueError(\n                    f\"Pixel indices out of bounds. Raster dimensions are {src.height} rows x {src.width} columns\"\n                )\n\n            # Create a window from pixel coordinates\n            window = Window(min_col, min_row, max_col - min_col, max_row - min_row)\n\n            # Calculate the geographic bounds for this window\n            window_transform = src.window_transform(window)\n            output_bounds = rasterio.transform.array_bounds(\n                window.height, window.width, window_transform\n            )\n            # Reorder to (minx, miny, maxx, maxy)\n            output_bounds = (\n                output_bounds[0],\n                output_bounds[1],\n                output_bounds[2],\n                output_bounds[3],\n            )\n\n        # Get window dimensions\n        window_width = int(window.width)\n        window_height = int(window.height)\n\n        # Check if the window is valid\n        if window_width &lt;= 0 or window_height &lt;= 0:\n            raise ValueError(\"Bounding box results in an empty window\")\n\n        # Handle band selection\n        if bands is None:\n            # Use all bands\n            bands_to_read = list(range(1, src.count + 1))\n        else:\n            # Validate band indices\n            if not all(1 &lt;= b &lt;= src.count for b in bands):\n                raise ValueError(f\"Band indices must be between 1 and {src.count}\")\n            bands_to_read = bands\n\n        # Calculate new transform for the clipped raster\n        new_transform = from_bounds(\n            output_bounds[0],\n            output_bounds[1],\n            output_bounds[2],\n            output_bounds[3],\n            window_width,\n            window_height,\n        )\n\n        # Create a metadata dictionary for the output\n        out_meta = src.meta.copy()\n        out_meta.update(\n            {\n                \"height\": window_height,\n                \"width\": window_width,\n                \"transform\": new_transform,\n                \"count\": len(bands_to_read),\n            }\n        )\n\n        # Read the data for the selected bands\n        data = []\n        for band_idx in bands_to_read:\n            band_data = src.read(band_idx, window=window)\n            data.append(band_data)\n\n        # Stack the bands into a single array\n        if len(data) &gt; 1:\n            clipped_data = np.stack(data)\n        else:\n            clipped_data = data[0][np.newaxis, :, :]\n\n        # Write the output raster\n        with rasterio.open(output_raster, \"w\", **out_meta) as dst:\n            dst.write(clipped_data)\n\n    return output_raster\n</code></pre>"},{"location":"utils/#geoai.utils.create_overview_image","title":"<code>create_overview_image(src, tile_coordinates, output_path, tile_size, stride, geojson_path=None)</code>","text":"<p>Create an overview image showing all tiles and their status, with optional GeoJSON export.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>rasterio.io.DatasetReader</code> <p>The source raster dataset.</p> required <code>tile_coordinates</code> <code>list</code> <p>A list of dictionaries containing tile information.</p> required <code>output_path</code> <code>str</code> <p>The path where the overview image will be saved.</p> required <code>tile_size</code> <code>int</code> <p>The size of each tile in pixels.</p> required <code>stride</code> <code>int</code> <p>The stride between tiles in pixels. Controls overlap between adjacent tiles.</p> required <code>geojson_path</code> <code>str</code> <p>If provided, exports the tile rectangles as GeoJSON to this path.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the saved overview image.</p> Source code in <code>geoai/utils.py</code> <pre><code>def create_overview_image(\n    src, tile_coordinates, output_path, tile_size, stride, geojson_path=None\n):\n    \"\"\"Create an overview image showing all tiles and their status, with optional GeoJSON export.\n\n    Args:\n        src (rasterio.io.DatasetReader): The source raster dataset.\n        tile_coordinates (list): A list of dictionaries containing tile information.\n        output_path (str): The path where the overview image will be saved.\n        tile_size (int): The size of each tile in pixels.\n        stride (int): The stride between tiles in pixels. Controls overlap between adjacent tiles.\n        geojson_path (str, optional): If provided, exports the tile rectangles as GeoJSON to this path.\n\n    Returns:\n        str: Path to the saved overview image.\n    \"\"\"\n    # Read a reduced version of the source image\n    overview_scale = max(\n        1, int(max(src.width, src.height) / 2000)\n    )  # Scale to max ~2000px\n    overview_width = src.width // overview_scale\n    overview_height = src.height // overview_scale\n\n    # Read downsampled image\n    overview_data = src.read(\n        out_shape=(src.count, overview_height, overview_width),\n        resampling=rasterio.enums.Resampling.average,\n    )\n\n    # Create RGB image for display\n    if overview_data.shape[0] &gt;= 3:\n        rgb = np.moveaxis(overview_data[:3], 0, -1)\n    else:\n        # For single band, create grayscale RGB\n        rgb = np.stack([overview_data[0], overview_data[0], overview_data[0]], axis=-1)\n\n    # Normalize for display\n    for i in range(rgb.shape[-1]):\n        band = rgb[..., i]\n        non_zero = band[band &gt; 0]\n        if len(non_zero) &gt; 0:\n            p2, p98 = np.percentile(non_zero, (2, 98))\n            rgb[..., i] = np.clip((band - p2) / (p98 - p2), 0, 1)\n\n    # Create figure\n    plt.figure(figsize=(12, 12))\n    plt.imshow(rgb)\n\n    # If GeoJSON export is requested, prepare GeoJSON structures\n    if geojson_path:\n        features = []\n\n    # Draw tile boundaries\n    for tile in tile_coordinates:\n        # Convert bounds to pixel coordinates in overview\n        bounds = tile[\"bounds\"]\n        # Calculate scaled pixel coordinates\n        x_min = int((tile[\"x\"]) / overview_scale)\n        y_min = int((tile[\"y\"]) / overview_scale)\n        width = int(tile_size / overview_scale)\n        height = int(tile_size / overview_scale)\n\n        # Draw rectangle\n        color = \"lime\" if tile[\"has_features\"] else \"red\"\n        rect = plt.Rectangle(\n            (x_min, y_min), width, height, fill=False, edgecolor=color, linewidth=0.5\n        )\n        plt.gca().add_patch(rect)\n\n        # Add tile number if not too crowded\n        if width &gt; 20 and height &gt; 20:\n            plt.text(\n                x_min + width / 2,\n                y_min + height / 2,\n                str(tile[\"index\"]),\n                color=\"white\",\n                ha=\"center\",\n                va=\"center\",\n                fontsize=8,\n            )\n\n        # Add to GeoJSON features if exporting\n        if geojson_path:\n            # Create a polygon from the bounds (already in geo-coordinates)\n            minx, miny, maxx, maxy = bounds\n            polygon = box(minx, miny, maxx, maxy)\n\n            # Calculate overlap with neighboring tiles\n            overlap = 0\n            if stride &lt; tile_size:\n                overlap = tile_size - stride\n\n            # Create a GeoJSON feature\n            feature = {\n                \"type\": \"Feature\",\n                \"geometry\": mapping(polygon),\n                \"properties\": {\n                    \"index\": tile[\"index\"],\n                    \"has_features\": tile[\"has_features\"],\n                    \"bounds_pixel\": [\n                        tile[\"x\"],\n                        tile[\"y\"],\n                        tile[\"x\"] + tile_size,\n                        tile[\"y\"] + tile_size,\n                    ],\n                    \"tile_size_px\": tile_size,\n                    \"stride_px\": stride,\n                    \"overlap_px\": overlap,\n                },\n            }\n\n            # Add any additional properties from the tile\n            for key, value in tile.items():\n                if key not in [\"x\", \"y\", \"index\", \"has_features\", \"bounds\"]:\n                    feature[\"properties\"][key] = value\n\n            features.append(feature)\n\n    plt.title(\"Tile Overview (Green = Contains Features, Red = Empty)\")\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\n    print(f\"Overview image saved to {output_path}\")\n\n    # Export GeoJSON if requested\n    if geojson_path:\n        geojson_collection = {\n            \"type\": \"FeatureCollection\",\n            \"features\": features,\n            \"properties\": {\n                \"crs\": (\n                    src.crs.to_string()\n                    if hasattr(src.crs, \"to_string\")\n                    else str(src.crs)\n                ),\n                \"total_tiles\": len(features),\n                \"source_raster_dimensions\": [src.width, src.height],\n            },\n        }\n\n        # Save to file\n        with open(geojson_path, \"w\") as f:\n            json.dump(geojson_collection, f)\n\n        print(f\"GeoJSON saved to {geojson_path}\")\n\n    return output_path\n</code></pre>"},{"location":"utils/#geoai.utils.create_split_map","title":"<code>create_split_map(left_layer='TERRAIN', right_layer='OpenTopoMap', left_args=None, right_args=None, left_array_args=None, right_array_args=None, zoom_control=True, fullscreen_control=True, layer_control=True, add_close_button=False, left_label=None, right_label=None, left_position='bottomleft', right_position='bottomright', widget_layout=None, draggable=True, center=[20, 0], zoom=2, height='600px', basemap=None, basemap_args=None, m=None, **kwargs)</code>","text":"<p>Adds split map.</p> <p>Parameters:</p> Name Type Description Default <code>left_layer</code> <code>str</code> <p>The left tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'TERRAIN'.</p> <code>'TERRAIN'</code> <code>right_layer</code> <code>str</code> <p>The right tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'OpenTopoMap'.</p> <code>'OpenTopoMap'</code> <code>left_args</code> <code>dict</code> <p>The arguments for the left tile layer. Defaults to {}.</p> <code>None</code> <code>right_args</code> <code>dict</code> <p>The arguments for the right tile layer. Defaults to {}.</p> <code>None</code> <code>left_array_args</code> <code>dict</code> <p>The arguments for array_to_image for the left layer. Defaults to {}.</p> <code>None</code> <code>right_array_args</code> <code>dict</code> <p>The arguments for array_to_image for the right layer. Defaults to {}.</p> <code>None</code> <code>zoom_control</code> <code>bool</code> <p>Whether to add zoom control. Defaults to True.</p> <code>True</code> <code>fullscreen_control</code> <code>bool</code> <p>Whether to add fullscreen control. Defaults to True.</p> <code>True</code> <code>layer_control</code> <code>bool</code> <p>Whether to add layer control. Defaults to True.</p> <code>True</code> <code>add_close_button</code> <code>bool</code> <p>Whether to add a close button. Defaults to False.</p> <code>False</code> <code>left_label</code> <code>str</code> <p>The label for the left layer. Defaults to None.</p> <code>None</code> <code>right_label</code> <code>str</code> <p>The label for the right layer. Defaults to None.</p> <code>None</code> <code>left_position</code> <code>str</code> <p>The position for the left label. Defaults to \"bottomleft\".</p> <code>'bottomleft'</code> <code>right_position</code> <code>str</code> <p>The position for the right label. Defaults to \"bottomright\".</p> <code>'bottomright'</code> <code>widget_layout</code> <code>dict</code> <p>The layout for the widget. Defaults to None.</p> <code>None</code> <code>draggable</code> <code>bool</code> <p>Whether the split map is draggable. Defaults to True.</p> <code>True</code> Source code in <code>geoai/utils.py</code> <pre><code>def create_split_map(\n    left_layer: Optional[str] = \"TERRAIN\",\n    right_layer: Optional[str] = \"OpenTopoMap\",\n    left_args: Optional[dict] = None,\n    right_args: Optional[dict] = None,\n    left_array_args: Optional[dict] = None,\n    right_array_args: Optional[dict] = None,\n    zoom_control: Optional[bool] = True,\n    fullscreen_control: Optional[bool] = True,\n    layer_control: Optional[bool] = True,\n    add_close_button: Optional[bool] = False,\n    left_label: Optional[str] = None,\n    right_label: Optional[str] = None,\n    left_position: Optional[str] = \"bottomleft\",\n    right_position: Optional[str] = \"bottomright\",\n    widget_layout: Optional[dict] = None,\n    draggable: Optional[bool] = True,\n    center: Optional[List[float]] = [20, 0],\n    zoom: Optional[int] = 2,\n    height: Optional[int] = \"600px\",\n    basemap: Optional[str] = None,\n    basemap_args: Optional[dict] = None,\n    m=None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Adds split map.\n\n    Args:\n        left_layer (str, optional): The left tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'TERRAIN'.\n        right_layer (str, optional): The right tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'OpenTopoMap'.\n        left_args (dict, optional): The arguments for the left tile layer. Defaults to {}.\n        right_args (dict, optional): The arguments for the right tile layer. Defaults to {}.\n        left_array_args (dict, optional): The arguments for array_to_image for the left layer. Defaults to {}.\n        right_array_args (dict, optional): The arguments for array_to_image for the right layer. Defaults to {}.\n        zoom_control (bool, optional): Whether to add zoom control. Defaults to True.\n        fullscreen_control (bool, optional): Whether to add fullscreen control. Defaults to True.\n        layer_control (bool, optional): Whether to add layer control. Defaults to True.\n        add_close_button (bool, optional): Whether to add a close button. Defaults to False.\n        left_label (str, optional): The label for the left layer. Defaults to None.\n        right_label (str, optional): The label for the right layer. Defaults to None.\n        left_position (str, optional): The position for the left label. Defaults to \"bottomleft\".\n        right_position (str, optional): The position for the right label. Defaults to \"bottomright\".\n        widget_layout (dict, optional): The layout for the widget. Defaults to None.\n        draggable (bool, optional): Whether the split map is draggable. Defaults to True.\n    \"\"\"\n\n    if left_args is None:\n        left_args = {}\n\n    if right_args is None:\n        right_args = {}\n\n    if left_array_args is None:\n        left_array_args = {}\n\n    if right_array_args is None:\n        right_array_args = {}\n\n    if basemap_args is None:\n        basemap_args = {}\n\n    if m is None:\n        m = leafmap.Map(center=center, zoom=zoom, height=height, **kwargs)\n        m.clear_layers()\n    if isinstance(basemap, str):\n        if basemap.endswith(\".tif\"):\n            if basemap.startswith(\"http\"):\n                m.add_cog_layer(basemap, name=\"Basemap\", **basemap_args)\n            else:\n                m.add_raster(basemap, name=\"Basemap\", **basemap_args)\n        else:\n            m.add_basemap(basemap)\n    m.split_map(\n        left_layer=left_layer,\n        right_layer=right_layer,\n        left_args=left_args,\n        right_args=right_args,\n        left_array_args=left_array_args,\n        right_array_args=right_array_args,\n        zoom_control=zoom_control,\n        fullscreen_control=fullscreen_control,\n        layer_control=layer_control,\n        add_close_button=add_close_button,\n        left_label=left_label,\n        right_label=right_label,\n        left_position=left_position,\n        right_position=right_position,\n        widget_layout=widget_layout,\n        draggable=draggable,\n    )\n\n    return m\n</code></pre>"},{"location":"utils/#geoai.utils.dict_to_image","title":"<code>dict_to_image(data_dict, output=None, **kwargs)</code>","text":"<p>Convert a dictionary containing spatial data to a rasterio dataset or save it to a file. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo dataset sampler.</p> <p>This function transforms a dictionary with CRS, bounding box, and image data into a rasterio DatasetReader using leafmap's array_to_image utility after first converting to a rioxarray DataArray.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>Dict[str, Any]</code> <p>A dictionary containing: - 'crs': A pyproj CRS object - 'bounds': A BoundingBox object with minx, maxx, miny, maxy attributes   and optionally mint, maxt for temporal bounds - 'image': A tensor or array-like object with image data</p> required <code>output</code> <code>Optional[str]</code> <p>Optional path to save the image to a file. If not provided, the image will be returned as a rasterio DatasetReader object.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to leafmap.array_to_image. Common options include: - colormap: str, name of the colormap (e.g., 'viridis', 'terrain') - vmin: float, minimum value for colormap scaling - vmax: float, maximum value for colormap scaling</p> <code>{}</code> <p>Returns:</p> Type Description <code>DatasetReader</code> <p>A rasterio DatasetReader object that can be used for visualization or further processing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image = dict_to_image(\n...     {'crs': CRS.from_epsg(26911), 'bounds': bbox, 'image': tensor},\n...     colormap='terrain'\n... )\n&gt;&gt;&gt; fig, ax = plt.subplots(figsize=(10, 10))\n&gt;&gt;&gt; show(image, ax=ax)\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def dict_to_image(\n    data_dict: Dict[str, Any], output: Optional[str] = None, **kwargs\n) -&gt; rasterio.DatasetReader:\n    \"\"\"Convert a dictionary containing spatial data to a rasterio dataset or save it to\n    a file. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\".\n    It can be generated from a TorchGeo dataset sampler.\n\n    This function transforms a dictionary with CRS, bounding box, and image data\n    into a rasterio DatasetReader using leafmap's array_to_image utility after\n    first converting to a rioxarray DataArray.\n\n    Args:\n        data_dict: A dictionary containing:\n            - 'crs': A pyproj CRS object\n            - 'bounds': A BoundingBox object with minx, maxx, miny, maxy attributes\n              and optionally mint, maxt for temporal bounds\n            - 'image': A tensor or array-like object with image data\n        output: Optional path to save the image to a file. If not provided, the image\n            will be returned as a rasterio DatasetReader object.\n        **kwargs: Additional keyword arguments to pass to leafmap.array_to_image.\n            Common options include:\n            - colormap: str, name of the colormap (e.g., 'viridis', 'terrain')\n            - vmin: float, minimum value for colormap scaling\n            - vmax: float, maximum value for colormap scaling\n\n    Returns:\n        A rasterio DatasetReader object that can be used for visualization or\n        further processing.\n\n    Examples:\n        &gt;&gt;&gt; image = dict_to_image(\n        ...     {'crs': CRS.from_epsg(26911), 'bounds': bbox, 'image': tensor},\n        ...     colormap='terrain'\n        ... )\n        &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(10, 10))\n        &gt;&gt;&gt; show(image, ax=ax)\n    \"\"\"\n    da = dict_to_rioxarray(data_dict)\n\n    if output is not None:\n        out_dir = os.path.abspath(os.path.dirname(output))\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir, exist_ok=True)\n        da.rio.to_raster(output)\n        return output\n    else:\n        image = leafmap.array_to_image(da, **kwargs)\n        return image\n</code></pre>"},{"location":"utils/#geoai.utils.dict_to_rioxarray","title":"<code>dict_to_rioxarray(data_dict)</code>","text":"<p>Convert a dictionary to a xarray DataArray. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo dataset sampler.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>Dict</code> <p>The dictionary containing the data.</p> required <p>Returns:</p> Type Description <code>xr.DataArray</code> <p>The xarray DataArray.</p> Source code in <code>geoai/utils.py</code> <pre><code>def dict_to_rioxarray(data_dict: Dict) -&gt; xr.DataArray:\n    \"\"\"Convert a dictionary to a xarray DataArray. The dictionary should contain the\n    following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo\n    dataset sampler.\n\n    Args:\n        data_dict (Dict): The dictionary containing the data.\n\n    Returns:\n        xr.DataArray: The xarray DataArray.\n    \"\"\"\n\n    from affine import Affine\n\n    # Extract components from the dictionary\n    crs = data_dict[\"crs\"]\n    bounds = data_dict[\"bounds\"]\n    image_tensor = data_dict[\"image\"]\n\n    # Convert tensor to numpy array if needed\n    if hasattr(image_tensor, \"numpy\"):\n        # For PyTorch tensors\n        image_array = image_tensor.numpy()\n    else:\n        # If it's already a numpy array or similar\n        image_array = np.array(image_tensor)\n\n    # Calculate pixel resolution\n    width = image_array.shape[2]  # Width is the size of the last dimension\n    height = image_array.shape[1]  # Height is the size of the middle dimension\n\n    res_x = (bounds.maxx - bounds.minx) / width\n    res_y = (bounds.maxy - bounds.miny) / height\n\n    # Create the transform matrix\n    transform = Affine(res_x, 0.0, bounds.minx, 0.0, -res_y, bounds.maxy)\n\n    # Create dimensions\n    x_coords = np.linspace(bounds.minx + res_x / 2, bounds.maxx - res_x / 2, width)\n    y_coords = np.linspace(bounds.maxy - res_y / 2, bounds.miny + res_y / 2, height)\n\n    # If time dimension exists in the bounds\n    if hasattr(bounds, \"mint\") and hasattr(bounds, \"maxt\"):\n        # Create a single time value or range if needed\n        t_coords = [\n            bounds.mint\n        ]  # Or np.linspace(bounds.mint, bounds.maxt, num_time_steps)\n\n        # Create DataArray with time dimension\n        dims = (\n            (\"band\", \"y\", \"x\")\n            if image_array.shape[0] &lt;= 10\n            else (\"time\", \"band\", \"y\", \"x\")\n        )\n\n        if dims[0] == \"band\":\n            # For multi-band single time\n            da = xr.DataArray(\n                image_array,\n                dims=dims,\n                coords={\n                    \"band\": np.arange(1, image_array.shape[0] + 1),\n                    \"y\": y_coords,\n                    \"x\": x_coords,\n                },\n            )\n        else:\n            # For multi-time multi-band\n            da = xr.DataArray(\n                image_array,\n                dims=dims,\n                coords={\n                    \"time\": t_coords,\n                    \"band\": np.arange(1, image_array.shape[1] + 1),\n                    \"y\": y_coords,\n                    \"x\": x_coords,\n                },\n            )\n    else:\n        # Create DataArray without time dimension\n        da = xr.DataArray(\n            image_array,\n            dims=(\"band\", \"y\", \"x\"),\n            coords={\n                \"band\": np.arange(1, image_array.shape[0] + 1),\n                \"y\": y_coords,\n                \"x\": x_coords,\n            },\n        )\n\n    # Set spatial attributes\n    da.rio.write_crs(crs, inplace=True)\n    da.rio.write_transform(transform, inplace=True)\n\n    return da\n</code></pre>"},{"location":"utils/#geoai.utils.download_file","title":"<code>download_file(url, output_path=None, overwrite=False)</code>","text":"<p>Download a file from a given URL with a progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the file to download.</p> required <code>output_path</code> <code>str</code> <p>The path where the downloaded file will be saved. If not provided, the filename from the URL will be used.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it already exists.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the downloaded file.</p> Source code in <code>geoai/utils.py</code> <pre><code>def download_file(url, output_path=None, overwrite=False):\n    \"\"\"\n    Download a file from a given URL with a progress bar.\n\n    Args:\n        url (str): The URL of the file to download.\n        output_path (str, optional): The path where the downloaded file will be saved.\n            If not provided, the filename from the URL will be used.\n        overwrite (bool, optional): Whether to overwrite the file if it already exists.\n\n    Returns:\n        str: The path to the downloaded file.\n    \"\"\"\n    # Get the filename from the URL if output_path is not provided\n    if output_path is None:\n        output_path = os.path.basename(url)\n\n    # Check if the file already exists\n    if os.path.exists(output_path) and not overwrite:\n        print(f\"File already exists: {output_path}\")\n        return output_path\n\n    # Send a streaming GET request\n    response = requests.get(url, stream=True, timeout=50)\n    response.raise_for_status()  # Raise an exception for HTTP errors\n\n    # Get the total file size if available\n    total_size = int(response.headers.get(\"content-length\", 0))\n\n    # Open the output file\n    with (\n        open(output_path, \"wb\") as file,\n        tqdm(\n            desc=os.path.basename(output_path),\n            total=total_size,\n            unit=\"B\",\n            unit_scale=True,\n            unit_divisor=1024,\n        ) as progress_bar,\n    ):\n\n        # Download the file in chunks and update the progress bar\n        for chunk in response.iter_content(chunk_size=1024):\n            if chunk:  # filter out keep-alive new chunks\n                file.write(chunk)\n                progress_bar.update(len(chunk))\n\n    return output_path\n</code></pre>"},{"location":"utils/#geoai.utils.export_geotiff_tiles","title":"<code>export_geotiff_tiles(in_raster, out_folder, in_class_data, tile_size=256, stride=128, class_value_field='class', buffer_radius=0, max_tiles=None, quiet=False, all_touched=True, create_overview=False, skip_empty_tiles=False)</code>","text":"<p>Export georeferenced GeoTIFF tiles and labels from raster and classification data.</p> <p>Parameters:</p> Name Type Description Default <code>in_raster</code> <code>str</code> <p>Path to input raster image</p> required <code>out_folder</code> <code>str</code> <p>Path to output folder</p> required <code>in_class_data</code> <code>str</code> <p>Path to classification data - can be vector file or raster</p> required <code>tile_size</code> <code>int</code> <p>Size of tiles in pixels (square)</p> <code>256</code> <code>stride</code> <code>int</code> <p>Step size between tiles</p> <code>128</code> <code>class_value_field</code> <code>str</code> <p>Field containing class values (for vector data)</p> <code>'class'</code> <code>buffer_radius</code> <code>float</code> <p>Buffer to add around features (in units of the CRS)</p> <code>0</code> <code>max_tiles</code> <code>int</code> <p>Maximum number of tiles to process (None for all)</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>If True, suppress non-essential output</p> <code>False</code> <code>all_touched</code> <code>bool</code> <p>Whether to use all_touched=True in rasterization (for vector data)</p> <code>True</code> <code>create_overview</code> <code>bool</code> <p>Whether to create an overview image of all tiles</p> <code>False</code> <code>skip_empty_tiles</code> <code>bool</code> <p>If True, skip tiles with no features</p> <code>False</code> Source code in <code>geoai/utils.py</code> <pre><code>def export_geotiff_tiles(\n    in_raster,\n    out_folder,\n    in_class_data,\n    tile_size=256,\n    stride=128,\n    class_value_field=\"class\",\n    buffer_radius=0,\n    max_tiles=None,\n    quiet=False,\n    all_touched=True,\n    create_overview=False,\n    skip_empty_tiles=False,\n):\n    \"\"\"\n    Export georeferenced GeoTIFF tiles and labels from raster and classification data.\n\n    Args:\n        in_raster (str): Path to input raster image\n        out_folder (str): Path to output folder\n        in_class_data (str): Path to classification data - can be vector file or raster\n        tile_size (int): Size of tiles in pixels (square)\n        stride (int): Step size between tiles\n        class_value_field (str): Field containing class values (for vector data)\n        buffer_radius (float): Buffer to add around features (in units of the CRS)\n        max_tiles (int): Maximum number of tiles to process (None for all)\n        quiet (bool): If True, suppress non-essential output\n        all_touched (bool): Whether to use all_touched=True in rasterization (for vector data)\n        create_overview (bool): Whether to create an overview image of all tiles\n        skip_empty_tiles (bool): If True, skip tiles with no features\n    \"\"\"\n    # Create output directories\n    os.makedirs(out_folder, exist_ok=True)\n    image_dir = os.path.join(out_folder, \"images\")\n    os.makedirs(image_dir, exist_ok=True)\n    label_dir = os.path.join(out_folder, \"labels\")\n    os.makedirs(label_dir, exist_ok=True)\n    ann_dir = os.path.join(out_folder, \"annotations\")\n    os.makedirs(ann_dir, exist_ok=True)\n\n    # Determine if class data is raster or vector\n    is_class_data_raster = False\n    if isinstance(in_class_data, str):\n        file_ext = Path(in_class_data).suffix.lower()\n        # Common raster extensions\n        if file_ext in [\".tif\", \".tiff\", \".img\", \".jp2\", \".png\", \".bmp\", \".gif\"]:\n            try:\n                with rasterio.open(in_class_data) as src:\n                    is_class_data_raster = True\n                    if not quiet:\n                        print(f\"Detected in_class_data as raster: {in_class_data}\")\n                        print(f\"Raster CRS: {src.crs}\")\n                        print(f\"Raster dimensions: {src.width} x {src.height}\")\n            except Exception:\n                is_class_data_raster = False\n                if not quiet:\n                    print(f\"Unable to open {in_class_data} as raster, trying as vector\")\n\n    # Open the input raster\n    with rasterio.open(in_raster) as src:\n        if not quiet:\n            print(f\"\\nRaster info for {in_raster}:\")\n            print(f\"  CRS: {src.crs}\")\n            print(f\"  Dimensions: {src.width} x {src.height}\")\n            print(f\"  Bounds: {src.bounds}\")\n\n        # Calculate number of tiles\n        num_tiles_x = math.ceil((src.width - tile_size) / stride) + 1\n        num_tiles_y = math.ceil((src.height - tile_size) / stride) + 1\n        total_tiles = num_tiles_x * num_tiles_y\n\n        if max_tiles is None:\n            max_tiles = total_tiles\n\n        # Process classification data\n        class_to_id = {}\n\n        if is_class_data_raster:\n            # Load raster class data\n            with rasterio.open(in_class_data) as class_src:\n                # Check if raster CRS matches\n                if class_src.crs != src.crs:\n                    warnings.warn(\n                        f\"CRS mismatch: Class raster ({class_src.crs}) doesn't match input raster ({src.crs}). \"\n                        f\"Results may be misaligned.\"\n                    )\n\n                # Get unique values from raster\n                # Sample to avoid loading huge rasters\n                sample_data = class_src.read(\n                    1,\n                    out_shape=(\n                        1,\n                        min(class_src.height, 1000),\n                        min(class_src.width, 1000),\n                    ),\n                )\n\n                unique_classes = np.unique(sample_data)\n                unique_classes = unique_classes[\n                    unique_classes &gt; 0\n                ]  # Remove 0 as it's typically background\n\n                if not quiet:\n                    print(\n                        f\"Found {len(unique_classes)} unique classes in raster: {unique_classes}\"\n                    )\n\n                # Create class mapping\n                class_to_id = {int(cls): i + 1 for i, cls in enumerate(unique_classes)}\n        else:\n            # Load vector class data\n            try:\n                gdf = gpd.read_file(in_class_data)\n                if not quiet:\n                    print(f\"Loaded {len(gdf)} features from {in_class_data}\")\n                    print(f\"Vector CRS: {gdf.crs}\")\n\n                # Always reproject to match raster CRS\n                if gdf.crs != src.crs:\n                    if not quiet:\n                        print(f\"Reprojecting features from {gdf.crs} to {src.crs}\")\n                    gdf = gdf.to_crs(src.crs)\n\n                # Apply buffer if specified\n                if buffer_radius &gt; 0:\n                    gdf[\"geometry\"] = gdf.buffer(buffer_radius)\n                    if not quiet:\n                        print(f\"Applied buffer of {buffer_radius} units\")\n\n                # Check if class_value_field exists\n                if class_value_field in gdf.columns:\n                    unique_classes = gdf[class_value_field].unique()\n                    if not quiet:\n                        print(\n                            f\"Found {len(unique_classes)} unique classes: {unique_classes}\"\n                        )\n                    # Create class mapping\n                    class_to_id = {cls: i + 1 for i, cls in enumerate(unique_classes)}\n                else:\n                    if not quiet:\n                        print(\n                            f\"WARNING: '{class_value_field}' not found in vector data. Using default class ID 1.\"\n                        )\n                    class_to_id = {1: 1}  # Default mapping\n            except Exception as e:\n                raise ValueError(f\"Error processing vector data: {e}\")\n\n        # Create progress bar\n        pbar = tqdm(\n            total=min(total_tiles, max_tiles),\n            desc=\"Generating tiles\",\n            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}&lt;{remaining}, {rate_fmt}]\",\n        )\n\n        # Track statistics for summary\n        stats = {\n            \"total_tiles\": 0,\n            \"tiles_with_features\": 0,\n            \"feature_pixels\": 0,\n            \"errors\": 0,\n            \"tile_coordinates\": [],  # For overview image\n        }\n\n        # Process tiles\n        tile_index = 0\n        for y in range(num_tiles_y):\n            for x in range(num_tiles_x):\n                if tile_index &gt;= max_tiles:\n                    break\n\n                # Calculate window coordinates\n                window_x = x * stride\n                window_y = y * stride\n\n                # Adjust for edge cases\n                if window_x + tile_size &gt; src.width:\n                    window_x = src.width - tile_size\n                if window_y + tile_size &gt; src.height:\n                    window_y = src.height - tile_size\n\n                # Define window\n                window = Window(window_x, window_y, tile_size, tile_size)\n\n                # Get window transform and bounds\n                window_transform = src.window_transform(window)\n\n                # Calculate window bounds\n                minx = window_transform[2]  # Upper left x\n                maxy = window_transform[5]  # Upper left y\n                maxx = minx + tile_size * window_transform[0]  # Add width\n                miny = maxy + tile_size * window_transform[4]  # Add height\n\n                window_bounds = box(minx, miny, maxx, maxy)\n\n                # Store tile coordinates for overview\n                if create_overview:\n                    stats[\"tile_coordinates\"].append(\n                        {\n                            \"index\": tile_index,\n                            \"x\": window_x,\n                            \"y\": window_y,\n                            \"bounds\": [minx, miny, maxx, maxy],\n                            \"has_features\": False,\n                        }\n                    )\n\n                # Create label mask\n                label_mask = np.zeros((tile_size, tile_size), dtype=np.uint8)\n                has_features = False\n\n                # Process classification data to create labels\n                if is_class_data_raster:\n                    # For raster class data\n                    with rasterio.open(in_class_data) as class_src:\n                        # Calculate window in class raster\n                        src_bounds = src.bounds\n                        class_bounds = class_src.bounds\n\n                        # Check if windows overlap\n                        if (\n                            src_bounds.left &gt; class_bounds.right\n                            or src_bounds.right &lt; class_bounds.left\n                            or src_bounds.bottom &gt; class_bounds.top\n                            or src_bounds.top &lt; class_bounds.bottom\n                        ):\n                            warnings.warn(\n                                \"Class raster and input raster do not overlap.\"\n                            )\n                        else:\n                            # Get corresponding window in class raster\n                            window_class = rasterio.windows.from_bounds(\n                                minx, miny, maxx, maxy, class_src.transform\n                            )\n\n                            # Read label data\n                            try:\n                                label_data = class_src.read(\n                                    1,\n                                    window=window_class,\n                                    boundless=True,\n                                    out_shape=(tile_size, tile_size),\n                                )\n\n                                # Remap class values if needed\n                                if class_to_id:\n                                    remapped_data = np.zeros_like(label_data)\n                                    for orig_val, new_val in class_to_id.items():\n                                        remapped_data[label_data == orig_val] = new_val\n                                    label_mask = remapped_data\n                                else:\n                                    label_mask = label_data\n\n                                # Check if we have any features\n                                if np.any(label_mask &gt; 0):\n                                    has_features = True\n                                    stats[\"feature_pixels\"] += np.count_nonzero(\n                                        label_mask\n                                    )\n                            except Exception as e:\n                                pbar.write(f\"Error reading class raster window: {e}\")\n                                stats[\"errors\"] += 1\n                else:\n                    # For vector class data\n                    # Find features that intersect with window\n                    window_features = gdf[gdf.intersects(window_bounds)]\n\n                    if len(window_features) &gt; 0:\n                        for idx, feature in window_features.iterrows():\n                            # Get class value\n                            if class_value_field in feature:\n                                class_val = feature[class_value_field]\n                                class_id = class_to_id.get(class_val, 1)\n                            else:\n                                class_id = 1\n\n                            # Get geometry in window coordinates\n                            geom = feature.geometry.intersection(window_bounds)\n                            if not geom.is_empty:\n                                try:\n                                    # Rasterize feature\n                                    feature_mask = features.rasterize(\n                                        [(geom, class_id)],\n                                        out_shape=(tile_size, tile_size),\n                                        transform=window_transform,\n                                        fill=0,\n                                        all_touched=all_touched,\n                                    )\n\n                                    # Add to label mask\n                                    label_mask = np.maximum(label_mask, feature_mask)\n\n                                    # Check if the feature was actually rasterized\n                                    if np.any(feature_mask):\n                                        has_features = True\n                                        if create_overview and tile_index &lt; len(\n                                            stats[\"tile_coordinates\"]\n                                        ):\n                                            stats[\"tile_coordinates\"][tile_index][\n                                                \"has_features\"\n                                            ] = True\n                                except Exception as e:\n                                    pbar.write(f\"Error rasterizing feature {idx}: {e}\")\n                                    stats[\"errors\"] += 1\n\n                # Skip tile if no features and skip_empty_tiles is True\n                if skip_empty_tiles and not has_features:\n                    pbar.update(1)\n                    tile_index += 1\n                    continue\n\n                # Read image data\n                image_data = src.read(window=window)\n\n                # Export image as GeoTIFF\n                image_path = os.path.join(image_dir, f\"tile_{tile_index:06d}.tif\")\n\n                # Create profile for image GeoTIFF\n                image_profile = src.profile.copy()\n                image_profile.update(\n                    {\n                        \"height\": tile_size,\n                        \"width\": tile_size,\n                        \"count\": image_data.shape[0],\n                        \"transform\": window_transform,\n                    }\n                )\n\n                # Save image as GeoTIFF\n                try:\n                    with rasterio.open(image_path, \"w\", **image_profile) as dst:\n                        dst.write(image_data)\n                    stats[\"total_tiles\"] += 1\n                except Exception as e:\n                    pbar.write(f\"ERROR saving image GeoTIFF: {e}\")\n                    stats[\"errors\"] += 1\n\n                # Create profile for label GeoTIFF\n                label_profile = {\n                    \"driver\": \"GTiff\",\n                    \"height\": tile_size,\n                    \"width\": tile_size,\n                    \"count\": 1,\n                    \"dtype\": \"uint8\",\n                    \"crs\": src.crs,\n                    \"transform\": window_transform,\n                }\n\n                # Export label as GeoTIFF\n                label_path = os.path.join(label_dir, f\"tile_{tile_index:06d}.tif\")\n                try:\n                    with rasterio.open(label_path, \"w\", **label_profile) as dst:\n                        dst.write(label_mask.astype(np.uint8), 1)\n\n                    if has_features:\n                        stats[\"tiles_with_features\"] += 1\n                        stats[\"feature_pixels\"] += np.count_nonzero(label_mask)\n                except Exception as e:\n                    pbar.write(f\"ERROR saving label GeoTIFF: {e}\")\n                    stats[\"errors\"] += 1\n\n                # Create XML annotation for object detection if using vector class data\n                if (\n                    not is_class_data_raster\n                    and \"gdf\" in locals()\n                    and len(window_features) &gt; 0\n                ):\n                    # Create XML annotation\n                    root = ET.Element(\"annotation\")\n                    ET.SubElement(root, \"folder\").text = \"images\"\n                    ET.SubElement(root, \"filename\").text = f\"tile_{tile_index:06d}.tif\"\n\n                    size = ET.SubElement(root, \"size\")\n                    ET.SubElement(size, \"width\").text = str(tile_size)\n                    ET.SubElement(size, \"height\").text = str(tile_size)\n                    ET.SubElement(size, \"depth\").text = str(image_data.shape[0])\n\n                    # Add georeference information\n                    geo = ET.SubElement(root, \"georeference\")\n                    ET.SubElement(geo, \"crs\").text = str(src.crs)\n                    ET.SubElement(geo, \"transform\").text = str(\n                        window_transform\n                    ).replace(\"\\n\", \"\")\n                    ET.SubElement(geo, \"bounds\").text = (\n                        f\"{minx}, {miny}, {maxx}, {maxy}\"\n                    )\n\n                    # Add objects\n                    for idx, feature in window_features.iterrows():\n                        # Get feature class\n                        if class_value_field in feature:\n                            class_val = feature[class_value_field]\n                        else:\n                            class_val = \"object\"\n\n                        # Get geometry bounds in pixel coordinates\n                        geom = feature.geometry.intersection(window_bounds)\n                        if not geom.is_empty:\n                            # Get bounds in world coordinates\n                            minx_f, miny_f, maxx_f, maxy_f = geom.bounds\n\n                            # Convert to pixel coordinates\n                            col_min, row_min = ~window_transform * (minx_f, maxy_f)\n                            col_max, row_max = ~window_transform * (maxx_f, miny_f)\n\n                            # Ensure coordinates are within tile bounds\n                            xmin = max(0, min(tile_size, int(col_min)))\n                            ymin = max(0, min(tile_size, int(row_min)))\n                            xmax = max(0, min(tile_size, int(col_max)))\n                            ymax = max(0, min(tile_size, int(row_max)))\n\n                            # Only add if the box has non-zero area\n                            if xmax &gt; xmin and ymax &gt; ymin:\n                                obj = ET.SubElement(root, \"object\")\n                                ET.SubElement(obj, \"name\").text = str(class_val)\n                                ET.SubElement(obj, \"difficult\").text = \"0\"\n\n                                bbox = ET.SubElement(obj, \"bndbox\")\n                                ET.SubElement(bbox, \"xmin\").text = str(xmin)\n                                ET.SubElement(bbox, \"ymin\").text = str(ymin)\n                                ET.SubElement(bbox, \"xmax\").text = str(xmax)\n                                ET.SubElement(bbox, \"ymax\").text = str(ymax)\n\n                    # Save XML\n                    tree = ET.ElementTree(root)\n                    xml_path = os.path.join(ann_dir, f\"tile_{tile_index:06d}.xml\")\n                    tree.write(xml_path)\n\n                # Update progress bar\n                pbar.update(1)\n                pbar.set_description(\n                    f\"Generated: {stats['total_tiles']}, With features: {stats['tiles_with_features']}\"\n                )\n\n                tile_index += 1\n                if tile_index &gt;= max_tiles:\n                    break\n\n            if tile_index &gt;= max_tiles:\n                break\n\n        # Close progress bar\n        pbar.close()\n\n        # Create overview image if requested\n        if create_overview and stats[\"tile_coordinates\"]:\n            try:\n                create_overview_image(\n                    src,\n                    stats[\"tile_coordinates\"],\n                    os.path.join(out_folder, \"overview.png\"),\n                    tile_size,\n                    stride,\n                )\n            except Exception as e:\n                print(f\"Failed to create overview image: {e}\")\n\n        # Report results\n        if not quiet:\n            print(\"\\n------- Export Summary -------\")\n            print(f\"Total tiles exported: {stats['total_tiles']}\")\n            print(\n                f\"Tiles with features: {stats['tiles_with_features']} ({stats['tiles_with_features']/max(1, stats['total_tiles'])*100:.1f}%)\"\n            )\n            if stats[\"tiles_with_features\"] &gt; 0:\n                print(\n                    f\"Average feature pixels per tile: {stats['feature_pixels']/stats['tiles_with_features']:.1f}\"\n                )\n            if stats[\"errors\"] &gt; 0:\n                print(f\"Errors encountered: {stats['errors']}\")\n            print(f\"Output saved to: {out_folder}\")\n\n            # Verify georeference in a sample image and label\n            if stats[\"total_tiles\"] &gt; 0:\n                print(\"\\n------- Georeference Verification -------\")\n                sample_image = os.path.join(image_dir, f\"tile_0.tif\")\n                sample_label = os.path.join(label_dir, f\"tile_0.tif\")\n\n                if os.path.exists(sample_image):\n                    try:\n                        with rasterio.open(sample_image) as img:\n                            print(f\"Image CRS: {img.crs}\")\n                            print(f\"Image transform: {img.transform}\")\n                            print(\n                                f\"Image has georeference: {img.crs is not None and img.transform is not None}\"\n                            )\n                            print(\n                                f\"Image dimensions: {img.width}x{img.height}, {img.count} bands, {img.dtypes[0]} type\"\n                            )\n                    except Exception as e:\n                        print(f\"Error verifying image georeference: {e}\")\n\n                if os.path.exists(sample_label):\n                    try:\n                        with rasterio.open(sample_label) as lbl:\n                            print(f\"Label CRS: {lbl.crs}\")\n                            print(f\"Label transform: {lbl.transform}\")\n                            print(\n                                f\"Label has georeference: {lbl.crs is not None and lbl.transform is not None}\"\n                            )\n                            print(\n                                f\"Label dimensions: {lbl.width}x{lbl.height}, {lbl.count} bands, {lbl.dtypes[0]} type\"\n                            )\n                    except Exception as e:\n                        print(f\"Error verifying label georeference: {e}\")\n\n        # Return statistics dictionary for further processing if needed\n        return stats\n</code></pre>"},{"location":"utils/#geoai.utils.export_tiles_to_geojson","title":"<code>export_tiles_to_geojson(tile_coordinates, src, output_path, tile_size=None, stride=None)</code>","text":"<p>Export tile rectangles directly to GeoJSON without creating an overview image.</p> <p>Parameters:</p> Name Type Description Default <code>tile_coordinates</code> <code>list</code> <p>A list of dictionaries containing tile information.</p> required <code>src</code> <code>rasterio.io.DatasetReader</code> <p>The source raster dataset.</p> required <code>output_path</code> <code>str</code> <p>The path where the GeoJSON will be saved.</p> required <code>tile_size</code> <code>int</code> <p>The size of each tile in pixels. Only needed if not in tile_coordinates.</p> <code>None</code> <code>stride</code> <code>int</code> <p>The stride between tiles in pixels. Used to calculate overlaps between tiles.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the saved GeoJSON file.</p> Source code in <code>geoai/utils.py</code> <pre><code>def export_tiles_to_geojson(\n    tile_coordinates, src, output_path, tile_size=None, stride=None\n):\n    \"\"\"\n    Export tile rectangles directly to GeoJSON without creating an overview image.\n\n    Args:\n        tile_coordinates (list): A list of dictionaries containing tile information.\n        src (rasterio.io.DatasetReader): The source raster dataset.\n        output_path (str): The path where the GeoJSON will be saved.\n        tile_size (int, optional): The size of each tile in pixels. Only needed if not in tile_coordinates.\n        stride (int, optional): The stride between tiles in pixels. Used to calculate overlaps between tiles.\n\n    Returns:\n        str: Path to the saved GeoJSON file.\n    \"\"\"\n    features = []\n\n    for tile in tile_coordinates:\n        # Get the size from the tile or use the provided parameter\n        tile_width = tile.get(\"width\", tile.get(\"size\", tile_size))\n        tile_height = tile.get(\"height\", tile.get(\"size\", tile_size))\n\n        if tile_width is None or tile_height is None:\n            raise ValueError(\n                \"Tile size not found in tile data and no tile_size parameter provided\"\n            )\n\n        # Get bounds from the tile\n        if \"bounds\" in tile:\n            # If bounds are already in geo coordinates\n            minx, miny, maxx, maxy = tile[\"bounds\"]\n        else:\n            # Try to calculate bounds from transform if available\n            if hasattr(src, \"transform\"):\n                # Convert pixel coordinates to geo coordinates\n                window_transform = src.transform\n                x, y = tile[\"x\"], tile[\"y\"]\n                minx = window_transform[2] + x * window_transform[0]\n                maxy = window_transform[5] + y * window_transform[4]\n                maxx = minx + tile_width * window_transform[0]\n                miny = maxy + tile_height * window_transform[4]\n            else:\n                raise ValueError(\n                    \"Cannot determine bounds. Neither 'bounds' in tile nor transform in src.\"\n                )\n\n        # Calculate overlap with neighboring tiles if stride is provided\n        overlap = 0\n        if stride is not None and stride &lt; tile_width:\n            overlap = tile_width - stride\n\n        # Create a polygon from the bounds\n        polygon = box(minx, miny, maxx, maxy)\n\n        # Create a GeoJSON feature\n        feature = {\n            \"type\": \"Feature\",\n            \"geometry\": mapping(polygon),\n            \"properties\": {\n                \"index\": tile[\"index\"],\n                \"has_features\": tile.get(\"has_features\", False),\n                \"tile_width_px\": tile_width,\n                \"tile_height_px\": tile_height,\n            },\n        }\n\n        # Add overlap information if stride is provided\n        if stride is not None:\n            feature[\"properties\"][\"stride_px\"] = stride\n            feature[\"properties\"][\"overlap_px\"] = overlap\n\n        # Add additional properties from the tile\n        for key, value in tile.items():\n            if key not in [\"bounds\", \"geometry\"]:\n                feature[\"properties\"][key] = value\n\n        features.append(feature)\n\n    # Create the GeoJSON collection\n    geojson_collection = {\n        \"type\": \"FeatureCollection\",\n        \"features\": features,\n        \"properties\": {\n            \"crs\": (\n                src.crs.to_string() if hasattr(src.crs, \"to_string\") else str(src.crs)\n            ),\n            \"total_tiles\": len(features),\n            \"source_raster_dimensions\": (\n                [src.width, src.height] if hasattr(src, \"width\") else None\n            ),\n        },\n    }\n\n    # Create directory if it doesn't exist\n    os.makedirs(os.path.dirname(os.path.abspath(output_path)) or \".\", exist_ok=True)\n\n    # Save to file\n    with open(output_path, \"w\") as f:\n        json.dump(geojson_collection, f)\n\n    print(f\"GeoJSON saved to {output_path}\")\n    return output_path\n</code></pre>"},{"location":"utils/#geoai.utils.export_training_data","title":"<code>export_training_data(in_raster, out_folder, in_class_data, image_chip_format='GEOTIFF', tile_size_x=256, tile_size_y=256, stride_x=None, stride_y=None, output_nofeature_tiles=True, metadata_format='PASCAL_VOC', start_index=0, class_value_field='class', buffer_radius=0, in_mask_polygons=None, rotation_angle=0, reference_system=None, blacken_around_feature=False, crop_mode='FIXED_SIZE', in_raster2=None, in_instance_data=None, instance_class_value_field=None, min_polygon_overlap_ratio=0.0, all_touched=True, save_geotiff=True, quiet=False)</code>","text":"<p>Export training data for deep learning using TorchGeo with progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>in_raster</code> <code>str</code> <p>Path to input raster image.</p> required <code>out_folder</code> <code>str</code> <p>Output folder path where chips and labels will be saved.</p> required <code>in_class_data</code> <code>str</code> <p>Path to vector file containing class polygons.</p> required <code>image_chip_format</code> <code>str</code> <p>Output image format (PNG, JPEG, TIFF, GEOTIFF).</p> <code>'GEOTIFF'</code> <code>tile_size_x</code> <code>int</code> <p>Width of image chips in pixels.</p> <code>256</code> <code>tile_size_y</code> <code>int</code> <p>Height of image chips in pixels.</p> <code>256</code> <code>stride_x</code> <code>int</code> <p>Horizontal stride between chips. If None, uses tile_size_x.</p> <code>None</code> <code>stride_y</code> <code>int</code> <p>Vertical stride between chips. If None, uses tile_size_y.</p> <code>None</code> <code>output_nofeature_tiles</code> <code>bool</code> <p>Whether to export chips without features.</p> <code>True</code> <code>metadata_format</code> <code>str</code> <p>Output metadata format (PASCAL_VOC, KITTI, COCO).</p> <code>'PASCAL_VOC'</code> <code>start_index</code> <code>int</code> <p>Starting index for chip filenames.</p> <code>0</code> <code>class_value_field</code> <code>str</code> <p>Field name in in_class_data containing class values.</p> <code>'class'</code> <code>buffer_radius</code> <code>float</code> <p>Buffer radius around features (in CRS units).</p> <code>0</code> <code>in_mask_polygons</code> <code>str</code> <p>Path to vector file containing mask polygons.</p> <code>None</code> <code>rotation_angle</code> <code>float</code> <p>Rotation angle in degrees.</p> <code>0</code> <code>reference_system</code> <code>str</code> <p>Reference system code.</p> <code>None</code> <code>blacken_around_feature</code> <code>bool</code> <p>Whether to mask areas outside of features.</p> <code>False</code> <code>crop_mode</code> <code>str</code> <p>Crop mode (FIXED_SIZE, CENTERED_ON_FEATURE).</p> <code>'FIXED_SIZE'</code> <code>in_raster2</code> <code>str</code> <p>Path to secondary raster image.</p> <code>None</code> <code>in_instance_data</code> <code>str</code> <p>Path to vector file containing instance polygons.</p> <code>None</code> <code>instance_class_value_field</code> <code>str</code> <p>Field name in in_instance_data for instance classes.</p> <code>None</code> <code>min_polygon_overlap_ratio</code> <code>float</code> <p>Minimum overlap ratio for polygons.</p> <code>0.0</code> <code>all_touched</code> <code>bool</code> <p>Whether to use all_touched=True in rasterization.</p> <code>True</code> <code>save_geotiff</code> <code>bool</code> <p>Whether to save as GeoTIFF with georeferencing.</p> <code>True</code> <code>quiet</code> <code>bool</code> <p>If True, suppress most output messages.</p> <code>False</code> Source code in <code>geoai/utils.py</code> <pre><code>def export_training_data(\n    in_raster,\n    out_folder,\n    in_class_data,\n    image_chip_format=\"GEOTIFF\",\n    tile_size_x=256,\n    tile_size_y=256,\n    stride_x=None,\n    stride_y=None,\n    output_nofeature_tiles=True,\n    metadata_format=\"PASCAL_VOC\",\n    start_index=0,\n    class_value_field=\"class\",\n    buffer_radius=0,\n    in_mask_polygons=None,\n    rotation_angle=0,\n    reference_system=None,\n    blacken_around_feature=False,\n    crop_mode=\"FIXED_SIZE\",  # Implemented but not fully used yet\n    in_raster2=None,\n    in_instance_data=None,\n    instance_class_value_field=None,  # Implemented but not fully used yet\n    min_polygon_overlap_ratio=0.0,\n    all_touched=True,\n    save_geotiff=True,\n    quiet=False,\n):\n    \"\"\"\n    Export training data for deep learning using TorchGeo with progress bar.\n\n    Args:\n        in_raster (str): Path to input raster image.\n        out_folder (str): Output folder path where chips and labels will be saved.\n        in_class_data (str): Path to vector file containing class polygons.\n        image_chip_format (str): Output image format (PNG, JPEG, TIFF, GEOTIFF).\n        tile_size_x (int): Width of image chips in pixels.\n        tile_size_y (int): Height of image chips in pixels.\n        stride_x (int): Horizontal stride between chips. If None, uses tile_size_x.\n        stride_y (int): Vertical stride between chips. If None, uses tile_size_y.\n        output_nofeature_tiles (bool): Whether to export chips without features.\n        metadata_format (str): Output metadata format (PASCAL_VOC, KITTI, COCO).\n        start_index (int): Starting index for chip filenames.\n        class_value_field (str): Field name in in_class_data containing class values.\n        buffer_radius (float): Buffer radius around features (in CRS units).\n        in_mask_polygons (str): Path to vector file containing mask polygons.\n        rotation_angle (float): Rotation angle in degrees.\n        reference_system (str): Reference system code.\n        blacken_around_feature (bool): Whether to mask areas outside of features.\n        crop_mode (str): Crop mode (FIXED_SIZE, CENTERED_ON_FEATURE).\n        in_raster2 (str): Path to secondary raster image.\n        in_instance_data (str): Path to vector file containing instance polygons.\n        instance_class_value_field (str): Field name in in_instance_data for instance classes.\n        min_polygon_overlap_ratio (float): Minimum overlap ratio for polygons.\n        all_touched (bool): Whether to use all_touched=True in rasterization.\n        save_geotiff (bool): Whether to save as GeoTIFF with georeferencing.\n        quiet (bool): If True, suppress most output messages.\n    \"\"\"\n    # Create output directories\n    image_dir = os.path.join(out_folder, \"images\")\n    os.makedirs(image_dir, exist_ok=True)\n\n    label_dir = os.path.join(out_folder, \"labels\")\n    os.makedirs(label_dir, exist_ok=True)\n\n    # Define annotation directories based on metadata format\n    if metadata_format == \"PASCAL_VOC\":\n        ann_dir = os.path.join(out_folder, \"annotations\")\n        os.makedirs(ann_dir, exist_ok=True)\n    elif metadata_format == \"COCO\":\n        ann_dir = os.path.join(out_folder, \"annotations\")\n        os.makedirs(ann_dir, exist_ok=True)\n        # Initialize COCO annotations dictionary\n        coco_annotations = {\"images\": [], \"annotations\": [], \"categories\": []}\n\n    # Initialize statistics dictionary\n    stats = {\n        \"total_tiles\": 0,\n        \"tiles_with_features\": 0,\n        \"feature_pixels\": 0,\n        \"errors\": 0,\n    }\n\n    # Open raster\n    with rasterio.open(in_raster) as src:\n        if not quiet:\n            print(f\"\\nRaster info for {in_raster}:\")\n            print(f\"  CRS: {src.crs}\")\n            print(f\"  Dimensions: {src.width} x {src.height}\")\n            print(f\"  Bounds: {src.bounds}\")\n\n        # Set defaults for stride if not provided\n        if stride_x is None:\n            stride_x = tile_size_x\n        if stride_y is None:\n            stride_y = tile_size_y\n\n        # Calculate number of tiles in x and y directions\n        num_tiles_x = math.ceil((src.width - tile_size_x) / stride_x) + 1\n        num_tiles_y = math.ceil((src.height - tile_size_y) / stride_y) + 1\n        total_tiles = num_tiles_x * num_tiles_y\n\n        # Read class data\n        gdf = gpd.read_file(in_class_data)\n        if not quiet:\n            print(f\"Loaded {len(gdf)} features from {in_class_data}\")\n            print(f\"Available columns: {gdf.columns.tolist()}\")\n            print(f\"GeoJSON CRS: {gdf.crs}\")\n\n        # Check if class_value_field exists\n        if class_value_field not in gdf.columns:\n            if not quiet:\n                print(\n                    f\"WARNING: '{class_value_field}' field not found in the input data. Using default class value 1.\"\n                )\n            # Add a default class column\n            gdf[class_value_field] = 1\n            unique_classes = [1]\n        else:\n            # Print unique classes for debugging\n            unique_classes = gdf[class_value_field].unique()\n            if not quiet:\n                print(f\"Found {len(unique_classes)} unique classes: {unique_classes}\")\n\n        # CRITICAL: Always reproject to match raster CRS to ensure proper alignment\n        if gdf.crs != src.crs:\n            if not quiet:\n                print(f\"Reprojecting features from {gdf.crs} to {src.crs}\")\n            gdf = gdf.to_crs(src.crs)\n        elif reference_system and gdf.crs != reference_system:\n            if not quiet:\n                print(\n                    f\"Reprojecting features to specified reference system {reference_system}\"\n                )\n            gdf = gdf.to_crs(reference_system)\n\n        # Check overlap between raster and vector data\n        raster_bounds = box(*src.bounds)\n        vector_bounds = box(*gdf.total_bounds)\n        if not raster_bounds.intersects(vector_bounds):\n            if not quiet:\n                print(\n                    \"WARNING: The vector data doesn't intersect with the raster extent!\"\n                )\n                print(f\"Raster bounds: {src.bounds}\")\n                print(f\"Vector bounds: {gdf.total_bounds}\")\n        else:\n            overlap = (\n                raster_bounds.intersection(vector_bounds).area / vector_bounds.area\n            )\n            if not quiet:\n                print(f\"Overlap between raster and vector: {overlap:.2%}\")\n\n        # Apply buffer if specified\n        if buffer_radius &gt; 0:\n            gdf[\"geometry\"] = gdf.buffer(buffer_radius)\n\n        # Initialize class mapping (ensure all classes are mapped to non-zero values)\n        class_to_id = {cls: i + 1 for i, cls in enumerate(unique_classes)}\n\n        # Store category info for COCO format\n        if metadata_format == \"COCO\":\n            for cls_val in unique_classes:\n                coco_annotations[\"categories\"].append(\n                    {\n                        \"id\": class_to_id[cls_val],\n                        \"name\": str(cls_val),\n                        \"supercategory\": \"object\",\n                    }\n                )\n\n        # Load mask polygons if provided\n        mask_gdf = None\n        if in_mask_polygons:\n            mask_gdf = gpd.read_file(in_mask_polygons)\n            if reference_system:\n                mask_gdf = mask_gdf.to_crs(reference_system)\n            elif mask_gdf.crs != src.crs:\n                mask_gdf = mask_gdf.to_crs(src.crs)\n\n        # Process instance data if provided\n        instance_gdf = None\n        if in_instance_data:\n            instance_gdf = gpd.read_file(in_instance_data)\n            if reference_system:\n                instance_gdf = instance_gdf.to_crs(reference_system)\n            elif instance_gdf.crs != src.crs:\n                instance_gdf = instance_gdf.to_crs(src.crs)\n\n        # Load secondary raster if provided\n        src2 = None\n        if in_raster2:\n            src2 = rasterio.open(in_raster2)\n\n        # Set up augmentation if rotation is specified\n        augmentation = None\n        if rotation_angle != 0:\n            # Fixed: Added data_keys parameter to AugmentationSequential\n            augmentation = torchgeo.transforms.AugmentationSequential(\n                torch.nn.ModuleList([RandomRotation(rotation_angle)]),\n                data_keys=[\"image\"],  # Add data_keys parameter\n            )\n\n        # Initialize annotation ID for COCO format\n        ann_id = 0\n\n        # Create progress bar\n        pbar = tqdm(\n            total=total_tiles,\n            desc=f\"Generating tiles (with features: 0)\",\n            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}&lt;{remaining}, {rate_fmt}]\",\n        )\n\n        # Generate tiles\n        chip_index = start_index\n        for y in range(num_tiles_y):\n            for x in range(num_tiles_x):\n                # Calculate window coordinates\n                window_x = x * stride_x\n                window_y = y * stride_y\n\n                # Adjust for edge cases\n                if window_x + tile_size_x &gt; src.width:\n                    window_x = src.width - tile_size_x\n                if window_y + tile_size_y &gt; src.height:\n                    window_y = src.height - tile_size_y\n\n                # Adjust window based on crop_mode\n                if crop_mode == \"CENTERED_ON_FEATURE\" and len(gdf) &gt; 0:\n                    # Find the nearest feature to the center of this window\n                    window_center_x = window_x + tile_size_x // 2\n                    window_center_y = window_y + tile_size_y // 2\n\n                    # Convert center to world coordinates\n                    center_x, center_y = src.xy(window_center_y, window_center_x)\n                    center_point = gpd.points_from_xy([center_x], [center_y])[0]\n\n                    # Find nearest feature\n                    distances = gdf.geometry.distance(center_point)\n                    nearest_idx = distances.idxmin()\n                    nearest_feature = gdf.iloc[nearest_idx]\n\n                    # Get centroid of nearest feature\n                    feature_centroid = nearest_feature.geometry.centroid\n\n                    # Convert feature centroid to pixel coordinates\n                    feature_row, feature_col = src.index(\n                        feature_centroid.x, feature_centroid.y\n                    )\n\n                    # Adjust window to center on feature\n                    window_x = max(\n                        0, min(src.width - tile_size_x, feature_col - tile_size_x // 2)\n                    )\n                    window_y = max(\n                        0, min(src.height - tile_size_y, feature_row - tile_size_y // 2)\n                    )\n\n                # Define window\n                window = Window(window_x, window_y, tile_size_x, tile_size_y)\n\n                # Get window transform and bounds in source CRS\n                window_transform = src.window_transform(window)\n\n                # Calculate window bounds more explicitly and accurately\n                minx = window_transform[2]  # Upper left x\n                maxy = window_transform[5]  # Upper left y\n                maxx = minx + tile_size_x * window_transform[0]  # Add width\n                miny = (\n                    maxy + tile_size_y * window_transform[4]\n                )  # Add height (note: transform[4] is typically negative)\n\n                window_bounds = box(minx, miny, maxx, maxy)\n\n                # Apply rotation if specified\n                if rotation_angle != 0:\n                    window_bounds = rotate(\n                        window_bounds, rotation_angle, origin=\"center\"\n                    )\n\n                # Find features that intersect with window\n                window_features = gdf[gdf.intersects(window_bounds)]\n\n                # Process instance data if provided\n                window_instances = None\n                if instance_gdf is not None and instance_class_value_field is not None:\n                    window_instances = instance_gdf[\n                        instance_gdf.intersects(window_bounds)\n                    ]\n                    if len(window_instances) &gt; 0:\n                        if not quiet:\n                            pbar.write(\n                                f\"Found {len(window_instances)} instances in tile {chip_index}\"\n                            )\n\n                # Skip if no features and output_nofeature_tiles is False\n                if not output_nofeature_tiles and len(window_features) == 0:\n                    pbar.update(1)  # Still update progress bar\n                    continue\n\n                # Check polygon overlap ratio if specified\n                if min_polygon_overlap_ratio &gt; 0 and len(window_features) &gt; 0:\n                    valid_features = []\n                    for _, feature in window_features.iterrows():\n                        overlap_ratio = (\n                            feature.geometry.intersection(window_bounds).area\n                            / feature.geometry.area\n                        )\n                        if overlap_ratio &gt;= min_polygon_overlap_ratio:\n                            valid_features.append(feature)\n\n                    if len(valid_features) &gt; 0:\n                        window_features = gpd.GeoDataFrame(valid_features)\n                    elif not output_nofeature_tiles:\n                        pbar.update(1)  # Still update progress bar\n                        continue\n\n                # Apply mask if provided\n                if mask_gdf is not None:\n                    mask_features = mask_gdf[mask_gdf.intersects(window_bounds)]\n                    if len(mask_features) == 0:\n                        pbar.update(1)  # Still update progress bar\n                        continue\n\n                # Read image data - keep original for GeoTIFF export\n                orig_image_data = src.read(window=window)\n\n                # Create a copy for processing\n                image_data = orig_image_data.copy().astype(np.float32)\n\n                # Normalize image data for processing\n                for band in range(image_data.shape[0]):\n                    band_min, band_max = np.percentile(image_data[band], (1, 99))\n                    if band_max &gt; band_min:\n                        image_data[band] = np.clip(\n                            (image_data[band] - band_min) / (band_max - band_min), 0, 1\n                        )\n\n                # Read secondary image data if provided\n                if src2:\n                    image_data2 = src2.read(window=window)\n                    # Stack the two images\n                    image_data = np.vstack((image_data, image_data2))\n\n                # Apply blacken_around_feature if needed\n                if blacken_around_feature and len(window_features) &gt; 0:\n                    mask = np.zeros((tile_size_y, tile_size_x), dtype=bool)\n                    for _, feature in window_features.iterrows():\n                        # Project feature to pixel coordinates\n                        feature_pixels = features.rasterize(\n                            [(feature.geometry, 1)],\n                            out_shape=(tile_size_y, tile_size_x),\n                            transform=window_transform,\n                        )\n                        mask = np.logical_or(mask, feature_pixels.astype(bool))\n\n                    # Apply mask to image\n                    for band in range(image_data.shape[0]):\n                        temp = image_data[band, :, :]\n                        temp[~mask] = 0\n                        image_data[band, :, :] = temp\n\n                # Apply rotation if specified\n                if augmentation:\n                    # Convert to torch tensor for augmentation\n                    image_tensor = torch.from_numpy(image_data).unsqueeze(\n                        0\n                    )  # Add batch dimension\n                    # Apply augmentation with proper data format\n                    augmented = augmentation({\"image\": image_tensor})\n                    image_data = (\n                        augmented[\"image\"].squeeze(0).numpy()\n                    )  # Remove batch dimension\n\n                # Create a processed version for regular image formats\n                processed_image = (image_data * 255).astype(np.uint8)\n\n                # Create label mask\n                label_mask = np.zeros((tile_size_y, tile_size_x), dtype=np.uint8)\n                has_features = False\n\n                if len(window_features) &gt; 0:\n                    for idx, feature in window_features.iterrows():\n                        # Get class value\n                        class_val = (\n                            feature[class_value_field]\n                            if class_value_field in feature\n                            else 1\n                        )\n                        if isinstance(class_val, str):\n                            # If class is a string, use its position in the unique classes list\n                            class_id = class_to_id.get(class_val, 1)\n                        else:\n                            # If class is already a number, use it directly\n                            class_id = int(class_val) if class_val &gt; 0 else 1\n\n                        # Get the geometry in pixel coordinates\n                        geom = feature.geometry.intersection(window_bounds)\n                        if not geom.is_empty:\n                            try:\n                                # Rasterize the feature\n                                feature_mask = features.rasterize(\n                                    [(geom, class_id)],\n                                    out_shape=(tile_size_y, tile_size_x),\n                                    transform=window_transform,\n                                    fill=0,\n                                    all_touched=all_touched,\n                                )\n\n                                # Update mask with higher class values taking precedence\n                                label_mask = np.maximum(label_mask, feature_mask)\n\n                                # Check if any pixels were added\n                                if np.any(feature_mask):\n                                    has_features = True\n                            except Exception as e:\n                                if not quiet:\n                                    pbar.write(f\"Error rasterizing feature {idx}: {e}\")\n                                stats[\"errors\"] += 1\n\n                # Save as GeoTIFF if requested\n                if save_geotiff or image_chip_format.upper() in [\n                    \"TIFF\",\n                    \"TIF\",\n                    \"GEOTIFF\",\n                ]:\n                    # Standardize extension to .tif for GeoTIFF files\n                    image_filename = f\"tile_{chip_index:06d}.tif\"\n                    image_path = os.path.join(image_dir, image_filename)\n\n                    # Create profile for the GeoTIFF\n                    profile = src.profile.copy()\n                    profile.update(\n                        {\n                            \"height\": tile_size_y,\n                            \"width\": tile_size_x,\n                            \"count\": orig_image_data.shape[0],\n                            \"transform\": window_transform,\n                        }\n                    )\n\n                    # Save the GeoTIFF with original data\n                    try:\n                        with rasterio.open(image_path, \"w\", **profile) as dst:\n                            dst.write(orig_image_data)\n                        stats[\"total_tiles\"] += 1\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(\n                                f\"ERROR saving image GeoTIFF for tile {chip_index}: {e}\"\n                            )\n                        stats[\"errors\"] += 1\n                else:\n                    # For non-GeoTIFF formats, use PIL to save the image\n                    image_filename = (\n                        f\"tile_{chip_index:06d}.{image_chip_format.lower()}\"\n                    )\n                    image_path = os.path.join(image_dir, image_filename)\n\n                    # Create PIL image for saving\n                    if processed_image.shape[0] == 1:\n                        img = Image.fromarray(processed_image[0])\n                    elif processed_image.shape[0] == 3:\n                        # For RGB, need to transpose and make sure it's the right data type\n                        rgb_data = np.transpose(processed_image, (1, 2, 0))\n                        img = Image.fromarray(rgb_data)\n                    else:\n                        # For multiband images, save only RGB or first three bands\n                        rgb_data = np.transpose(processed_image[:3], (1, 2, 0))\n                        img = Image.fromarray(rgb_data)\n\n                    # Save image\n                    try:\n                        img.save(image_path)\n                        stats[\"total_tiles\"] += 1\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(f\"ERROR saving image for tile {chip_index}: {e}\")\n                        stats[\"errors\"] += 1\n\n                # Save label as GeoTIFF\n                label_filename = f\"tile_{chip_index:06d}.tif\"\n                label_path = os.path.join(label_dir, label_filename)\n\n                # Create profile for label GeoTIFF\n                label_profile = {\n                    \"driver\": \"GTiff\",\n                    \"height\": tile_size_y,\n                    \"width\": tile_size_x,\n                    \"count\": 1,\n                    \"dtype\": \"uint8\",\n                    \"crs\": src.crs,\n                    \"transform\": window_transform,\n                }\n\n                # Save label GeoTIFF\n                try:\n                    with rasterio.open(label_path, \"w\", **label_profile) as dst:\n                        dst.write(label_mask, 1)\n\n                    if has_features:\n                        pixel_count = np.count_nonzero(label_mask)\n                        stats[\"tiles_with_features\"] += 1\n                        stats[\"feature_pixels\"] += pixel_count\n                except Exception as e:\n                    if not quiet:\n                        pbar.write(f\"ERROR saving label for tile {chip_index}: {e}\")\n                    stats[\"errors\"] += 1\n\n                # Also save a PNG version for easy visualization if requested\n                if metadata_format == \"PASCAL_VOC\":\n                    try:\n                        # Ensure correct data type for PIL\n                        png_label = label_mask.astype(np.uint8)\n                        label_img = Image.fromarray(png_label)\n                        label_png_path = os.path.join(\n                            label_dir, f\"tile_{chip_index:06d}.png\"\n                        )\n                        label_img.save(label_png_path)\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(\n                                f\"ERROR saving PNG label for tile {chip_index}: {e}\"\n                            )\n                            pbar.write(\n                                f\"  Label mask shape: {label_mask.shape}, dtype: {label_mask.dtype}\"\n                            )\n                            # Try again with explicit conversion\n                            try:\n                                # Alternative approach for problematic arrays\n                                png_data = np.zeros(\n                                    (tile_size_y, tile_size_x), dtype=np.uint8\n                                )\n                                np.copyto(png_data, label_mask, casting=\"unsafe\")\n                                label_img = Image.fromarray(png_data)\n                                label_img.save(label_png_path)\n                                pbar.write(\n                                    f\"  Succeeded using alternative conversion method\"\n                                )\n                            except Exception as e2:\n                                pbar.write(f\"  Second attempt also failed: {e2}\")\n                                stats[\"errors\"] += 1\n\n                # Generate annotations\n                if metadata_format == \"PASCAL_VOC\" and len(window_features) &gt; 0:\n                    # Create XML annotation\n                    root = ET.Element(\"annotation\")\n                    ET.SubElement(root, \"folder\").text = \"images\"\n                    ET.SubElement(root, \"filename\").text = image_filename\n\n                    size = ET.SubElement(root, \"size\")\n                    ET.SubElement(size, \"width\").text = str(tile_size_x)\n                    ET.SubElement(size, \"height\").text = str(tile_size_y)\n                    ET.SubElement(size, \"depth\").text = str(min(image_data.shape[0], 3))\n\n                    # Add georeference information\n                    geo = ET.SubElement(root, \"georeference\")\n                    ET.SubElement(geo, \"crs\").text = str(src.crs)\n                    ET.SubElement(geo, \"transform\").text = str(\n                        window_transform\n                    ).replace(\"\\n\", \"\")\n                    ET.SubElement(geo, \"bounds\").text = (\n                        f\"{minx}, {miny}, {maxx}, {maxy}\"\n                    )\n\n                    for _, feature in window_features.iterrows():\n                        # Convert feature geometry to pixel coordinates\n                        feature_bounds = feature.geometry.intersection(window_bounds)\n                        if feature_bounds.is_empty:\n                            continue\n\n                        # Get pixel coordinates of bounds\n                        minx_f, miny_f, maxx_f, maxy_f = feature_bounds.bounds\n\n                        # Convert to pixel coordinates\n                        col_min, row_min = ~window_transform * (minx_f, maxy_f)\n                        col_max, row_max = ~window_transform * (maxx_f, miny_f)\n\n                        # Ensure coordinates are within bounds\n                        xmin = max(0, min(tile_size_x, int(col_min)))\n                        ymin = max(0, min(tile_size_y, int(row_min)))\n                        xmax = max(0, min(tile_size_x, int(col_max)))\n                        ymax = max(0, min(tile_size_y, int(row_max)))\n\n                        # Skip if box is too small\n                        if xmax - xmin &lt; 1 or ymax - ymin &lt; 1:\n                            continue\n\n                        obj = ET.SubElement(root, \"object\")\n                        ET.SubElement(obj, \"name\").text = str(\n                            feature[class_value_field]\n                        )\n                        ET.SubElement(obj, \"difficult\").text = \"0\"\n\n                        bbox = ET.SubElement(obj, \"bndbox\")\n                        ET.SubElement(bbox, \"xmin\").text = str(xmin)\n                        ET.SubElement(bbox, \"ymin\").text = str(ymin)\n                        ET.SubElement(bbox, \"xmax\").text = str(xmax)\n                        ET.SubElement(bbox, \"ymax\").text = str(ymax)\n\n                    # Save XML\n                    try:\n                        tree = ET.ElementTree(root)\n                        xml_path = os.path.join(ann_dir, f\"tile_{chip_index:06d}.xml\")\n                        tree.write(xml_path)\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(\n                                f\"ERROR saving XML annotation for tile {chip_index}: {e}\"\n                            )\n                        stats[\"errors\"] += 1\n\n                elif metadata_format == \"COCO\" and len(window_features) &gt; 0:\n                    # Add image info\n                    image_id = chip_index\n                    coco_annotations[\"images\"].append(\n                        {\n                            \"id\": image_id,\n                            \"file_name\": image_filename,\n                            \"width\": tile_size_x,\n                            \"height\": tile_size_y,\n                            \"crs\": str(src.crs),\n                            \"transform\": str(window_transform),\n                        }\n                    )\n\n                    # Add annotations for each feature\n                    for _, feature in window_features.iterrows():\n                        feature_bounds = feature.geometry.intersection(window_bounds)\n                        if feature_bounds.is_empty:\n                            continue\n\n                        # Get pixel coordinates of bounds\n                        minx_f, miny_f, maxx_f, maxy_f = feature_bounds.bounds\n\n                        # Convert to pixel coordinates\n                        col_min, row_min = ~window_transform * (minx_f, maxy_f)\n                        col_max, row_max = ~window_transform * (maxx_f, miny_f)\n\n                        # Ensure coordinates are within bounds\n                        xmin = max(0, min(tile_size_x, int(col_min)))\n                        ymin = max(0, min(tile_size_y, int(row_min)))\n                        xmax = max(0, min(tile_size_x, int(col_max)))\n                        ymax = max(0, min(tile_size_y, int(row_max)))\n\n                        # Skip if box is too small\n                        if xmax - xmin &lt; 1 or ymax - ymin &lt; 1:\n                            continue\n\n                        width = xmax - xmin\n                        height = ymax - ymin\n\n                        # Add annotation\n                        ann_id += 1\n                        category_id = class_to_id[feature[class_value_field]]\n\n                        coco_annotations[\"annotations\"].append(\n                            {\n                                \"id\": ann_id,\n                                \"image_id\": image_id,\n                                \"category_id\": category_id,\n                                \"bbox\": [xmin, ymin, width, height],\n                                \"area\": width * height,\n                                \"iscrowd\": 0,\n                            }\n                        )\n\n                # Update progress bar\n                pbar.update(1)\n                pbar.set_description(\n                    f\"Generated: {stats['total_tiles']}, With features: {stats['tiles_with_features']}\"\n                )\n\n                chip_index += 1\n\n        # Close progress bar\n        pbar.close()\n\n        # Save COCO annotations if applicable\n        if metadata_format == \"COCO\":\n            try:\n                with open(os.path.join(ann_dir, \"instances.json\"), \"w\") as f:\n                    json.dump(coco_annotations, f)\n            except Exception as e:\n                if not quiet:\n                    print(f\"ERROR saving COCO annotations: {e}\")\n                stats[\"errors\"] += 1\n\n        # Close secondary raster if opened\n        if src2:\n            src2.close()\n\n    # Print summary\n    if not quiet:\n        print(\"\\n------- Export Summary -------\")\n        print(f\"Total tiles exported: {stats['total_tiles']}\")\n        print(\n            f\"Tiles with features: {stats['tiles_with_features']} ({stats['tiles_with_features']/max(1, stats['total_tiles'])*100:.1f}%)\"\n        )\n        if stats[\"tiles_with_features\"] &gt; 0:\n            print(\n                f\"Average feature pixels per tile: {stats['feature_pixels']/stats['tiles_with_features']:.1f}\"\n            )\n        if stats[\"errors\"] &gt; 0:\n            print(f\"Errors encountered: {stats['errors']}\")\n        print(f\"Output saved to: {out_folder}\")\n\n        # Verify georeference in a sample image and label\n        if stats[\"total_tiles\"] &gt; 0:\n            print(\"\\n------- Georeference Verification -------\")\n            sample_image = os.path.join(image_dir, f\"tile_{start_index}.tif\")\n            sample_label = os.path.join(label_dir, f\"tile_{start_index}.tif\")\n\n            if os.path.exists(sample_image):\n                try:\n                    with rasterio.open(sample_image) as img:\n                        print(f\"Image CRS: {img.crs}\")\n                        print(f\"Image transform: {img.transform}\")\n                        print(\n                            f\"Image has georeference: {img.crs is not None and img.transform is not None}\"\n                        )\n                        print(\n                            f\"Image dimensions: {img.width}x{img.height}, {img.count} bands, {img.dtypes[0]} type\"\n                        )\n                except Exception as e:\n                    print(f\"Error verifying image georeference: {e}\")\n\n            if os.path.exists(sample_label):\n                try:\n                    with rasterio.open(sample_label) as lbl:\n                        print(f\"Label CRS: {lbl.crs}\")\n                        print(f\"Label transform: {lbl.transform}\")\n                        print(\n                            f\"Label has georeference: {lbl.crs is not None and lbl.transform is not None}\"\n                        )\n                        print(\n                            f\"Label dimensions: {lbl.width}x{lbl.height}, {lbl.count} bands, {lbl.dtypes[0]} type\"\n                        )\n                except Exception as e:\n                    print(f\"Error verifying label georeference: {e}\")\n\n    # Return statistics\n    return stats, out_folder\n</code></pre>"},{"location":"utils/#geoai.utils.get_raster_info","title":"<code>get_raster_info(raster_path)</code>","text":"<p>Display basic information about a raster dataset.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the basic information about the raster</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_info(raster_path):\n    \"\"\"Display basic information about a raster dataset.\n\n    Args:\n        raster_path (str): Path to the raster file\n\n    Returns:\n        dict: Dictionary containing the basic information about the raster\n    \"\"\"\n    # Open the raster dataset\n    with rasterio.open(raster_path) as src:\n        # Get basic metadata\n        info = {\n            \"driver\": src.driver,\n            \"width\": src.width,\n            \"height\": src.height,\n            \"count\": src.count,\n            \"dtype\": src.dtypes[0],\n            \"crs\": src.crs.to_string() if src.crs else \"No CRS defined\",\n            \"transform\": src.transform,\n            \"bounds\": src.bounds,\n            \"resolution\": (src.transform[0], -src.transform[4]),\n            \"nodata\": src.nodata,\n        }\n\n        # Calculate statistics for each band\n        stats = []\n        for i in range(1, src.count + 1):\n            band = src.read(i, masked=True)\n            band_stats = {\n                \"band\": i,\n                \"min\": float(band.min()),\n                \"max\": float(band.max()),\n                \"mean\": float(band.mean()),\n                \"std\": float(band.std()),\n            }\n            stats.append(band_stats)\n\n        info[\"band_stats\"] = stats\n\n    return info\n</code></pre>"},{"location":"utils/#geoai.utils.get_raster_info_gdal","title":"<code>get_raster_info_gdal(raster_path)</code>","text":"<p>Get basic information about a raster dataset using GDAL.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the basic information about the raster,     or None if the file cannot be opened</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_info_gdal(raster_path):\n    \"\"\"Get basic information about a raster dataset using GDAL.\n\n    Args:\n        raster_path (str): Path to the raster file\n\n    Returns:\n        dict: Dictionary containing the basic information about the raster,\n            or None if the file cannot be opened\n    \"\"\"\n\n    from osgeo import gdal\n\n    # Open the dataset\n    ds = gdal.Open(raster_path)\n    if ds is None:\n        print(f\"Error: Could not open {raster_path}\")\n        return None\n\n    # Get basic information\n    info = {\n        \"driver\": ds.GetDriver().ShortName,\n        \"width\": ds.RasterXSize,\n        \"height\": ds.RasterYSize,\n        \"count\": ds.RasterCount,\n        \"projection\": ds.GetProjection(),\n        \"geotransform\": ds.GetGeoTransform(),\n    }\n\n    # Calculate resolution\n    gt = ds.GetGeoTransform()\n    if gt:\n        info[\"resolution\"] = (abs(gt[1]), abs(gt[5]))\n        info[\"origin\"] = (gt[0], gt[3])\n\n    # Get band information\n    bands_info = []\n    for i in range(1, ds.RasterCount + 1):\n        band = ds.GetRasterBand(i)\n        stats = band.GetStatistics(True, True)\n        band_info = {\n            \"band\": i,\n            \"datatype\": gdal.GetDataTypeName(band.DataType),\n            \"min\": stats[0],\n            \"max\": stats[1],\n            \"mean\": stats[2],\n            \"std\": stats[3],\n            \"nodata\": band.GetNoDataValue(),\n        }\n        bands_info.append(band_info)\n\n    info[\"bands\"] = bands_info\n\n    # Close the dataset\n    ds = None\n\n    return info\n</code></pre>"},{"location":"utils/#geoai.utils.get_raster_stats","title":"<code>get_raster_stats(raster_path, divide_by=1.0)</code>","text":"<p>Calculate statistics for each band in a raster dataset.</p> <p>This function computes min, max, mean, and standard deviation values for each band in the provided raster, returning results in a dictionary with lists for each statistic type.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <code>divide_by</code> <code>float</code> <p>Value to divide pixel values by. Defaults to 1.0, which keeps the original pixel</p> <code>1.0</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing lists of statistics with keys:     - 'min': List of minimum values for each band     - 'max': List of maximum values for each band     - 'mean': List of mean values for each band     - 'std': List of standard deviation values for each band</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_stats(raster_path, divide_by=1.0):\n    \"\"\"Calculate statistics for each band in a raster dataset.\n\n    This function computes min, max, mean, and standard deviation values\n    for each band in the provided raster, returning results in a dictionary\n    with lists for each statistic type.\n\n    Args:\n        raster_path (str): Path to the raster file\n        divide_by (float, optional): Value to divide pixel values by.\n            Defaults to 1.0, which keeps the original pixel\n\n    Returns:\n        dict: Dictionary containing lists of statistics with keys:\n            - 'min': List of minimum values for each band\n            - 'max': List of maximum values for each band\n            - 'mean': List of mean values for each band\n            - 'std': List of standard deviation values for each band\n    \"\"\"\n    # Initialize the results dictionary with empty lists\n    stats = {\"min\": [], \"max\": [], \"mean\": [], \"std\": []}\n\n    # Open the raster dataset\n    with rasterio.open(raster_path) as src:\n        # Calculate statistics for each band\n        for i in range(1, src.count + 1):\n            band = src.read(i, masked=True)\n\n            # Append statistics for this band to each list\n            stats[\"min\"].append(float(band.min()) / divide_by)\n            stats[\"max\"].append(float(band.max()) / divide_by)\n            stats[\"mean\"].append(float(band.mean()) / divide_by)\n            stats[\"std\"].append(float(band.std()) / divide_by)\n\n    return stats\n</code></pre>"},{"location":"utils/#geoai.utils.get_vector_info","title":"<code>get_vector_info(vector_path)</code>","text":"<p>Display basic information about a vector dataset using GeoPandas.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the basic information about the vector dataset</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_vector_info(vector_path):\n    \"\"\"Display basic information about a vector dataset using GeoPandas.\n\n    Args:\n        vector_path (str): Path to the vector file\n\n    Returns:\n        dict: Dictionary containing the basic information about the vector dataset\n    \"\"\"\n    # Open the vector dataset\n    gdf = (\n        gpd.read_parquet(vector_path)\n        if vector_path.endswith(\".parquet\")\n        else gpd.read_file(vector_path)\n    )\n\n    # Get basic metadata\n    info = {\n        \"file_path\": vector_path,\n        \"driver\": os.path.splitext(vector_path)[1][1:].upper(),  # Format from extension\n        \"feature_count\": len(gdf),\n        \"crs\": str(gdf.crs),\n        \"geometry_type\": str(gdf.geom_type.value_counts().to_dict()),\n        \"attribute_count\": len(gdf.columns) - 1,  # Subtract the geometry column\n        \"attribute_names\": list(gdf.columns[gdf.columns != \"geometry\"]),\n        \"bounds\": gdf.total_bounds.tolist(),\n    }\n\n    # Add statistics about numeric attributes\n    numeric_columns = gdf.select_dtypes(include=[\"number\"]).columns\n    attribute_stats = {}\n    for col in numeric_columns:\n        if col != \"geometry\":\n            attribute_stats[col] = {\n                \"min\": gdf[col].min(),\n                \"max\": gdf[col].max(),\n                \"mean\": gdf[col].mean(),\n                \"std\": gdf[col].std(),\n                \"null_count\": gdf[col].isna().sum(),\n            }\n\n    info[\"attribute_stats\"] = attribute_stats\n\n    return info\n</code></pre>"},{"location":"utils/#geoai.utils.get_vector_info_ogr","title":"<code>get_vector_info_ogr(vector_path)</code>","text":"<p>Get basic information about a vector dataset using OGR.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the basic information about the vector dataset,     or None if the file cannot be opened</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_vector_info_ogr(vector_path):\n    \"\"\"Get basic information about a vector dataset using OGR.\n\n    Args:\n        vector_path (str): Path to the vector file\n\n    Returns:\n        dict: Dictionary containing the basic information about the vector dataset,\n            or None if the file cannot be opened\n    \"\"\"\n    from osgeo import ogr\n\n    # Register all OGR drivers\n    ogr.RegisterAll()\n\n    # Open the dataset\n    ds = ogr.Open(vector_path)\n    if ds is None:\n        print(f\"Error: Could not open {vector_path}\")\n        return None\n\n    # Basic dataset information\n    info = {\n        \"file_path\": vector_path,\n        \"driver\": ds.GetDriver().GetName(),\n        \"layer_count\": ds.GetLayerCount(),\n        \"layers\": [],\n    }\n\n    # Extract information for each layer\n    for i in range(ds.GetLayerCount()):\n        layer = ds.GetLayer(i)\n        layer_info = {\n            \"name\": layer.GetName(),\n            \"feature_count\": layer.GetFeatureCount(),\n            \"geometry_type\": ogr.GeometryTypeToName(layer.GetGeomType()),\n            \"spatial_ref\": (\n                layer.GetSpatialRef().ExportToWkt() if layer.GetSpatialRef() else \"None\"\n            ),\n            \"extent\": layer.GetExtent(),\n            \"fields\": [],\n        }\n\n        # Get field information\n        defn = layer.GetLayerDefn()\n        for j in range(defn.GetFieldCount()):\n            field_defn = defn.GetFieldDefn(j)\n            field_info = {\n                \"name\": field_defn.GetName(),\n                \"type\": field_defn.GetTypeName(),\n                \"width\": field_defn.GetWidth(),\n                \"precision\": field_defn.GetPrecision(),\n            }\n            layer_info[\"fields\"].append(field_info)\n\n        info[\"layers\"].append(layer_info)\n\n    # Close the dataset\n    ds = None\n\n    return info\n</code></pre>"},{"location":"utils/#geoai.utils.hybrid_regularization","title":"<code>hybrid_regularization(building_polygons)</code>","text":"<p>A comprehensive hybrid approach to building footprint regularization.</p> <p>Applies different strategies based on building characteristics.</p> <p>Parameters:</p> Name Type Description Default <code>building_polygons</code> <p>GeoDataFrame or list of shapely Polygons containing building footprints</p> required <p>Returns:</p> Type Description <p>GeoDataFrame or list of shapely Polygons with regularized building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def hybrid_regularization(building_polygons):\n    \"\"\"\n    A comprehensive hybrid approach to building footprint regularization.\n\n    Applies different strategies based on building characteristics.\n\n    Args:\n        building_polygons: GeoDataFrame or list of shapely Polygons containing building footprints\n\n    Returns:\n        GeoDataFrame or list of shapely Polygons with regularized building footprints\n    \"\"\"\n    from shapely.geometry import Polygon\n    from shapely.affinity import rotate\n\n    # Use minimum_rotated_rectangle instead of oriented_envelope\n    try:\n        from shapely.minimum_rotated_rectangle import minimum_rotated_rectangle\n    except ImportError:\n        # For older Shapely versions\n        def minimum_rotated_rectangle(geom):\n            \"\"\"Calculate the minimum rotated rectangle for a geometry\"\"\"\n            # For older Shapely versions, implement a simple version\n            return geom.minimum_rotated_rectangle\n\n    # Determine input type for correct return\n    is_gdf = isinstance(building_polygons, gpd.GeoDataFrame)\n\n    # Extract geometries if GeoDataFrame\n    if is_gdf:\n        geom_objects = building_polygons.geometry\n    else:\n        geom_objects = building_polygons\n\n    results = []\n\n    for building in geom_objects:\n        # 1. Analyze building characteristics\n        if not hasattr(building, \"exterior\") or building.is_empty:\n            results.append(building)\n            continue\n\n        # Calculate shape complexity metrics\n        complexity = building.length / (4 * np.sqrt(building.area))\n\n        # Calculate dominant angle\n        coords = np.array(building.exterior.coords)[:-1]\n        segments = np.diff(np.vstack([coords, coords[0]]), axis=0)\n        segment_lengths = np.sqrt(segments[:, 0] ** 2 + segments[:, 1] ** 2)\n        segment_angles = np.arctan2(segments[:, 1], segments[:, 0]) * 180 / np.pi\n\n        # Weight angles by segment length\n        hist, bins = np.histogram(\n            segment_angles % 180, bins=36, range=(0, 180), weights=segment_lengths\n        )\n        bin_centers = (bins[:-1] + bins[1:]) / 2\n        dominant_angle = bin_centers[np.argmax(hist)]\n\n        # Check if building is close to orthogonal\n        is_orthogonal = min(dominant_angle % 45, 45 - (dominant_angle % 45)) &lt; 5\n\n        # 2. Apply appropriate regularization strategy\n        if complexity &gt; 1.5:\n            # Complex buildings: use minimum rotated rectangle\n            result = minimum_rotated_rectangle(building)\n        elif is_orthogonal:\n            # Near-orthogonal buildings: orthogonalize in place\n            rotated = rotate(building, -dominant_angle, origin=\"centroid\")\n\n            # Create orthogonal hull in rotated space\n            bounds = rotated.bounds\n            ortho_hull = Polygon(\n                [\n                    (bounds[0], bounds[1]),\n                    (bounds[2], bounds[1]),\n                    (bounds[2], bounds[3]),\n                    (bounds[0], bounds[3]),\n                ]\n            )\n\n            result = rotate(ortho_hull, dominant_angle, origin=\"centroid\")\n        else:\n            # Diagonal buildings: use custom approach for diagonal buildings\n            # Rotate to align with axes\n            rotated = rotate(building, -dominant_angle, origin=\"centroid\")\n\n            # Simplify in rotated space\n            simplified = rotated.simplify(0.3, preserve_topology=True)\n\n            # Get the bounds in rotated space\n            bounds = simplified.bounds\n            min_x, min_y, max_x, max_y = bounds\n\n            # Create a rectangular hull in rotated space\n            rect_poly = Polygon(\n                [(min_x, min_y), (max_x, min_y), (max_x, max_y), (min_x, max_y)]\n            )\n\n            # Rotate back to original orientation\n            result = rotate(rect_poly, dominant_angle, origin=\"centroid\")\n\n        results.append(result)\n\n    # Return in same format as input\n    if is_gdf:\n        return gpd.GeoDataFrame(geometry=results, crs=building_polygons.crs)\n    else:\n        return results\n</code></pre>"},{"location":"utils/#geoai.utils.install_package","title":"<code>install_package(package)</code>","text":"<p>Install a Python package.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str | list</code> <p>The package name or a GitHub URL or a list of package names or GitHub URLs.</p> required Source code in <code>geoai/utils.py</code> <pre><code>def install_package(package):\n    \"\"\"Install a Python package.\n\n    Args:\n        package (str | list): The package name or a GitHub URL or a list of package names or GitHub URLs.\n    \"\"\"\n    import subprocess\n\n    if isinstance(package, str):\n        packages = [package]\n    elif isinstance(package, list):\n        packages = package\n    else:\n        raise ValueError(\"The package argument must be a string or a list of strings.\")\n\n    for package in packages:\n        if package.startswith(\"https\"):\n            package = f\"git+{package}\"\n\n        # Execute pip install command and show output in real-time\n        command = f\"pip install {package}\"\n        process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n\n        # Print output in real-time\n        while True:\n            output = process.stdout.readline()\n            if output == b\"\" and process.poll() is not None:\n                break\n            if output:\n                print(output.decode(\"utf-8\").strip())\n\n        # Wait for process to complete\n        process.wait()\n</code></pre>"},{"location":"utils/#geoai.utils.masks_to_vector","title":"<code>masks_to_vector(mask_path, output_path=None, simplify_tolerance=1.0, mask_threshold=0.5, min_object_area=100, max_object_area=None, nms_iou_threshold=0.5)</code>","text":"<p>Convert a building mask GeoTIFF to vector polygons and save as a vector dataset.</p> <p>Parameters:</p> Name Type Description Default <code>mask_path</code> <p>Path to the building masks GeoTIFF</p> required <code>output_path</code> <p>Path to save the output GeoJSON (default: mask_path with .geojson extension)</p> <code>None</code> <code>simplify_tolerance</code> <p>Tolerance for polygon simplification (default: self.simplify_tolerance)</p> <code>1.0</code> <code>mask_threshold</code> <p>Threshold for mask binarization (default: self.mask_threshold)</p> <code>0.5</code> <code>min_object_area</code> <p>Minimum area in pixels to keep a building (default: self.min_object_area)</p> <code>100</code> <code>max_object_area</code> <p>Maximum area in pixels to keep a building (default: self.max_object_area)</p> <code>None</code> <code>nms_iou_threshold</code> <p>IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)</p> <code>0.5</code> <p>Returns:</p> Type Description <p>GeoDataFrame with building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def masks_to_vector(\n    mask_path,\n    output_path=None,\n    simplify_tolerance=1.0,\n    mask_threshold=0.5,\n    min_object_area=100,\n    max_object_area=None,\n    nms_iou_threshold=0.5,\n):\n    \"\"\"\n    Convert a building mask GeoTIFF to vector polygons and save as a vector dataset.\n\n    Args:\n        mask_path: Path to the building masks GeoTIFF\n        output_path: Path to save the output GeoJSON (default: mask_path with .geojson extension)\n        simplify_tolerance: Tolerance for polygon simplification (default: self.simplify_tolerance)\n        mask_threshold: Threshold for mask binarization (default: self.mask_threshold)\n        min_object_area: Minimum area in pixels to keep a building (default: self.min_object_area)\n        max_object_area: Maximum area in pixels to keep a building (default: self.max_object_area)\n        nms_iou_threshold: IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)\n\n    Returns:\n        GeoDataFrame with building footprints\n    \"\"\"\n    # Set default output path if not provided\n    # if output_path is None:\n    #     output_path = os.path.splitext(mask_path)[0] + \".geojson\"\n\n    print(f\"Converting mask to GeoJSON with parameters:\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n    print(f\"- Min building area: {min_object_area}\")\n    print(f\"- Simplify tolerance: {simplify_tolerance}\")\n    print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n\n    # Open the mask raster\n    with rasterio.open(mask_path) as src:\n        # Read the mask data\n        mask_data = src.read(1)\n        transform = src.transform\n        crs = src.crs\n\n        # Print mask statistics\n        print(f\"Mask dimensions: {mask_data.shape}\")\n        print(f\"Mask value range: {mask_data.min()} to {mask_data.max()}\")\n\n        # Prepare for connected component analysis\n        # Binarize the mask based on threshold\n        binary_mask = (mask_data &gt; (mask_threshold * 255)).astype(np.uint8)\n\n        # Apply morphological operations for better results (optional)\n        kernel = np.ones((3, 3), np.uint8)\n        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find connected components\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n            binary_mask, connectivity=8\n        )\n\n        print(f\"Found {num_labels-1} potential buildings\")  # Subtract 1 for background\n\n        # Create list to store polygons and confidence values\n        all_polygons = []\n        all_confidences = []\n\n        # Process each component (skip the first one which is background)\n        for i in tqdm(range(1, num_labels)):\n            # Extract this building\n            area = stats[i, cv2.CC_STAT_AREA]\n\n            # Skip if too small\n            if area &lt; min_object_area:\n                continue\n\n            # Skip if too large\n            if max_object_area is not None and area &gt; max_object_area:\n                continue\n\n            # Create a mask for this building\n            building_mask = (labels == i).astype(np.uint8)\n\n            # Find contours\n            contours, _ = cv2.findContours(\n                building_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n            )\n\n            # Process each contour\n            for contour in contours:\n                # Skip if too few points\n                if contour.shape[0] &lt; 3:\n                    continue\n\n                # Simplify contour if it has many points\n                if contour.shape[0] &gt; 50 and simplify_tolerance &gt; 0:\n                    epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                    contour = cv2.approxPolyDP(contour, epsilon, True)\n\n                # Convert to list of (x, y) coordinates\n                polygon_points = contour.reshape(-1, 2)\n\n                # Convert pixel coordinates to geographic coordinates\n                geo_points = []\n                for x, y in polygon_points:\n                    gx, gy = transform * (x, y)\n                    geo_points.append((gx, gy))\n\n                # Create Shapely polygon\n                if len(geo_points) &gt;= 3:\n                    try:\n                        shapely_poly = Polygon(geo_points)\n                        if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                            all_polygons.append(shapely_poly)\n\n                            # Calculate \"confidence\" as normalized size\n                            # This is a proxy since we don't have model confidence scores\n                            normalized_size = min(1.0, area / 1000)  # Cap at 1.0\n                            all_confidences.append(normalized_size)\n                    except Exception as e:\n                        print(f\"Error creating polygon: {e}\")\n\n        print(f\"Created {len(all_polygons)} valid polygons\")\n\n        # Create GeoDataFrame\n        if not all_polygons:\n            print(\"No valid polygons found\")\n            return None\n\n        gdf = gpd.GeoDataFrame(\n            {\n                \"geometry\": all_polygons,\n                \"confidence\": all_confidences,\n                \"class\": 1,  # Building class\n            },\n            crs=crs,\n        )\n\n        def filter_overlapping_polygons(gdf, **kwargs):\n            \"\"\"\n            Filter overlapping polygons using non-maximum suppression.\n\n            Args:\n                gdf: GeoDataFrame with polygons\n                **kwargs: Optional parameters:\n                    nms_iou_threshold: IoU threshold for filtering\n\n            Returns:\n                Filtered GeoDataFrame\n            \"\"\"\n            if len(gdf) &lt;= 1:\n                return gdf\n\n            # Get parameters from kwargs or use instance defaults\n            iou_threshold = kwargs.get(\"nms_iou_threshold\", nms_iou_threshold)\n\n            # Sort by confidence\n            gdf = gdf.sort_values(\"confidence\", ascending=False)\n\n            # Fix any invalid geometries\n            gdf[\"geometry\"] = gdf[\"geometry\"].apply(\n                lambda geom: geom.buffer(0) if not geom.is_valid else geom\n            )\n\n            keep_indices = []\n            polygons = gdf.geometry.values\n\n            for i in range(len(polygons)):\n                if i in keep_indices:\n                    continue\n\n                keep = True\n                for j in keep_indices:\n                    # Skip invalid geometries\n                    if not polygons[i].is_valid or not polygons[j].is_valid:\n                        continue\n\n                    # Calculate IoU\n                    try:\n                        intersection = polygons[i].intersection(polygons[j]).area\n                        union = polygons[i].area + polygons[j].area - intersection\n                        iou = intersection / union if union &gt; 0 else 0\n\n                        if iou &gt; iou_threshold:\n                            keep = False\n                            break\n                    except Exception:\n                        # Skip on topology exceptions\n                        continue\n\n                if keep:\n                    keep_indices.append(i)\n\n            return gdf.iloc[keep_indices]\n\n        # Apply non-maximum suppression to remove overlapping polygons\n        gdf = filter_overlapping_polygons(gdf, nms_iou_threshold=nms_iou_threshold)\n\n        print(f\"Final building count after filtering: {len(gdf)}\")\n\n        # Save to file\n        if output_path is not None:\n            gdf.to_file(output_path)\n            print(f\"Saved {len(gdf)} building footprints to {output_path}\")\n\n        return gdf\n</code></pre>"},{"location":"utils/#geoai.utils.plot_batch","title":"<code>plot_batch(batch, bright=1.0, cols=4, width=5, chnls=[2, 1, 0], cmap='Blues')</code>","text":"<p>Plot a batch of images and masks. This function is adapted from the plot_batch() function in the torchgeo library at https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html Credit to the torchgeo developers for the original implementation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch containing images and masks.</p> required <code>bright</code> <code>float</code> <p>The brightness factor. Defaults to 1.0.</p> <code>1.0</code> <code>cols</code> <code>int</code> <p>The number of columns in the plot grid. Defaults to 4.</p> <code>4</code> <code>width</code> <code>int</code> <p>The width of each plot. Defaults to 5.</p> <code>5</code> <code>chnls</code> <code>List[int]</code> <p>The channels to use for RGB. Defaults to [2, 1, 0].</p> <code>[2, 1, 0]</code> <code>cmap</code> <code>str</code> <p>The colormap to use for masks. Defaults to \"Blues\".</p> <code>'Blues'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_batch(\n    batch: Dict[str, Any],\n    bright: float = 1.0,\n    cols: int = 4,\n    width: int = 5,\n    chnls: List[int] = [2, 1, 0],\n    cmap: str = \"Blues\",\n) -&gt; None:\n    \"\"\"\n    Plot a batch of images and masks. This function is adapted from the plot_batch()\n    function in the torchgeo library at\n    https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html\n    Credit to the torchgeo developers for the original implementation.\n\n    Args:\n        batch (Dict[str, Any]): The batch containing images and masks.\n        bright (float, optional): The brightness factor. Defaults to 1.0.\n        cols (int, optional): The number of columns in the plot grid. Defaults to 4.\n        width (int, optional): The width of each plot. Defaults to 5.\n        chnls (List[int], optional): The channels to use for RGB. Defaults to [2, 1, 0].\n        cmap (str, optional): The colormap to use for masks. Defaults to \"Blues\".\n\n    Returns:\n        None\n    \"\"\"\n    # Get the samples and the number of items in the batch\n    samples = unbind_samples(batch.copy())\n\n    # if batch contains images and masks, the number of images will be doubled\n    n = 2 * len(samples) if (\"image\" in batch) and (\"mask\" in batch) else len(samples)\n\n    # calculate the number of rows in the grid\n    rows = n // cols + (1 if n % cols != 0 else 0)\n\n    # create a grid\n    _, axs = plt.subplots(rows, cols, figsize=(cols * width, rows * width))\n\n    if (\"image\" in batch) and (\"mask\" in batch):\n        # plot the images on the even axis\n        plot_images(\n            images=map(lambda x: x[\"image\"], samples),\n            axs=axs.reshape(-1)[::2],\n            chnls=chnls,\n            bright=bright,\n        )\n\n        # plot the masks on the odd axis\n        plot_masks(masks=map(lambda x: x[\"mask\"], samples), axs=axs.reshape(-1)[1::2])\n\n    else:\n        if \"image\" in batch:\n            plot_images(\n                images=map(lambda x: x[\"image\"], samples),\n                axs=axs.reshape(-1),\n                chnls=chnls,\n                bright=bright,\n            )\n\n        elif \"mask\" in batch:\n            plot_masks(\n                masks=map(lambda x: x[\"mask\"], samples), axs=axs.reshape(-1), cmap=cmap\n            )\n</code></pre>"},{"location":"utils/#geoai.utils.plot_images","title":"<code>plot_images(images, axs, chnls=[2, 1, 0], bright=1.0)</code>","text":"<p>Plot a list of images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Iterable[torch.Tensor]</code> <p>The images to plot.</p> required <code>axs</code> <code>Iterable[plt.Axes]</code> <p>The axes to plot the images on.</p> required <code>chnls</code> <code>List[int]</code> <p>The channels to use for RGB. Defaults to [2, 1, 0].</p> <code>[2, 1, 0]</code> <code>bright</code> <code>float</code> <p>The brightness factor. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_images(\n    images: Iterable[torch.Tensor],\n    axs: Iterable[plt.Axes],\n    chnls: List[int] = [2, 1, 0],\n    bright: float = 1.0,\n) -&gt; None:\n    \"\"\"\n    Plot a list of images.\n\n    Args:\n        images (Iterable[torch.Tensor]): The images to plot.\n        axs (Iterable[plt.Axes]): The axes to plot the images on.\n        chnls (List[int], optional): The channels to use for RGB. Defaults to [2, 1, 0].\n        bright (float, optional): The brightness factor. Defaults to 1.0.\n\n    Returns:\n        None\n    \"\"\"\n    for img, ax in zip(images, axs):\n        arr = torch.clamp(bright * img, min=0, max=1).numpy()\n        rgb = arr.transpose(1, 2, 0)[:, :, chnls]\n        ax.imshow(rgb)\n        ax.axis(\"off\")\n</code></pre>"},{"location":"utils/#geoai.utils.plot_masks","title":"<code>plot_masks(masks, axs, cmap='Blues')</code>","text":"<p>Plot a list of masks.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>Iterable[torch.Tensor]</code> <p>The masks to plot.</p> required <code>axs</code> <code>Iterable[plt.Axes]</code> <p>The axes to plot the masks on.</p> required <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \"Blues\".</p> <code>'Blues'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_masks(\n    masks: Iterable[torch.Tensor], axs: Iterable[plt.Axes], cmap: str = \"Blues\"\n) -&gt; None:\n    \"\"\"\n    Plot a list of masks.\n\n    Args:\n        masks (Iterable[torch.Tensor]): The masks to plot.\n        axs (Iterable[plt.Axes]): The axes to plot the masks on.\n        cmap (str, optional): The colormap to use. Defaults to \"Blues\".\n\n    Returns:\n        None\n    \"\"\"\n    for mask, ax in zip(masks, axs):\n        ax.imshow(mask.squeeze().numpy(), cmap=cmap)\n        ax.axis(\"off\")\n</code></pre>"},{"location":"utils/#geoai.utils.print_raster_info","title":"<code>print_raster_info(raster_path, show_preview=True, figsize=(10, 8))</code>","text":"<p>Print formatted information about a raster dataset and optionally show a preview.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <code>show_preview</code> <code>bool</code> <p>Whether to display a visual preview of the raster. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (10, 8).</p> <code>(10, 8)</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing raster information if successful, None otherwise</p> Source code in <code>geoai/utils.py</code> <pre><code>def print_raster_info(raster_path, show_preview=True, figsize=(10, 8)):\n    \"\"\"Print formatted information about a raster dataset and optionally show a preview.\n\n    Args:\n        raster_path (str): Path to the raster file\n        show_preview (bool, optional): Whether to display a visual preview of the raster.\n            Defaults to True.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n\n    Returns:\n        dict: Dictionary containing raster information if successful, None otherwise\n    \"\"\"\n    try:\n        info = get_raster_info(raster_path)\n\n        # Print basic information\n        print(f\"===== RASTER INFORMATION: {raster_path} =====\")\n        print(f\"Driver: {info['driver']}\")\n        print(f\"Dimensions: {info['width']} x {info['height']} pixels\")\n        print(f\"Number of bands: {info['count']}\")\n        print(f\"Data type: {info['dtype']}\")\n        print(f\"Coordinate Reference System: {info['crs']}\")\n        print(f\"Georeferenced Bounds: {info['bounds']}\")\n        print(f\"Pixel Resolution: {info['resolution'][0]}, {info['resolution'][1]}\")\n        print(f\"NoData Value: {info['nodata']}\")\n\n        # Print band statistics\n        print(\"\\n----- Band Statistics -----\")\n        for band_stat in info[\"band_stats\"]:\n            print(f\"Band {band_stat['band']}:\")\n            print(f\"  Min: {band_stat['min']:.2f}\")\n            print(f\"  Max: {band_stat['max']:.2f}\")\n            print(f\"  Mean: {band_stat['mean']:.2f}\")\n            print(f\"  Std Dev: {band_stat['std']:.2f}\")\n\n        # Show a preview if requested\n        if show_preview:\n            with rasterio.open(raster_path) as src:\n                # For multi-band images, show RGB composite or first band\n                if src.count &gt;= 3:\n                    # Try to show RGB composite\n                    rgb = np.dstack([src.read(i) for i in range(1, 4)])\n                    plt.figure(figsize=figsize)\n                    plt.imshow(rgb)\n                    plt.title(f\"RGB Preview: {raster_path}\")\n                else:\n                    # Show first band for single-band images\n                    plt.figure(figsize=figsize)\n                    show(\n                        src.read(1),\n                        cmap=\"viridis\",\n                        title=f\"Band 1 Preview: {raster_path}\",\n                    )\n                    plt.colorbar(label=\"Pixel Value\")\n                plt.show()\n\n    except Exception as e:\n        print(f\"Error reading raster: {str(e)}\")\n</code></pre>"},{"location":"utils/#geoai.utils.print_vector_info","title":"<code>print_vector_info(vector_path, show_preview=True, figsize=(10, 8))</code>","text":"<p>Print formatted information about a vector dataset and optionally show a preview.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <code>show_preview</code> <code>bool</code> <p>Whether to display a visual preview of the vector data. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (10, 8).</p> <code>(10, 8)</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing vector information if successful, None otherwise</p> Source code in <code>geoai/utils.py</code> <pre><code>def print_vector_info(vector_path, show_preview=True, figsize=(10, 8)):\n    \"\"\"Print formatted information about a vector dataset and optionally show a preview.\n\n    Args:\n        vector_path (str): Path to the vector file\n        show_preview (bool, optional): Whether to display a visual preview of the vector data.\n            Defaults to True.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n\n    Returns:\n        dict: Dictionary containing vector information if successful, None otherwise\n    \"\"\"\n    try:\n        info = get_vector_info(vector_path)\n\n        # Print basic information\n        print(f\"===== VECTOR INFORMATION: {vector_path} =====\")\n        print(f\"Driver: {info['driver']}\")\n        print(f\"Feature count: {info['feature_count']}\")\n        print(f\"Geometry types: {info['geometry_type']}\")\n        print(f\"Coordinate Reference System: {info['crs']}\")\n        print(f\"Bounds: {info['bounds']}\")\n        print(f\"Number of attributes: {info['attribute_count']}\")\n        print(f\"Attribute names: {', '.join(info['attribute_names'])}\")\n\n        # Print attribute statistics\n        if info[\"attribute_stats\"]:\n            print(\"\\n----- Attribute Statistics -----\")\n            for attr, stats in info[\"attribute_stats\"].items():\n                print(f\"Attribute: {attr}\")\n                for stat_name, stat_value in stats.items():\n                    print(\n                        f\"  {stat_name}: {stat_value:.4f}\"\n                        if isinstance(stat_value, float)\n                        else f\"  {stat_name}: {stat_value}\"\n                    )\n\n        # Show a preview if requested\n        if show_preview:\n            gdf = (\n                gpd.read_parquet(vector_path)\n                if vector_path.endswith(\".parquet\")\n                else gpd.read_file(vector_path)\n            )\n            fig, ax = plt.subplots(figsize=figsize)\n            gdf.plot(ax=ax, cmap=\"viridis\")\n            ax.set_title(f\"Preview: {vector_path}\")\n            plt.tight_layout()\n            plt.show()\n\n            # # Show a sample of the attribute table\n            # if not gdf.empty:\n            #     print(\"\\n----- Sample of attribute table (first 5 rows) -----\")\n            #     print(gdf.head().to_string())\n\n    except Exception as e:\n        print(f\"Error reading vector data: {str(e)}\")\n</code></pre>"},{"location":"utils/#geoai.utils.raster_to_vector","title":"<code>raster_to_vector(raster_path, output_path=None, threshold=0, min_area=10, simplify_tolerance=None, class_values=None, attribute_name='class', output_format='geojson', plot_result=False)</code>","text":"<p>Convert a raster label mask to vector polygons.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the input raster file (e.g., GeoTIFF).</p> required <code>output_path</code> <code>str</code> <p>Path to save the output vector file. If None, returns GeoDataFrame without saving.</p> <code>None</code> <code>threshold</code> <code>int/float</code> <p>Pixel values greater than this threshold will be vectorized.</p> <code>0</code> <code>min_area</code> <code>float</code> <p>Minimum polygon area in square map units to keep.</p> <code>10</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for geometry simplification. None for no simplification.</p> <code>None</code> <code>class_values</code> <code>list</code> <p>Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.</p> <code>None</code> <code>attribute_name</code> <code>str</code> <p>Name of the attribute field for the class values.</p> <code>'class'</code> <code>output_format</code> <code>str</code> <p>Format for output file - 'geojson', 'shapefile', 'gpkg'.</p> <code>'geojson'</code> <code>plot_result</code> <code>bool</code> <p>Whether to plot the resulting polygons overlaid on the raster.</p> <code>False</code> <p>Returns:</p> Type Description <code>geopandas.GeoDataFrame</code> <p>A GeoDataFrame containing the vectorized polygons.</p> Source code in <code>geoai/utils.py</code> <pre><code>def raster_to_vector(\n    raster_path,\n    output_path=None,\n    threshold=0,\n    min_area=10,\n    simplify_tolerance=None,\n    class_values=None,\n    attribute_name=\"class\",\n    output_format=\"geojson\",\n    plot_result=False,\n):\n    \"\"\"\n    Convert a raster label mask to vector polygons.\n\n    Args:\n        raster_path (str): Path to the input raster file (e.g., GeoTIFF).\n        output_path (str): Path to save the output vector file. If None, returns GeoDataFrame without saving.\n        threshold (int/float): Pixel values greater than this threshold will be vectorized.\n        min_area (float): Minimum polygon area in square map units to keep.\n        simplify_tolerance (float): Tolerance for geometry simplification. None for no simplification.\n        class_values (list): Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.\n        attribute_name (str): Name of the attribute field for the class values.\n        output_format (str): Format for output file - 'geojson', 'shapefile', 'gpkg'.\n        plot_result (bool): Whether to plot the resulting polygons overlaid on the raster.\n\n    Returns:\n        geopandas.GeoDataFrame: A GeoDataFrame containing the vectorized polygons.\n    \"\"\"\n    # Open the raster file\n    with rasterio.open(raster_path) as src:\n        # Read the data\n        data = src.read(1)\n\n        # Get metadata\n        transform = src.transform\n        crs = src.crs\n\n        # Create mask based on threshold and class values\n        if class_values is not None:\n            # Create a mask for each specified class value\n            masks = {val: (data == val) for val in class_values}\n        else:\n            # Create a mask for values above threshold\n            masks = {1: (data &gt; threshold)}\n            class_values = [1]  # Default class\n\n        # Initialize list to store features\n        all_features = []\n\n        # Process each class value\n        for class_val in class_values:\n            mask = masks[class_val]\n\n            # Vectorize the mask\n            for geom, value in features.shapes(\n                mask.astype(np.uint8), mask=mask, transform=transform\n            ):\n                # Convert to shapely geometry\n                geom = shape(geom)\n\n                # Skip small polygons\n                if geom.area &lt; min_area:\n                    continue\n\n                # Simplify geometry if requested\n                if simplify_tolerance is not None:\n                    geom = geom.simplify(simplify_tolerance)\n\n                # Add to features list with class value\n                all_features.append({\"geometry\": geom, attribute_name: class_val})\n\n        # Create GeoDataFrame\n        if all_features:\n            gdf = gpd.GeoDataFrame(all_features, crs=crs)\n        else:\n            print(\"Warning: No features were extracted from the raster.\")\n            # Return empty GeoDataFrame with correct CRS\n            gdf = gpd.GeoDataFrame([], geometry=[], crs=crs)\n\n        # Save to file if requested\n        if output_path is not None:\n            # Create directory if it doesn't exist\n            os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n\n            # Save to file based on format\n            if output_format.lower() == \"geojson\":\n                gdf.to_file(output_path, driver=\"GeoJSON\")\n            elif output_format.lower() == \"shapefile\":\n                gdf.to_file(output_path)\n            elif output_format.lower() == \"gpkg\":\n                gdf.to_file(output_path, driver=\"GPKG\")\n            else:\n                raise ValueError(f\"Unsupported output format: {output_format}\")\n\n            print(f\"Vectorized data saved to {output_path}\")\n\n        # Plot result if requested\n        if plot_result:\n            fig, ax = plt.subplots(figsize=(12, 12))\n\n            # Plot raster\n            raster_img = src.read()\n            if raster_img.shape[0] == 1:\n                plt.imshow(raster_img[0], cmap=\"viridis\", alpha=0.7)\n            else:\n                # Use first 3 bands for RGB display\n                rgb = raster_img[:3].transpose(1, 2, 0)\n                # Normalize for display\n                rgb = np.clip(rgb / rgb.max(), 0, 1)\n                plt.imshow(rgb)\n\n            # Plot vector boundaries\n            if not gdf.empty:\n                gdf.plot(ax=ax, facecolor=\"none\", edgecolor=\"red\", linewidth=2)\n\n            plt.title(\"Raster with Vectorized Boundaries\")\n            plt.axis(\"off\")\n            plt.tight_layout()\n            plt.show()\n\n        return gdf\n</code></pre>"},{"location":"utils/#geoai.utils.read_raster","title":"<code>read_raster(source, band=None, masked=True, **kwargs)</code>","text":"<p>Reads raster data from various formats using rioxarray.</p> <p>This function reads raster data from local files or URLs into a rioxarray data structure with preserved geospatial metadata.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <p>String path to the raster file or URL.</p> required <code>band</code> <p>Integer or list of integers specifying which band(s) to read. Defaults to None (all bands).</p> <code>None</code> <code>masked</code> <p>Boolean indicating whether to mask nodata values. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to rioxarray.open_rasterio.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xarray.DataArray</code> <p>A DataArray containing the raster data with geospatial     metadata preserved.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the file format is not supported or source cannot be accessed.</p> <p>Examples:</p> <p>Read a local GeoTIFF</p> <pre><code>&gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\")\n&gt;&gt;&gt;\nRead only band 1 from a remote GeoTIFF\n&gt;&gt;&gt; raster = read_raster(\"https://example.com/data.tif\", band=1)\n&gt;&gt;&gt;\nRead a raster without masking nodata values\n&gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\", masked=False)\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def read_raster(source, band=None, masked=True, **kwargs):\n    \"\"\"Reads raster data from various formats using rioxarray.\n\n    This function reads raster data from local files or URLs into a rioxarray\n    data structure with preserved geospatial metadata.\n\n    Args:\n        source: String path to the raster file or URL.\n        band: Integer or list of integers specifying which band(s) to read.\n            Defaults to None (all bands).\n        masked: Boolean indicating whether to mask nodata values.\n            Defaults to True.\n        **kwargs: Additional keyword arguments to pass to rioxarray.open_rasterio.\n\n    Returns:\n        xarray.DataArray: A DataArray containing the raster data with geospatial\n            metadata preserved.\n\n    Raises:\n        ValueError: If the file format is not supported or source cannot be accessed.\n\n    Examples:\n        Read a local GeoTIFF\n        &gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\")\n        &gt;&gt;&gt;\n        Read only band 1 from a remote GeoTIFF\n        &gt;&gt;&gt; raster = read_raster(\"https://example.com/data.tif\", band=1)\n        &gt;&gt;&gt;\n        Read a raster without masking nodata values\n        &gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\", masked=False)\n    \"\"\"\n    import urllib.parse\n    from rasterio.errors import RasterioIOError\n\n    # Determine if source is a URL or local file\n    parsed_url = urllib.parse.urlparse(source)\n    is_url = parsed_url.scheme in [\"http\", \"https\"]\n\n    # If it's a local file, check if it exists\n    if not is_url and not os.path.exists(source):\n        raise ValueError(f\"Raster file does not exist: {source}\")\n\n    try:\n        # Open the raster with rioxarray\n        raster = rxr.open_rasterio(source, masked=masked, **kwargs)\n\n        # Handle band selection if specified\n        if band is not None:\n            if isinstance(band, (list, tuple)):\n                # Convert from 1-based indexing to 0-based indexing\n                band_indices = [b - 1 for b in band]\n                raster = raster.isel(band=band_indices)\n            else:\n                # Single band selection (convert from 1-based to 0-based indexing)\n                raster = raster.isel(band=band - 1)\n\n        return raster\n\n    except RasterioIOError as e:\n        raise ValueError(f\"Could not read raster from source '{source}': {str(e)}\")\n    except Exception as e:\n        raise ValueError(f\"Error reading raster data: {str(e)}\")\n</code></pre>"},{"location":"utils/#geoai.utils.read_vector","title":"<code>read_vector(source, layer=None, **kwargs)</code>","text":"<p>Reads vector data from various formats including GeoParquet.</p> <p>This function dynamically determines the file type based on extension and reads it into a GeoDataFrame. It supports both local files and HTTP/HTTPS URLs.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <p>String path to the vector file or URL.</p> required <code>layer</code> <p>String or integer specifying which layer to read from multi-layer files (only applicable for formats like GPKG, GeoJSON, etc.). Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the underlying reader.</p> <code>{}</code> <p>Returns:</p> Type Description <code>geopandas.GeoDataFrame</code> <p>A GeoDataFrame containing the vector data.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the file format is not supported or source cannot be accessed.</p> <p>Examples:</p> <p>Read a local shapefile</p> <pre><code>&gt;&gt;&gt; gdf = read_vector(\"path/to/data.shp\")\n&gt;&gt;&gt;\nRead a GeoParquet file from URL\n&gt;&gt;&gt; gdf = read_vector(\"https://example.com/data.parquet\")\n&gt;&gt;&gt;\nRead a specific layer from a GeoPackage\n&gt;&gt;&gt; gdf = read_vector(\"path/to/data.gpkg\", layer=\"layer_name\")\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def read_vector(source, layer=None, **kwargs):\n    \"\"\"Reads vector data from various formats including GeoParquet.\n\n    This function dynamically determines the file type based on extension\n    and reads it into a GeoDataFrame. It supports both local files and HTTP/HTTPS URLs.\n\n    Args:\n        source: String path to the vector file or URL.\n        layer: String or integer specifying which layer to read from multi-layer\n            files (only applicable for formats like GPKG, GeoJSON, etc.).\n            Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the underlying reader.\n\n    Returns:\n        geopandas.GeoDataFrame: A GeoDataFrame containing the vector data.\n\n    Raises:\n        ValueError: If the file format is not supported or source cannot be accessed.\n\n    Examples:\n        Read a local shapefile\n        &gt;&gt;&gt; gdf = read_vector(\"path/to/data.shp\")\n        &gt;&gt;&gt;\n        Read a GeoParquet file from URL\n        &gt;&gt;&gt; gdf = read_vector(\"https://example.com/data.parquet\")\n        &gt;&gt;&gt;\n        Read a specific layer from a GeoPackage\n        &gt;&gt;&gt; gdf = read_vector(\"path/to/data.gpkg\", layer=\"layer_name\")\n    \"\"\"\n\n    import fiona\n    import urllib.parse\n\n    # Determine if source is a URL or local file\n    parsed_url = urllib.parse.urlparse(source)\n    is_url = parsed_url.scheme in [\"http\", \"https\"]\n\n    # If it's a local file, check if it exists\n    if not is_url and not os.path.exists(source):\n        raise ValueError(f\"File does not exist: {source}\")\n\n    # Get file extension\n    _, ext = os.path.splitext(source)\n    ext = ext.lower()\n\n    # Handle GeoParquet files\n    if ext in [\".parquet\", \".pq\", \".geoparquet\"]:\n        return gpd.read_parquet(source, **kwargs)\n\n    # Handle common vector formats\n    if ext in [\".shp\", \".geojson\", \".json\", \".gpkg\", \".gml\", \".kml\", \".gpx\"]:\n        # For formats that might have multiple layers\n        if ext in [\".gpkg\", \".gml\"] and layer is not None:\n            return gpd.read_file(source, layer=layer, **kwargs)\n        return gpd.read_file(source, **kwargs)\n\n    # Try to use fiona to identify valid layers for formats that might have them\n    # Only attempt this for local files as fiona.listlayers might not work with URLs\n    if layer is None and ext in [\".gpkg\", \".gml\"] and not is_url:\n        try:\n            layers = fiona.listlayers(source)\n            if layers:\n                return gpd.read_file(source, layer=layers[0], **kwargs)\n        except Exception:\n            # If listing layers fails, we'll fall through to the generic read attempt\n            pass\n\n    # For other formats or when layer listing fails, attempt to read using GeoPandas\n    try:\n        return gpd.read_file(source, **kwargs)\n    except Exception as e:\n        raise ValueError(f\"Could not read from source '{source}': {str(e)}\")\n</code></pre>"},{"location":"utils/#geoai.utils.region_groups","title":"<code>region_groups(image, connectivity=1, min_size=10, max_size=None, threshold=None, properties=None, intensity_image=None, out_csv=None, out_vector=None, out_image=None, **kwargs)</code>","text":"<p>Segment regions in an image and filter them based on size.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, xr.DataArray, np.ndarray]</code> <p>Input image, can be a file path, xarray DataArray, or numpy array.</p> required <code>connectivity</code> <code>int</code> <p>Connectivity for labeling. Defaults to 1 for 4-connectivity. Use 2 for 8-connectivity.</p> <code>1</code> <code>min_size</code> <code>int</code> <p>Minimum size of regions to keep. Defaults to 10.</p> <code>10</code> <code>max_size</code> <code>Optional[int]</code> <p>Maximum size of regions to keep. Defaults to None.</p> <code>None</code> <code>threshold</code> <code>Optional[int]</code> <p>Threshold for filling holes. Defaults to None, which is equal to min_size.</p> <code>None</code> <code>properties</code> <code>Optional[List[str]]</code> <p>List of properties to measure. See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops Defaults to None.</p> <code>None</code> <code>intensity_image</code> <code>Optional[Union[str, xr.DataArray, np.ndarray]]</code> <p>Intensity image to measure properties. Defaults to None.</p> <code>None</code> <code>out_csv</code> <code>Optional[str]</code> <p>Path to save the properties as a CSV file. Defaults to None.</p> <code>None</code> <code>out_vector</code> <code>Optional[str]</code> <p>Path to save the vector file. Defaults to None.</p> <code>None</code> <code>out_image</code> <code>Optional[str]</code> <p>Path to save the output image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]</code> <p>Labeled image and properties DataFrame.</p> Source code in <code>geoai/utils.py</code> <pre><code>def region_groups(\n    image: Union[str, \"xr.DataArray\", np.ndarray],\n    connectivity: int = 1,\n    min_size: int = 10,\n    max_size: Optional[int] = None,\n    threshold: Optional[int] = None,\n    properties: Optional[List[str]] = None,\n    intensity_image: Optional[Union[str, \"xr.DataArray\", np.ndarray]] = None,\n    out_csv: Optional[str] = None,\n    out_vector: Optional[str] = None,\n    out_image: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Union[Tuple[np.ndarray, \"pd.DataFrame\"], Tuple[\"xr.DataArray\", \"pd.DataFrame\"]]:\n    \"\"\"\n    Segment regions in an image and filter them based on size.\n\n    Args:\n        image (Union[str, xr.DataArray, np.ndarray]): Input image, can be a file\n            path, xarray DataArray, or numpy array.\n        connectivity (int, optional): Connectivity for labeling. Defaults to 1\n            for 4-connectivity. Use 2 for 8-connectivity.\n        min_size (int, optional): Minimum size of regions to keep. Defaults to 10.\n        max_size (Optional[int], optional): Maximum size of regions to keep.\n            Defaults to None.\n        threshold (Optional[int], optional): Threshold for filling holes.\n            Defaults to None, which is equal to min_size.\n        properties (Optional[List[str]], optional): List of properties to measure.\n            See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops\n            Defaults to None.\n        intensity_image (Optional[Union[str, xr.DataArray, np.ndarray]], optional):\n            Intensity image to measure properties. Defaults to None.\n        out_csv (Optional[str], optional): Path to save the properties as a CSV file.\n            Defaults to None.\n        out_vector (Optional[str], optional): Path to save the vector file.\n            Defaults to None.\n        out_image (Optional[str], optional): Path to save the output image.\n            Defaults to None.\n\n    Returns:\n        Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.\n    \"\"\"\n    from skimage import measure\n    import scipy.ndimage as ndi\n\n    if isinstance(image, str):\n        ds = rxr.open_rasterio(image)\n        da = ds.sel(band=1)\n        array = da.values.squeeze()\n    elif isinstance(image, xr.DataArray):\n        da = image\n        array = image.values.squeeze()\n    elif isinstance(image, np.ndarray):\n        array = image\n    else:\n        raise ValueError(\n            \"The input image must be a file path, xarray DataArray, or numpy array.\"\n        )\n\n    if threshold is None:\n        threshold = min_size\n\n    # Define a custom function to calculate median intensity\n    def intensity_median(region, intensity_image):\n        # Extract the intensity values for the region\n        return np.median(intensity_image[region])\n\n    # Add your custom function to the list of extra properties\n    if intensity_image is not None:\n        extra_props = (intensity_median,)\n    else:\n        extra_props = None\n\n    if properties is None:\n        properties = [\n            \"label\",\n            \"area\",\n            \"area_bbox\",\n            \"area_convex\",\n            \"area_filled\",\n            \"major_length\",\n            \"minor_length\",\n            \"eccentricity\",\n            \"diameter_areagth\",\n            \"extent\",\n            \"orientation\",\n            \"perimeter\",\n            \"solidity\",\n        ]\n\n        if intensity_image is not None:\n\n            properties += [\n                \"intensity_max\",\n                \"intensity_mean\",\n                \"intensity_min\",\n                \"intensity_std\",\n            ]\n\n    if intensity_image is not None:\n        if isinstance(intensity_image, str):\n            ds = rxr.open_rasterio(intensity_image)\n            intensity_da = ds.sel(band=1)\n            intensity_image = intensity_da.values.squeeze()\n        elif isinstance(intensity_image, xr.DataArray):\n            intensity_image = intensity_image.values.squeeze()\n        elif isinstance(intensity_image, np.ndarray):\n            pass\n        else:\n            raise ValueError(\n                \"The intensity_image must be a file path, xarray DataArray, or numpy array.\"\n            )\n\n    label_image = measure.label(array, connectivity=connectivity)\n    props = measure.regionprops_table(\n        label_image, properties=properties, intensity_image=intensity_image, **kwargs\n    )\n\n    df = pd.DataFrame(props)\n\n    # Get the labels of regions with area smaller than the threshold\n    small_regions = df[df[\"area\"] &lt; min_size][\"label\"].values\n    # Set the corresponding labels in the label_image to zero\n    for region_label in small_regions:\n        label_image[label_image == region_label] = 0\n\n    if max_size is not None:\n        large_regions = df[df[\"area\"] &gt; max_size][\"label\"].values\n        for region_label in large_regions:\n            label_image[label_image == region_label] = 0\n\n    # Find the background (holes) which are zeros\n    holes = label_image == 0\n\n    # Label the holes (connected components in the background)\n    labeled_holes, _ = ndi.label(holes)\n\n    # Measure properties of the labeled holes, including area and bounding box\n    hole_props = measure.regionprops(labeled_holes)\n\n    # Loop through each hole and fill it if it is smaller than the threshold\n    for prop in hole_props:\n        if prop.area &lt; threshold:\n            # Get the coordinates of the small hole\n            coords = prop.coords\n\n            # Find the surrounding region's ID (non-zero value near the hole)\n            surrounding_region_values = []\n            for coord in coords:\n                x, y = coord\n                # Get a 3x3 neighborhood around the hole pixel\n                neighbors = label_image[max(0, x - 1) : x + 2, max(0, y - 1) : y + 2]\n                # Exclude the hole pixels (zeros) and get region values\n                region_values = neighbors[neighbors != 0]\n                if region_values.size &gt; 0:\n                    surrounding_region_values.append(\n                        region_values[0]\n                    )  # Take the first non-zero value\n\n            if surrounding_region_values:\n                # Fill the hole with the mode (most frequent) of the surrounding region values\n                fill_value = max(\n                    set(surrounding_region_values), key=surrounding_region_values.count\n                )\n                label_image[coords[:, 0], coords[:, 1]] = fill_value\n\n    label_image, num_labels = measure.label(\n        label_image, connectivity=connectivity, return_num=True\n    )\n    props = measure.regionprops_table(\n        label_image,\n        properties=properties,\n        intensity_image=intensity_image,\n        extra_properties=extra_props,\n        **kwargs,\n    )\n\n    df = pd.DataFrame(props)\n    df[\"elongation\"] = df[\"major_length\"] / df[\"minor_length\"]\n\n    dtype = \"uint8\"\n    if num_labels &gt; 255 and num_labels &lt;= 65535:\n        dtype = \"uint16\"\n    elif num_labels &gt; 65535:\n        dtype = \"uint32\"\n\n    if out_csv is not None:\n        df.to_csv(out_csv, index=False)\n\n    if isinstance(image, np.ndarray):\n        return label_image, df\n    else:\n        da.values = label_image\n        if out_image is not None:\n            da.rio.to_raster(out_image, dtype=dtype)\n            if out_vector is not None:\n                tmp_vector = temp_file_path(\".gpkg\")\n                raster_to_vector(out_image, tmp_vector)\n                gdf = gpd.read_file(tmp_vector)\n                gdf[\"label\"] = gdf[\"value\"].astype(int)\n                gdf.drop(columns=[\"value\"], inplace=True)\n                gdf2 = pd.merge(gdf, df, on=\"label\", how=\"left\")\n                gdf2.to_file(out_vector)\n                gdf2.sort_values(\"label\", inplace=True)\n                df = gdf2\n        return da, df\n</code></pre>"},{"location":"utils/#geoai.utils.regularization","title":"<code>regularization(building_polygons, angle_tolerance=10, simplify_tolerance=0.5, orthogonalize=True, preserve_topology=True)</code>","text":"<p>Regularizes building footprint polygons with multiple techniques beyond minimum rotated rectangles.</p> <p>Parameters:</p> Name Type Description Default <code>building_polygons</code> <p>GeoDataFrame or list of shapely Polygons containing building footprints</p> required <code>angle_tolerance</code> <p>Degrees within which angles will be regularized to 90/180 degrees</p> <code>10</code> <code>simplify_tolerance</code> <p>Distance tolerance for Douglas-Peucker simplification</p> <code>0.5</code> <code>orthogonalize</code> <p>Whether to enforce orthogonal angles in the final polygons</p> <code>True</code> <code>preserve_topology</code> <p>Whether to preserve topology during simplification</p> <code>True</code> <p>Returns:</p> Type Description <p>GeoDataFrame or list of shapely Polygons with regularized building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def regularization(\n    building_polygons,\n    angle_tolerance=10,\n    simplify_tolerance=0.5,\n    orthogonalize=True,\n    preserve_topology=True,\n):\n    \"\"\"\n    Regularizes building footprint polygons with multiple techniques beyond minimum\n    rotated rectangles.\n\n    Args:\n        building_polygons: GeoDataFrame or list of shapely Polygons containing building footprints\n        angle_tolerance: Degrees within which angles will be regularized to 90/180 degrees\n        simplify_tolerance: Distance tolerance for Douglas-Peucker simplification\n        orthogonalize: Whether to enforce orthogonal angles in the final polygons\n        preserve_topology: Whether to preserve topology during simplification\n\n    Returns:\n        GeoDataFrame or list of shapely Polygons with regularized building footprints\n    \"\"\"\n    from shapely.geometry import Polygon, shape\n    from shapely.affinity import rotate, translate\n    from shapely import wkt\n\n    regularized_buildings = []\n\n    # Check if we're dealing with a GeoDataFrame\n    if isinstance(building_polygons, gpd.GeoDataFrame):\n        geom_objects = building_polygons.geometry\n    else:\n        geom_objects = building_polygons\n\n    for building in geom_objects:\n        # Handle potential string representations of geometries\n        if isinstance(building, str):\n            try:\n                # Try to parse as WKT\n                building = wkt.loads(building)\n            except Exception:\n                print(f\"Failed to parse geometry string: {building[:30]}...\")\n                continue\n\n        # Ensure we have a valid geometry\n        if not hasattr(building, \"simplify\"):\n            print(f\"Invalid geometry type: {type(building)}\")\n            continue\n\n        # Step 1: Simplify to remove noise and small vertices\n        simplified = building.simplify(\n            simplify_tolerance, preserve_topology=preserve_topology\n        )\n\n        if orthogonalize:\n            # Make sure we have a valid polygon with an exterior\n            if not hasattr(simplified, \"exterior\") or simplified.exterior is None:\n                print(f\"Simplified geometry has no exterior: {simplified}\")\n                regularized_buildings.append(building)  # Use original instead\n                continue\n\n            # Step 2: Get the dominant angle to rotate building\n            coords = np.array(simplified.exterior.coords)\n\n            # Make sure we have enough coordinates for angle calculation\n            if len(coords) &lt; 3:\n                print(f\"Not enough coordinates for angle calculation: {len(coords)}\")\n                regularized_buildings.append(building)  # Use original instead\n                continue\n\n            segments = np.diff(coords, axis=0)\n            angles = np.arctan2(segments[:, 1], segments[:, 0]) * 180 / np.pi\n\n            # Find most common angle classes (0, 90, 180, 270 degrees)\n            binned_angles = np.round(angles / 90) * 90\n            dominant_angle = np.bincount(binned_angles.astype(int) % 180).argmax()\n\n            # Step 3: Rotate to align with axes, regularize, then rotate back\n            rotated = rotate(simplified, -dominant_angle, origin=\"centroid\")\n\n            # Step 4: Rectify coordinates to enforce right angles\n            ext_coords = np.array(rotated.exterior.coords)\n            rect_coords = []\n\n            # Regularize each vertex to create orthogonal corners\n            for i in range(len(ext_coords) - 1):\n                rect_coords.append(ext_coords[i])\n\n                # Check if we need to add a right-angle vertex\n                angle = (\n                    np.arctan2(\n                        ext_coords[(i + 1) % (len(ext_coords) - 1), 1]\n                        - ext_coords[i, 1],\n                        ext_coords[(i + 1) % (len(ext_coords) - 1), 0]\n                        - ext_coords[i, 0],\n                    )\n                    * 180\n                    / np.pi\n                )\n\n                if abs(angle % 90) &gt; angle_tolerance and abs(angle % 90) &lt; (\n                    90 - angle_tolerance\n                ):\n                    # Add intermediate point to create right angle\n                    rect_coords.append(\n                        [\n                            ext_coords[(i + 1) % (len(ext_coords) - 1), 0],\n                            ext_coords[i, 1],\n                        ]\n                    )\n\n            # Close the polygon by adding the first point again\n            rect_coords.append(rect_coords[0])\n\n            # Create regularized polygon and rotate back\n            regularized = Polygon(rect_coords)\n            final_building = rotate(regularized, dominant_angle, origin=\"centroid\")\n        else:\n            final_building = simplified\n\n        regularized_buildings.append(final_building)\n\n    # If input was a GeoDataFrame, return a GeoDataFrame\n    if isinstance(building_polygons, gpd.GeoDataFrame):\n        return gpd.GeoDataFrame(\n            geometry=regularized_buildings, crs=building_polygons.crs\n        )\n    else:\n        return regularized_buildings\n</code></pre>"},{"location":"utils/#geoai.utils.temp_file_path","title":"<code>temp_file_path(ext)</code>","text":"<p>Returns a temporary file path.</p> <p>Parameters:</p> Name Type Description Default <code>ext</code> <code>str</code> <p>The file extension.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The temporary file path.</p> Source code in <code>geoai/utils.py</code> <pre><code>def temp_file_path(ext):\n    \"\"\"Returns a temporary file path.\n\n    Args:\n        ext (str): The file extension.\n\n    Returns:\n        str: The temporary file path.\n    \"\"\"\n\n    import tempfile\n    import uuid\n\n    if not ext.startswith(\".\"):\n        ext = \".\" + ext\n    file_id = str(uuid.uuid4())\n    file_path = os.path.join(tempfile.gettempdir(), f\"{file_id}{ext}\")\n\n    return file_path\n</code></pre>"},{"location":"utils/#geoai.utils.vector_to_raster","title":"<code>vector_to_raster(vector_path, output_path=None, reference_raster=None, attribute_field=None, output_shape=None, transform=None, pixel_size=None, bounds=None, crs=None, all_touched=False, fill_value=0, dtype=&lt;class 'numpy.uint8'&gt;, nodata=None, plot_result=False)</code>","text":"<p>Convert vector data to a raster.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str or GeoDataFrame</code> <p>Path to the input vector file or a GeoDataFrame.</p> required <code>output_path</code> <code>str</code> <p>Path to save the output raster file. If None, returns the array without saving.</p> <code>None</code> <code>reference_raster</code> <code>str</code> <p>Path to a reference raster for dimensions, transform and CRS.</p> <code>None</code> <code>attribute_field</code> <code>str</code> <p>Field name in the vector data to use for pixel values. If None, all vector features will be burned with value 1.</p> <code>None</code> <code>output_shape</code> <code>tuple</code> <p>Shape of the output raster as (height, width). Required if reference_raster is not provided.</p> <code>None</code> <code>transform</code> <code>affine.Affine</code> <p>Affine transformation matrix. Required if reference_raster is not provided.</p> <code>None</code> <code>pixel_size</code> <code>float or tuple</code> <p>Pixel size (resolution) as single value or (x_res, y_res). Used to calculate transform if transform is not provided.</p> <code>None</code> <code>bounds</code> <code>tuple</code> <p>Bounds of the output raster as (left, bottom, right, top). Used to calculate transform if transform is not provided.</p> <code>None</code> <code>crs</code> <code>str or CRS</code> <p>Coordinate reference system of the output raster. Required if reference_raster is not provided.</p> <code>None</code> <code>all_touched</code> <code>bool</code> <p>If True, all pixels touched by geometries will be burned in. If False, only pixels whose center is within the geometry will be burned in.</p> <code>False</code> <code>fill_value</code> <code>int</code> <p>Value to fill the raster with before burning in features.</p> <code>0</code> <code>dtype</code> <code>numpy.dtype</code> <p>Data type of the output raster.</p> <code>&lt;class 'numpy.uint8'&gt;</code> <code>nodata</code> <code>int</code> <p>No data value for the output raster.</p> <code>None</code> <code>plot_result</code> <code>bool</code> <p>Whether to plot the resulting raster.</p> <code>False</code> <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>The rasterized data array if output_path is None, else None.</p> Source code in <code>geoai/utils.py</code> <pre><code>def vector_to_raster(\n    vector_path,\n    output_path=None,\n    reference_raster=None,\n    attribute_field=None,\n    output_shape=None,\n    transform=None,\n    pixel_size=None,\n    bounds=None,\n    crs=None,\n    all_touched=False,\n    fill_value=0,\n    dtype=np.uint8,\n    nodata=None,\n    plot_result=False,\n):\n    \"\"\"\n    Convert vector data to a raster.\n\n    Args:\n        vector_path (str or GeoDataFrame): Path to the input vector file or a GeoDataFrame.\n        output_path (str): Path to save the output raster file. If None, returns the array without saving.\n        reference_raster (str): Path to a reference raster for dimensions, transform and CRS.\n        attribute_field (str): Field name in the vector data to use for pixel values.\n            If None, all vector features will be burned with value 1.\n        output_shape (tuple): Shape of the output raster as (height, width).\n            Required if reference_raster is not provided.\n        transform (affine.Affine): Affine transformation matrix.\n            Required if reference_raster is not provided.\n        pixel_size (float or tuple): Pixel size (resolution) as single value or (x_res, y_res).\n            Used to calculate transform if transform is not provided.\n        bounds (tuple): Bounds of the output raster as (left, bottom, right, top).\n            Used to calculate transform if transform is not provided.\n        crs (str or CRS): Coordinate reference system of the output raster.\n            Required if reference_raster is not provided.\n        all_touched (bool): If True, all pixels touched by geometries will be burned in.\n            If False, only pixels whose center is within the geometry will be burned in.\n        fill_value (int): Value to fill the raster with before burning in features.\n        dtype (numpy.dtype): Data type of the output raster.\n        nodata (int): No data value for the output raster.\n        plot_result (bool): Whether to plot the resulting raster.\n\n    Returns:\n        numpy.ndarray: The rasterized data array if output_path is None, else None.\n    \"\"\"\n    # Load vector data\n    if isinstance(vector_path, gpd.GeoDataFrame):\n        gdf = vector_path\n    else:\n        gdf = gpd.read_file(vector_path)\n\n    # Check if vector data is empty\n    if gdf.empty:\n        warnings.warn(\"The input vector data is empty. Creating an empty raster.\")\n\n    # Get CRS from vector data if not provided\n    if crs is None and reference_raster is None:\n        crs = gdf.crs\n\n    # Get transform and output shape from reference raster if provided\n    if reference_raster is not None:\n        with rasterio.open(reference_raster) as src:\n            transform = src.transform\n            output_shape = src.shape\n            crs = src.crs\n            if nodata is None:\n                nodata = src.nodata\n    else:\n        # Check if we have all required parameters\n        if transform is None:\n            if pixel_size is None or bounds is None:\n                raise ValueError(\n                    \"Either reference_raster, transform, or both pixel_size and bounds must be provided.\"\n                )\n\n            # Calculate transform from pixel size and bounds\n            if isinstance(pixel_size, (int, float)):\n                x_res = y_res = float(pixel_size)\n            else:\n                x_res, y_res = pixel_size\n                y_res = abs(y_res) * -1  # Convert to negative for north-up raster\n\n            left, bottom, right, top = bounds\n            transform = rasterio.transform.from_bounds(\n                left,\n                bottom,\n                right,\n                top,\n                int((right - left) / x_res),\n                int((top - bottom) / abs(y_res)),\n            )\n\n        if output_shape is None:\n            # Calculate output shape from bounds and pixel size\n            if bounds is None or pixel_size is None:\n                raise ValueError(\n                    \"output_shape must be provided if reference_raster is not provided and \"\n                    \"cannot be calculated from bounds and pixel_size.\"\n                )\n\n            if isinstance(pixel_size, (int, float)):\n                x_res = y_res = float(pixel_size)\n            else:\n                x_res, y_res = pixel_size\n\n            left, bottom, right, top = bounds\n            width = int((right - left) / x_res)\n            height = int((top - bottom) / abs(y_res))\n            output_shape = (height, width)\n\n    # Ensure CRS is set\n    if crs is None:\n        raise ValueError(\n            \"CRS must be provided either directly, from reference_raster, or from input vector data.\"\n        )\n\n    # Reproject vector data if its CRS doesn't match the output CRS\n    if gdf.crs != crs:\n        print(f\"Reprojecting vector data from {gdf.crs} to {crs}\")\n        gdf = gdf.to_crs(crs)\n\n    # Create empty raster filled with fill_value\n    raster_data = np.full(output_shape, fill_value, dtype=dtype)\n\n    # Burn vector features into raster\n    if not gdf.empty:\n        # Prepare shapes for burning\n        if attribute_field is not None and attribute_field in gdf.columns:\n            # Use attribute field for values\n            shapes = [\n                (geom, value) for geom, value in zip(gdf.geometry, gdf[attribute_field])\n            ]\n        else:\n            # Burn with value 1\n            shapes = [(geom, 1) for geom in gdf.geometry]\n\n        # Burn shapes into raster\n        burned = features.rasterize(\n            shapes=shapes,\n            out_shape=output_shape,\n            transform=transform,\n            fill=fill_value,\n            all_touched=all_touched,\n            dtype=dtype,\n        )\n\n        # Update raster data\n        raster_data = burned\n\n    # Save raster if output path is provided\n    if output_path is not None:\n        # Create directory if it doesn't exist\n        os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n\n        # Define metadata\n        metadata = {\n            \"driver\": \"GTiff\",\n            \"height\": output_shape[0],\n            \"width\": output_shape[1],\n            \"count\": 1,\n            \"dtype\": raster_data.dtype,\n            \"crs\": crs,\n            \"transform\": transform,\n        }\n\n        # Add nodata value if provided\n        if nodata is not None:\n            metadata[\"nodata\"] = nodata\n\n        # Write raster\n        with rasterio.open(output_path, \"w\", **metadata) as dst:\n            dst.write(raster_data, 1)\n\n        print(f\"Rasterized data saved to {output_path}\")\n\n    # Plot result if requested\n    if plot_result:\n        fig, ax = plt.subplots(figsize=(10, 10))\n\n        # Plot raster\n        im = ax.imshow(raster_data, cmap=\"viridis\")\n        plt.colorbar(im, ax=ax, label=attribute_field if attribute_field else \"Value\")\n\n        # Plot vector boundaries for reference\n        if output_path is not None:\n            # Get the extent of the raster\n            with rasterio.open(output_path) as src:\n                bounds = src.bounds\n                raster_bbox = box(*bounds)\n        else:\n            # Calculate extent from transform and shape\n            height, width = output_shape\n            left, top = transform * (0, 0)\n            right, bottom = transform * (width, height)\n            raster_bbox = box(left, bottom, right, top)\n\n        # Clip vector to raster extent for clarity in plot\n        if not gdf.empty:\n            gdf_clipped = gpd.clip(gdf, raster_bbox)\n            if not gdf_clipped.empty:\n                gdf_clipped.boundary.plot(ax=ax, color=\"red\", linewidth=1)\n\n        plt.title(\"Rasterized Vector Data\")\n        plt.tight_layout()\n        plt.show()\n\n    return raster_data\n</code></pre>"},{"location":"utils/#geoai.utils.view_image","title":"<code>view_image(image, transpose=False, bdx=None, scale_factor=1.0, figsize=(10, 5), axis_off=True, title=None, **kwargs)</code>","text":"<p>Visualize an image using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[np.ndarray, torch.Tensor]</code> <p>The image to visualize.</p> required <code>transpose</code> <code>bool</code> <p>Whether to transpose the image. Defaults to False.</p> <code>False</code> <code>bdx</code> <code>Optional[int]</code> <p>The band index to visualize. Defaults to None.</p> <code>None</code> <code>scale_factor</code> <code>float</code> <p>The scale factor to apply to the image. Defaults to 1.0.</p> <code>1.0</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>The size of the figure. Defaults to (10, 5).</p> <code>(10, 5)</code> <code>axis_off</code> <code>bool</code> <p>Whether to turn off the axis. Defaults to True.</p> <code>True</code> <code>title</code> <code>Optional[str]</code> <p>The title of the plot. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for plt.imshow().</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def view_image(\n    image: Union[np.ndarray, torch.Tensor],\n    transpose: bool = False,\n    bdx: Optional[int] = None,\n    scale_factor: float = 1.0,\n    figsize: Tuple[int, int] = (10, 5),\n    axis_off: bool = True,\n    title: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Visualize an image using matplotlib.\n\n    Args:\n        image (Union[np.ndarray, torch.Tensor]): The image to visualize.\n        transpose (bool, optional): Whether to transpose the image. Defaults to False.\n        bdx (Optional[int], optional): The band index to visualize. Defaults to None.\n        scale_factor (float, optional): The scale factor to apply to the image. Defaults to 1.0.\n        figsize (Tuple[int, int], optional): The size of the figure. Defaults to (10, 5).\n        axis_off (bool, optional): Whether to turn off the axis. Defaults to True.\n        title (Optional[str], optional): The title of the plot. Defaults to None.\n        **kwargs (Any): Additional keyword arguments for plt.imshow().\n\n    Returns:\n        None\n    \"\"\"\n\n    if isinstance(image, torch.Tensor):\n        image = image.cpu().numpy()\n    elif isinstance(image, str):\n        image = rasterio.open(image).read().transpose(1, 2, 0)\n\n    plt.figure(figsize=figsize)\n\n    if transpose:\n        image = image.transpose(1, 2, 0)\n\n    if bdx is not None:\n        image = image[:, :, bdx]\n\n    if len(image.shape) &gt; 2 and image.shape[2] &gt; 3:\n        image = image[:, :, 0:3]\n\n    if scale_factor != 1.0:\n        image = np.clip(image * scale_factor, 0, 1)\n\n    plt.imshow(image, **kwargs)\n    if axis_off:\n        plt.axis(\"off\")\n    if title is not None:\n        plt.title(title)\n    plt.show()\n    plt.close()\n</code></pre>"},{"location":"utils/#geoai.utils.view_raster","title":"<code>view_raster(source, indexes=None, colormap=None, vmin=None, vmax=None, nodata=None, attribution=None, layer_name='Raster', layer_index=None, zoom_to_layer=True, visible=True, opacity=1.0, array_args={}, client_args={'cors_all': False}, basemap='OpenStreetMap', **kwargs)</code>","text":"<p>Visualize a raster using leafmap.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the raster.</p> required <code>indexes</code> <code>Optional[int]</code> <p>The band indexes to visualize. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>Optional[str]</code> <p>The colormap to apply. Defaults to None.</p> <code>None</code> <code>vmin</code> <code>Optional[float]</code> <p>The minimum value for colormap scaling. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>Optional[float]</code> <p>The maximum value for colormap scaling. Defaults to None.</p> <code>None</code> <code>nodata</code> <code>Optional[float]</code> <p>The nodata value. Defaults to None.</p> <code>None</code> <code>attribution</code> <code>Optional[str]</code> <p>The attribution for the raster. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>Optional[str]</code> <p>The name of the layer. Defaults to \"Raster\".</p> <code>'Raster'</code> <code>layer_index</code> <code>Optional[int]</code> <p>The index of the layer. Defaults to None.</p> <code>None</code> <code>zoom_to_layer</code> <code>Optional[bool]</code> <p>Whether to zoom to the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>Optional[bool]</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>opacity</code> <code>Optional[float]</code> <p>The opacity of the layer. Defaults to 1.0.</p> <code>1.0</code> <code>array_args</code> <code>Optional[Dict]</code> <p>Additional arguments for array processing. Defaults to {}.</p> <code>{}</code> <code>client_args</code> <code>Optional[Dict]</code> <p>Additional arguments for the client. Defaults to {\"cors_all\": False}.</p> <code>{'cors_all': False}</code> <code>basemap</code> <code>Optional[str]</code> <p>The basemap to use. Defaults to \"OpenStreetMap\".</p> <code>'OpenStreetMap'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>leafmap.Map</code> <p>The map object with the raster layer added.</p> Source code in <code>geoai/utils.py</code> <pre><code>def view_raster(\n    source: str,\n    indexes: Optional[int] = None,\n    colormap: Optional[str] = None,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n    nodata: Optional[float] = None,\n    attribution: Optional[str] = None,\n    layer_name: Optional[str] = \"Raster\",\n    layer_index: Optional[int] = None,\n    zoom_to_layer: Optional[bool] = True,\n    visible: Optional[bool] = True,\n    opacity: Optional[float] = 1.0,\n    array_args: Optional[Dict] = {},\n    client_args: Optional[Dict] = {\"cors_all\": False},\n    basemap: Optional[str] = \"OpenStreetMap\",\n    **kwargs,\n):\n    \"\"\"\n    Visualize a raster using leafmap.\n\n    Args:\n        source (str): The source of the raster.\n        indexes (Optional[int], optional): The band indexes to visualize. Defaults to None.\n        colormap (Optional[str], optional): The colormap to apply. Defaults to None.\n        vmin (Optional[float], optional): The minimum value for colormap scaling. Defaults to None.\n        vmax (Optional[float], optional): The maximum value for colormap scaling. Defaults to None.\n        nodata (Optional[float], optional): The nodata value. Defaults to None.\n        attribution (Optional[str], optional): The attribution for the raster. Defaults to None.\n        layer_name (Optional[str], optional): The name of the layer. Defaults to \"Raster\".\n        layer_index (Optional[int], optional): The index of the layer. Defaults to None.\n        zoom_to_layer (Optional[bool], optional): Whether to zoom to the layer. Defaults to True.\n        visible (Optional[bool], optional): Whether the layer is visible. Defaults to True.\n        opacity (Optional[float], optional): The opacity of the layer. Defaults to 1.0.\n        array_args (Optional[Dict], optional): Additional arguments for array processing. Defaults to {}.\n        client_args (Optional[Dict], optional): Additional arguments for the client. Defaults to {\"cors_all\": False}.\n        basemap (Optional[str], optional): The basemap to use. Defaults to \"OpenStreetMap\".\n        **kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        leafmap.Map: The map object with the raster layer added.\n    \"\"\"\n\n    m = leafmap.Map(basemap=basemap)\n\n    if isinstance(source, dict):\n        source = dict_to_image(source)\n\n    m.add_raster(\n        source=source,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        attribution=attribution,\n        layer_name=layer_name,\n        layer_index=layer_index,\n        zoom_to_layer=zoom_to_layer,\n        visible=visible,\n        opacity=opacity,\n        array_args=array_args,\n        client_args=client_args,\n        **kwargs,\n    )\n    return m\n</code></pre>"},{"location":"utils/#geoai.utils.view_vector","title":"<code>view_vector(vector_data, column=None, cmap='viridis', figsize=(10, 10), title=None, legend=True, basemap=False, alpha=0.7, edge_color='black', classification='quantiles', n_classes=5, highlight_index=None, highlight_color='red', scheme=None, save_path=None, dpi=300)</code>","text":"<p>Visualize vector datasets with options for styling, classification, basemaps and more.</p> <p>This function visualizes GeoDataFrame objects with customizable symbology. It supports different vector types (points, lines, polygons), attribute-based classification, and background basemaps.</p> <p>Parameters:</p> Name Type Description Default <code>vector_data</code> <code>geopandas.GeoDataFrame</code> <p>The vector dataset to visualize.</p> required <code>column</code> <code>str</code> <p>Column to use for choropleth mapping. If None, a single color will be used. Defaults to None.</p> <code>None</code> <code>cmap</code> <code>str or matplotlib.colors.Colormap</code> <p>Colormap to use for choropleth mapping. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height) in inches. Defaults to (10, 10).</p> <code>(10, 10)</code> <code>title</code> <code>str</code> <p>Title for the plot. Defaults to None.</p> <code>None</code> <code>legend</code> <code>bool</code> <p>Whether to display a legend. Defaults to True.</p> <code>True</code> <code>basemap</code> <code>bool</code> <p>Whether to add a web basemap. Requires contextily. Defaults to False.</p> <code>False</code> <code>alpha</code> <code>float</code> <p>Transparency of the vector features, between 0-1. Defaults to 0.7.</p> <code>0.7</code> <code>edge_color</code> <code>str</code> <p>Color for feature edges. Defaults to \"black\".</p> <code>'black'</code> <code>classification</code> <code>str</code> <p>Classification method for choropleth maps. Options: \"quantiles\", \"equal_interval\", \"natural_breaks\". Defaults to \"quantiles\".</p> <code>'quantiles'</code> <code>n_classes</code> <code>int</code> <p>Number of classes for choropleth maps. Defaults to 5.</p> <code>5</code> <code>highlight_index</code> <code>list</code> <p>List of indices to highlight. Defaults to None.</p> <code>None</code> <code>highlight_color</code> <code>str</code> <p>Color to use for highlighted features. Defaults to \"red\".</p> <code>'red'</code> <code>scheme</code> <code>str</code> <p>MapClassify classification scheme. Overrides classification parameter if provided. Defaults to None.</p> <code>None</code> <code>save_path</code> <code>str</code> <p>Path to save the figure. If None, the figure is not saved. Defaults to None.</p> <code>None</code> <code>dpi</code> <code>int</code> <p>DPI for saved figure. Defaults to 300.</p> <code>300</code> <p>Returns:</p> Type Description <code>matplotlib.axes.Axes</code> <p>The Axes object containing the plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n&gt;&gt;&gt; view_vector(cities, \"population\", cmap=\"Reds\", basemap=True)\n</code></pre> <pre><code>&gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n&gt;&gt;&gt; view_vector(roads, \"type\", basemap=True, figsize=(12, 8))\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def view_vector(\n    vector_data,\n    column=None,\n    cmap=\"viridis\",\n    figsize=(10, 10),\n    title=None,\n    legend=True,\n    basemap=False,\n    alpha=0.7,\n    edge_color=\"black\",\n    classification=\"quantiles\",\n    n_classes=5,\n    highlight_index=None,\n    highlight_color=\"red\",\n    scheme=None,\n    save_path=None,\n    dpi=300,\n):\n    \"\"\"\n    Visualize vector datasets with options for styling, classification, basemaps and more.\n\n    This function visualizes GeoDataFrame objects with customizable symbology.\n    It supports different vector types (points, lines, polygons), attribute-based\n    classification, and background basemaps.\n\n    Args:\n        vector_data (geopandas.GeoDataFrame): The vector dataset to visualize.\n        column (str, optional): Column to use for choropleth mapping. If None,\n            a single color will be used. Defaults to None.\n        cmap (str or matplotlib.colors.Colormap, optional): Colormap to use for\n            choropleth mapping. Defaults to \"viridis\".\n        figsize (tuple, optional): Figure size as (width, height) in inches.\n            Defaults to (10, 10).\n        title (str, optional): Title for the plot. Defaults to None.\n        legend (bool, optional): Whether to display a legend. Defaults to True.\n        basemap (bool, optional): Whether to add a web basemap. Requires contextily.\n            Defaults to False.\n        alpha (float, optional): Transparency of the vector features, between 0-1.\n            Defaults to 0.7.\n        edge_color (str, optional): Color for feature edges. Defaults to \"black\".\n        classification (str, optional): Classification method for choropleth maps.\n            Options: \"quantiles\", \"equal_interval\", \"natural_breaks\".\n            Defaults to \"quantiles\".\n        n_classes (int, optional): Number of classes for choropleth maps.\n            Defaults to 5.\n        highlight_index (list, optional): List of indices to highlight.\n            Defaults to None.\n        highlight_color (str, optional): Color to use for highlighted features.\n            Defaults to \"red\".\n        scheme (str, optional): MapClassify classification scheme. Overrides\n            classification parameter if provided. Defaults to None.\n        save_path (str, optional): Path to save the figure. If None, the figure\n            is not saved. Defaults to None.\n        dpi (int, optional): DPI for saved figure. Defaults to 300.\n\n    Returns:\n        matplotlib.axes.Axes: The Axes object containing the plot.\n\n    Examples:\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n        &gt;&gt;&gt; view_vector(cities, \"population\", cmap=\"Reds\", basemap=True)\n\n        &gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n        &gt;&gt;&gt; view_vector(roads, \"type\", basemap=True, figsize=(12, 8))\n    \"\"\"\n    import contextily as ctx\n\n    if isinstance(vector_data, str):\n        vector_data = gpd.read_file(vector_data)\n\n    # Check if input is a GeoDataFrame\n    if not isinstance(vector_data, gpd.GeoDataFrame):\n        raise TypeError(\"Input data must be a GeoDataFrame\")\n\n    # Make a copy to avoid changing the original data\n    gdf = vector_data.copy()\n\n    # Set up figure and axis\n    fig, ax = plt.subplots(figsize=figsize)\n\n    # Determine geometry type\n    geom_type = gdf.geometry.iloc[0].geom_type\n\n    # Plotting parameters\n    plot_kwargs = {\"alpha\": alpha, \"ax\": ax}\n\n    # Set up keyword arguments based on geometry type\n    if \"Point\" in geom_type:\n        plot_kwargs[\"markersize\"] = 50\n        plot_kwargs[\"edgecolor\"] = edge_color\n    elif \"Line\" in geom_type:\n        plot_kwargs[\"linewidth\"] = 1\n    elif \"Polygon\" in geom_type:\n        plot_kwargs[\"edgecolor\"] = edge_color\n\n    # Classification options\n    if column is not None:\n        if scheme is not None:\n            # Use mapclassify scheme if provided\n            plot_kwargs[\"scheme\"] = scheme\n        else:\n            # Use classification parameter\n            if classification == \"quantiles\":\n                plot_kwargs[\"scheme\"] = \"quantiles\"\n            elif classification == \"equal_interval\":\n                plot_kwargs[\"scheme\"] = \"equal_interval\"\n            elif classification == \"natural_breaks\":\n                plot_kwargs[\"scheme\"] = \"fisher_jenks\"\n\n        plot_kwargs[\"k\"] = n_classes\n        plot_kwargs[\"cmap\"] = cmap\n        plot_kwargs[\"column\"] = column\n        plot_kwargs[\"legend\"] = legend\n\n    # Plot the main data\n    gdf.plot(**plot_kwargs)\n\n    # Highlight specific features if requested\n    if highlight_index is not None:\n        gdf.iloc[highlight_index].plot(\n            ax=ax, color=highlight_color, edgecolor=\"black\", linewidth=2, zorder=5\n        )\n\n    # Add basemap if requested\n    if basemap:\n        try:\n            ctx.add_basemap(ax, crs=gdf.crs, source=ctx.providers.OpenStreetMap.Mapnik)\n        except Exception as e:\n            print(f\"Could not add basemap: {e}\")\n\n    # Set title if provided\n    if title:\n        ax.set_title(title, fontsize=14)\n\n    # Remove axes if not needed\n    ax.set_axis_off()\n\n    # Adjust layout\n    plt.tight_layout()\n\n    # Save figure if a path is provided\n    if save_path:\n        plt.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n\n    return ax\n</code></pre>"},{"location":"utils/#geoai.utils.view_vector_interactive","title":"<code>view_vector_interactive(vector_data, layer_name='Vector Layer', tiles_args=None, **kwargs)</code>","text":"<p>Visualize vector datasets with options for styling, classification, basemaps and more.</p> <p>This function visualizes GeoDataFrame objects with customizable symbology. It supports different vector types (points, lines, polygons), attribute-based classification, and background basemaps.</p> <p>Parameters:</p> Name Type Description Default <code>vector_data</code> <code>geopandas.GeoDataFrame</code> <p>The vector dataset to visualize.</p> required <code>layer_name</code> <code>str</code> <p>The name of the layer. Defaults to \"Vector Layer\".</p> <code>'Vector Layer'</code> <code>tiles_args</code> <code>dict</code> <p>Additional arguments for the localtileserver client. get_folium_tile_layer function. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to GeoDataFrame.explore() function.</p> <code>{}</code> <code>See</code> <code>https</code> <p>//geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.explore.html</p> required <p>Returns:</p> Type Description <code>folium.Map</code> <p>The map object with the vector data added.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n&gt;&gt;&gt; view_vector_interactive(cities)\n</code></pre> <pre><code>&gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n&gt;&gt;&gt; view_vector_interactive(roads, figsize=(12, 8))\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def view_vector_interactive(\n    vector_data,\n    layer_name=\"Vector Layer\",\n    tiles_args=None,\n    **kwargs,\n):\n    \"\"\"\n    Visualize vector datasets with options for styling, classification, basemaps and more.\n\n    This function visualizes GeoDataFrame objects with customizable symbology.\n    It supports different vector types (points, lines, polygons), attribute-based\n    classification, and background basemaps.\n\n    Args:\n        vector_data (geopandas.GeoDataFrame): The vector dataset to visualize.\n        layer_name (str, optional): The name of the layer. Defaults to \"Vector Layer\".\n        tiles_args (dict, optional): Additional arguments for the localtileserver client.\n            get_folium_tile_layer function. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to GeoDataFrame.explore() function.\n        See https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.explore.html\n\n    Returns:\n        folium.Map: The map object with the vector data added.\n\n    Examples:\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n        &gt;&gt;&gt; view_vector_interactive(cities)\n\n        &gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n        &gt;&gt;&gt; view_vector_interactive(roads, figsize=(12, 8))\n    \"\"\"\n    import folium\n    import folium.plugins as plugins\n    from localtileserver import get_folium_tile_layer, TileClient\n    from leafmap import cog_tile\n\n    google_tiles = {\n        \"Roadmap\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=m&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Maps\",\n        },\n        \"Satellite\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=s&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Satellite\",\n        },\n        \"Terrain\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=p&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Terrain\",\n        },\n        \"Hybrid\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=y&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Hybrid\",\n        },\n    }\n\n    basemap_layer_name = None\n    raster_layer = None\n\n    if \"tiles\" in kwargs and isinstance(kwargs[\"tiles\"], str):\n        if kwargs[\"tiles\"].title() in google_tiles:\n            basemap_layer_name = google_tiles[kwargs[\"tiles\"].title()][\"name\"]\n            kwargs[\"tiles\"] = google_tiles[kwargs[\"tiles\"].title()][\"url\"]\n            kwargs[\"attr\"] = \"Google\"\n        elif kwargs[\"tiles\"].lower().endswith(\".tif\"):\n            if tiles_args is None:\n                tiles_args = {}\n            if kwargs[\"tiles\"].lower().startswith(\"http\"):\n                basemap_layer_name = \"Remote Raster\"\n                kwargs[\"tiles\"] = cog_tile(kwargs[\"tiles\"], **tiles_args)\n                kwargs[\"attr\"] = \"TiTiler\"\n            else:\n                basemap_layer_name = \"Local Raster\"\n                client = TileClient(kwargs[\"tiles\"])\n                raster_layer = get_folium_tile_layer(client, **tiles_args)\n                kwargs[\"tiles\"] = raster_layer.tiles\n                kwargs[\"attr\"] = \"localtileserver\"\n\n    if \"max_zoom\" not in kwargs:\n        kwargs[\"max_zoom\"] = 30\n\n    if isinstance(vector_data, str):\n        vector_data = gpd.read_file(vector_data)\n\n    # Check if input is a GeoDataFrame\n    if not isinstance(vector_data, gpd.GeoDataFrame):\n        raise TypeError(\"Input data must be a GeoDataFrame\")\n\n    layer_control = kwargs.pop(\"layer_control\", True)\n    fullscreen_control = kwargs.pop(\"fullscreen_control\", True)\n\n    m = vector_data.explore(**kwargs)\n\n    # Change the layer name\n    for layer in m._children.values():\n        if isinstance(layer, folium.GeoJson):\n            layer.layer_name = layer_name\n        if isinstance(layer, folium.TileLayer) and basemap_layer_name:\n            layer.layer_name = basemap_layer_name\n\n    if layer_control:\n        m.add_child(folium.LayerControl())\n\n    if fullscreen_control:\n        plugins.Fullscreen().add_to(m)\n\n    return m\n</code></pre>"},{"location":"utils/#geoai.utils.visualize_vector_by_attribute","title":"<code>visualize_vector_by_attribute(vector_path, attribute_name, cmap='viridis', figsize=(10, 8))</code>","text":"<p>Create a thematic map visualization of vector data based on an attribute.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <code>attribute_name</code> <code>str</code> <p>Name of the attribute to visualize</p> required <code>cmap</code> <code>str</code> <p>Matplotlib colormap name. Defaults to 'viridis'.</p> <code>'viridis'</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (10, 8).</p> <code>(10, 8)</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if visualization was successful, False otherwise</p> Source code in <code>geoai/utils.py</code> <pre><code>def visualize_vector_by_attribute(\n    vector_path, attribute_name, cmap=\"viridis\", figsize=(10, 8)\n):\n    \"\"\"Create a thematic map visualization of vector data based on an attribute.\n\n    Args:\n        vector_path (str): Path to the vector file\n        attribute_name (str): Name of the attribute to visualize\n        cmap (str, optional): Matplotlib colormap name. Defaults to 'viridis'.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n\n    Returns:\n        bool: True if visualization was successful, False otherwise\n    \"\"\"\n    try:\n        # Read the vector data\n        gdf = gpd.read_file(vector_path)\n\n        # Check if attribute exists\n        if attribute_name not in gdf.columns:\n            print(f\"Attribute '{attribute_name}' not found in the dataset\")\n            return False\n\n        # Create the plot\n        fig, ax = plt.subplots(figsize=figsize)\n\n        # Determine plot type based on data type\n        if pd.api.types.is_numeric_dtype(gdf[attribute_name]):\n            # Continuous data\n            gdf.plot(column=attribute_name, cmap=cmap, legend=True, ax=ax)\n        else:\n            # Categorical data\n            gdf.plot(column=attribute_name, categorical=True, legend=True, ax=ax)\n\n        # Add title and labels\n        ax.set_title(f\"{os.path.basename(vector_path)} - {attribute_name}\")\n        ax.set_xlabel(\"Longitude\")\n        ax.set_ylabel(\"Latitude\")\n\n        # Add basemap or additional elements if available\n        # Note: Additional options could be added here for more complex maps\n\n        plt.tight_layout()\n        plt.show()\n\n    except Exception as e:\n        print(f\"Error visualizing data: {str(e)}\")\n</code></pre>"},{"location":"examples/building_footprints_usa/","title":"Building footprints usa","text":"In\u00a0[1]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[2]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[3]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\nvector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" In\u00a0[4]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) <pre>\rnaip_train.tif:   0%|          | 0.00/12.1M [00:00&lt;?, ?B/s]</pre> <pre>\rnaip_train.tif:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 9.09M/12.1M [00:00&lt;00:00, 95.3MB/s]</pre> <pre>\rnaip_train.tif: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.1M/12.1M [00:00&lt;00:00, 100MB/s] </pre> <pre>\n</pre> In\u00a0[5]: Copied! <pre>vector_path = geoai.download_file(vector_url)\n</pre> vector_path = geoai.download_file(vector_url) <pre>\rnaip_train_buildings.geojson:   0%|          | 0.00/456k [00:00&lt;?, ?B/s]</pre> <pre>\rnaip_train_buildings.geojson:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 385k/456k [00:00&lt;00:00, 2.34MB/s]</pre> <pre>\rnaip_train_buildings.geojson: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 456k/456k [00:00&lt;00:00, 1.83MB/s]</pre> <pre>\n</pre> In\u00a0[6]: Copied! <pre>extractor = geoai.BuildingFootprintExtractor()\n</pre> extractor = geoai.BuildingFootprintExtractor() <pre>Model path not specified, downloading from Hugging Face...\n</pre> <pre>Model downloaded to: /home/runner/.cache/huggingface/hub/models--giswqs--geoai/snapshots/92a3d4371b88466e0fc1ab3b0964f45782fca4d0/building_footprints_usa.pth\nModel loaded successfully\n</pre> In\u00a0[7]: Copied! <pre>mask_path = extractor.save_masks_as_geotiff(\n    raster_path=raster_path,\n    output_path=\"building_masks.tif\",\n    confidence_threshold=0.5,\n    mask_threshold=0.5,\n)\n</pre> mask_path = extractor.save_masks_as_geotiff(     raster_path=raster_path,     output_path=\"building_masks.tif\",     confidence_threshold=0.5,     mask_threshold=0.5, ) <pre>Processing masks with parameters:\n- Confidence threshold: 0.5\n- Chip size: (512, 512)\n- Mask threshold: 0.5\nDataset initialized with 3 rows and 5 columns of chips\nImage dimensions: 2503 x 1126 pixels\nChip size: 512 x 512 pixels\nCRS: EPSG:26911\n</pre> <pre>Processing raster with 4 batches\n</pre> <pre>\r  0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>\r 25%|\u2588\u2588\u258c       | 1/4 [00:12&lt;00:36, 12.27s/it]</pre> <pre>\r 50%|\u2588\u2588\u2588\u2588\u2588     | 2/4 [00:24&lt;00:24, 12.21s/it]</pre> <pre>Resizing masks at image edges (set verbose=True for details)\n</pre> <pre>\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:35&lt;00:11, 11.85s/it]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:44&lt;00:00, 10.40s/it]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:44&lt;00:00, 11.01s/it]</pre> <pre>Object masks saved to building_masks.tif\n</pre> <pre>\n</pre> <p>Convert raster to vector</p> In\u00a0[8]: Copied! <pre>gdf = extractor.masks_to_vector(\n    mask_path=mask_path,\n    output_path=\"building_masks.geojson\",\n    simplify_tolerance=1.0,\n)\n</pre> gdf = extractor.masks_to_vector(     mask_path=mask_path,     output_path=\"building_masks.geojson\",     simplify_tolerance=1.0, ) <pre>Converting mask to GeoJSON with parameters:\n- Mask threshold: 0.5\n- Min object area: 100\n- Max object area: None\n- Simplify tolerance: 1.0\n- NMS IoU threshold: 0.5\n- Regularize objects: True\n- Angle threshold: 15\u00b0 from 90\u00b0\n- Rectangularity threshold: 70.0%\nMask dimensions: (1126, 2503)\nMask value range: 0 to 255\nFound 642 potential objects\n</pre> <pre>\r  0%|          | 0/642 [00:00&lt;?, ?it/s]</pre> <pre>\r  9%|\u2589         | 57/642 [00:00&lt;00:01, 562.63it/s]</pre> <pre>\r 18%|\u2588\u258a        | 114/642 [00:00&lt;00:00, 554.94it/s]</pre> <pre>\r 26%|\u2588\u2588\u258b       | 170/642 [00:00&lt;00:00, 549.91it/s]</pre> <pre>\r 35%|\u2588\u2588\u2588\u258c      | 226/642 [00:00&lt;00:00, 550.22it/s]</pre> <pre>\r 45%|\u2588\u2588\u2588\u2588\u258d     | 287/642 [00:00&lt;00:00, 571.09it/s]</pre> <pre>\r 54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 348/642 [00:00&lt;00:00, 581.62it/s]</pre> <pre>\r 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 407/642 [00:00&lt;00:00, 575.12it/s]</pre> <pre>\r 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 465/642 [00:00&lt;00:00, 563.55it/s]</pre> <pre>\r 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 522/642 [00:00&lt;00:00, 560.98it/s]</pre> <pre>\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 579/642 [00:01&lt;00:00, 557.66it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 642/642 [00:01&lt;00:00, 576.96it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 642/642 [00:01&lt;00:00, 567.19it/s]</pre> <pre>\n</pre> <pre>Created 623 valid polygons\n</pre> <pre>Object count after NMS filtering: 623\nRegularizing 623 objects...\n- Angle threshold: 15\u00b0 from 90\u00b0\n- Min orthogonality: 30.0% of angles\n- Min rectangularity: 70.0% of bounding box area\n</pre> <pre>\r  0%|          | 0/623 [00:00&lt;?, ?it/s]</pre> <pre>\r 21%|\u2588\u2588        | 132/623 [00:00&lt;00:00, 1313.81it/s]</pre> <pre>\r 43%|\u2588\u2588\u2588\u2588\u258e     | 268/623 [00:00&lt;00:00, 1337.09it/s]</pre> <pre>\r 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 408/623 [00:00&lt;00:00, 1362.43it/s]</pre> <pre>\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 558/623 [00:00&lt;00:00, 1413.68it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 623/623 [00:00&lt;00:00, 1404.92it/s]</pre> <pre>\n</pre> <pre>Regularization completed:\n- Total objects: 623\n- Rectangular objects: 619 (99.4%)\n- Other regularized objects: 0 (0.0%)\n- Unmodified objects: 4 (0.6%)\nSaved 623 objects to building_masks.geojson\n</pre> In\u00a0[9]: Copied! <pre>output_path = \"naip_buildings.geojson\"\ngdf = extractor.process_raster(\n    raster_path,\n    output_path=\"buildings.geojson\",\n    batch_size=4,\n    confidence_threshold=0.5,\n    overlap=0.25,\n    nms_iou_threshold=0.5,\n    min_object_area=100,\n    max_object_area=None,\n    mask_threshold=0.5,\n    simplify_tolerance=1.0,\n)\n</pre> output_path = \"naip_buildings.geojson\" gdf = extractor.process_raster(     raster_path,     output_path=\"buildings.geojson\",     batch_size=4,     confidence_threshold=0.5,     overlap=0.25,     nms_iou_threshold=0.5,     min_object_area=100,     max_object_area=None,     mask_threshold=0.5,     simplify_tolerance=1.0, ) <pre>Processing with parameters:\n- Confidence threshold: 0.5\n- Tile overlap: 0.25\n- Chip size: (512, 512)\n- NMS IoU threshold: 0.5\n- Mask threshold: 0.5\n- Min object area: 100\n- Max object area: None\n- Simplify tolerance: 1.0\n- Filter edge objects: True\n- Edge buffer size: 20 pixels\nDataset initialized with 3 rows and 5 columns of chips\nImage dimensions: 2503 x 1126 pixels\nChip size: 512 x 512 pixels\nCRS: EPSG:26911\n</pre> <pre>Processing raster with 4 batches\n</pre> <pre>\r  0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>\r 25%|\u2588\u2588\u258c       | 1/4 [00:11&lt;00:33, 11.04s/it]</pre> <pre>\r 50%|\u2588\u2588\u2588\u2588\u2588     | 2/4 [00:22&lt;00:22, 11.45s/it]</pre> <pre>\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:34&lt;00:11, 11.40s/it]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:42&lt;00:00, 10.06s/it]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:42&lt;00:00, 10.53s/it]</pre> <pre>\n</pre> <pre>Objects before filtering: 684\nObjects after filtering: 679\nSaved 679 objects to buildings.geojson\n</pre> In\u00a0[10]: Copied! <pre>gdf_regularized = extractor.regularize_buildings(\n    gdf=gdf,\n    min_area=100,\n    angle_threshold=15,\n    orthogonality_threshold=0.3,\n    rectangularity_threshold=0.7,\n)\n</pre> gdf_regularized = extractor.regularize_buildings(     gdf=gdf,     min_area=100,     angle_threshold=15,     orthogonality_threshold=0.3,     rectangularity_threshold=0.7, ) <pre>Regularizing 679 objects...\n- Angle threshold: 15\u00b0 from 90\u00b0\n- Min orthogonality: 30.0% of angles\n- Min rectangularity: 70.0% of bounding box area\n</pre> <pre>\r  0%|          | 0/679 [00:00&lt;?, ?it/s]</pre> <pre>\r 21%|\u2588\u2588        | 142/679 [00:00&lt;00:00, 1411.71it/s]</pre> <pre>\r 42%|\u2588\u2588\u2588\u2588\u258f     | 287/679 [00:00&lt;00:00, 1429.48it/s]</pre> <pre>\r 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 437/679 [00:00&lt;00:00, 1458.05it/s]</pre> <pre>\r 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 583/679 [00:00&lt;00:00, 1449.50it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 679/679 [00:00&lt;00:00, 1459.25it/s]</pre> <pre>Regularization completed:\n- Total objects: 679\n- Rectangular objects: 648 (95.4%)\n- Other regularized objects: 0 (0.0%)\n- Unmodified objects: 31 (4.6%)\n</pre> <pre>\n</pre> In\u00a0[11]: Copied! <pre>gdf.head()\n</pre> gdf.head() Out[11]: geometry confidence class 624 POLYGON ((455181.6 5277618.6, 455177.4 5277614... 0.994213 1 616 POLYGON ((454990.2 5277628.2, 454990.2 5277627... 0.992599 1 617 POLYGON ((454855.8 5277628.2, 454855.8 5277626... 0.989381 1 278 POLYGON ((454981.8 5277798, 454981.2 5277797.4... 0.986561 1 279 POLYGON ((455052 5277904.8, 455050.8 5277903.6... 0.982763 1 In\u00a0[12]: Copied! <pre>geoai.view_vector_interactive(\n    gdf, column=\"confidence\", layer_name=\"Building\", tiles=\"Satellite\"\n)\n</pre> geoai.view_vector_interactive(     gdf, column=\"confidence\", layer_name=\"Building\", tiles=\"Satellite\" ) Out[12]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[13]: Copied! <pre>geoai.view_vector_interactive(\n    gdf, column=\"confidence\", layer_name=\"Building\", tiles=raster_url\n)\n</pre> geoai.view_vector_interactive(     gdf, column=\"confidence\", layer_name=\"Building\", tiles=raster_url ) Out[13]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[14]: Copied! <pre>geoai.view_vector_interactive(\n    gdf_regularized, column=\"confidence\", layer_name=\"Building\", tiles=raster_url\n)\n</pre> geoai.view_vector_interactive(     gdf_regularized, column=\"confidence\", layer_name=\"Building\", tiles=raster_url ) Out[14]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[15]: Copied! <pre>extractor.visualize_results(raster_path, gdf, output_path=\"naip_buildings.png\")\n</pre> extractor.visualize_results(raster_path, gdf, output_path=\"naip_buildings.png\") <pre>Using confidence values (range: 0.50 - 0.99)\n</pre> <pre>Visualization saved to naip_buildings.png\n</pre> <pre>Sample visualization saved to naip_buildings_sample.png\n</pre> In\u00a0[16]: Copied! <pre>extractor.visualize_results(\n    raster_path, gdf_regularized, output_path=\"naip_buildings_regularized.png\"\n)\n</pre> extractor.visualize_results(     raster_path, gdf_regularized, output_path=\"naip_buildings_regularized.png\" ) <pre>Using confidence values (range: 0.50 - 0.99)\n</pre> <pre>Visualization saved to naip_buildings_regularized.png\n</pre> <pre>Sample visualization saved to naip_buildings_regularized_sample.png\n</pre>"},{"location":"examples/building_footprints_usa/#building-footprint-extraction-for-the-usa","title":"Building Footprint Extraction for the USA\u00b6","text":""},{"location":"examples/building_footprints_usa/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/building_footprints_usa/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/building_footprints_usa/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/building_footprints_usa/#initialize-building-footprint-extraction-pretrained-model","title":"Initialize building footprint extraction pretrained model\u00b6","text":"<p>The pretained model is adapted from the Esri building footprint extraction model for the USA. Credits to Esri for the model.</p>"},{"location":"examples/building_footprints_usa/#extract-building-footprints","title":"Extract building footprints\u00b6","text":""},{"location":"examples/building_footprints_usa/#option-1-extract-building-footprints-as-raster","title":"Option 1: Extract building footprints as raster\u00b6","text":""},{"location":"examples/building_footprints_usa/#option-2-extract-building-footprints-as-vector","title":"Option 2: Extract building footprints as vector\u00b6","text":""},{"location":"examples/building_footprints_usa/#regularize-building-footprints","title":"Regularize building footprints\u00b6","text":""},{"location":"examples/building_footprints_usa/#visualize-building-footprints","title":"Visualize building footprints\u00b6","text":""},{"location":"examples/building_regularization/","title":"Building regularization","text":"In\u00a0[1]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[2]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[3]: Copied! <pre>naip_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\nmasks_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_building_masks.tif\"\n</pre> naip_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) masks_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_building_masks.tif\" In\u00a0[4]: Copied! <pre>masks_path = geoai.download_file(masks_url)\nnaip_path = geoai.download_file(naip_url)\n</pre> masks_path = geoai.download_file(masks_url) naip_path = geoai.download_file(naip_url) <pre>\rnaip_building_masks.tif:   0%|          | 0.00/61.6k [00:00&lt;?, ?B/s]</pre> <pre>\rnaip_building_masks.tif: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61.6k/61.6k [00:00&lt;00:00, 15.1MB/s]</pre> <pre>File already exists: naip_train.tif\n</pre> <pre>\n</pre> In\u00a0[5]: Copied! <pre>geoai.view_image(masks_path, figsize=(18, 10))\n</pre> geoai.view_image(masks_path, figsize=(18, 10)) In\u00a0[6]: Copied! <pre>geoai.create_split_map(\n    left_layer=masks_url,\n    right_layer=naip_url,\n    left_label=\"Building Masks\",\n    right_label=\"NAIP Imagery\",\n    left_args={\n        \"colormap\": {\"255\": \"#0000ff\"},\n        \"opacity\": 0.5,\n    },\n    basemap=naip_url,\n)\n</pre> geoai.create_split_map(     left_layer=masks_url,     right_layer=naip_url,     left_label=\"Building Masks\",     right_label=\"NAIP Imagery\",     left_args={         \"colormap\": {\"255\": \"#0000ff\"},         \"opacity\": 0.5,     },     basemap=naip_url, ) Out[6]: In\u00a0[7]: Copied! <pre>gdf = geoai.raster_to_vector(masks_path, output_path=\"naip_building_masks.geojson\")\n</pre> gdf = geoai.raster_to_vector(masks_path, output_path=\"naip_building_masks.geojson\") <pre>Vectorized data saved to naip_building_masks.geojson\n</pre> In\u00a0[8]: Copied! <pre>geoai.view_vector_interactive(\n    gdf, style_kwds={\"color\": \"blue\", \"fillOpacity\": 0.4}, tiles=naip_url\n)\n</pre> geoai.view_vector_interactive(     gdf, style_kwds={\"color\": \"blue\", \"fillOpacity\": 0.4}, tiles=naip_url ) Out[8]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[9]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf,\n    right_layer=naip_url,\n    left_label=\"Buildings\",\n    right_label=\"NAIP Imagery\",\n    left_args={\"style\": {\"color\": \"blue\"}, \"fillOpacity\": 0.4},\n    basemap=naip_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf,     right_layer=naip_url,     left_label=\"Buildings\",     right_label=\"NAIP Imagery\",     left_args={\"style\": {\"color\": \"blue\"}, \"fillOpacity\": 0.4},     basemap=naip_url, ) Out[9]: In\u00a0[10]: Copied! <pre>gdf_regularized = geoai.regularization(\n    building_polygons=gdf,\n    angle_tolerance=10,\n    simplify_tolerance=0.5,\n    orthogonalize=True,\n    preserve_topology=True,\n)\n</pre> gdf_regularized = geoai.regularization(     building_polygons=gdf,     angle_tolerance=10,     simplify_tolerance=0.5,     orthogonalize=True,     preserve_topology=True, ) In\u00a0[11]: Copied! <pre>geoai.view_vector_interactive(\n    gdf_regularized, style_kwds={\"color\": \"red\", \"fillOpacity\": 0.4}, tiles=naip_url\n)\n</pre> geoai.view_vector_interactive(     gdf_regularized, style_kwds={\"color\": \"red\", \"fillOpacity\": 0.4}, tiles=naip_url ) Out[11]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[12]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_regularized,\n    right_layer=naip_url,\n    left_label=\"Regularized Buildings\",\n    right_label=\"NAIP Imagery\",\n    left_args={\"style\": {\"color\": \"red\"}, \"fillOpacity\": 0.4},\n    basemap=naip_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_regularized,     right_layer=naip_url,     left_label=\"Regularized Buildings\",     right_label=\"NAIP Imagery\",     left_args={\"style\": {\"color\": \"red\"}, \"fillOpacity\": 0.4},     basemap=naip_url, ) Out[12]: In\u00a0[13]: Copied! <pre>gdf_hybrid = geoai.hybrid_regularization(gdf)\n</pre> gdf_hybrid = geoai.hybrid_regularization(gdf) In\u00a0[14]: Copied! <pre>geoai.view_vector_interactive(\n    gdf_hybrid, style_kwds={\"color\": \"green\", \"fillOpacity\": 0.4}, tiles=naip_url\n)\n</pre> geoai.view_vector_interactive(     gdf_hybrid, style_kwds={\"color\": \"green\", \"fillOpacity\": 0.4}, tiles=naip_url ) Out[14]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[15]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_regularized,\n    right_layer=naip_url,\n    left_label=\"Regularized Buildings\",\n    right_label=\"NAIP Imagery\",\n    left_args={\"style\": {\"color\": \"green\", \"fillOpacity\": 0.4}},\n    basemap=naip_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_regularized,     right_layer=naip_url,     left_label=\"Regularized Buildings\",     right_label=\"NAIP Imagery\",     left_args={\"style\": {\"color\": \"green\", \"fillOpacity\": 0.4}},     basemap=naip_url, ) Out[15]: In\u00a0[16]: Copied! <pre>gdf_adaptive = geoai.adaptive_regularization(\n    building_polygons=gdf,\n    simplify_tolerance=0.5,\n    area_threshold=0.9,\n    preserve_shape=True,\n)\n</pre> gdf_adaptive = geoai.adaptive_regularization(     building_polygons=gdf,     simplify_tolerance=0.5,     area_threshold=0.9,     preserve_shape=True, ) In\u00a0[17]: Copied! <pre>geoai.view_vector_interactive(\n    gdf_adaptive, style_kwds={\"color\": \"yellow\", \"fillOpacity\": 0.4}, tiles=naip_url\n)\n</pre> geoai.view_vector_interactive(     gdf_adaptive, style_kwds={\"color\": \"yellow\", \"fillOpacity\": 0.4}, tiles=naip_url ) Out[17]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[18]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_adaptive,\n    right_layer=naip_url,\n    left_label=\"Adaptive Regularization Buildings\",\n    right_label=\"NAIP Imagery\",\n    left_args={\"style\": {\"color\": \"yellow\", \"fillOpacity\": 0.4}},\n    basemap=naip_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_adaptive,     right_layer=naip_url,     left_label=\"Adaptive Regularization Buildings\",     right_label=\"NAIP Imagery\",     left_args={\"style\": {\"color\": \"yellow\", \"fillOpacity\": 0.4}},     basemap=naip_url, ) Out[18]: In\u00a0[19]: Copied! <pre>import leafmap.foliumap as leafmap\n</pre> import leafmap.foliumap as leafmap In\u00a0[20]: Copied! <pre>m = leafmap.Map()\nm.add_basemap(\"SATELLITE\")\nm.add_gdf(gdf, layer_name=\"Original\")\nm.add_gdf(\n    gdf_regularized, style={\"color\": \"red\", \"fillOpacity\": 0}, layer_name=\"Regularized\"\n)\nm.add_gdf(gdf_hybrid, style={\"color\": \"green\", \"fillOpacity\": 0}, layer_name=\"Hybrid\")\nm.add_gdf(\n    gdf_adaptive, style={\"color\": \"yellow\", \"fillOpacity\": 0}, layer_name=\"Adaptive\"\n)\nlegend = {\n    \"Original\": \"blue\",\n    \"Regularized\": \"red\",\n    \"Hybrid\": \"green\",\n    \"Adaptive\": \"yellow\",\n}\nm.add_legend(title=\"Building Footprints\", legend_dict=legend)\nm\n</pre> m = leafmap.Map() m.add_basemap(\"SATELLITE\") m.add_gdf(gdf, layer_name=\"Original\") m.add_gdf(     gdf_regularized, style={\"color\": \"red\", \"fillOpacity\": 0}, layer_name=\"Regularized\" ) m.add_gdf(gdf_hybrid, style={\"color\": \"green\", \"fillOpacity\": 0}, layer_name=\"Hybrid\") m.add_gdf(     gdf_adaptive, style={\"color\": \"yellow\", \"fillOpacity\": 0}, layer_name=\"Adaptive\" ) legend = {     \"Original\": \"blue\",     \"Regularized\": \"red\",     \"Hybrid\": \"green\",     \"Adaptive\": \"yellow\", } m.add_legend(title=\"Building Footprints\", legend_dict=legend) m Out[20]:"},{"location":"examples/building_regularization/#building-regularization","title":"Building Regularization\u00b6","text":""},{"location":"examples/building_regularization/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/building_regularization/#import-package","title":"Import package\u00b6","text":""},{"location":"examples/building_regularization/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/building_regularization/#convert-raster-to-vector","title":"Convert raster to vector\u00b6","text":""},{"location":"examples/building_regularization/#building-regularization","title":"Building regularization\u00b6","text":""},{"location":"examples/building_regularization/#hybrid-regularization","title":"Hybrid regularization\u00b6","text":""},{"location":"examples/building_regularization/#adaptive-regularization","title":"Adaptive regularization\u00b6","text":""},{"location":"examples/building_regularization/#compare-regularization-methods","title":"Compare regularization methods\u00b6","text":""},{"location":"examples/data_visualization/","title":"Data visualization","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>from torchgeo.datasets import NAIP\nfrom torchgeo.samplers import RandomGeoSampler, GridGeoSampler\nfrom geoai.utils import view_image, view_raster, dict_to_image\nfrom geoai.download import download_naip\n</pre> from torchgeo.datasets import NAIP from torchgeo.samplers import RandomGeoSampler, GridGeoSampler from geoai.utils import view_image, view_raster, dict_to_image from geoai.download import download_naip In\u00a0[\u00a0]: Copied! <pre>root = \"naip_data\"\n</pre> root = \"naip_data\" In\u00a0[\u00a0]: Copied! <pre>bbox = (-117.6029, 47.65, -117.5936, 47.6563)\ndownloaded_files = download_naip(\n    bbox=bbox,\n    output_dir=root,\n    max_items=1,\n)\n</pre> bbox = (-117.6029, 47.65, -117.5936, 47.6563) downloaded_files = download_naip(     bbox=bbox,     output_dir=root,     max_items=1, ) <ul> <li>torchgeo.datasets.NAIP: Provides access to the National Agriculture Imagery Program (NAIP) dataset, which offers high-resolution aerial imagery across the United States.</li> <li>torchgeo.samplers: Contains sampling strategies for geospatial data:<ul> <li>RandomGeoSampler: Samples random patches from the dataset</li> <li>GridGeoSampler: Samples patches in a grid pattern with specified stride</li> </ul> </li> <li>geoai.utils: Custom utility functions for visualization:<ul> <li>view_image: Visualizes tensor images</li> <li>view_raster: Displays georeferenced data on an interactive map</li> <li>dict_to_image: Converts dictionary representation to image format</li> </ul> </li> </ul> <p>Load the NAIP dataset from the specified root directory:</p> In\u00a0[\u00a0]: Copied! <pre>dataset = NAIP(root)\n</pre> dataset = NAIP(root) <p>Examine the dataset object to understand its properties:</p> In\u00a0[\u00a0]: Copied! <pre>dataset\n</pre> dataset <p>This will display information about the NAIP dataset including available imagery dates, coverage area, and other metadata.</p> <p>Check the Coordinate Reference System (CRS) used by the dataset:</p> In\u00a0[\u00a0]: Copied! <pre>dataset.crs\n</pre> dataset.crs <p>The CRS defines how the geospatial data is projected onto a coordinate system, which is essential for accurate visualization and analysis.</p> In\u00a0[\u00a0]: Copied! <pre>train_sampler = RandomGeoSampler(dataset, size=256, length=1000)\n</pre> train_sampler = RandomGeoSampler(dataset, size=256, length=1000) <p>This creates a sampler that will randomly select 1000 patches, each 256x256 pixels in size. This sampling strategy is commonly used for training machine learning models where you need a diverse set of examples.</p> <p>Extract a bounding box from the random sampler:</p> In\u00a0[\u00a0]: Copied! <pre>train_bbox = next(iter(train_sampler))\ntrain_bbox\n</pre> train_bbox = next(iter(train_sampler)) train_bbox <p>The bounding box contains the coordinates defining the spatial extent of our randomly sampled patch.</p> <p>Load the actual image data corresponding to the randomly selected bounding box:</p> In\u00a0[\u00a0]: Copied! <pre>train_image = dataset[next(iter(train_sampler))][\"image\"]\n</pre> train_image = dataset[next(iter(train_sampler))][\"image\"] <p>Examine the complete data dictionary returned for a sample, which includes both the image and metadata:</p> In\u00a0[\u00a0]: Copied! <pre>dataset[next(iter(train_sampler))]\n</pre> dataset[next(iter(train_sampler))] <p>This returns a dictionary containing the image tensor and associated metadata such as the bounding box, CRS, and other properties.</p> In\u00a0[\u00a0]: Copied! <pre>view_image(\n    train_image, transpose=True, scale_factor=(1 / 250), title=\"Random GeoSampler\"\n)\n</pre> view_image(     train_image, transpose=True, scale_factor=(1 / 250), title=\"Random GeoSampler\" ) <ul> <li>transpose=True: Rearranges the dimensions for proper display (from [C,H,W] to [H,W,C])</li> <li>scale_factor=(1/250): Scales the pixel values for better visualization</li> <li>title=\"Random GeoSampler\": Adds a descriptive title to the plot</li> </ul> In\u00a0[\u00a0]: Copied! <pre>test_sampler = GridGeoSampler(dataset, size=256, stride=128)\n</pre> test_sampler = GridGeoSampler(dataset, size=256, stride=128) <p>This sampler extracts 256x256 pixel patches in a grid pattern with a stride of 128 pixels, meaning patches will overlap by 128 pixels. Grid sampling is typically used for inference or testing, where systematic coverage of the area is important.</p> <p>Extract a bounding box from the grid sampler:</p> In\u00a0[\u00a0]: Copied! <pre>test_bbox = next(iter(test_sampler))\ntest_bbox\n</pre> test_bbox = next(iter(test_sampler)) test_bbox <p>Load the image data for a patch selected by the grid sampler:</p> In\u00a0[\u00a0]: Copied! <pre>test_image = dataset[next(iter(test_sampler))][\"image\"]\n</pre> test_image = dataset[next(iter(test_sampler))][\"image\"] In\u00a0[\u00a0]: Copied! <pre>view_image(test_image, transpose=True, scale_factor=(1 / 250), title=\"Grid GeoSampler\")\n</pre> view_image(test_image, transpose=True, scale_factor=(1 / 250), title=\"Grid GeoSampler\") <p>The visualization shows a systematically sampled patch from the dataset.</p> In\u00a0[\u00a0]: Copied! <pre>data = dataset[next(iter(test_sampler))]\n</pre> data = dataset[next(iter(test_sampler))] <p>Visualize the raster data on an interactive map with Google Satellite imagery as the background:</p> In\u00a0[\u00a0]: Copied! <pre>view_raster(data, basemap=\"Google Satellite\")\n</pre> view_raster(data, basemap=\"Google Satellite\") <p>This interactive visualization places the sampled data in its real-world geographic context, allowing you to see how it aligns with the Google Satellite imagery.</p>"},{"location":"examples/data_visualization/#data-visualization","title":"Data visualization\u00b6","text":"<p>This notebook demonstrates how to work with geospatial imagery data using TorchGeo and GeoAI. We'll explore how to load data, sample it using different strategies, and visualize the results.</p>"},{"location":"examples/data_visualization/#install-package","title":"Install Package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/data_visualization/#importing-required-libraries","title":"Importing Required Libraries\u00b6","text":"<p>First, we import the necessary libraries for our geospatial data visualization workflow:</p>"},{"location":"examples/data_visualization/#download-naip-imagery","title":"Download NAIP imagery\u00b6","text":""},{"location":"examples/data_visualization/#setting-up-the-dataset","title":"Setting Up the Dataset\u00b6","text":""},{"location":"examples/data_visualization/#random-sampling-of-geospatial-data","title":"Random Sampling of Geospatial Data\u00b6","text":"<p>Create a random sampler to extract patches from the dataset:</p>"},{"location":"examples/data_visualization/#visualizing-randomly-sampled-data","title":"Visualizing Randomly Sampled Data\u00b6","text":"<p>Display the randomly sampled image:</p>"},{"location":"examples/data_visualization/#grid-sampling-of-geospatial-data","title":"Grid Sampling of Geospatial Data\u00b6","text":"<p>Create a grid sampler to extract patches in a systematic pattern:</p>"},{"location":"examples/data_visualization/#visualizing-grid-sampled-data","title":"Visualizing Grid Sampled Data\u00b6","text":"<p>Display the image from the grid sampler:</p>"},{"location":"examples/data_visualization/#advanced-visualization-with-geospatial-context","title":"Advanced Visualization with Geospatial Context\u00b6","text":"<p>Load a complete data sample including all metadata:</p>"},{"location":"examples/data_visualization/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ol> <li>TorchGeo provides a flexible framework for working with geospatial datasets like NAIP.</li> <li>Different sampling strategies (random vs. grid) serve different purposes in geospatial machine learning workflows.</li> <li>Visualization tools help understand the data in both pixel space (view_image) and geographic space (view_raster).</li> <li>Working with geospatial data requires attention to coordinate reference systems (CRS) and proper handling of georeferenced data.</li> </ol>"},{"location":"examples/download_data/","title":"Download data","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom geoai.download import (\n    download_naip,\n    download_overture_buildings,\n    extract_building_stats,\n)\n</pre> import leafmap from geoai.download import (     download_naip,     download_overture_buildings,     extract_building_stats, ) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[47.6526, -117.5923], zoom=16)\nm.add_basemap(\"Google Satellite\")\nm\n</pre> m = leafmap.Map(center=[47.6526, -117.5923], zoom=16) m.add_basemap(\"Google Satellite\") m <p>Use the drawing tools to draw a rectangle on the map. If no rectangle is drawn, the default ROI will be used.</p> In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = (-117.6029, 47.65, -117.5936, 47.6563)\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = (-117.6029, 47.65, -117.5936, 47.6563) In\u00a0[\u00a0]: Copied! <pre># Download NAIP imagery for the specified region\ndownloaded_files = download_naip(\n    bbox=bbox,\n    output_dir=\"naip_data\",\n    max_items=1,\n    # year=2020,\n)\n\nprint(f\"Downloaded {len(downloaded_files)} files.\")\n</pre> # Download NAIP imagery for the specified region downloaded_files = download_naip(     bbox=bbox,     output_dir=\"naip_data\",     max_items=1,     # year=2020, )  print(f\"Downloaded {len(downloaded_files)} files.\") In\u00a0[\u00a0]: Copied! <pre># Download buildings\ndata_file = download_overture_buildings(\n    bbox=bbox,\n    output_file=\"buildings.geojson\",\n    output_format=\"geojson\",\n    data_type=\"building\",\n    verbose=True,\n)\n</pre> # Download buildings data_file = download_overture_buildings(     bbox=bbox,     output_file=\"buildings.geojson\",     output_format=\"geojson\",     data_type=\"building\",     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>if data_file:\n    stats = extract_building_stats(data_file)\n    print(stats)\n</pre> if data_file:     stats = extract_building_stats(data_file)     print(stats) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"naip_data/m_4711720_sw_11_060_20230701_20230911.tif\", layer_name=\"NAIP\")\nm.add_geojson(\"buildings.geojson\", layer_name=\"Buildings\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"naip_data/m_4711720_sw_11_060_20230701_20230911.tif\", layer_name=\"NAIP\") m.add_geojson(\"buildings.geojson\", layer_name=\"Buildings\") m <p></p>"},{"location":"examples/download_data/#download-data","title":"Download Data\u00b6","text":""},{"location":"examples/download_data/#install-package","title":"Install Package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/download_data/#import-libraries","title":"Import Libraries\u00b6","text":"<p>These modules allow downloading NAIP imagery and extracting building data statistics.</p>"},{"location":"examples/download_data/#define-bounding-box","title":"Define Bounding Box\u00b6","text":"<p>Define the geographic extent (longitude and latitude) for data downloads.</p>"},{"location":"examples/download_data/#download-naip-imagery","title":"Download NAIP Imagery\u00b6","text":"<p>Fetch NAIP aerial imagery for the specified bounding box. The <code>max_items</code> parameter limits the number of downloaded files.</p>"},{"location":"examples/download_data/#download-building-data","title":"Download Building Data\u00b6","text":"<p>Retrieve building footprint data in GeoJSON format within the bounding box. The <code>verbose</code> flag provides detailed output.</p>"},{"location":"examples/download_data/#extract-building-statistics","title":"Extract Building Statistics\u00b6","text":"<p>If the building data file is successfully downloaded, extract and display relevant statistics such as area, count, and footprint details.</p>"},{"location":"examples/download_data/#visualize-datasets","title":"Visualize Datasets\u00b6","text":""},{"location":"examples/geometric_properties/","title":"Geometric properties","text":"In\u00a0[1]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[2]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[3]: Copied! <pre>vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_buildings_masks.geojson\"\nraster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\n</pre> vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_buildings_masks.geojson\" raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) In\u00a0[4]: Copied! <pre>gdf = geoai.read_vector(vector_url)\n</pre> gdf = geoai.read_vector(vector_url) In\u00a0[5]: Copied! <pre>gdf.head()\n</pre> gdf.head() Out[5]: confidence class geometry 0 0.994213 1 POLYGON ((455181.6 5277618.6, 455177.4 5277614... 1 0.992599 1 POLYGON ((454990.2 5277628.2, 454990.2 5277627... 2 0.989381 1 POLYGON ((454855.8 5277628.2, 454855.8 5277626... 3 0.986561 1 POLYGON ((454981.8 5277798, 454981.2 5277797.4... 4 0.982763 1 POLYGON ((455052 5277904.8, 455050.8 5277903.6... In\u00a0[6]: Copied! <pre>geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url) Out[6]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[7]: Copied! <pre>gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\")\n</pre> gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\") In\u00a0[8]: Copied! <pre>gdf_props.head()\n</pre> gdf_props.head() Out[8]: confidence class geometry area_m2 length_m perimeter_m area_bbox_m2 area_convex_m2 area_filled_m2 major_length_m minor_length_m eccentricity orientation elongation extent solidity complexity 0 0.994213 1 POLYGON ((455181.6 5277618.6, 455177.4 5277614... 184.50 53.407316 53.407316 241.92 189.18 184.50 19.2 12.6 0.754544 0.0 1.523810 0.762649 0.975262 1.109169 1 0.992599 1 POLYGON ((454990.2 5277628.2, 454990.2 5277627... 527.22 92.836753 92.836753 546.84 533.52 527.22 29.4 18.6 0.774435 90.0 1.580645 0.964121 0.988192 1.140562 2 0.989381 1 POLYGON ((454855.8 5277628.2, 454855.8 5277626... 599.40 97.988225 97.988225 617.40 604.44 599.40 29.4 21.0 0.699854 90.0 1.400000 0.970845 0.991662 1.129043 3 0.986561 1 POLYGON ((454981.8 5277798, 454981.2 5277797.4... 741.96 129.976450 129.976450 866.52 786.24 741.96 49.8 17.4 0.936975 0.0 2.862069 0.856253 0.943681 1.346076 4 0.982763 1 POLYGON ((455052 5277904.8, 455050.8 5277903.6... 748.98 130.122035 130.122035 876.96 795.78 748.98 50.4 17.4 0.938515 90.0 2.896552 0.854064 0.941190 1.341253 In\u00a0[9]: Copied! <pre>geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=raster_url) Out[9]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[10]: Copied! <pre>geoai.view_vector_interactive(gdf_props, column=\"elongation\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf_props, column=\"elongation\", tiles=raster_url) Out[10]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[11]: Copied! <pre>gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &lt; 2000) &amp; (gdf_props[\"elongation\"] &lt; 5)]\n</pre> gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &lt; 2000) &amp; (gdf_props[\"elongation\"] &lt; 5)] In\u00a0[12]: Copied! <pre>geoai.view_vector_interactive(gdf_filtered, column=\"elongation\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf_filtered, column=\"elongation\", tiles=raster_url) Out[12]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook"},{"location":"examples/geometric_properties/#geometric-properties","title":"Geometric Properties\u00b6","text":"<p>This notebook demonstrates how to calculate geometric properties of objects in a vector dataset and filter out unwanted objects based on these properties.</p>"},{"location":"examples/geometric_properties/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/geometric_properties/#import-package","title":"Import package\u00b6","text":""},{"location":"examples/geometric_properties/#load-data","title":"Load data\u00b6","text":""},{"location":"examples/geometric_properties/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"examples/geometric_properties/#add-geometric-properties","title":"Add geometric properties\u00b6","text":""},{"location":"examples/geometric_properties/#visualize-geometric-properties","title":"Visualize geometric properties\u00b6","text":""},{"location":"examples/geometric_properties/#filter-objects-based-on-geometric-properties","title":"Filter objects based on geometric properties\u00b6","text":""},{"location":"examples/geometric_properties/#visualize-filtered-objects","title":"Visualize filtered objects\u00b6","text":""},{"location":"examples/image_chips/","title":"Image chips","text":"In\u00a0[1]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[2]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[3]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\nvector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" In\u00a0[4]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) <pre>File already exists: naip_train.tif\n</pre> In\u00a0[5]: Copied! <pre>vector_path = geoai.download_file(vector_url)\n</pre> vector_path = geoai.download_file(vector_url) <pre>File already exists: naip_train_buildings.geojson\n</pre> In\u00a0[6]: Copied! <pre>geoai.view_image(raster_path, figsize=(18, 10))\n</pre> geoai.view_image(raster_path, figsize=(18, 10)) In\u00a0[7]: Copied! <pre>geoai.view_vector(vector_path, basemap=True, alpha=0.5, figsize=(18, 10))\n</pre> geoai.view_vector(vector_path, basemap=True, alpha=0.5, figsize=(18, 10)) Out[7]: <pre>&lt;Axes: &gt;</pre> In\u00a0[8]: Copied! <pre>geoai.view_vector_interactive(vector_path)\n</pre> geoai.view_vector_interactive(vector_path) Out[8]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[9]: Copied! <pre>output_path = vector_path.replace(\".geojson\", \".tif\")\ngeoai.vector_to_raster(vector_path, output_path, reference_raster=raster_path)\n</pre> output_path = vector_path.replace(\".geojson\", \".tif\") geoai.vector_to_raster(vector_path, output_path, reference_raster=raster_path) <pre>Reprojecting vector data from EPSG:4326 to EPSG:26911\nRasterized data saved to naip_train_buildings.tif\n</pre> Out[9]: <pre>array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)</pre> In\u00a0[10]: Copied! <pre>geoai.view_image(output_path, figsize=(18, 10))\n</pre> geoai.view_image(output_path, figsize=(18, 10)) In\u00a0[11]: Copied! <pre>tiles = geoai.export_geotiff_tiles(\n    in_raster=raster_path,\n    out_folder=\"output\",\n    in_class_data=vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n    create_overview=True,\n)\n</pre> tiles = geoai.export_geotiff_tiles(     in_raster=raster_path,     out_folder=\"output\",     in_class_data=vector_path,     tile_size=512,     stride=256,     buffer_radius=0,     create_overview=True, ) <pre>\nRaster info for naip_train.tif:\n  CRS: EPSG:26911\n  Dimensions: 2503 x 1126\n  Bounds: BoundingBox(left=454780.8, bottom=5277567.0, right=456282.6, top=5278242.6)\nLoaded 722 features from naip_train_buildings.geojson\nVector CRS: EPSG:4326\nReprojecting features from EPSG:4326 to EPSG:26911\nFound 6 unique classes: ['apartments' None 'terrace' 'detached' 'house' 'shed']\n</pre> <pre>\rGenerating tiles:   0%|          | 0/36 [00:00&lt;?, ?it/s]</pre> <pre>\rGenerated: 1, With features: 1:   3%|\u258e         | 1/36 [00:00&lt;00:03, 11.08it/s]</pre> <pre>\rGenerated: 1, With features: 1:   6%|\u258c         | 2/36 [00:00&lt;00:06,  5.52it/s]</pre> <pre>\rGenerated: 2, With features: 2:   6%|\u258c         | 2/36 [00:00&lt;00:06,  5.52it/s]</pre> <pre>\rGenerated: 3, With features: 3:   8%|\u258a         | 3/36 [00:00&lt;00:05,  5.52it/s]</pre> <pre>\rGenerated: 3, With features: 3:  11%|\u2588         | 4/36 [00:00&lt;00:04,  7.30it/s]</pre> <pre>\rGenerated: 4, With features: 4:  11%|\u2588         | 4/36 [00:00&lt;00:04,  7.30it/s]</pre> <pre>\rGenerated: 4, With features: 4:  14%|\u2588\u258d        | 5/36 [00:00&lt;00:04,  6.97it/s]</pre> <pre>\rGenerated: 5, With features: 5:  14%|\u2588\u258d        | 5/36 [00:00&lt;00:04,  6.97it/s]</pre> <pre>\rGenerated: 5, With features: 5:  17%|\u2588\u258b        | 6/36 [00:00&lt;00:04,  6.95it/s]</pre> <pre>\rGenerated: 6, With features: 6:  17%|\u2588\u258b        | 6/36 [00:00&lt;00:04,  6.95it/s]</pre> <pre>\rGenerated: 6, With features: 6:  19%|\u2588\u2589        | 7/36 [00:01&lt;00:04,  6.76it/s]</pre> <pre>\rGenerated: 7, With features: 7:  19%|\u2588\u2589        | 7/36 [00:01&lt;00:04,  6.76it/s]</pre> <pre>\rGenerated: 7, With features: 7:  22%|\u2588\u2588\u258f       | 8/36 [00:01&lt;00:04,  6.36it/s]</pre> <pre>\rGenerated: 8, With features: 8:  22%|\u2588\u2588\u258f       | 8/36 [00:01&lt;00:04,  6.36it/s]</pre> <pre>\rGenerated: 8, With features: 8:  25%|\u2588\u2588\u258c       | 9/36 [00:01&lt;00:04,  6.19it/s]</pre> <pre>\rGenerated: 9, With features: 9:  25%|\u2588\u2588\u258c       | 9/36 [00:01&lt;00:04,  6.19it/s]</pre> <pre>\rGenerated: 9, With features: 9:  28%|\u2588\u2588\u258a       | 10/36 [00:01&lt;00:03,  6.69it/s]</pre> <pre>\rGenerated: 10, With features: 10:  28%|\u2588\u2588\u258a       | 10/36 [00:01&lt;00:03,  6.69it/s]</pre> <pre>\rGenerated: 10, With features: 10:  31%|\u2588\u2588\u2588       | 11/36 [00:01&lt;00:03,  7.09it/s]</pre> <pre>\rGenerated: 11, With features: 11:  31%|\u2588\u2588\u2588       | 11/36 [00:01&lt;00:03,  7.09it/s]</pre> <pre>\rGenerated: 11, With features: 11:  33%|\u2588\u2588\u2588\u258e      | 12/36 [00:01&lt;00:03,  7.39it/s]</pre> <pre>\rGenerated: 12, With features: 12:  33%|\u2588\u2588\u2588\u258e      | 12/36 [00:01&lt;00:03,  7.39it/s]</pre> <pre>\rGenerated: 12, With features: 12:  36%|\u2588\u2588\u2588\u258c      | 13/36 [00:01&lt;00:03,  7.24it/s]</pre> <pre>\rGenerated: 13, With features: 13:  36%|\u2588\u2588\u2588\u258c      | 13/36 [00:01&lt;00:03,  7.24it/s]</pre> <pre>\rGenerated: 13, With features: 13:  39%|\u2588\u2588\u2588\u2589      | 14/36 [00:02&lt;00:03,  6.82it/s]</pre> <pre>\rGenerated: 14, With features: 14:  39%|\u2588\u2588\u2588\u2589      | 14/36 [00:02&lt;00:03,  6.82it/s]</pre> <pre>\rGenerated: 14, With features: 14:  42%|\u2588\u2588\u2588\u2588\u258f     | 15/36 [00:02&lt;00:03,  6.02it/s]</pre> <pre>\rGenerated: 15, With features: 15:  42%|\u2588\u2588\u2588\u2588\u258f     | 15/36 [00:02&lt;00:03,  6.02it/s]</pre> <pre>\rGenerated: 15, With features: 15:  44%|\u2588\u2588\u2588\u2588\u258d     | 16/36 [00:02&lt;00:03,  5.60it/s]</pre> <pre>\rGenerated: 16, With features: 16:  44%|\u2588\u2588\u2588\u2588\u258d     | 16/36 [00:02&lt;00:03,  5.60it/s]</pre> <pre>\rGenerated: 16, With features: 16:  47%|\u2588\u2588\u2588\u2588\u258b     | 17/36 [00:02&lt;00:03,  5.36it/s]</pre> <pre>\rGenerated: 17, With features: 17:  47%|\u2588\u2588\u2588\u2588\u258b     | 17/36 [00:02&lt;00:03,  5.36it/s]</pre> <pre>\rGenerated: 17, With features: 17:  50%|\u2588\u2588\u2588\u2588\u2588     | 18/36 [00:02&lt;00:03,  5.25it/s]</pre> <pre>\rGenerated: 18, With features: 18:  50%|\u2588\u2588\u2588\u2588\u2588     | 18/36 [00:02&lt;00:03,  5.25it/s]</pre> <pre>\rGenerated: 18, With features: 18:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 19/36 [00:03&lt;00:02,  5.79it/s]</pre> <pre>\rGenerated: 19, With features: 19:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 19/36 [00:03&lt;00:02,  5.79it/s]</pre> <pre>\rGenerated: 19, With features: 19:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 20/36 [00:03&lt;00:02,  6.08it/s]</pre> <pre>\rGenerated: 20, With features: 20:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 20/36 [00:03&lt;00:02,  6.08it/s]</pre> <pre>\rGenerated: 20, With features: 20:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 21/36 [00:03&lt;00:02,  6.30it/s]</pre> <pre>\rGenerated: 21, With features: 21:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 21/36 [00:03&lt;00:02,  6.30it/s]</pre> <pre>\rGenerated: 21, With features: 21:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 22/36 [00:03&lt;00:02,  6.77it/s]</pre> <pre>\rGenerated: 22, With features: 22:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 22/36 [00:03&lt;00:02,  6.77it/s]</pre> <pre>\rGenerated: 22, With features: 22:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 23/36 [00:03&lt;00:01,  7.06it/s]</pre> <pre>\rGenerated: 23, With features: 23:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 23/36 [00:03&lt;00:01,  7.06it/s]</pre> <pre>\rGenerated: 23, With features: 23:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 24/36 [00:03&lt;00:01,  6.26it/s]</pre> <pre>\rGenerated: 24, With features: 24:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 24/36 [00:03&lt;00:01,  6.26it/s]</pre> <pre>\rGenerated: 24, With features: 24:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 25/36 [00:03&lt;00:01,  6.18it/s]</pre> <pre>\rGenerated: 25, With features: 25:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 25/36 [00:03&lt;00:01,  6.18it/s]</pre> <pre>\rGenerated: 25, With features: 25:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 26/36 [00:04&lt;00:01,  5.78it/s]</pre> <pre>\rGenerated: 26, With features: 26:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 26/36 [00:04&lt;00:01,  5.78it/s]</pre> <pre>\rGenerated: 26, With features: 26:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 27/36 [00:04&lt;00:01,  5.81it/s]</pre> <pre>\rGenerated: 27, With features: 27:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 27/36 [00:04&lt;00:01,  5.81it/s]</pre> <pre>\rGenerated: 27, With features: 27:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 28/36 [00:04&lt;00:01,  6.11it/s]</pre> <pre>\rGenerated: 28, With features: 28:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 28/36 [00:04&lt;00:01,  6.11it/s]</pre> <pre>\rGenerated: 28, With features: 28:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 29/36 [00:04&lt;00:01,  5.98it/s]</pre> <pre>\rGenerated: 29, With features: 29:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 29/36 [00:04&lt;00:01,  5.98it/s]</pre> <pre>\rGenerated: 29, With features: 29:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 30/36 [00:04&lt;00:00,  6.50it/s]</pre> <pre>\rGenerated: 30, With features: 30:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 30/36 [00:04&lt;00:00,  6.50it/s]</pre> <pre>\rGenerated: 30, With features: 30:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 31/36 [00:04&lt;00:00,  7.00it/s]</pre> <pre>\rGenerated: 31, With features: 31:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 31/36 [00:04&lt;00:00,  7.00it/s]</pre> <pre>\rGenerated: 31, With features: 31:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 32/36 [00:04&lt;00:00,  7.01it/s]</pre> <pre>\rGenerated: 32, With features: 32:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 32/36 [00:04&lt;00:00,  7.01it/s]</pre> <pre>\rGenerated: 32, With features: 32:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 33/36 [00:05&lt;00:00,  6.67it/s]</pre> <pre>\rGenerated: 33, With features: 33:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 33/36 [00:05&lt;00:00,  6.67it/s]</pre> <pre>\rGenerated: 33, With features: 33:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 34/36 [00:05&lt;00:00,  6.30it/s]</pre> <pre>\rGenerated: 34, With features: 34:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 34/36 [00:05&lt;00:00,  6.30it/s]</pre> <pre>\rGenerated: 34, With features: 34:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 35/36 [00:05&lt;00:00,  6.26it/s]</pre> <pre>\rGenerated: 35, With features: 35:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 35/36 [00:05&lt;00:00,  6.26it/s]</pre> <pre>\rGenerated: 35, With features: 35: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 36/36 [00:05&lt;00:00,  5.93it/s]</pre> <pre>\rGenerated: 36, With features: 36: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 36/36 [00:05&lt;00:00,  5.93it/s]</pre> <pre>\rGenerated: 36, With features: 36: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 36/36 [00:05&lt;00:00,  6.32it/s]</pre> <pre>\n</pre> <pre>Overview image saved to output/overview.png\n\n------- Export Summary -------\nTotal tiles exported: 36\nTiles with features: 36 (100.0%)\nAverage feature pixels per tile: 45966.3\nOutput saved to: output\n\n------- Georeference Verification -------\n</pre> In\u00a0[12]: Copied! <pre>geoai.view_image(\"output/overview.png\", figsize=(18, 10))\n</pre> geoai.view_image(\"output/overview.png\", figsize=(18, 10)) In\u00a0[13]: Copied! <pre>geoai.view_image(\"output/images/tile_000000.tif\")\n</pre> geoai.view_image(\"output/images/tile_000000.tif\") In\u00a0[14]: Copied! <pre>geoai.view_image(\"output/labels/tile_000000.tif\")\n</pre> geoai.view_image(\"output/labels/tile_000000.tif\") In\u00a0[15]: Copied! <pre>geoai.view_image(\"output/images/tile_000001.tif\")\n</pre> geoai.view_image(\"output/images/tile_000001.tif\") In\u00a0[16]: Copied! <pre>geoai.view_image(\"output/labels/tile_000001.tif\")\n</pre> geoai.view_image(\"output/labels/tile_000001.tif\")"},{"location":"examples/image_chips/#generate-image-chips","title":"Generate Image Chips\u00b6","text":""},{"location":"examples/image_chips/#install-package","title":"Install Package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/image_chips/#import-packages","title":"Import Packages\u00b6","text":""},{"location":"examples/image_chips/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/image_chips/#preview-data","title":"Preview data\u00b6","text":""},{"location":"examples/image_chips/#convert-vector-to-raster","title":"Convert vector to raster\u00b6","text":""},{"location":"examples/image_chips/#generate-image-chips","title":"Generate image chips\u00b6","text":""},{"location":"examples/image_chips/#preview-image-chips","title":"Preview image chips\u00b6","text":""},{"location":"examples/view_metadata/","title":"View metadata","text":"In\u00a0[1]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py <p>Import the package</p> In\u00a0[2]: Copied! <pre>import geoai\n</pre> import geoai <p>Define URLs for sample datasets: a NAIP imagery raster file and a building footprints vector file</p> In\u00a0[3]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\nvector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" <p>Download the raster file (NAIP imagery) and save it locally</p> In\u00a0[4]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) <pre>File already exists: naip_train.tif\n</pre> <p>Download the vector file (building footprints) and save it locally</p> In\u00a0[5]: Copied! <pre>vector_path = geoai.download_file(vector_url)\n</pre> vector_path = geoai.download_file(vector_url) <pre>File already exists: naip_train_buildings.geojson\n</pre> <p>Display metadata about the raster file, including dimensions, resolution, projection, and bands</p> In\u00a0[6]: Copied! <pre>geoai.get_raster_info(raster_path)[\"band_stats\"]\n</pre> geoai.get_raster_info(raster_path)[\"band_stats\"] Out[6]: <pre>[{'band': 1,\n  'min': 12.0,\n  'max': 251.0,\n  'mean': 150.6730747259594,\n  'std': 48.01908734374099},\n {'band': 2,\n  'min': 49.0,\n  'max': 251.0,\n  'mean': 141.92468895229808,\n  'std': 43.46595463573497},\n {'band': 3,\n  'min': 53.0,\n  'max': 251.0,\n  'mean': 120.89909373405554,\n  'std': 41.78086244480775},\n {'band': 4,\n  'min': 22.0,\n  'max': 251.0,\n  'mean': 159.68995855062735,\n  'std': 54.95588423977727}]</pre> In\u00a0[7]: Copied! <pre>geoai.print_raster_info(raster_path, figsize=(18, 10))\n</pre> geoai.print_raster_info(raster_path, figsize=(18, 10)) <pre>===== RASTER INFORMATION: naip_train.tif =====\nDriver: GTiff\nDimensions: 2503 x 1126 pixels\nNumber of bands: 4\nData type: uint8\nCoordinate Reference System: EPSG:26911\nGeoreferenced Bounds: BoundingBox(left=454780.8, bottom=5277567.0, right=456282.6, top=5278242.6)\nPixel Resolution: 0.5999999999999953, 0.5999999999996691\nNoData Value: None\n\n----- Band Statistics -----\nBand 1:\n  Min: 12.00\n  Max: 251.00\n  Mean: 150.67\n  Std Dev: 48.02\nBand 2:\n  Min: 49.00\n  Max: 251.00\n  Mean: 141.92\n  Std Dev: 43.47\nBand 3:\n  Min: 53.00\n  Max: 251.00\n  Mean: 120.90\n  Std Dev: 41.78\nBand 4:\n  Min: 22.00\n  Max: 251.00\n  Mean: 159.69\n  Std Dev: 54.96\n</pre> <p>Display metadata about the vector file, including geometry type, feature count, extent, and attributes</p> In\u00a0[8]: Copied! <pre>geoai.print_vector_info(vector_path, figsize=(18, 10))\n</pre> geoai.print_vector_info(vector_path, figsize=(18, 10)) <pre>===== VECTOR INFORMATION: naip_train_buildings.geojson =====\nDriver: GEOJSON\nFeature count: 722\nGeometry types: {'Polygon': 722}\nCoordinate Reference System: EPSG:4326\nBounds: [-117.6017984, 47.65016239407519, -117.58246913360121, 47.655846]\nNumber of attributes: 8\nAttribute names: id, version, sources, subtype, class, height, has_parts, is_underground\n\n----- Attribute Statistics -----\nAttribute: version\n  min: 0\n  max: 0\n  mean: 0.0000\n  std: 0.0000\n  null_count: 0\nAttribute: height\n  min: 2.0198\n  max: 6.3780\n  mean: 4.9893\n  std: 0.8086\n  null_count: 397\n</pre> <p>Analyze the \"height\" attribute of buildings to obtain statistical information</p> In\u00a0[9]: Copied! <pre>geoai.analyze_vector_attributes(vector_path, \"height\")\n</pre> geoai.analyze_vector_attributes(vector_path, \"height\") Out[9]: <pre>{'attribute': 'height',\n 'type': 'numeric',\n 'count': np.int64(325),\n 'null_count': np.int64(397),\n 'min': np.float64(2.0197694),\n 'max': np.float64(6.3780174),\n 'mean': np.float64(4.989292356615385),\n 'median': np.float64(5.2050776),\n 'std': np.float64(0.8086136381053566),\n 'unique_values': 325}</pre> <p>Create a visualization of building footprints colored by their height values</p> In\u00a0[10]: Copied! <pre>geoai.visualize_vector_by_attribute(vector_path, \"height\")\n</pre> geoai.visualize_vector_by_attribute(vector_path, \"height\") <p>Clip the raster file to a specified extent</p> In\u00a0[11]: Copied! <pre>clip_raster_path = \"naip_clip.tif\"\ngeoai.clip_raster_by_bbox(\n    raster_path,\n    clip_raster_path,\n    bbox=(0, 0, 500, 500),\n    bands=[1, 2, 3],\n    bbox_type=\"pixel\",\n)\n</pre> clip_raster_path = \"naip_clip.tif\" geoai.clip_raster_by_bbox(     raster_path,     clip_raster_path,     bbox=(0, 0, 500, 500),     bands=[1, 2, 3],     bbox_type=\"pixel\", ) Out[11]: <pre>'naip_clip.tif'</pre> In\u00a0[12]: Copied! <pre>geoai.view_image(clip_raster_path)\n</pre> geoai.view_image(clip_raster_path)"},{"location":"examples/view_metadata/#view-metadata","title":"View Metadata\u00b6","text":"<p>Install Package</p> <p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/dataviz/lidar_viz/","title":"Lidar viz","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"leafmap[lidar]\" open3d\n</pre> # %pip install \"leafmap[lidar]\" open3d In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap <p>Download a sample LiDAR dataset from Google Drive. The zip file is 52.1 MB and the uncompressed LAS file is 109 MB.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://open.gishub.org/data/lidar/madison.zip\"\nfilename = \"madison.las\"\n</pre> url = \"https://open.gishub.org/data/lidar/madison.zip\" filename = \"madison.las\" In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(url, \"madison.zip\", unzip=True)\n</pre> leafmap.download_file(url, \"madison.zip\", unzip=True) <p>Read the LiDAR data</p> In\u00a0[\u00a0]: Copied! <pre>las = leafmap.read_lidar(filename)\n</pre> las = leafmap.read_lidar(filename) <p>The LAS header.</p> In\u00a0[\u00a0]: Copied! <pre>las.header\n</pre> las.header <p>The number of points.</p> In\u00a0[\u00a0]: Copied! <pre>las.header.point_count\n</pre> las.header.point_count <p>The list of features.</p> In\u00a0[\u00a0]: Copied! <pre>list(las.point_format.dimension_names)\n</pre> list(las.point_format.dimension_names) <p>Inspect data.</p> In\u00a0[\u00a0]: Copied! <pre>las.X\n</pre> las.X In\u00a0[\u00a0]: Copied! <pre>las.Y\n</pre> las.Y In\u00a0[\u00a0]: Copied! <pre>las.Z\n</pre> las.Z In\u00a0[\u00a0]: Copied! <pre>las.intensity\n</pre> las.intensity In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"pyvista\")\n</pre> leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"pyvista\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, backend=\"ipygany\", background=\"white\")\n</pre> leafmap.view_lidar(filename, backend=\"ipygany\", background=\"white\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"panel\", background=\"white\")\n</pre> leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"panel\", background=\"white\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, backend=\"open3d\")\n</pre> leafmap.view_lidar(filename, backend=\"open3d\") <p></p>"},{"location":"examples/dataviz/lidar_viz/#visualizing-lidar-data-with-leafmap","title":"Visualizing LiDAR Data with Leafmap\u00b6","text":"<p>This notebook demonstrates how to visualize LiDAR data using leafmap.</p>"},{"location":"examples/dataviz/lidar_viz/#installation","title":"Installation\u00b6","text":"<p>Uncomment and run the following cell to install the required Python packages.</p>"},{"location":"examples/dataviz/lidar_viz/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#download-data","title":"Download data\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#metadata","title":"Metadata\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#read-data","title":"Read data\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#pyvista","title":"PyVista\u00b6","text":"<p>Visualize LiDAR data using the pyvista backend.</p>"},{"location":"examples/dataviz/lidar_viz/#ipygany","title":"ipygany\u00b6","text":"<p>Visualize LiDAR data using the ipygany backend.</p>"},{"location":"examples/dataviz/lidar_viz/#panel","title":"Panel\u00b6","text":"<p>Visualize LiDAR data using the panel backend.</p>"},{"location":"examples/dataviz/lidar_viz/#open3d","title":"Open3D\u00b6","text":"<p>Visualize LiDAR data using the open3d backend.</p>"},{"location":"examples/dataviz/raster_viz/","title":"Raster viz","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"leafmap[raster]\"\n</pre> # %pip install \"leafmap[raster]\" In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nurl = \"https://opendata.digitalglobe.com/events/california-fire-2020/pre-event/2018-02-16/pine-gulch-fire20/1030010076004E00.tif\"\nm.add_cog_layer(url, name=\"Fire (pre-event)\")\nm\n</pre> m = leafmap.Map() url = \"https://opendata.digitalglobe.com/events/california-fire-2020/pre-event/2018-02-16/pine-gulch-fire20/1030010076004E00.tif\" m.add_cog_layer(url, name=\"Fire (pre-event)\") m <p>You can add multiple COGs to the map. Let's add another COG to the map.</p> In\u00a0[\u00a0]: Copied! <pre>url2 = \"https://opendata.digitalglobe.com/events/california-fire-2020/post-event/2020-08-14/pine-gulch-fire20/10300100AAC8DD00.tif\"\nm.add_cog_layer(url2, name=\"Fire (post-event)\")\nm\n</pre> url2 = \"https://opendata.digitalglobe.com/events/california-fire-2020/post-event/2020-08-14/pine-gulch-fire20/10300100AAC8DD00.tif\" m.add_cog_layer(url2, name=\"Fire (post-event)\") m <p></p> <p>Create a split map for comparing two COGs.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.split_map(left_layer=url, right_layer=url2)\nm\n</pre> m = leafmap.Map() m.split_map(left_layer=url, right_layer=url2) m <p></p> In\u00a0[\u00a0]: Copied! <pre>dem_url = \"https://open.gishub.org/data/raster/srtm90.tif\"\nleafmap.download_file(dem_url, unzip=False)\n</pre> dem_url = \"https://open.gishub.org/data/raster/srtm90.tif\" leafmap.download_file(dem_url, unzip=False) <p>Visualize a single-band raster.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"srtm90.tif\", cmap=\"terrain\", layer_name=\"DEM\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"srtm90.tif\", cmap=\"terrain\", layer_name=\"DEM\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>landsat_url = \"https://open.gishub.org/data/raster/cog.tif\"\nleafmap.download_file(landsat_url)\n</pre> landsat_url = \"https://open.gishub.org/data/raster/cog.tif\" leafmap.download_file(landsat_url) <p>Visualize a multi-band raster.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"cog.tif\", bands=[4, 3, 2], layer_name=\"Landsat\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"cog.tif\", bands=[4, 3, 2], layer_name=\"Landsat\") m <p></p> <p>Create an interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://canada-spot-ortho.s3.amazonaws.com/canada_spot_orthoimages/canada_spot5_orthoimages/S5_2007/S5_11055_6057_20070622/S5_11055_6057_20070622.json\"\nleafmap.stac_bands(url)\n</pre> url = \"https://canada-spot-ortho.s3.amazonaws.com/canada_spot_orthoimages/canada_spot5_orthoimages/S5_2007/S5_11055_6057_20070622/S5_11055_6057_20070622.json\" leafmap.stac_bands(url) <p>Add STAC layers to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_stac_layer(url, bands=[\"pan\"], name=\"Panchromatic\")\nm.add_stac_layer(url, bands=[\"B3\", \"B2\", \"B1\"], name=\"False color\")\nm\n</pre> m = leafmap.Map() m.add_stac_layer(url, bands=[\"pan\"], name=\"Panchromatic\") m.add_stac_layer(url, bands=[\"B3\", \"B2\", \"B1\"], name=\"False color\") m <p></p> <p>Provide custom STAC API endpoints as a dictionary in the format of <code>{\"name\": \"url\"}</code>. The name will show up in the dropdown menu, while the url is the STAC API endpoint that will be used to search for items.</p> In\u00a0[\u00a0]: Copied! <pre>catalogs = {\n    \"Element84 Earth Search\": \"https://earth-search.aws.element84.com/v1\",\n    \"Microsoft Planetary Computer\": \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n}\n</pre> catalogs = {     \"Element84 Earth Search\": \"https://earth-search.aws.element84.com/v1\",     \"Microsoft Planetary Computer\": \"https://planetarycomputer.microsoft.com/api/stac/v1\", } In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40, -100], zoom=4)\nm.set_catalog_source(catalogs)\nm.add_stac_gui()\nm\n</pre> m = leafmap.Map(center=[40, -100], zoom=4) m.set_catalog_source(catalogs) m.add_stac_gui() m <p>Once the catalog panel is open, you can search for items from the custom STAC API endpoints. Simply draw a bounding box on the map or zoom to a location of interest. Click on the Collections button to retrieve the collections from the custom STAC API endpoints. Next, select a collection from the dropdown menu. Then, click on the Items button to retrieve the items from the selected collection. Finally, click on the Display button to add the selected item to the map.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>m.stac_gdf  # The GeoDataFrame of the STAC search results\n</pre> m.stac_gdf  # The GeoDataFrame of the STAC search results In\u00a0[\u00a0]: Copied! <pre>m.stac_dict  # The STAC search results as a dictionary\n</pre> m.stac_dict  # The STAC search results as a dictionary In\u00a0[\u00a0]: Copied! <pre>m.stac_item  # The selected STAC item of the search result\n</pre> m.stac_item  # The selected STAC item of the search result <p>To Be able to run this notebook you'll need to have AWS credential available as environment variables. Uncomment the following lines to set the environment variables.</p> In\u00a0[\u00a0]: Copied! <pre># os.environ[\"AWS_ACCESS_KEY_ID\"] = \"YOUR AWS ACCESS ID HERE\"\n# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"YOUR AWS ACCESS KEY HERE\"\n</pre> # os.environ[\"AWS_ACCESS_KEY_ID\"] = \"YOUR AWS ACCESS ID HERE\" # os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"YOUR AWS ACCESS KEY HERE\" <p>In this example, we will use datasets from the Maxar Open Data Program on AWS.</p> In\u00a0[\u00a0]: Copied! <pre>BUCKET = \"maxar-opendata\"\nFOLDER = \"events/Kahramanmaras-turkey-earthquake-23/\"\n</pre> BUCKET = \"maxar-opendata\" FOLDER = \"events/Kahramanmaras-turkey-earthquake-23/\" <p>List all the datasets in the bucket. Specify a file extension to filter the results if needed.</p> In\u00a0[\u00a0]: Copied! <pre>items = leafmap.s3_list_objects(BUCKET, FOLDER, ext=\".tif\")\nitems[:10]\n</pre> items = leafmap.s3_list_objects(BUCKET, FOLDER, ext=\".tif\") items[:10] <p>Visualize raster datasets from the bucket.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_cog_layer(items[2], name=\"Maxar\")\nm\n</pre> m = leafmap.Map() m.add_cog_layer(items[2], name=\"Maxar\") m <p></p>"},{"location":"examples/dataviz/raster_viz/#visualizing-raster-data-with-leafmap","title":"Visualizing Raster Data with Leafmap\u00b6","text":"<p>This notebook demonstrates how to visualize raster data using leafmap. Leafmap can visualize raster data (e.g., Cloud Optimized GeoTIFF) stored in a local file or on the cloud (e.g., AWS S3). It can also visualize raster data stored in a STAC catalog.</p>"},{"location":"examples/dataviz/raster_viz/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install the required packages if needed.</p>"},{"location":"examples/dataviz/raster_viz/#import-packages","title":"Import packages\u00b6","text":""},{"location":"examples/dataviz/raster_viz/#cog","title":"COG\u00b6","text":"<p>A Cloud Optimized GeoTIFF (COG) is a regular GeoTIFF file, aimed at being hosted on a HTTP file server, with an internal organization that enables more efficient workflows on the cloud. It does this by leveraging the ability of clients issuing HTTP GET range requests to ask for just the parts of a file they need. More information about COG can be found at https://www.cogeo.org/in-depth.html</p> <p>For this demo, we will use data from https://www.maxar.com/open-data/california-colorado-fires for mapping California and Colorado fires. Let's create an interactive map and add the COG to the map.</p>"},{"location":"examples/dataviz/raster_viz/#local-raster","title":"Local Raster\u00b6","text":"<p>Leafmap can also visualize local GeoTIFF files. Let's download some sample data</p>"},{"location":"examples/dataviz/raster_viz/#stac","title":"STAC\u00b6","text":"<p>The SpatioTemporal Asset Catalog (STAC) specification provides a common language to describe a range of geospatial information so that it can more easily be indexed and discovered. A SpatioTemporal Asset is any file that represents information about the earth captured in a certain space and time. STAC aims to enable that next generation of geospatial search engines, while also supporting web best practices so geospatial information is more easily surfaced in traditional search engines. More information about STAC can be found at the STAC website. In this example, we will use a STAC item from the SPOT Orthoimages of Canada available through the link below:</p>"},{"location":"examples/dataviz/raster_viz/#custom-stac-catalog","title":"Custom STAC Catalog\u00b6","text":""},{"location":"examples/dataviz/raster_viz/#aws-s3","title":"AWS S3\u00b6","text":""},{"location":"examples/dataviz/vector_viz/","title":"Vector viz","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"leafmap[vector]\"\n</pre> # %pip install \"leafmap[vector]\" In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[0, 0], zoom=2)\ndata = \"https://open.gishub.org/data/vector/cables.geojson\"\nm.add_vector(data, layer_name=\"Cable lines\", info_mode=\"on_hover\")\nm\n</pre> m = leafmap.Map(center=[0, 0], zoom=2) data = \"https://open.gishub.org/data/vector/cables.geojson\" m.add_vector(data, layer_name=\"Cable lines\", info_mode=\"on_hover\") m <p>You can style the vector with custom style callback functions.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[20, 0], zoom=2)\nm.add_basemap(\"CartoDB.DarkMatter\")\ndata = \"https://open.gishub.org/data/vector/cables.geojson\"\ncallback = lambda feat: {\"color\": feat[\"properties\"][\"color\"], \"weight\": 1}\nm.add_vector(data, layer_name=\"Cable lines\", style_callback=callback)\nm\n</pre> m = leafmap.Map(center=[20, 0], zoom=2) m.add_basemap(\"CartoDB.DarkMatter\") data = \"https://open.gishub.org/data/vector/cables.geojson\" callback = lambda feat: {\"color\": feat[\"properties\"][\"color\"], \"weight\": 1} m.add_vector(data, layer_name=\"Cable lines\", style_callback=callback) m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\ndata = \"https://raw.githubusercontent.com/opengeos/leafmap/master/docs/examples/data/countries.geojson\"\nm.add_data(\n    data, column=\"POP_EST\", scheme=\"Quantiles\", cmap=\"Blues\", legend_title=\"Population\"\n)\nm\n</pre> m = leafmap.Map() data = \"https://raw.githubusercontent.com/opengeos/leafmap/master/docs/examples/data/countries.geojson\" m.add_data(     data, column=\"POP_EST\", scheme=\"Quantiles\", cmap=\"Blues\", legend_title=\"Population\" ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_data(\n    data,\n    column=\"POP_EST\",\n    scheme=\"EqualInterval\",\n    cmap=\"Blues\",\n    legend_title=\"Population\",\n)\nm\n</pre> m = leafmap.Map() m.add_data(     data,     column=\"POP_EST\",     scheme=\"EqualInterval\",     cmap=\"Blues\",     legend_title=\"Population\", ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://open.gishub.org/data/duckdb/cities.parquet\"\ngdf = leafmap.read_parquet(url, return_type=\"gdf\", src_crs=\"EPSG:4326\")\ngdf.head()\n</pre> url = \"https://open.gishub.org/data/duckdb/cities.parquet\" gdf = leafmap.read_parquet(url, return_type=\"gdf\", src_crs=\"EPSG:4326\") gdf.head() <p>Visualize point data.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_vector(\n    gdf,\n    get_radius=20000,\n    get_fill_color=\"blue\",\n    zoom_to_layer=False,\n    map_args={\"center\": (40, -100), \"zoom\": 3, \"height\": 500},\n)\n</pre> leafmap.view_vector(     gdf,     get_radius=20000,     get_fill_color=\"blue\",     zoom_to_layer=False,     map_args={\"center\": (40, -100), \"zoom\": 3, \"height\": 500}, ) <p></p> <p>Visualizing polygon data.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://data.source.coop/giswqs/nwi/wetlands/DC_Wetlands.parquet\"\ngdf = leafmap.read_parquet(\n    url, return_type=\"gdf\", src_crs=\"EPSG:5070\", dst_crs=\"EPSG:4326\"\n)\ngdf.head()\n</pre> url = \"https://data.source.coop/giswqs/nwi/wetlands/DC_Wetlands.parquet\" gdf = leafmap.read_parquet(     url, return_type=\"gdf\", src_crs=\"EPSG:5070\", dst_crs=\"EPSG:4326\" ) gdf.head() In\u00a0[\u00a0]: Copied! <pre>leafmap.view_vector(gdf, get_fill_color=[0, 0, 255, 128])\n</pre> leafmap.view_vector(gdf, get_fill_color=[0, 0, 255, 128]) <p></p> <p>Alternatively, you can specify a color map to visualize the data.</p> In\u00a0[\u00a0]: Copied! <pre>color_map = {\n    \"Freshwater Forested/Shrub Wetland\": (0, 136, 55),\n    \"Freshwater Emergent Wetland\": (127, 195, 28),\n    \"Freshwater Pond\": (104, 140, 192),\n    \"Estuarine and Marine Wetland\": (102, 194, 165),\n    \"Riverine\": (1, 144, 191),\n    \"Lake\": (19, 0, 124),\n    \"Estuarine and Marine Deepwater\": (0, 124, 136),\n    \"Other\": (178, 134, 86),\n}\n</pre> color_map = {     \"Freshwater Forested/Shrub Wetland\": (0, 136, 55),     \"Freshwater Emergent Wetland\": (127, 195, 28),     \"Freshwater Pond\": (104, 140, 192),     \"Estuarine and Marine Wetland\": (102, 194, 165),     \"Riverine\": (1, 144, 191),     \"Lake\": (19, 0, 124),     \"Estuarine and Marine Deepwater\": (0, 124, 136),     \"Other\": (178, 134, 86), } In\u00a0[\u00a0]: Copied! <pre>leafmap.view_vector(gdf, color_column=\"WETLAND_TYPE\", color_map=color_map, opacity=0.5)\n</pre> leafmap.view_vector(gdf, color_column=\"WETLAND_TYPE\", color_map=color_map, opacity=0.5) <p></p> <p>Display a legend for the data.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.Legend(title=\"Wetland Type\", legend_dict=color_map)\n</pre> leafmap.Legend(title=\"Wetland Type\", legend_dict=color_map) <p></p> In\u00a0[\u00a0]: Copied! <pre>import leafmap.foliumap as leafmap\n</pre> import leafmap.foliumap as leafmap <p>Check the metadata of the PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://storage.googleapis.com/ahp-research/overture/pmtiles/overture.pmtiles\"\nmetadata = leafmap.pmtiles_metadata(url)\nprint(f\"layer names: {metadata['layer_names']}\")\nprint(f\"bounds: {metadata['bounds']}\")\n</pre> url = \"https://storage.googleapis.com/ahp-research/overture/pmtiles/overture.pmtiles\" metadata = leafmap.pmtiles_metadata(url) print(f\"layer names: {metadata['layer_names']}\") print(f\"bounds: {metadata['bounds']}\") <p>Visualize the PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_basemap(\"CartoDB.DarkMatter\")\n\nstyle = {\n    \"version\": 8,\n    \"sources\": {\n        \"example_source\": {\n            \"type\": \"vector\",\n            \"url\": \"pmtiles://\" + url,\n            \"attribution\": \"PMTiles\",\n        }\n    },\n    \"layers\": [\n        {\n            \"id\": \"admins\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"admins\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#BDD3C7\", \"fill-opacity\": 0.1},\n        },\n        {\n            \"id\": \"buildings\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"buildings\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#FFFFB3\", \"fill-opacity\": 0.5},\n        },\n        {\n            \"id\": \"places\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"places\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#BEBADA\", \"fill-opacity\": 0.5},\n        },\n        {\n            \"id\": \"roads\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"roads\",\n            \"type\": \"line\",\n            \"paint\": {\"line-color\": \"#FB8072\"},\n        },\n    ],\n}\n\nm.add_pmtiles(\n    url, name=\"PMTiles\", style=style, overlay=True, show=True, zoom_to_layer=True\n)\n\nlegend_dict = {\n    \"admins\": \"BDD3C7\",\n    \"buildings\": \"FFFFB3\",\n    \"places\": \"BEBADA\",\n    \"roads\": \"FB8072\",\n}\n\nm.add_legend(legend_dict=legend_dict)\nm\n</pre> m = leafmap.Map() m.add_basemap(\"CartoDB.DarkMatter\")  style = {     \"version\": 8,     \"sources\": {         \"example_source\": {             \"type\": \"vector\",             \"url\": \"pmtiles://\" + url,             \"attribution\": \"PMTiles\",         }     },     \"layers\": [         {             \"id\": \"admins\",             \"source\": \"example_source\",             \"source-layer\": \"admins\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#BDD3C7\", \"fill-opacity\": 0.1},         },         {             \"id\": \"buildings\",             \"source\": \"example_source\",             \"source-layer\": \"buildings\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#FFFFB3\", \"fill-opacity\": 0.5},         },         {             \"id\": \"places\",             \"source\": \"example_source\",             \"source-layer\": \"places\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#BEBADA\", \"fill-opacity\": 0.5},         },         {             \"id\": \"roads\",             \"source\": \"example_source\",             \"source-layer\": \"roads\",             \"type\": \"line\",             \"paint\": {\"line-color\": \"#FB8072\"},         },     ], }  m.add_pmtiles(     url, name=\"PMTiles\", style=style, overlay=True, show=True, zoom_to_layer=True )  legend_dict = {     \"admins\": \"BDD3C7\",     \"buildings\": \"FFFFB3\",     \"places\": \"BEBADA\",     \"roads\": \"FB8072\", }  m.add_legend(legend_dict=legend_dict) m <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://data.source.coop/vida/google-microsoft-open-buildings/pmtiles/go_ms_building_footprints.pmtiles\"\nmetadata = leafmap.pmtiles_metadata(url)\nprint(f\"layer names: {metadata['layer_names']}\")\nprint(f\"bounds: {metadata['bounds']}\")\n</pre> url = \"https://data.source.coop/vida/google-microsoft-open-buildings/pmtiles/go_ms_building_footprints.pmtiles\" metadata = leafmap.pmtiles_metadata(url) print(f\"layer names: {metadata['layer_names']}\") print(f\"bounds: {metadata['bounds']}\") <p>Visualize the PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[20, 0], zoom=2)\nm.add_basemap(\"CartoDB.DarkMatter\")\nm.add_basemap(\"Esri.WorldImagery\", show=False)\n\nstyle = {\n    \"version\": 8,\n    \"sources\": {\n        \"example_source\": {\n            \"type\": \"vector\",\n            \"url\": \"pmtiles://\" + url,\n            \"attribution\": \"PMTiles\",\n        }\n    },\n    \"layers\": [\n        {\n            \"id\": \"buildings\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"building_footprints\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},\n        },\n    ],\n}\n\nm.add_pmtiles(\n    url, name=\"Buildings\", style=style, overlay=True, show=True, zoom_to_layer=False\n)\n\nm\n</pre> m = leafmap.Map(center=[20, 0], zoom=2) m.add_basemap(\"CartoDB.DarkMatter\") m.add_basemap(\"Esri.WorldImagery\", show=False)  style = {     \"version\": 8,     \"sources\": {         \"example_source\": {             \"type\": \"vector\",             \"url\": \"pmtiles://\" + url,             \"attribution\": \"PMTiles\",         }     },     \"layers\": [         {             \"id\": \"buildings\",             \"source\": \"example_source\",             \"source-layer\": \"building_footprints\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},         },     ], }  m.add_pmtiles(     url, name=\"Buildings\", style=style, overlay=True, show=True, zoom_to_layer=False )  m <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://raw.githubusercontent.com/opengeos/open-data/main/datasets/libya/Derna_buildings.geojson\"\nleafmap.download_file(url, \"buildings.geojson\")\n</pre> url = \"https://raw.githubusercontent.com/opengeos/open-data/main/datasets/libya/Derna_buildings.geojson\" leafmap.download_file(url, \"buildings.geojson\") <p>Convert vector to PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>pmtiles = \"buildings.pmtiles\"\nleafmap.geojson_to_pmtiles(\n    \"buildings.geojson\", pmtiles, layer_name=\"buildings\", overwrite=True, quiet=True\n)\n</pre> pmtiles = \"buildings.pmtiles\" leafmap.geojson_to_pmtiles(     \"buildings.geojson\", pmtiles, layer_name=\"buildings\", overwrite=True, quiet=True ) <p>Start a HTTP Sever</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.start_server(port=8000)\n</pre> leafmap.start_server(port=8000) In\u00a0[\u00a0]: Copied! <pre>url = f\"http://127.0.0.1:8000/{pmtiles}\"\nleafmap.pmtiles_metadata(url)\n</pre> url = f\"http://127.0.0.1:8000/{pmtiles}\" leafmap.pmtiles_metadata(url) <p>Display the PMTiles on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\n\nstyle = {\n    \"version\": 8,\n    \"sources\": {\n        \"example_source\": {\n            \"type\": \"vector\",\n            \"url\": \"pmtiles://\" + url,\n            \"attribution\": \"PMTiles\",\n        }\n    },\n    \"layers\": [\n        {\n            \"id\": \"buildings\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"buildings\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},\n        },\n    ],\n}\n\nm.add_pmtiles(url, name=\"Buildings\", show=True, zoom_to_layer=True, style=style)\nm\n</pre> m = leafmap.Map()  style = {     \"version\": 8,     \"sources\": {         \"example_source\": {             \"type\": \"vector\",             \"url\": \"pmtiles://\" + url,             \"attribution\": \"PMTiles\",         }     },     \"layers\": [         {             \"id\": \"buildings\",             \"source\": \"example_source\",             \"source-layer\": \"buildings\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},         },     ], }  m.add_pmtiles(url, name=\"Buildings\", show=True, zoom_to_layer=True, style=style) m <p></p>"},{"location":"examples/dataviz/vector_viz/#visualizing-vector-data-with-leafmap","title":"Visualizing Vector Data with Leafmap\u00b6","text":""},{"location":"examples/dataviz/vector_viz/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install leafmap if needed.</p>"},{"location":"examples/dataviz/vector_viz/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/dataviz/vector_viz/#visualize-vector-data","title":"Visualize vector data\u00b6","text":"<p>You can visualize vector data using the <code>add_vector</code> function. It supports common vector data formats, including GeoJSON, Shapefile, GeoPackage, and any other formats supported by geopandas.</p>"},{"location":"examples/dataviz/vector_viz/#choropleth-map","title":"Choropleth map\u00b6","text":"<p>You can create a choropleth map using the <code>add_data</code> function. It supports GeoJSON, Shapefile, GeoPackage, and any other formats supported by geopandas.</p>"},{"location":"examples/dataviz/vector_viz/#geoparquet","title":"GeoParquet\u00b6","text":"<p>Visualize GeoParquet data with leafmap and lonboard.</p>"},{"location":"examples/dataviz/vector_viz/#pmtiles","title":"PMTiles\u00b6","text":"<p>PMTiles is a single-file archive format for tiled data. A PMTiles archive can be hosted on a commodity storage platform such as S3, and enables low-cost, zero-maintenance map applications that are \"serverless\" - free of a custom tile backend or third party provider.</p>"},{"location":"examples/dataviz/vector_viz/#remote-pmtiles","title":"Remote PMTiles\u00b6","text":"<p>Leafmap can visualize PMTiles hosted locally or remotely.</p>"},{"location":"examples/dataviz/vector_viz/#overture-data","title":"Overture data\u00b6","text":""},{"location":"examples/dataviz/vector_viz/#source-cooperative","title":"Source Cooperative\u00b6","text":"<p>Visualize the Google-Microsoft Open Buildings data hosted on Source Cooperative.</p> <p>Check the metadata of the PMTiles.</p>"},{"location":"examples/dataviz/vector_viz/#local-pmtiles","title":"Local PMTiles\u00b6","text":"<p>tippecanoe is required to convert vector data to pmtiles. Install it with <code>conda install -c conda-forge tippecanoe</code>.</p> <p>Download building footprints of Derna, Libya.</p>"},{"location":"examples/rastervision/semantic_segmentation/","title":"Semantic segmentation","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport torch\nfrom matplotlib import pyplot as plt\n\nfrom rastervision.core.data import ClassConfig, SemanticSegmentationLabels\n\nimport albumentations as A\n\nfrom rastervision.pytorch_learner import (\n    SemanticSegmentationRandomWindowGeoDataset,\n    SemanticSegmentationSlidingWindowGeoDataset,\n    SemanticSegmentationVisualizer,\n    SemanticSegmentationGeoDataConfig,\n    SemanticSegmentationLearnerConfig,\n    SolverConfig,\n    SemanticSegmentationLearner,\n)\n</pre> import os import torch from matplotlib import pyplot as plt  from rastervision.core.data import ClassConfig, SemanticSegmentationLabels  import albumentations as A  from rastervision.pytorch_learner import (     SemanticSegmentationRandomWindowGeoDataset,     SemanticSegmentationSlidingWindowGeoDataset,     SemanticSegmentationVisualizer,     SemanticSegmentationGeoDataConfig,     SemanticSegmentationLearnerConfig,     SolverConfig,     SemanticSegmentationLearner, ) In\u00a0[\u00a0]: Copied! <pre>os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\"\n</pre> os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\" In\u00a0[\u00a0]: Copied! <pre>class_config = ClassConfig(\n    names=[\"background\", \"building\"],\n    colors=[\"lightgray\", \"darkred\"],\n    null_class=\"background\",\n)\n\nviz = SemanticSegmentationVisualizer(\n    class_names=class_config.names, class_colors=class_config.colors\n)\n</pre> class_config = ClassConfig(     names=[\"background\", \"building\"],     colors=[\"lightgray\", \"darkred\"],     null_class=\"background\", )  viz = SemanticSegmentationVisualizer(     class_names=class_config.names, class_colors=class_config.colors ) In\u00a0[\u00a0]: Copied! <pre>train_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/images/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13.tif\"\ntrain_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/labels/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13_Buildings.geojson\"\n\nval_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\"\nval_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\"\n</pre> train_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/images/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13.tif\" train_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/labels/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13_Buildings.geojson\"  val_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\" val_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\" In\u00a0[\u00a0]: Copied! <pre>pred_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\"\npred_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\"\n</pre> pred_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\" pred_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\" In\u00a0[\u00a0]: Copied! <pre>data_augmentation_transform = A.Compose(\n    [\n        A.Flip(),\n        A.ShiftScaleRotate(),\n        A.OneOf(\n            [\n                A.HueSaturationValue(hue_shift_limit=10),\n                A.RGBShift(),\n                A.ToGray(),\n                A.ToSepia(),\n                A.RandomBrightness(),\n                A.RandomGamma(),\n            ]\n        ),\n        A.CoarseDropout(max_height=32, max_width=32, max_holes=5),\n    ]\n)\n</pre> data_augmentation_transform = A.Compose(     [         A.Flip(),         A.ShiftScaleRotate(),         A.OneOf(             [                 A.HueSaturationValue(hue_shift_limit=10),                 A.RGBShift(),                 A.ToGray(),                 A.ToSepia(),                 A.RandomBrightness(),                 A.RandomGamma(),             ]         ),         A.CoarseDropout(max_height=32, max_width=32, max_holes=5),     ] ) In\u00a0[\u00a0]: Copied! <pre>train_ds = SemanticSegmentationRandomWindowGeoDataset.from_uris(\n    class_config=class_config,\n    image_uri=train_image_uri,\n    label_vector_uri=train_label_uri,\n    label_vector_default_class_id=class_config.get_class_id(\"building\"),\n    size_lims=(150, 200),\n    out_size=256,\n    max_windows=400,\n    transform=data_augmentation_transform,\n)\n\nlen(train_ds)\n</pre> train_ds = SemanticSegmentationRandomWindowGeoDataset.from_uris(     class_config=class_config,     image_uri=train_image_uri,     label_vector_uri=train_label_uri,     label_vector_default_class_id=class_config.get_class_id(\"building\"),     size_lims=(150, 200),     out_size=256,     max_windows=400,     transform=data_augmentation_transform, )  len(train_ds) In\u00a0[\u00a0]: Copied! <pre>x, y = viz.get_batch(train_ds, 4)\nviz.plot_batch(x, y, show=True)\n</pre> x, y = viz.get_batch(train_ds, 4) viz.plot_batch(x, y, show=True) In\u00a0[\u00a0]: Copied! <pre>val_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(\n    class_config=class_config,\n    image_uri=val_image_uri,\n    label_vector_uri=val_label_uri,\n    label_vector_default_class_id=class_config.get_class_id(\"building\"),\n    size=200,\n    stride=100,\n    transform=A.Resize(256, 256),\n)\nlen(val_ds)\n</pre> val_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(     class_config=class_config,     image_uri=val_image_uri,     label_vector_uri=val_label_uri,     label_vector_default_class_id=class_config.get_class_id(\"building\"),     size=200,     stride=100,     transform=A.Resize(256, 256), ) len(val_ds) In\u00a0[\u00a0]: Copied! <pre>x, y = viz.get_batch(val_ds, 4)\nviz.plot_batch(x, y, show=True)\n</pre> x, y = viz.get_batch(val_ds, 4) viz.plot_batch(x, y, show=True) In\u00a0[\u00a0]: Copied! <pre>pred_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(\n    class_config=class_config,\n    image_uri=pred_image_uri,\n    size=200,\n    stride=100,\n    transform=A.Resize(256, 256),\n)\nlen(pred_ds)\n</pre> pred_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(     class_config=class_config,     image_uri=pred_image_uri,     size=200,     stride=100,     transform=A.Resize(256, 256), ) len(pred_ds) In\u00a0[\u00a0]: Copied! <pre>model = torch.hub.load(\n    \"AdeelH/pytorch-fpn:0.3\",\n    \"make_fpn_resnet\",\n    name=\"resnet18\",\n    fpn_type=\"panoptic\",\n    num_classes=len(class_config),\n    fpn_channels=128,\n    in_channels=3,\n    out_size=(256, 256),\n    pretrained=True,\n)\n</pre> model = torch.hub.load(     \"AdeelH/pytorch-fpn:0.3\",     \"make_fpn_resnet\",     name=\"resnet18\",     fpn_type=\"panoptic\",     num_classes=len(class_config),     fpn_channels=128,     in_channels=3,     out_size=(256, 256),     pretrained=True, ) In\u00a0[\u00a0]: Copied! <pre>data_cfg = SemanticSegmentationGeoDataConfig(\n    class_names=class_config.names,\n    class_colors=class_config.colors,\n    num_workers=0,  # increase to use multi-processing\n)\n</pre> data_cfg = SemanticSegmentationGeoDataConfig(     class_names=class_config.names,     class_colors=class_config.colors,     num_workers=0,  # increase to use multi-processing ) In\u00a0[\u00a0]: Copied! <pre>solver_cfg = SolverConfig(batch_sz=8, lr=3e-2, class_loss_weights=[1.0, 10.0])\n</pre> solver_cfg = SolverConfig(batch_sz=8, lr=3e-2, class_loss_weights=[1.0, 10.0]) In\u00a0[\u00a0]: Copied! <pre>learner_cfg = SemanticSegmentationLearnerConfig(data=data_cfg, solver=solver_cfg)\n</pre> learner_cfg = SemanticSegmentationLearnerConfig(data=data_cfg, solver=solver_cfg) In\u00a0[\u00a0]: Copied! <pre>learner = SemanticSegmentationLearner(\n    cfg=learner_cfg,\n    output_dir=\"./train-demo/\",\n    model=model,\n    train_ds=train_ds,\n    valid_ds=val_ds,\n)\n</pre> learner = SemanticSegmentationLearner(     cfg=learner_cfg,     output_dir=\"./train-demo/\",     model=model,     train_ds=train_ds,     valid_ds=val_ds, ) In\u00a0[\u00a0]: Copied! <pre>learner.log_data_stats()\n</pre> learner.log_data_stats() In\u00a0[\u00a0]: Copied! <pre>%load_ext tensorboard\n</pre> %load_ext tensorboard In\u00a0[\u00a0]: Copied! <pre>%tensorboard --bind_all --logdir \"./train-demo/tb-logs\" --reload_interval 10\n</pre> %tensorboard --bind_all --logdir \"./train-demo/tb-logs\" --reload_interval 10 In\u00a0[\u00a0]: Copied! <pre>learner.train(epochs=3)\n</pre> learner.train(epochs=3) In\u00a0[\u00a0]: Copied! <pre>learner.train(epochs=1)\n</pre> learner.train(epochs=1) In\u00a0[\u00a0]: Copied! <pre>learner.plot_predictions(split=\"valid\", show=True)\n</pre> learner.plot_predictions(split=\"valid\", show=True) In\u00a0[\u00a0]: Copied! <pre>learner.save_model_bundle()\n</pre> learner.save_model_bundle() In\u00a0[\u00a0]: Copied! <pre>learner = SemanticSegmentationLearner.from_model_bundle(\n    model_bundle_uri=\"./train-demo/model-bundle.zip\",\n    output_dir=\"./train-demo/\",\n    model=model,\n)\n</pre> learner = SemanticSegmentationLearner.from_model_bundle(     model_bundle_uri=\"./train-demo/model-bundle.zip\",     output_dir=\"./train-demo/\",     model=model, ) In\u00a0[\u00a0]: Copied! <pre>learner = SemanticSegmentationLearner.from_model_bundle(\n    model_bundle_uri=\"./train-demo/model-bundle.zip\",\n    output_dir=\"./train-demo/\",\n    model=model,\n    train_ds=train_ds,\n    valid_ds=val_ds,\n    training=True,\n)\n</pre> learner = SemanticSegmentationLearner.from_model_bundle(     model_bundle_uri=\"./train-demo/model-bundle.zip\",     output_dir=\"./train-demo/\",     model=model,     train_ds=train_ds,     valid_ds=val_ds,     training=True, ) In\u00a0[\u00a0]: Copied! <pre>learner.train(epochs=1)\n</pre> learner.train(epochs=1) In\u00a0[\u00a0]: Copied! <pre>learner.plot_predictions(split=\"valid\", show=True)\n</pre> learner.plot_predictions(split=\"valid\", show=True) In\u00a0[\u00a0]: Copied! <pre>predictions = learner.predict_dataset(\n    pred_ds,\n    raw_out=True,\n    numpy_out=True,\n    predict_kw=dict(out_shape=(325, 325)),\n    progress_bar=True,\n)\n</pre> predictions = learner.predict_dataset(     pred_ds,     raw_out=True,     numpy_out=True,     predict_kw=dict(out_shape=(325, 325)),     progress_bar=True, ) In\u00a0[\u00a0]: Copied! <pre>pred_labels = SemanticSegmentationLabels.from_predictions(\n    pred_ds.windows,\n    predictions,\n    smooth=True,\n    extent=pred_ds.scene.extent,\n    num_classes=len(class_config),\n)\n</pre> pred_labels = SemanticSegmentationLabels.from_predictions(     pred_ds.windows,     predictions,     smooth=True,     extent=pred_ds.scene.extent,     num_classes=len(class_config), ) In\u00a0[\u00a0]: Copied! <pre>scores = pred_labels.get_score_arr(pred_labels.extent)\n</pre> scores = pred_labels.get_score_arr(pred_labels.extent) In\u00a0[\u00a0]: Copied! <pre>pred_labels.save(\n    uri=f\"predict\",\n    crs_transformer=pred_ds.scene.raster_source.crs_transformer,\n    class_config=class_config,\n)\n</pre> pred_labels.save(     uri=f\"predict\",     crs_transformer=pred_ds.scene.raster_source.crs_transformer,     class_config=class_config, )"},{"location":"examples/samgeo/arcgis/","title":"Arcgis","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo\n\n%matplotlib inline\n</pre> import os import leafmap from samgeo import SamGeo  %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>workspace = os.path.dirname(arcpy.env.workspace)\nos.chdir(workspace)\narcpy.env.overwriteOutput = True\n</pre> workspace = os.path.dirname(arcpy.env.workspace) os.chdir(workspace) arcpy.env.overwriteOutput = True In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/buildings.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/buildings.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/agriculture.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/agriculture.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/water.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/water.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>image = \"agriculture.tif\"\n</pre> image = \"agriculture.tif\" <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p> In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"ag_masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"ag_masks.tif\", foreground=True, unique=True) <p>If you run into GPU memory errors, uncomment the following code block and run it to empty cuda cache then rerun the code block above.</p> In\u00a0[\u00a0]: Copied! <pre># sam.clear_cuda_cache()\n</pre> # sam.clear_cuda_cache() <p>Show the segmentation result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"ag_annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"ag_annotations.tif\") <p>Add layers to ArcGIS Pro.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.arc_active_map()\n</pre> m = leafmap.arc_active_map() In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"agriculture.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"agriculture.tif\")) In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"ag_annotations.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"ag_annotations.tif\")) <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>in_raster = os.path.join(workspace, \"ag_masks.tif\")\nout_shp = os.path.join(workspace, \"ag_masks.shp\")\n</pre> in_raster = os.path.join(workspace, \"ag_masks.tif\") out_shp = os.path.join(workspace, \"ag_masks.shp\") In\u00a0[\u00a0]: Copied! <pre>arcpy.conversion.RasterToPolygon(in_raster, out_shp)\n</pre> arcpy.conversion.RasterToPolygon(in_raster, out_shp) In\u00a0[\u00a0]: Copied! <pre>image = \"water.tif\"\n</pre> image = \"water.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"water_masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"water_masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre># sam.clear_cuda_cache()\n</pre> # sam.clear_cuda_cache() In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"water_annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"water_annotations.tif\") In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"water.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"water.tif\")) In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"water_annotations.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"water_annotations.tif\")) In\u00a0[\u00a0]: Copied! <pre>in_raster = os.path.join(workspace, \"water_masks.tif\")\nout_shp = os.path.join(workspace, \"water_masks.shp\")\n</pre> in_raster = os.path.join(workspace, \"water_masks.tif\") out_shp = os.path.join(workspace, \"water_masks.shp\") In\u00a0[\u00a0]: Copied! <pre>arcpy.conversion.RasterToPolygon(in_raster, out_shp)\n</pre> arcpy.conversion.RasterToPolygon(in_raster, out_shp) In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(\"agriculture.tif\", output=\"ag_masks2.tif\", foreground=True)\n</pre> sam.generate(\"agriculture.tif\", output=\"ag_masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=0.5, output=\"ag_annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=0.5, output=\"ag_annotations2.tif\")"},{"location":"examples/samgeo/arcgis/#using-the-segment-geospatial-python-package-with-arcgis-pro","title":"Using the Segment-Geospatial Python Package with ArcGIS Pro\u00b6","text":"<p>The notebook shows step-by-step instructions for using the Segment Anything Model (SAM) with ArcGIS Pro. Check out the YouTube tutorial here and the Resources for Unlocking the Power of Deep Learning Applications Using ArcGIS. Credit goes to Esri.</p> <p></p>"},{"location":"examples/samgeo/arcgis/#installation","title":"Installation\u00b6","text":"<ol> <li><p>Open Windows Registry Editor (<code>regedit.exe</code>) and navigate to <code>Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem</code>. Change the value of <code>LongPathsEnabled</code> to <code>1</code>. See this screenshot. This is a known issue with the deep learning libraries for ArcGIS Pro 3.1. A future release might fix this issue.</p> </li> <li><p>Navigate to the Start Menu -&gt; All apps -&gt; ArcGIS folder, then open the Python Command Prompt.</p> </li> <li><p>Create a new conda environment and install mamba and Python 3.9.x from the Esri Anaconda channel. Mamba is a drop-in replacement for conda that is mach faster for installing Python packages and their dependencies.</p> <p><code>conda create conda-forge::mamba esri::python --name samgeo</code></p> </li> <li><p>Activate the new conda environment.</p> <p><code>conda activate samgeo</code></p> </li> <li><p>Install arcpy, deep-learning-essentials, segment-geospatial, and other dependencies (~4GB download).</p> <p><code>mamba install arcpy deep-learning-essentials leafmap localtileserver segment-geospatial -c esri -c conda-forge</code></p> </li> <li><p>Activate the new environment in ArcGIS Pro.</p> <p><code>proswap samgeo</code></p> </li> <li><p>Close the Python Command Prompt and open ArcGIS Pro.</p> </li> <li><p>Download this notebook and run it in ArcGIS Pro.</p> </li> </ol>"},{"location":"examples/samgeo/arcgis/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/samgeo/arcgis/#download-sample-data","title":"Download sample data\u00b6","text":"<p>In this example, we will use the high-resolution aerial imagery from the USDA National Agricultural Imagery Program (NAIP). You can download NAIP imagery using the USDA Data Gateway or the USDA NCRS Box Drive. I have downloaded some NAIP imagery and clipped them to a smaller area, which are available here.</p>"},{"location":"examples/samgeo/arcgis/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/arcgis/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Specify the file path to the image we downloaded earlier.</p>"},{"location":"examples/samgeo/arcgis/#segment-waterbodies","title":"Segment waterbodies\u00b6","text":""},{"location":"examples/samgeo/arcgis/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/samgeo/automatic_mask_generator/","title":"Automatic mask generator","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, show_image, download_file, overlay_images, tms_to_geotiff\n</pre> import os import leafmap from samgeo import SamGeo, show_image, download_file, overlay_images, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.2659, 37.8682, -122.2521, 37.8741]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.2659, 37.8682, -122.2521, 37.8741] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\") m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\")\n</pre> overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\") <p></p>"},{"location":"examples/samgeo/automatic_mask_generator/#automatically-generating-object-masks-with-sam","title":"Automatically generating object masks with SAM\u00b6","text":"<p>This notebook shows how to segment objects from an image using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p> <p>The notebook is adapted from segment-anything/notebooks/automatic_mask_generator_example.ipynb, but I have made it much easier to save the segmentation results and visualize them.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/","title":"Automatic mask generator hq","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo.hq_sam import (\n    SamGeo,\n    show_image,\n    download_file,\n    overlay_images,\n    tms_to_geotiff,\n)\n</pre> import os import leafmap from samgeo.hq_sam import (     SamGeo,     show_image,     download_file,     overlay_images,     tms_to_geotiff, ) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.2659, 37.8682, -122.2521, 37.8741]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.2659, 37.8682, -122.2521, 37.8741] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\") m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\")\n</pre> overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\") <p></p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#automatically-generating-object-masks-with-hq-sam","title":"Automatically generating object masks with HQ-SAM\u00b6","text":"<p>This notebook shows how to segment objects from an image using the High-Quality Segment Anything Model (HQ-SAM) with a few lines of code.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator_hq/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator_hq/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/samgeo/box_prompts/","title":"Box prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo import SamGeo\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo import SamGeo In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) <p>Display the map. Use the drawing tools to draw some rectangles around the features you want to extract, such as trees, buildings.</p> In\u00a0[\u00a0]: Copied! <pre>m\n</pre> m In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-51.2546, -22.1771, -51.2541, -22.1767],\n        [-51.2538, -22.1764, -51.2535, -22.1761],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-51.2546, -22.1771, -51.2541, -22.1767],         [-51.2538, -22.1764, -51.2535, -22.1761],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m In\u00a0[\u00a0]: Copied! <pre>url = \"https://opengeos.github.io/data/sam/tree_boxes.geojson\"\ngeojson = \"tree_boxes.geojson\"\nleafmap.download_file(url, geojson)\n</pre> url = \"https://opengeos.github.io/data/sam/tree_boxes.geojson\" geojson = \"tree_boxes.geojson\" leafmap.download_file(url, geojson) <p>Display the vector data on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"Image.tif\", layer_name=\"image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"Image.tif\", layer_name=\"image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\") m In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=\"mask2.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=\"mask2.tif\", dtype=\"uint8\") <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask2.tif\", cmap=\"Greens\", nodata=0, opacity=0.5, layer_name=\"Tree masks\")\nm\n</pre> m.add_raster(\"mask2.tif\", cmap=\"Greens\", nodata=0, opacity=0.5, layer_name=\"Tree masks\") m <p></p>"},{"location":"examples/samgeo/box_prompts/#segmenting-remote-sensing-imagery-with-box-prompts","title":"Segmenting remote sensing imagery with box prompts\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/box_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/box_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/box_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/box_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p> <p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p> <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p>"},{"location":"examples/samgeo/box_prompts/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"examples/samgeo/box_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"examples/samgeo/box_prompts/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"examples/samgeo/box_prompts/#use-an-existing-vector-file-as-box-prompts","title":"Use an existing vector file as box prompts\u00b6","text":"<p>Alternatively, you can specify a file path to a vector file. Let's download a sample vector file from GitHub.</p>"},{"location":"examples/samgeo/box_prompts/#segment-image-with-box-prompts","title":"Segment image with box prompts\u00b6","text":"<p>Segment the image using the specified file path to the vector mask.</p>"},{"location":"examples/samgeo/fast_sam/","title":"Fast sam","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial segment-anything-fast\n</pre> # %pip install segment-geospatial segment-anything-fast In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.fast_sam import SamGeo\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.fast_sam import SamGeo In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>from samgeo.fast_sam import SamGeo\n\nsam = SamGeo(model=\"FastSAM-x.pt\")\n</pre> from samgeo.fast_sam import SamGeo  sam = SamGeo(model=\"FastSAM-x.pt\") <p>Set the image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(\"Image.tif\")\n</pre> sam.set_image(\"Image.tif\") <p>Segment the image with <code>everything_prompt</code>. You can also try <code>point_prompt</code>, <code>box_prompt</code>, or <code>text_prompt</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam.everything_prompt(output=\"mask.tif\")\n</pre> sam.everything_prompt(output=\"mask.tif\") <p>Show the annotated image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\"mask.png\")\n</pre> sam.show_anns(\"mask.png\") <p></p> <p>Convert the segmentation results from GeoTIFF to vector.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"mask.tif\", \"mask.geojson\")\n</pre> sam.raster_to_vector(\"mask.tif\", \"mask.geojson\") <p>Show the segmentation results on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", opacity=0.5, layer_name=\"Mask\")\nm.add_vector(\"mask.geojson\", layer_name=\"Mask Vector\")\nm\n</pre> m.add_raster(\"mask.tif\", opacity=0.5, layer_name=\"Mask\") m.add_vector(\"mask.geojson\", layer_name=\"Mask Vector\") m <p></p>"},{"location":"examples/samgeo/fast_sam/#segmenting-remote-sensing-imagery-with-fastsam","title":"Segmenting remote sensing imagery with FastSAM\u00b6","text":"<p>FastSAM: https://github.com/CASIA-IVA-Lab/FastSAM</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/fast_sam/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/fast_sam/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/fast_sam/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/fast_sam/#initialize-samgeo-class","title":"Initialize SamGeo class\u00b6","text":"<p>The initialization of the SamGeo class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/input_prompts/","title":"Input prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, tms_to_geotiff\n</pre> import os import leafmap from samgeo import SamGeo, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p>"},{"location":"examples/samgeo/input_prompts/#generating-object-masks-from-input-prompts-with-sam","title":"Generating object masks from input prompts with SAM\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p> <p>The notebook is adapted from segment-anything/notebooks/predictor_example.ipynb, but I have made it much easier to save the segmentation results and visualize them.</p>"},{"location":"examples/samgeo/input_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/input_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/input_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/input_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/input_prompts/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/samgeo/input_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/samgeo/input_prompts_hq/","title":"Input prompts hq","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo.hq_sam import SamGeo, tms_to_geotiff\n</pre> import os import leafmap from samgeo.hq_sam import SamGeo, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p>"},{"location":"examples/samgeo/input_prompts_hq/#generating-object-masks-from-input-prompts-with-hq-sam","title":"Generating object masks from input prompts with HQ-SAM\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the High-Quality Segment Anything Model (HQ-SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/input_prompts_hq/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/input_prompts_hq/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/input_prompts_hq/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/input_prompts_hq/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/input_prompts_hq/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/samgeo/input_prompts_hq/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/samgeo/maxar_open_data/","title":"Maxar open data","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, raster_to_vector, overlay_images\n</pre> import os import leafmap from samgeo import SamGeo, raster_to_vector, overlay_images In\u00a0[\u00a0]: Copied! <pre>url = (\n    \"https://drive.google.com/file/d/1jIIC5hvSPeJEC0fbDhtxVWk2XV9AxsQD/view?usp=sharing\"\n)\n</pre> url = (     \"https://drive.google.com/file/d/1jIIC5hvSPeJEC0fbDhtxVWk2XV9AxsQD/view?usp=sharing\" ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(url, output=\"image.tif\")\n</pre> leafmap.download_file(url, output=\"image.tif\") In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(height=\"600px\")\nm.add_basemap(\"SATELLITE\")\nm.add_raster(\"image.tif\", layer_name=\"Image\")\nm.add_layer_manager()\nm\n</pre> m = leafmap.Map(height=\"600px\") m.add_basemap(\"SATELLITE\") m.add_raster(\"image.tif\", layer_name=\"Image\") m.add_layer_manager() m <p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p> In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 80,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 80, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(\"image.tif\", output=\"mask.tif\", foreground=True)\n</pre> sam.generate(\"image.tif\", output=\"mask.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>raster_to_vector(\"mask.tif\", output=\"mask.shp\")\n</pre> raster_to_vector(\"mask.tif\", output=\"mask.shp\") In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Display the annotations (each mask with a random color).</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotation.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotation.tif\") In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"image.tif\",\n    \"annotation.tif\",\n    label1=\"Image\",\n    label2=\"Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"image.tif\",     \"annotation.tif\",     label1=\"Image\",     label2=\"Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(\"image.tif\", \"annotation.tif\", backend=\"TkAgg\")\n</pre> overlay_images(\"image.tif\", \"annotation.tif\", backend=\"TkAgg\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", layer_name=\"Mask\", nodata=0)\nm.add_raster(\"annotation.tif\", layer_name=\"Annotation\")\nm\n</pre> m.add_raster(\"mask.tif\", layer_name=\"Mask\", nodata=0) m.add_raster(\"annotation.tif\", layer_name=\"Annotation\") m In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"mask.shp\", layer_name=\"Vector\", info_mode=None)\n</pre> m.add_vector(\"mask.shp\", layer_name=\"Vector\", info_mode=None) <p></p>"},{"location":"examples/samgeo/maxar_open_data/#segmenting-satellite-imagery-from-the-maxar-open-data-program","title":"Segmenting satellite imagery from the Maxar Open Data Program\u00b6","text":"<p>This notebook shows how to segment satellite imagery from the Maxar Open Data program for Libya floods.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/maxar_open_data/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/maxar_open_data/#download-sample-data","title":"Download sample data\u00b6","text":"<p>First, let's download a sample image of Derna, Libya from here.</p>"},{"location":"examples/samgeo/maxar_open_data/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#segment-the-image","title":"Segment the image\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#convert-raster-to-vector","title":"Convert raster to vector\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#display-the-segmentation-result","title":"Display the segmentation result\u00b6","text":"<p>First, let's show the result as a binary image.</p>"},{"location":"examples/samgeo/maxar_open_data/#compare-images-with-a-slider","title":"Compare images with a slider\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#display-images-on-an-interactive-map","title":"Display images on an interactive map.\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/","title":"Satellite predictor","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeoPredictor, tms_to_geotiff, get_basemaps\nfrom segment_anything import sam_model_registry\n</pre> import os import leafmap from samgeo import SamGeoPredictor, tms_to_geotiff, get_basemaps from segment_anything import sam_model_registry In\u00a0[\u00a0]: Copied! <pre>zoom = 16\nm = leafmap.Map(center=[45, -123], zoom=zoom)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> zoom = 16 m = leafmap.Map(center=[45, -123], zoom=zoom) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-123.0127, 44.9957, -122.9874, 45.0045]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-123.0127, 44.9957, -122.9874, 45.0045] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\n# image = '/path/to/your/own/image.tif'\n</pre> image = \"satellite.tif\" # image = '/path/to/your/own/image.tif' <p>Besides the <code>satellite</code> basemap, you can use any of the following basemaps returned by the <code>get_basemaps()</code> function:</p> In\u00a0[\u00a0]: Copied! <pre># get_basemaps().keys()\n</pre> # get_basemaps().keys() <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>tms_to_geotiff(\n    output=image, bbox=bbox, zoom=zoom + 1, source=\"Satellite\", overwrite=True\n)\n</pre> tms_to_geotiff(     output=image, bbox=bbox, zoom=zoom + 1, source=\"Satellite\", overwrite=True ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.add_raster(image, layer_name=\"Image\") m <p>Use the draw tools to draw a rectangle from which to subset segmentations on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    clip_box = m.user_roi_bounds()\nelse:\n    clip_box = [-123.0064, 44.9988, -123.0005, 45.0025]\n</pre> if m.user_roi_bounds() is not None:     clip_box = m.user_roi_bounds() else:     clip_box = [-123.0064, 44.9988, -123.0005, 45.0025] In\u00a0[\u00a0]: Copied! <pre>clip_box\n</pre> clip_box In\u00a0[\u00a0]: Copied! <pre>out_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\ncheckpoint = os.path.join(out_dir, \"sam_vit_h_4b8939.pth\")\n</pre> out_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\") checkpoint = os.path.join(out_dir, \"sam_vit_h_4b8939.pth\") In\u00a0[\u00a0]: Copied! <pre>import cv2\n\nimg_arr = cv2.imread(image)\n\nmodel_type = \"vit_h\"\n\nsam = sam_model_registry[model_type](checkpoint=checkpoint)\n\npredictor = SamGeoPredictor(sam)\n\npredictor.set_image(img_arr)\n\nmasks, _, _ = predictor.predict(src_fp=image, geo_box=clip_box)\n</pre> import cv2  img_arr = cv2.imread(image)  model_type = \"vit_h\"  sam = sam_model_registry[model_type](checkpoint=checkpoint)  predictor = SamGeoPredictor(sam)  predictor.set_image(img_arr)  masks, _, _ = predictor.predict(src_fp=image, geo_box=clip_box) In\u00a0[\u00a0]: Copied! <pre>masks_img = \"preds.tif\"\npredictor.masks_to_geotiff(image, masks_img, masks.astype(\"uint8\"))\n</pre> masks_img = \"preds.tif\" predictor.masks_to_geotiff(image, masks_img, masks.astype(\"uint8\")) In\u00a0[\u00a0]: Copied! <pre>vector = \"feats.geojson\"\ngdf = predictor.geotiff_to_geojson(masks_img, vector, bidx=1)\ngdf.plot()\n</pre> vector = \"feats.geojson\" gdf = predictor.geotiff_to_geojson(masks_img, vector, bidx=1) gdf.plot() In\u00a0[\u00a0]: Copied! <pre>style = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(vector, layer_name=\"Vector\", style=style)\nm\n</pre> style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(vector, layer_name=\"Vector\", style=style) m"},{"location":"examples/samgeo/satellite-predictor/#segment-anything-model-for-geospatial-data","title":"Segment Anything Model for Geospatial Data\u00b6","text":"<p>This notebook shows how to use segment satellite imagery using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/satellite-predictor/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/satellite-predictor/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"examples/samgeo/satellite-predictor/#initialize-samgeopredictor-class","title":"Initialize SamGeoPredictor class\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/samgeo/satellite/","title":"Satellite","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, tms_to_geotiff, get_basemaps\n</pre> import os import leafmap from samgeo import SamGeo, tms_to_geotiff, get_basemaps In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[29.676840, -95.369222], zoom=19)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[29.676840, -95.369222], zoom=19) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-95.3704, 29.6762, -95.368, 29.6775]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-95.3704, 29.6762, -95.368, 29.6775] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\n</pre> image = \"satellite.tif\" <p>Besides the <code>satellite</code> basemap, you can use any of the following basemaps returned by the <code>get_basemaps()</code> function:</p> In\u00a0[\u00a0]: Copied! <pre># get_basemaps().keys()\n</pre> # get_basemaps().keys() <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>tms_to_geotiff(output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True)\n</pre> tms_to_geotiff(output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False  # turn off the basemap\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False  # turn off the basemap m.add_raster(image, layer_name=\"Image\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    checkpoint=\"sam_vit_h_4b8939.pth\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     checkpoint=\"sam_vit_h_4b8939.pth\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>mask = \"segment.tif\"\nsam.generate(\n    image, mask, batch=True, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255\n)\n</pre> mask = \"segment.tif\" sam.generate(     image, mask, batch=True, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255 ) In\u00a0[\u00a0]: Copied! <pre>vector = \"segment.gpkg\"\nsam.tiff_to_gpkg(mask, vector, simplify_tolerance=None)\n</pre> vector = \"segment.gpkg\" sam.tiff_to_gpkg(mask, vector, simplify_tolerance=None) <p>You can also save the segmentation results as any vector data format supported by GeoPandas.</p> In\u00a0[\u00a0]: Copied! <pre>shapefile = \"segment.shp\"\nsam.tiff_to_vector(mask, shapefile)\n</pre> shapefile = \"segment.shp\" sam.tiff_to_vector(mask, shapefile) In\u00a0[\u00a0]: Copied! <pre>style = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(vector, layer_name=\"Vector\", style=style)\nm\n</pre> style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(vector, layer_name=\"Vector\", style=style) m <p></p>"},{"location":"examples/samgeo/satellite/#segment-anything-model-for-geospatial-data","title":"Segment Anything Model for Geospatial Data\u00b6","text":"<p>This notebook shows how to use segment satellite imagery using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/satellite/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/satellite/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/samgeo/satellite/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/satellite/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"examples/samgeo/satellite/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/samgeo/satellite/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Set <code>batch=True</code> to segment the image in batches. This is useful for large images that cannot fit in memory.</p>"},{"location":"examples/samgeo/satellite/#polygonize-the-raster-data","title":"Polygonize the raster data\u00b6","text":"<p>Save the segmentation results as a GeoPackage file.</p>"},{"location":"examples/samgeo/satellite/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/samgeo/swimming_pools/","title":"Swimming pools","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[34.040984, -118.491668], zoom=19, height=\"600px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[34.040984, -118.491668], zoom=19, height=\"600px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-118.4932, 34.0404, -118.4903, 34.0417]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-118.4932, 34.0404, -118.4903, 34.0417] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"swimming pool\"\n</pre> text_prompt = \"swimming pool\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Blues\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Swimming Pools\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Blues\",     box_color=\"red\",     title=\"Automatic Segmentation of Swimming Pools\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Blues\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Swimming Pools\",\n)\n</pre> sam.show_anns(     cmap=\"Blues\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Swimming Pools\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Swimming Pools\",\n    blend=False,\n    output=\"pools.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Swimming Pools\",     blend=False,     output=\"pools.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"pools.tif\", \"pools.shp\")\n</pre> sam.raster_to_vector(\"pools.tif\", \"pools.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"pools.tif\", layer_name=\"Pools\", palette=\"Blues\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"pools.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"pools.tif\", layer_name=\"Pools\", palette=\"Blues\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"pools.shp\", layer_name=\"Vector\", style=style) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/samgeo/swimming_pools/#mapping-swimming-pools-with-text-prompts","title":"Mapping swimming pools with text prompts\u00b6","text":"<p>This notebook shows how to map swimming pools with text prompts and the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/swimming_pools/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/swimming_pools/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/swimming_pools/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/swimming_pools/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/swimming_pools/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/samgeo/swimming_pools/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/samgeo/swimming_pools/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/samgeo/swimming_pools/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/samgeo/text_prompts/","title":"Text prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"trees.tif\", \"trees.shp\")\n</pre> sam.raster_to_vector(\"trees.tif\", \"trees.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style) m In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/samgeo/text_prompts/#segmenting-remote-sensing-imagery-with-text-prompts-and-the-segment-anything-model-sam","title":"Segmenting remote sensing imagery with text prompts and the Segment Anything Model (SAM)\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/text_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/text_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/text_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/text_prompts/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/text_prompts/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/samgeo/text_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/samgeo/text_prompts/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/samgeo/text_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/","title":"Text prompts batch","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff, split_raster\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff, split_raster from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.1278, -51.4430], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.1278, -51.4430], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.4494, -22.1307, -51.4371, -22.1244]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.4494, -22.1307, -51.4371, -22.1244] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>split_raster(image, out_dir=\"tiles\", tile_size=(1000, 1000), overlap=0)\n</pre> split_raster(image, out_dir=\"tiles\", tile_size=(1000, 1000), overlap=0) In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict_batch(\n    images=\"tiles\",\n    out_dir=\"masks\",\n    text_prompt=text_prompt,\n    box_threshold=0.24,\n    text_threshold=0.24,\n    mask_multiplier=255,\n    dtype=\"uint8\",\n    merge=True,\n    verbose=True,\n)\n</pre> sam.predict_batch(     images=\"tiles\",     out_dir=\"masks\",     text_prompt=text_prompt,     box_threshold=0.24,     text_threshold=0.24,     mask_multiplier=255,     dtype=\"uint8\",     merge=True,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"masks/merged.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm.add_layer_manager()\nm\n</pre> m.add_raster(\"masks/merged.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m.add_layer_manager() m <p></p>"},{"location":"examples/samgeo/text_prompts_batch/#batch-segmentation-with-text-prompts","title":"Batch segmentation with text prompts\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/text_prompts_batch/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/text_prompts_batch/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/text_prompts_batch/#split-the-image-into-tiles","title":"Split the image into tiles\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/text_prompts_batch/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/#segment-images","title":"Segment images\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/samgeo/text_prompts_batch/#visualize-the-results","title":"Visualize the results\u00b6","text":""}]}