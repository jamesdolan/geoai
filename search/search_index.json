{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GeoAI: Artificial Intelligence for Geospatial Data","text":"<p>A powerful Python package for integrating Artificial Intelligence with geospatial data analysis and visualization</p> <p>GeoAI bridges the gap between AI and geospatial analysis, providing tools for processing, analyzing, and visualizing geospatial data using advanced machine learning techniques. Whether you're working with satellite imagery, LiDAR point clouds, or vector data, GeoAI offers intuitive interfaces to apply cutting-edge AI models.</p> <ul> <li>\ud83d\udcd6 Documentation: https://geoai.gishub.org</li> <li>\ud83d\udcac Community: GitHub Discussions</li> <li>\ud83d\udc1b Issue Tracker: GitHub Issues</li> </ul>"},{"location":"#key-features","title":"\ud83d\ude80 Key Features","text":"<p>\u2757 Important notes: The GeoAI package is under active development and new features are being added regularly. Not all features listed below are available in the current release. If you have a feature request or would like to contribute, please let us know!</p>"},{"location":"#advanced-geospatial-data-visualization","title":"\ud83d\udcca Advanced Geospatial Data Visualization","text":"<ul> <li>Interactive multi-layer visualization of vector, raster, and point cloud data</li> <li>Customizable styling and symbology</li> <li>Time-series data visualization capabilities</li> </ul>"},{"location":"#data-preparation-processing","title":"\ud83d\udee0\ufe0f Data Preparation &amp; Processing","text":"<ul> <li>Streamlined access to satellite and aerial imagery from providers like Sentinel, Landsat, NAIP, and other open datasets</li> <li>Tools for downloading, mosaicking, and preprocessing remote sensing data</li> <li>Automated generation of training datasets with image chips and corresponding labels</li> <li>Vector-to-raster and raster-to-vector conversion utilities optimized for AI workflows</li> <li>Data augmentation techniques specific to geospatial data</li> <li>Support for integrating Overture Maps data and other open datasets for training and validation</li> </ul>"},{"location":"#image-segmentation","title":"\ud83d\uddbc\ufe0f Image Segmentation","text":"<ul> <li>Integration with Meta's Segment Anything Model (SAM) for automatic feature extraction</li> <li>Specialized segmentation algorithms optimized for satellite and aerial imagery</li> <li>Streamlined workflows for segmenting buildings, roads, vegetation, and water bodies</li> <li>Export capabilities to standard geospatial formats (GeoJSON, Shapefile, GeoPackage, GeoParquet)</li> </ul>"},{"location":"#image-classification","title":"\ud83d\udd0d Image Classification","text":"<ul> <li>Pre-trained models for land cover and land use classification</li> <li>Transfer learning utilities for fine-tuning models with your own data</li> <li>Multi-temporal classification support for change detection</li> <li>Accuracy assessment and validation tools</li> </ul>"},{"location":"#additional-capabilities","title":"\ud83c\udf0d Additional Capabilities","text":"<ul> <li>Terrain analysis with AI-enhanced feature extraction</li> <li>Point cloud classification and segmentation</li> <li>Object detection in aerial and satellite imagery</li> <li>Georeferencing utilities for AI model outputs</li> </ul>"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":""},{"location":"#using-pip","title":"Using pip","text":"<pre><code>pip install geoai-py\n</code></pre>"},{"location":"#using-conda","title":"Using conda","text":"<pre><code>conda install -c conda-forge geoai\n</code></pre>"},{"location":"#using-mamba","title":"Using mamba","text":"<pre><code>mamba install -c conda-forge geoai\n</code></pre>"},{"location":"#documentation","title":"\ud83d\udccb Documentation","text":"<p>Comprehensive documentation is available at https://geoai.gishub.org, including:</p> <ul> <li>Detailed API reference</li> <li>Tutorials and example notebooks</li> <li>Explanation of algorithms and models</li> <li>Best practices for geospatial AI</li> </ul>"},{"location":"#video-tutorials","title":"\ud83d\udcfa\u00a0Video Tutorials","text":"<p>Check out our YouTube channel for video tutorials on using GeoAI for geospatial data analysis and visualization.</p> <p></p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions of all kinds! See our contributing guide for ways to get started.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>GeoAI is free and open source software, licensed under the MIT License.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-aug-11-2023","title":"v0.0.1 - Aug 11, 2023","text":"<p>Initial release</p>"},{"location":"classify/","title":"classify module","text":"<p>The module for training semantic segmentation models for classifying remote sensing imagery.</p>"},{"location":"classify/#geoai.classify.classify_image","title":"<code>classify_image(image_path, model_path, output_path=None, chip_size=1024, overlap=256, batch_size=4, colormap=None, **kwargs)</code>","text":"<p>Classify a geospatial image using a trained semantic segmentation model.</p> <p>This function handles the full image classification pipeline with special attention to edge handling: 1. Process the image in a grid pattern with overlapping tiles 2. Use central regions of tiles for interior parts 3. Special handling for edges to ensure complete coverage 4. Merge results into a single georeferenced output</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the input GeoTIFF image.</p> required <code>model_path</code> <code>str</code> <p>Path to the trained model checkpoint.</p> required <code>output_path</code> <code>str</code> <p>Path to save the output classified image.                         Defaults to \"[input_name]_classified.tif\".</p> <code>None</code> <code>chip_size</code> <code>int</code> <p>Size of chips for processing. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>Overlap size between adjacent tiles. Defaults to 256.</p> <code>256</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference. Defaults to 4.</p> <code>4</code> <code>colormap</code> <code>dict</code> <p>Colormap to apply to the output image.                        Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for DataLoader.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the saved classified image.</p> Source code in <code>geoai/classify.py</code> <pre><code>def classify_image(\n    image_path,\n    model_path,\n    output_path=None,\n    chip_size=1024,\n    overlap=256,\n    batch_size=4,\n    colormap=None,\n    **kwargs,\n):\n    \"\"\"\n    Classify a geospatial image using a trained semantic segmentation model.\n\n    This function handles the full image classification pipeline with special\n    attention to edge handling:\n    1. Process the image in a grid pattern with overlapping tiles\n    2. Use central regions of tiles for interior parts\n    3. Special handling for edges to ensure complete coverage\n    4. Merge results into a single georeferenced output\n\n    Parameters:\n        image_path (str): Path to the input GeoTIFF image.\n        model_path (str): Path to the trained model checkpoint.\n        output_path (str, optional): Path to save the output classified image.\n                                    Defaults to \"[input_name]_classified.tif\".\n        chip_size (int, optional): Size of chips for processing. Defaults to 1024.\n        overlap (int, optional): Overlap size between adjacent tiles. Defaults to 256.\n        batch_size (int, optional): Batch size for inference. Defaults to 4.\n        colormap (dict, optional): Colormap to apply to the output image.\n                                   Defaults to None.\n        **kwargs: Additional keyword arguments for DataLoader.\n\n    Returns:\n        str: Path to the saved classified image.\n    \"\"\"\n    import timeit\n\n    import torch\n    from torchgeo.trainers import SemanticSegmentationTask\n\n    import rasterio\n\n    import warnings\n    from rasterio.errors import NotGeoreferencedWarning\n\n    # Disable specific GDAL/rasterio warnings\n    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"rasterio._.*\")\n    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"rasterio\")\n    warnings.filterwarnings(\"ignore\", category=NotGeoreferencedWarning)\n\n    # Also suppress GDAL error reports\n    import logging\n\n    logging.getLogger(\"rasterio\").setLevel(logging.ERROR)\n\n    # Set default output path if not provided\n    if output_path is None:\n        base_name = os.path.splitext(os.path.basename(image_path))[0]\n        output_path = f\"{base_name}_classified.tif\"\n\n    # Make sure output directory exists\n    output_dir = os.path.dirname(output_path)\n    if output_dir and not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Load the model\n    print(f\"Loading model from {model_path}...\")\n    task = SemanticSegmentationTask.load_from_checkpoint(model_path)\n    task.model.eval()\n    task.model.cuda()\n\n    # Process the image using a modified tiling approach\n    with rasterio.open(image_path) as src:\n        # Get image dimensions and metadata\n        height = src.height\n        width = src.width\n        profile = src.profile.copy()\n\n        # Prepare output array for the final result\n        output_image = np.zeros((height, width), dtype=np.uint8)\n        confidence_map = np.zeros((height, width), dtype=np.float32)\n\n        # Calculate number of tiles needed with overlap\n        # Ensure we have tiles that specifically cover the edges\n        effective_stride = chip_size - overlap\n\n        # Calculate x positions ensuring leftmost and rightmost edges are covered\n        x_positions = []\n        # Always include the leftmost position\n        x_positions.append(0)\n        # Add regular grid positions\n        for x in range(effective_stride, width - chip_size, effective_stride):\n            x_positions.append(x)\n        # Always include rightmost position that still fits\n        if width &gt; chip_size and x_positions[-1] + chip_size &lt; width:\n            x_positions.append(width - chip_size)\n\n        # Calculate y positions ensuring top and bottom edges are covered\n        y_positions = []\n        # Always include the topmost position\n        y_positions.append(0)\n        # Add regular grid positions\n        for y in range(effective_stride, height - chip_size, effective_stride):\n            y_positions.append(y)\n        # Always include bottommost position that still fits\n        if height &gt; chip_size and y_positions[-1] + chip_size &lt; height:\n            y_positions.append(height - chip_size)\n\n        # Create list of all tile positions\n        tile_positions = []\n        for y in y_positions:\n            for x in x_positions:\n                y_end = min(y + chip_size, height)\n                x_end = min(x + chip_size, width)\n                tile_positions.append((y, x, y_end, x_end))\n\n        # Print information about the tiling\n        print(\n            f\"Processing {len(tile_positions)} patches covering an image of size {height}x{width}...\"\n        )\n        start_time = timeit.default_timer()\n\n        # Process tiles in batches\n        for batch_start in range(0, len(tile_positions), batch_size):\n            batch_end = min(batch_start + batch_size, len(tile_positions))\n            batch_positions = tile_positions[batch_start:batch_end]\n            batch_data = []\n\n            # Load data for current batch\n            for y_start, x_start, y_end, x_end in batch_positions:\n                # Calculate actual tile size\n                actual_height = y_end - y_start\n                actual_width = x_end - x_start\n\n                # Read the tile data\n                tile_data = src.read(window=((y_start, y_end), (x_start, x_end)))\n\n                # Handle different sized tiles by padding if necessary\n                if tile_data.shape[1] != chip_size or tile_data.shape[2] != chip_size:\n                    padded_data = np.zeros(\n                        (tile_data.shape[0], chip_size, chip_size),\n                        dtype=tile_data.dtype,\n                    )\n                    padded_data[:, : tile_data.shape[1], : tile_data.shape[2]] = (\n                        tile_data\n                    )\n                    tile_data = padded_data\n\n                # Convert to tensor\n\n                tile_tensor = torch.from_numpy(tile_data).float() / 255.0\n                batch_data.append(tile_tensor)\n\n            # Convert batch to tensor\n            batch_tensor = torch.stack(batch_data)\n\n            # Run inference\n            with torch.no_grad():\n                logits = task.model.predict(batch_tensor.cuda())\n                probs = torch.softmax(logits, dim=1)\n                confidence, predictions = torch.max(probs, dim=1)\n                predictions = predictions.cpu().numpy()\n                confidence = confidence.cpu().numpy()\n\n            # Process each prediction\n            for idx, (y_start, x_start, y_end, x_end) in enumerate(batch_positions):\n                pred = predictions[idx]\n                conf = confidence[idx]\n\n                # Calculate actual tile size\n                actual_height = y_end - y_start\n                actual_width = x_end - x_start\n\n                # Get the actual prediction (removing padding if needed)\n                valid_pred = pred[:actual_height, :actual_width]\n                valid_conf = conf[:actual_height, :actual_width]\n\n                # Create confidence weights that favor central parts of tiles\n                # but still allow edge tiles to contribute fully at the image edges\n                is_edge_x = (x_start == 0) or (x_end == width)\n                is_edge_y = (y_start == 0) or (y_end == height)\n\n                # Create a mask that gives higher weight to central regions\n                # but ensures proper edge handling for boundary tiles\n                weight_mask = np.ones((actual_height, actual_width), dtype=np.float32)\n\n                # Only apply central weighting if not at an image edge\n                border = overlap // 2\n                if not is_edge_x and actual_width &gt; 2 * border:\n                    # Apply horizontal edge falloff (linear)\n                    for i in range(border):\n                        # Left edge\n                        weight_mask[:, i] = (i + 1) / (border + 1)\n                        # Right edge (if not at image edge)\n                        if i &lt; actual_width - border:\n                            weight_mask[:, actual_width - i - 1] = (i + 1) / (\n                                border + 1\n                            )\n\n                if not is_edge_y and actual_height &gt; 2 * border:\n                    # Apply vertical edge falloff (linear)\n                    for i in range(border):\n                        # Top edge\n                        weight_mask[i, :] = (i + 1) / (border + 1)\n                        # Bottom edge (if not at image edge)\n                        if i &lt; actual_height - border:\n                            weight_mask[actual_height - i - 1, :] = (i + 1) / (\n                                border + 1\n                            )\n\n                # Combine with prediction confidence\n                final_weight = weight_mask * valid_conf\n\n                # Update the output image based on confidence\n                current_conf = confidence_map[y_start:y_end, x_start:x_end]\n                update_mask = final_weight &gt; current_conf\n\n                if np.any(update_mask):\n                    # Update only pixels where this prediction has higher confidence\n                    output_image[y_start:y_end, x_start:x_end][update_mask] = (\n                        valid_pred[update_mask]\n                    )\n                    confidence_map[y_start:y_end, x_start:x_end][update_mask] = (\n                        final_weight[update_mask]\n                    )\n\n        # Update profile for output\n        profile.update({\"count\": 1, \"dtype\": \"uint8\", \"nodata\": 0})\n\n        # Save the result\n        print(f\"Saving classified image to {output_path}...\")\n        with rasterio.open(output_path, \"w\", **profile) as dst:\n            dst.write(output_image[np.newaxis, :, :])\n            if isinstance(colormap, dict):\n                dst.write_colormap(1, colormap)\n\n        # Calculate timing\n        total_time = timeit.default_timer() - start_time\n        print(f\"Total processing time: {total_time:.2f} seconds\")\n        print(f\"Successfully saved classified image to {output_path}\")\n\n    return output_path\n</code></pre>"},{"location":"classify/#geoai.classify.classify_images","title":"<code>classify_images(image_paths, model_path, output_dir=None, chip_size=1024, batch_size=4, colormap=None, file_extension='.tif', **kwargs)</code>","text":"<p>Classify multiple geospatial images using a trained semantic segmentation model.</p> <p>This function accepts either a list of image paths or a directory containing images and applies the classify_image function to each image, saving the results in the specified output directory.</p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>str or list</code> <p>Either a directory path containing images or a list of paths to input GeoTIFF images.</p> required <code>model_path</code> <code>str</code> <p>Path to the trained model checkpoint.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save the output classified images. Defaults to None (same directory as input images for a list, or a new \"classified\" subdirectory for a directory input).</p> <code>None</code> <code>chip_size</code> <code>int</code> <p>Size of chips for processing. Defaults to 1024.</p> <code>1024</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference. Defaults to 4.</p> <code>4</code> <code>colormap</code> <code>dict</code> <p>Colormap to apply to the output images. Defaults to None.</p> <code>None</code> <code>file_extension</code> <code>str</code> <p>File extension to filter by when image_paths is a directory. Defaults to \".tif\".</p> <code>'.tif'</code> <code>**kwargs</code> <p>Additional keyword arguments for the classify_image function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list</code> <p>List of paths to the saved classified images.</p> Source code in <code>geoai/classify.py</code> <pre><code>def classify_images(\n    image_paths,\n    model_path,\n    output_dir=None,\n    chip_size=1024,\n    batch_size=4,\n    colormap=None,\n    file_extension=\".tif\",\n    **kwargs,\n):\n    \"\"\"\n    Classify multiple geospatial images using a trained semantic segmentation model.\n\n    This function accepts either a list of image paths or a directory containing images\n    and applies the classify_image function to each image, saving the results in the\n    specified output directory.\n\n    Parameters:\n        image_paths (str or list): Either a directory path containing images or a list\n            of paths to input GeoTIFF images.\n        model_path (str): Path to the trained model checkpoint.\n        output_dir (str, optional): Directory to save the output classified images.\n            Defaults to None (same directory as input images for a list, or a new\n            \"classified\" subdirectory for a directory input).\n        chip_size (int, optional): Size of chips for processing. Defaults to 1024.\n        batch_size (int, optional): Batch size for inference. Defaults to 4.\n        colormap (dict, optional): Colormap to apply to the output images.\n            Defaults to None.\n        file_extension (str, optional): File extension to filter by when image_paths\n            is a directory. Defaults to \".tif\".\n        **kwargs: Additional keyword arguments for the classify_image function.\n\n    Returns:\n        list: List of paths to the saved classified images.\n    \"\"\"\n    # Import required libraries\n    from tqdm import tqdm\n    import glob\n\n    # Process directory input\n    if isinstance(image_paths, str) and os.path.isdir(image_paths):\n        # Set default output directory if not provided\n        if output_dir is None:\n            output_dir = os.path.join(image_paths, \"classified\")\n\n        # Get all images with the specified extension\n        image_path_list = glob.glob(os.path.join(image_paths, f\"*{file_extension}\"))\n\n        # Check if any images were found\n        if not image_path_list:\n            print(f\"No files with extension '{file_extension}' found in {image_paths}\")\n            return []\n\n        print(f\"Found {len(image_path_list)} images in directory {image_paths}\")\n\n    # Process list input\n    elif isinstance(image_paths, list):\n        image_path_list = image_paths\n\n        # Set default output directory if not provided\n        if output_dir is None and len(image_path_list) &gt; 0:\n            output_dir = os.path.dirname(image_path_list[0])\n\n    # Invalid input\n    else:\n        raise ValueError(\n            \"image_paths must be either a directory path or a list of file paths\"\n        )\n\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    classified_image_paths = []\n\n    # Create progress bar\n    for image_path in tqdm(image_path_list, desc=\"Classifying images\", unit=\"image\"):\n        try:\n            # Get just the filename without extension\n            base_filename = os.path.splitext(os.path.basename(image_path))[0]\n\n            # Create output path within output_dir\n            output_path = os.path.join(\n                output_dir, f\"{base_filename}_classified{file_extension}\"\n            )\n\n            # Perform classification\n            classified_image_path = classify_image(\n                image_path,\n                model_path,\n                output_path=output_path,\n                chip_size=chip_size,\n                batch_size=batch_size,\n                colormap=colormap,\n                **kwargs,\n            )\n            classified_image_paths.append(classified_image_path)\n        except Exception as e:\n            print(f\"Error processing {image_path}: {str(e)}\")\n\n    print(\n        f\"Classification complete. Processed {len(classified_image_paths)} images successfully.\"\n    )\n    return classified_image_paths\n</code></pre>"},{"location":"classify/#geoai.classify.train_classifier","title":"<code>train_classifier(image_root, label_root, output_dir='output', in_channels=4, num_classes=14, epochs=20, img_size=256, batch_size=8, sample_size=500, model='unet', backbone='resnet50', weights=True, num_filters=3, loss='ce', class_weights=None, ignore_index=None, lr=0.001, patience=10, freeze_backbone=False, freeze_decoder=False, transforms=None, use_augmentation=False, seed=42, train_val_test_split=(0.6, 0.2, 0.2), accelerator='auto', devices='auto', logger=None, callbacks=None, log_every_n_steps=10, use_distributed_sampler=False, monitor_metric='val_loss', mode='min', save_top_k=1, save_last=True, checkpoint_filename='best_model', checkpoint_path=None, every_n_epochs=1, **kwargs)</code>","text":"<p>Train a semantic segmentation model on geospatial imagery.</p> <p>This function sets up datasets, model, trainer, and executes the training process for semantic segmentation tasks using geospatial data. It supports training from scratch or resuming from a checkpoint if available.</p> <p>Parameters:</p> Name Type Description Default <code>image_root</code> <code>str</code> <p>Path to directory containing imagery.</p> required <code>label_root</code> <code>str</code> <p>Path to directory containing land cover labels.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save model outputs and checkpoints. Defaults to \"output\".</p> <code>'output'</code> <code>in_channels</code> <code>int</code> <p>Number of input channels in the imagery. Defaults to 4.</p> <code>4</code> <code>num_classes</code> <code>int</code> <p>Number of classes in the segmentation task. Defaults to 14.</p> <code>14</code> <code>epochs</code> <code>int</code> <p>Number of training epochs. Defaults to 20.</p> <code>20</code> <code>img_size</code> <code>int</code> <p>Size of image patches for training. Defaults to 256.</p> <code>256</code> <code>batch_size</code> <code>int</code> <p>Batch size for training. Defaults to 8.</p> <code>8</code> <code>sample_size</code> <code>int</code> <p>Number of samples per epoch. Defaults to 500.</p> <code>500</code> <code>model</code> <code>str</code> <p>Model architecture to use. Defaults to \"unet\".</p> <code>'unet'</code> <code>backbone</code> <code>str</code> <p>Backbone network for the model. Defaults to \"resnet50\".</p> <code>'resnet50'</code> <code>weights</code> <code>bool</code> <p>Whether to use pretrained weights. Defaults to True.</p> <code>True</code> <code>num_filters</code> <code>int</code> <p>Number of filters for the model. Defaults to 3.</p> <code>3</code> <code>loss</code> <code>str</code> <p>Loss function to use ('ce', 'jaccard', or 'focal'). Defaults to \"ce\".</p> <code>'ce'</code> <code>class_weights</code> <code>list</code> <p>Class weights for loss function. Defaults to None.</p> <code>None</code> <code>ignore_index</code> <code>int</code> <p>Index to ignore in loss calculation. Defaults to None.</p> <code>None</code> <code>lr</code> <code>float</code> <p>Learning rate. Defaults to 0.001.</p> <code>0.001</code> <code>patience</code> <code>int</code> <p>Number of epochs with no improvement after which training will stop. Defaults to 10.</p> <code>10</code> <code>freeze_backbone</code> <code>bool</code> <p>Whether to freeze backbone. Defaults to False.</p> <code>False</code> <code>freeze_decoder</code> <code>bool</code> <p>Whether to freeze decoder. Defaults to False.</p> <code>False</code> <code>transforms</code> <code>callable</code> <p>Transforms to apply to the data. Defaults to None.</p> <code>None</code> <code>use_augmentation</code> <code>bool</code> <p>Whether to apply data augmentation. Defaults to False.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <code>train_val_test_split</code> <code>list</code> <p>Proportions for train/val/test split. Defaults to [0.6, 0.2, 0.2].</p> <code>(0.6, 0.2, 0.2)</code> <code>accelerator</code> <code>str</code> <p>Accelerator to use for training ('cpu', 'gpu', etc.). Defaults to \"auto\".</p> <code>'auto'</code> <code>devices</code> <code>str</code> <p>Number of devices to use for training. Defaults to \"auto\".</p> <code>'auto'</code> <code>logger</code> <code>object</code> <p>Logger for tracking training progress. Defaults to None.</p> <code>None</code> <code>callbacks</code> <code>list</code> <p>List of callbacks for the trainer. Defaults to None.</p> <code>None</code> <code>log_every_n_steps</code> <code>int</code> <p>Frequency of logging training progress. Defaults to 10.</p> <code>10</code> <code>use_distributed_sampler</code> <code>bool</code> <p>Whether to use distributed sampling. Defaults to False.</p> <code>False</code> <code>monitor_metric</code> <code>str</code> <p>Metric to monitor for saving best model. Defaults to \"val_loss\".</p> <code>'val_loss'</code> <code>mode</code> <code>str</code> <p>Mode for monitoring metric ('min' or 'max'). Use 'min' for losses and 'max' for metrics like accuracy. Defaults to \"min\".</p> <code>'min'</code> <code>save_top_k</code> <code>int</code> <p>Number of best models to save. Defaults to 1.</p> <code>1</code> <code>save_last</code> <code>bool</code> <p>Whether to save the model from the last epoch. Defaults to True.</p> <code>True</code> <code>checkpoint_filename</code> <code>str</code> <p>Filename pattern for saved checkpoints. Defaults to \"best_model_{epoch:02d}_{val_loss:.4f}\".</p> <code>'best_model'</code> <code>checkpoint_path</code> <code>str</code> <p>Path to a checkpoint file to resume training.</p> <code>None</code> <code>every_n_epochs</code> <code>int</code> <p>Save a checkpoint every N epochs. Defaults to 1.</p> <code>1</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the datasets.</p> <code>{}</code> <p>Returns:</p> Type Description <code>object</code> <p>Trained SemanticSegmentationTask model.</p> Source code in <code>geoai/classify.py</code> <pre><code>def train_classifier(\n    image_root,\n    label_root,\n    output_dir=\"output\",\n    in_channels=4,\n    num_classes=14,\n    epochs=20,\n    img_size=256,\n    batch_size=8,\n    sample_size=500,\n    model=\"unet\",\n    backbone=\"resnet50\",\n    weights=True,\n    num_filters=3,\n    loss=\"ce\",\n    class_weights=None,\n    ignore_index=None,\n    lr=0.001,\n    patience=10,\n    freeze_backbone=False,\n    freeze_decoder=False,\n    transforms=None,\n    use_augmentation=False,\n    seed=42,\n    train_val_test_split=(0.6, 0.2, 0.2),\n    accelerator=\"auto\",\n    devices=\"auto\",\n    logger=None,\n    callbacks=None,\n    log_every_n_steps=10,\n    use_distributed_sampler=False,\n    monitor_metric=\"val_loss\",\n    mode=\"min\",\n    save_top_k=1,\n    save_last=True,\n    checkpoint_filename=\"best_model\",\n    checkpoint_path=None,\n    every_n_epochs=1,\n    **kwargs,\n):\n    \"\"\"Train a semantic segmentation model on geospatial imagery.\n\n    This function sets up datasets, model, trainer, and executes the training process\n    for semantic segmentation tasks using geospatial data. It supports training\n    from scratch or resuming from a checkpoint if available.\n\n    Args:\n        image_root (str): Path to directory containing imagery.\n        label_root (str): Path to directory containing land cover labels.\n        output_dir (str, optional): Directory to save model outputs and checkpoints.\n            Defaults to \"output\".\n        in_channels (int, optional): Number of input channels in the imagery.\n            Defaults to 4.\n        num_classes (int, optional): Number of classes in the segmentation task.\n            Defaults to 14.\n        epochs (int, optional): Number of training epochs. Defaults to 20.\n        img_size (int, optional): Size of image patches for training. Defaults to 256.\n        batch_size (int, optional): Batch size for training. Defaults to 8.\n        sample_size (int, optional): Number of samples per epoch. Defaults to 500.\n        model (str, optional): Model architecture to use. Defaults to \"unet\".\n        backbone (str, optional): Backbone network for the model. Defaults to \"resnet50\".\n        weights (bool, optional): Whether to use pretrained weights. Defaults to True.\n        num_filters (int, optional): Number of filters for the model. Defaults to 3.\n        loss (str, optional): Loss function to use ('ce', 'jaccard', or 'focal').\n            Defaults to \"ce\".\n        class_weights (list, optional): Class weights for loss function. Defaults to None.\n        ignore_index (int, optional): Index to ignore in loss calculation. Defaults to None.\n        lr (float, optional): Learning rate. Defaults to 0.001.\n        patience (int, optional): Number of epochs with no improvement after which\n            training will stop. Defaults to 10.\n        freeze_backbone (bool, optional): Whether to freeze backbone. Defaults to False.\n        freeze_decoder (bool, optional): Whether to freeze decoder. Defaults to False.\n        transforms (callable, optional): Transforms to apply to the data. Defaults to None.\n        use_augmentation (bool, optional): Whether to apply data augmentation.\n            Defaults to False.\n        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n        train_val_test_split (list, optional): Proportions for train/val/test split.\n            Defaults to [0.6, 0.2, 0.2].\n        accelerator (str, optional): Accelerator to use for training ('cpu', 'gpu', etc.).\n            Defaults to \"auto\".\n        devices (str, optional): Number of devices to use for training. Defaults to \"auto\".\n        logger (object, optional): Logger for tracking training progress. Defaults to None.\n        callbacks (list, optional): List of callbacks for the trainer. Defaults to None.\n        log_every_n_steps (int, optional): Frequency of logging training progress.\n            Defaults to 10.\n        use_distributed_sampler (bool, optional): Whether to use distributed sampling.\n            Defaults to False.\n        monitor_metric (str, optional): Metric to monitor for saving best model.\n            Defaults to \"val_loss\".\n        mode (str, optional): Mode for monitoring metric ('min' or 'max').\n            Use 'min' for losses and 'max' for metrics like accuracy.\n            Defaults to \"min\".\n        save_top_k (int, optional): Number of best models to save.\n            Defaults to 1.\n        save_last (bool, optional): Whether to save the model from the last epoch.\n            Defaults to True.\n        checkpoint_filename (str, optional): Filename pattern for saved checkpoints.\n            Defaults to \"best_model_{epoch:02d}_{val_loss:.4f}\".\n        checkpoint_path (str, optional): Path to a checkpoint file to resume training.\n        every_n_epochs (int, optional): Save a checkpoint every N epochs.\n            Defaults to 1.\n        **kwargs: Additional keyword arguments to pass to the datasets.\n\n    Returns:\n        object: Trained SemanticSegmentationTask model.\n    \"\"\"\n    import lightning.pytorch as pl\n    from torch.utils.data import DataLoader\n    from torchgeo.datasets import stack_samples, RasterDataset\n    from torchgeo.datasets.splits import random_bbox_assignment\n    from torchgeo.samplers import (\n        RandomGeoSampler,\n        RandomBatchGeoSampler,\n        GridGeoSampler,\n    )\n    import torch\n    import multiprocessing as mp\n    import timeit\n    import albumentations as A\n    from torchgeo.datamodules import GeoDataModule\n    from torchgeo.trainers import SemanticSegmentationTask\n    from lightning.pytorch.callbacks import ModelCheckpoint\n    from lightning.pytorch.loggers import CSVLogger\n\n    # Create a wrapper class for albumentations to work with TorchGeo format\n    class AlbumentationsWrapper:\n        def __init__(self, transform):\n            self.transform = transform\n\n        def __call__(self, sample):\n            # Extract image and mask from TorchGeo sample format\n            if \"image\" not in sample or \"mask\" not in sample:\n                return sample\n\n            image = sample[\"image\"]\n            mask = sample[\"mask\"]\n\n            # Albumentations expects channels last, but TorchGeo uses channels first\n            # Convert (C, H, W) to (H, W, C) for image\n            image_np = image.permute(1, 2, 0).numpy()\n            mask_np = mask.squeeze(0).numpy() if mask.dim() &gt; 2 else mask.numpy()\n\n            # Apply transformation with named arguments\n            transformed = self.transform(image=image_np, mask=mask_np)\n\n            # Convert back to PyTorch tensors with channels first\n            transformed_image = torch.from_numpy(transformed[\"image\"]).permute(2, 0, 1)\n            transformed_mask = torch.from_numpy(transformed[\"mask\"]).unsqueeze(0)\n\n            # Update the sample dictionary\n            result = sample.copy()\n            result[\"image\"] = transformed_image\n            result[\"mask\"] = transformed_mask\n\n            return result\n\n    # Set up data augmentation if requested\n    if use_augmentation:\n        aug_transforms = A.Compose(\n            [\n                A.HorizontalFlip(p=0.5),\n                A.VerticalFlip(p=0.5),\n                A.RandomRotate90(p=0.5),\n                A.ShiftScaleRotate(\n                    p=0.5, shift_limit=0.0625, scale_limit=0.1, rotate_limit=45\n                ),\n                A.RandomBrightnessContrast(\n                    p=0.5, brightness_limit=0.2, contrast_limit=0.2\n                ),\n                A.GaussianBlur(p=0.3),\n                A.GaussNoise(p=0.3),\n                A.CoarseDropout(p=0.3, max_holes=8, max_height=32, max_width=32),\n            ]\n        )\n        # Wrap the albumentations transforms\n        transforms = AlbumentationsWrapper(aug_transforms)\n\n    # # Set up device configuration\n    # device, num_devices = (\n    #     (\"cuda\", torch.cuda.device_count())\n    #     if torch.cuda.is_available()\n    #     else (\"cpu\", mp.cpu_count())\n    # )\n    workers = mp.cpu_count()\n    # print(f\"Running on {num_devices} {device}(s)\")\n\n    # Define datasets\n    class ImageDatasetClass(RasterDataset):\n        filename_glob = \"*.tif\"\n        is_image = True\n        separate_files = False\n\n    class LabelDatasetClass(RasterDataset):\n        filename_glob = \"*.tif\"\n        is_image = False\n        separate_files = False\n\n    # Prepare output directory\n    test_dir = os.path.join(output_dir, \"models\")\n    if not os.path.exists(test_dir):\n        os.makedirs(test_dir)\n\n    # Set up logger and checkpoint callback\n    if logger is None:\n        logger = CSVLogger(test_dir, name=\"lightning_logs\")\n\n    if callbacks is None:\n        checkpoint_callback = ModelCheckpoint(\n            dirpath=test_dir,\n            filename=checkpoint_filename,\n            save_top_k=save_top_k,\n            monitor=monitor_metric,\n            mode=mode,\n            save_last=save_last,\n            every_n_epochs=every_n_epochs,\n            verbose=True,\n        )\n        callbacks = [checkpoint_callback]\n\n    # Initialize the segmentation task\n    task = SemanticSegmentationTask(\n        model=model,\n        backbone=backbone,\n        weights=weights,\n        in_channels=in_channels,\n        num_classes=num_classes,\n        num_filters=num_filters,\n        loss=loss,\n        class_weights=class_weights,\n        ignore_index=ignore_index,\n        lr=lr,\n        patience=patience,\n        freeze_backbone=freeze_backbone,\n        freeze_decoder=freeze_decoder,\n    )\n\n    # Set up trainer\n    trainer = pl.Trainer(\n        accelerator=accelerator,\n        devices=devices,\n        max_epochs=epochs,\n        callbacks=callbacks,\n        logger=logger,\n        log_every_n_steps=log_every_n_steps,\n        use_distributed_sampler=use_distributed_sampler,\n        **kwargs,  # Pass any additional kwargs to the trainer\n    )\n\n    # Load datasets with transforms if augmentation is enabled\n\n    if isinstance(image_root, RasterDataset):\n        images = image_root\n    else:\n        images = ImageDatasetClass(paths=image_root, transforms=transforms, **kwargs)\n\n    if isinstance(label_root, RasterDataset):\n        labels = label_root\n    else:\n        labels = LabelDatasetClass(paths=label_root, **kwargs)\n\n    # Create intersection dataset\n    dataset = images &amp; labels\n\n    # Define custom datamodule for training\n    class CustomGeoDataModule(GeoDataModule):\n        def setup(self, stage: str) -&gt; None:\n            \"\"\"Set up datasets.\n\n            Args:\n                stage: Either 'fit', 'validate', 'test', or 'predict'.\n            \"\"\"\n            self.dataset = self.dataset_class(**self.kwargs)\n\n            generator = torch.Generator().manual_seed(seed)\n            (\n                self.train_dataset,\n                self.val_dataset,\n                self.test_dataset,\n            ) = random_bbox_assignment(dataset, train_val_test_split, generator)\n\n            if stage in [\"fit\"]:\n                self.train_batch_sampler = RandomBatchGeoSampler(\n                    self.train_dataset, self.patch_size, self.batch_size, self.length\n                )\n            if stage in [\"fit\", \"validate\"]:\n                self.val_sampler = GridGeoSampler(\n                    self.val_dataset, self.patch_size, self.patch_size\n                )\n            if stage in [\"test\"]:\n                self.test_sampler = GridGeoSampler(\n                    self.test_dataset, self.patch_size, self.patch_size\n                )\n\n    # Create datamodule\n    datamodule = CustomGeoDataModule(\n        dataset_class=type(dataset),\n        batch_size=batch_size,\n        patch_size=img_size,\n        length=sample_size,\n        num_workers=workers,\n        dataset1=images,\n        dataset2=labels,\n        collate_fn=stack_samples,\n    )\n\n    # Start training timer\n    start = timeit.default_timer()\n\n    # Check for existing checkpoint\n    if checkpoint_path is not None:\n        checkpoint_file = os.path.abspath(checkpoint_path)\n    else:\n        checkpoint_file = os.path.join(test_dir, \"last.ckpt\")\n\n    if os.path.isfile(checkpoint_file):\n        print(\"Resuming training from previous checkpoint...\")\n        trainer.fit(model=task, datamodule=datamodule, ckpt_path=checkpoint_file)\n    else:\n        print(\"Starting training from scratch...\")\n        trainer.fit(\n            model=task,\n            datamodule=datamodule,\n        )\n\n    training_time = timeit.default_timer() - start\n    print(f\"The time taken to train was: {training_time:.2f} seconds\")\n\n    best_model_path = checkpoint_callback.best_model_path\n    print(f\"Best model saved at: {best_model_path}\")\n\n    # Test the model\n    trainer.test(model=task, datamodule=datamodule)\n\n    return task\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/opengeos/geoai/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>geoai could always use more documentation, whether as part of the official geoai docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/opengeos/geoai/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up geoai for local development.</p> <ol> <li> <p>Fork the geoai repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/geoai.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv geoai\n$ cd geoai/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 geoai tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and     for PyPy. Check https://github.com/opengeos/geoai/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"download/","title":"download module","text":"<p>This module provides functions to download data, including NAIP imagery and building data from Overture Maps.</p>"},{"location":"download/#geoai.download.convert_vector_format","title":"<code>convert_vector_format(input_file, output_format='geojson', filter_expression=None)</code>","text":"<p>Convert the downloaded data to a different format or filter it.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>Path to the input file.</p> required <code>output_format</code> <code>str</code> <p>Format to convert to, one of \"geojson\", \"parquet\", \"shapefile\", \"csv\".</p> <code>'geojson'</code> <code>filter_expression</code> <code>Optional[str]</code> <p>Optional GeoDataFrame query expression to filter the data.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the converted file.</p> Source code in <code>geoai/download.py</code> <pre><code>def convert_vector_format(\n    input_file: str,\n    output_format: str = \"geojson\",\n    filter_expression: Optional[str] = None,\n) -&gt; str:\n    \"\"\"Convert the downloaded data to a different format or filter it.\n\n    Args:\n        input_file: Path to the input file.\n        output_format: Format to convert to, one of \"geojson\", \"parquet\", \"shapefile\", \"csv\".\n        filter_expression: Optional GeoDataFrame query expression to filter the data.\n\n    Returns:\n        Path to the converted file.\n    \"\"\"\n    try:\n        # Read the input file\n        logger.info(f\"Reading {input_file}\")\n        gdf = gpd.read_file(input_file)\n\n        # Apply filter if specified\n        if filter_expression:\n            logger.info(f\"Filtering data using expression: {filter_expression}\")\n            gdf = gdf.query(filter_expression)\n            logger.info(f\"After filtering: {len(gdf)} features\")\n\n        # Define output file path\n        base_path = os.path.splitext(input_file)[0]\n\n        if output_format == \"geojson\":\n            output_file = f\"{base_path}.geojson\"\n            logger.info(f\"Converting to GeoJSON: {output_file}\")\n            gdf.to_file(output_file, driver=\"GeoJSON\")\n        elif output_format == \"parquet\":\n            output_file = f\"{base_path}.parquet\"\n            logger.info(f\"Converting to Parquet: {output_file}\")\n            gdf.to_parquet(output_file)\n        elif output_format == \"shapefile\":\n            output_file = f\"{base_path}.shp\"\n            logger.info(f\"Converting to Shapefile: {output_file}\")\n            gdf.to_file(output_file)\n        elif output_format == \"csv\":\n            output_file = f\"{base_path}.csv\"\n            logger.info(f\"Converting to CSV: {output_file}\")\n\n            # For CSV, we need to convert geometry to WKT\n            gdf[\"geometry_wkt\"] = gdf.geometry.apply(lambda g: g.wkt)\n\n            # Save to CSV with geometry as WKT\n            gdf.drop(columns=[\"geometry\"]).to_csv(output_file, index=False)\n        else:\n            raise ValueError(f\"Unsupported output format: {output_format}\")\n\n        return output_file\n\n    except Exception as e:\n        logger.error(f\"Error converting data: {str(e)}\")\n        raise\n</code></pre>"},{"location":"download/#geoai.download.download_naip","title":"<code>download_naip(bbox, output_dir, year=None, max_items=10, overwrite=False, preview=False, **kwargs)</code>","text":"<p>Download NAIP imagery from Planetary Computer based on a bounding box.</p> <p>This function searches for NAIP (National Agriculture Imagery Program) imagery from Microsoft's Planetary Computer that intersects with the specified bounding box. It downloads the imagery and saves it as GeoTIFF files.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save the downloaded imagery.</p> required <code>year</code> <code>Optional[int]</code> <p>Specific year of NAIP imagery to download (e.g., 2020). If None, returns imagery from all available years.</p> <code>None</code> <code>max_items</code> <code>int</code> <p>Maximum number of items to download.</p> <code>10</code> <code>overwrite</code> <code>bool</code> <p>If True, overwrite existing files with the same name.</p> <code>False</code> <code>preview</code> <code>bool</code> <p>If True, display a preview of the downloaded imagery.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of downloaded file paths.</p> <p>Exceptions:</p> Type Description <code>Exception</code> <p>If there is an error downloading or saving the imagery.</p> Source code in <code>geoai/download.py</code> <pre><code>def download_naip(\n    bbox: Tuple[float, float, float, float],\n    output_dir: str,\n    year: Optional[int] = None,\n    max_items: int = 10,\n    overwrite: bool = False,\n    preview: bool = False,\n    **kwargs: Any,\n) -&gt; List[str]:\n    \"\"\"Download NAIP imagery from Planetary Computer based on a bounding box.\n\n    This function searches for NAIP (National Agriculture Imagery Program) imagery\n    from Microsoft's Planetary Computer that intersects with the specified bounding box.\n    It downloads the imagery and saves it as GeoTIFF files.\n\n    Args:\n        bbox: Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.\n        output_dir: Directory to save the downloaded imagery.\n        year: Specific year of NAIP imagery to download (e.g., 2020). If None, returns imagery from all available years.\n        max_items: Maximum number of items to download.\n        overwrite: If True, overwrite existing files with the same name.\n        preview: If True, display a preview of the downloaded imagery.\n\n    Returns:\n        List of downloaded file paths.\n\n    Raises:\n        Exception: If there is an error downloading or saving the imagery.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Create a geometry from the bounding box\n    geometry = box(*bbox)\n\n    # Connect to Planetary Computer STAC API\n    catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n\n    # Build query for NAIP data\n    search_params = {\n        \"collections\": [\"naip\"],\n        \"intersects\": geometry,\n        \"limit\": max_items,\n    }\n\n    # Add year filter if specified\n    if year:\n        search_params[\"query\"] = {\"naip:year\": {\"eq\": year}}\n\n    for key, value in kwargs.items():\n        search_params[key] = value\n\n    # Search for NAIP imagery\n    search_results = catalog.search(**search_params)\n    items = list(search_results.items())\n\n    if len(items) &gt; max_items:\n        items = items[:max_items]\n\n    if not items:\n        print(\"No NAIP imagery found for the specified region and parameters.\")\n        return []\n\n    print(f\"Found {len(items)} NAIP items.\")\n\n    # Download and save each item\n    downloaded_files = []\n    for i, item in enumerate(items):\n        # Sign the assets (required for Planetary Computer)\n        signed_item = pc.sign(item)\n\n        # Get the RGB asset URL\n        rgb_asset = signed_item.assets.get(\"image\")\n        if not rgb_asset:\n            print(f\"No RGB asset found for item {i+1}\")\n            continue\n\n        # Use the original filename from the asset\n        original_filename = os.path.basename(\n            rgb_asset.href.split(\"?\")[0]\n        )  # Remove query parameters\n        output_path = os.path.join(output_dir, original_filename)\n        if not overwrite and os.path.exists(output_path):\n            print(f\"Skipping existing file: {output_path}\")\n            downloaded_files.append(output_path)\n            continue\n\n        print(f\"Downloading item {i+1}/{len(items)}: {original_filename}\")\n\n        try:\n            # Open and save the data with progress bar\n            # For direct file download with progress bar\n            if rgb_asset.href.startswith(\"http\"):\n                download_with_progress(rgb_asset.href, output_path)\n                #\n            else:\n                # Fallback to direct rioxarray opening (less common case)\n                data = rxr.open_rasterio(rgb_asset.href)\n                data.rio.to_raster(output_path)\n\n            downloaded_files.append(output_path)\n            print(f\"Successfully saved to {output_path}\")\n\n            # Optional: Display a preview (uncomment if needed)\n            if preview:\n                data = rxr.open_rasterio(output_path)\n                preview_raster(data)\n\n        except Exception as e:\n            print(f\"Error downloading item {i+1}: {str(e)}\")\n\n    return downloaded_files\n</code></pre>"},{"location":"download/#geoai.download.download_overture_buildings","title":"<code>download_overture_buildings(bbox, output, overture_type='building', **kwargs)</code>","text":"<p>Download building data from Overture Maps for a given bounding box using the overturemaps CLI tool.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.</p> required <code>output</code> <code>str</code> <p>Path to save the output file.</p> required <code>overture_type</code> <code>str</code> <p>The Overture Maps data type to download (building, place, etc.).</p> <code>'building'</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the output file.</p> Source code in <code>geoai/download.py</code> <pre><code>def download_overture_buildings(\n    bbox: Tuple[float, float, float, float],\n    output: str,\n    overture_type: str = \"building\",\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Download building data from Overture Maps for a given bounding box using the overturemaps CLI tool.\n\n    Args:\n        bbox: Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.\n        output: Path to save the output file.\n        overture_type: The Overture Maps data type to download (building, place, etc.).\n\n    Returns:\n        Path to the output file.\n    \"\"\"\n\n    return get_overture_data(\n        overture_type=overture_type, bbox=bbox, output=output, **kwargs\n    )\n</code></pre>"},{"location":"download/#geoai.download.download_pc_stac_item","title":"<code>download_pc_stac_item(item_url, bands=None, output_dir=None, show_progress=True, merge_bands=False, merged_filename=None, overwrite=False, cell_size=None)</code>","text":"<p>Downloads a STAC item from Microsoft Planetary Computer with specified bands.</p> <p>This function fetches a STAC item by URL, signs the assets using Planetary Computer credentials, and downloads the specified bands with a progress bar. Can optionally merge bands into a single multi-band GeoTIFF.</p> <p>Parameters:</p> Name Type Description Default <code>item_url</code> <code>str</code> <p>The URL of the STAC item to download.</p> required <code>bands</code> <code>list</code> <p>List of specific bands to download (e.g., ['B01', 'B02']).                    If None, all available bands will be downloaded.</p> <code>None</code> <code>output_dir</code> <code>str</code> <p>Directory to save downloaded bands. If None,                        bands are returned as xarray DataArrays.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Whether to display a progress bar. Default is True.</p> <code>True</code> <code>merge_bands</code> <code>bool</code> <p>Whether to merge downloaded bands into a single                          multi-band GeoTIFF file. Default is False.</p> <code>False</code> <code>merged_filename</code> <code>str</code> <p>Filename for the merged bands. If None and                             merge_bands is True, uses \"{item_id}_merged.tif\".</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing files. Default is False.</p> <code>False</code> <code>cell_size</code> <code>float</code> <p>Resolution in meters for the merged output. If None,                         uses the resolution of the first band.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary mapping band names to their corresponding xarray DataArrays       or file paths if output_dir is provided. If merge_bands is True, also       includes a 'merged' key with the path to the merged file.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the item cannot be retrieved or a requested band is not available.</p> Source code in <code>geoai/download.py</code> <pre><code>def download_pc_stac_item(\n    item_url,\n    bands=None,\n    output_dir=None,\n    show_progress=True,\n    merge_bands=False,\n    merged_filename=None,\n    overwrite=False,\n    cell_size=None,\n):\n    \"\"\"\n    Downloads a STAC item from Microsoft Planetary Computer with specified bands.\n\n    This function fetches a STAC item by URL, signs the assets using Planetary Computer\n    credentials, and downloads the specified bands with a progress bar. Can optionally\n    merge bands into a single multi-band GeoTIFF.\n\n    Args:\n        item_url (str): The URL of the STAC item to download.\n        bands (list, optional): List of specific bands to download (e.g., ['B01', 'B02']).\n                               If None, all available bands will be downloaded.\n        output_dir (str, optional): Directory to save downloaded bands. If None,\n                                   bands are returned as xarray DataArrays.\n        show_progress (bool, optional): Whether to display a progress bar. Default is True.\n        merge_bands (bool, optional): Whether to merge downloaded bands into a single\n                                     multi-band GeoTIFF file. Default is False.\n        merged_filename (str, optional): Filename for the merged bands. If None and\n                                        merge_bands is True, uses \"{item_id}_merged.tif\".\n        overwrite (bool, optional): Whether to overwrite existing files. Default is False.\n        cell_size (float, optional): Resolution in meters for the merged output. If None,\n                                    uses the resolution of the first band.\n\n    Returns:\n        dict: Dictionary mapping band names to their corresponding xarray DataArrays\n              or file paths if output_dir is provided. If merge_bands is True, also\n              includes a 'merged' key with the path to the merged file.\n\n    Raises:\n        ValueError: If the item cannot be retrieved or a requested band is not available.\n    \"\"\"\n    from rasterio.enums import Resampling\n\n    # Get the item ID from the URL\n    item_id = item_url.split(\"/\")[-1]\n    collection = item_url.split(\"/collections/\")[1].split(\"/items/\")[0]\n\n    # Connect to the Planetary Computer STAC API\n    catalog = Client.open(\n        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n        modifier=pc.sign_inplace,\n    )\n\n    # Search for the specific item\n    search = catalog.search(collections=[collection], ids=[item_id])\n\n    # Get the first item from the search results\n    items = list(search.get_items())\n    if not items:\n        raise ValueError(f\"Item with ID {item_id} not found\")\n\n    item = items[0]\n\n    # Determine which bands to download\n    available_assets = list(item.assets.keys())\n\n    if bands is None:\n        # If no bands specified, download all band assets\n        bands_to_download = [\n            asset for asset in available_assets if asset.startswith(\"B\")\n        ]\n    else:\n        # Verify all requested bands exist\n        missing_bands = [band for band in bands if band not in available_assets]\n        if missing_bands:\n            raise ValueError(\n                f\"The following bands are not available: {missing_bands}. \"\n                f\"Available assets are: {available_assets}\"\n            )\n        bands_to_download = bands\n\n    # Create output directory if specified and doesn't exist\n    if output_dir and not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    result = {}\n    band_data_arrays = []\n    resampled_arrays = []\n    band_names = []  # Track band names in order\n\n    # Set up progress bar\n    progress_iter = (\n        tqdm(bands_to_download, desc=\"Downloading bands\")\n        if show_progress\n        else bands_to_download\n    )\n\n    # Download each requested band\n    for band in progress_iter:\n        if band not in item.assets:\n            if show_progress and not isinstance(progress_iter, list):\n                progress_iter.write(\n                    f\"Warning: Band {band} not found in assets, skipping.\"\n                )\n            continue\n\n        band_url = item.assets[band].href\n\n        if output_dir:\n            file_path = os.path.join(output_dir, f\"{item.id}_{band}.tif\")\n\n            # Check if file exists and skip if overwrite is False\n            if os.path.exists(file_path) and not overwrite:\n                if show_progress and not isinstance(progress_iter, list):\n                    progress_iter.write(\n                        f\"File {file_path} already exists, skipping (use overwrite=True to force download).\"\n                    )\n                # Still need to open the file to get the data for merging\n                if merge_bands:\n                    band_data = rxr.open_rasterio(file_path)\n                    band_data_arrays.append((band, band_data))\n                    band_names.append(band)\n                result[band] = file_path\n                continue\n\n        if show_progress and not isinstance(progress_iter, list):\n            progress_iter.set_description(f\"Downloading {band}\")\n\n        band_data = rxr.open_rasterio(band_url)\n\n        # Store the data array for potential merging later\n        if merge_bands:\n            band_data_arrays.append((band, band_data))\n            band_names.append(band)\n\n        if output_dir:\n            file_path = os.path.join(output_dir, f\"{item.id}_{band}.tif\")\n            band_data.rio.to_raster(file_path)\n            result[band] = file_path\n        else:\n            result[band] = band_data\n\n    # Merge bands if requested\n    if merge_bands and output_dir:\n        if merged_filename is None:\n            merged_filename = f\"{item.id}_merged.tif\"\n\n        merged_path = os.path.join(output_dir, merged_filename)\n\n        # Check if merged file exists and skip if overwrite is False\n        if os.path.exists(merged_path) and not overwrite:\n            if show_progress:\n                print(\n                    f\"Merged file {merged_path} already exists, skipping (use overwrite=True to force creation).\"\n                )\n            result[\"merged\"] = merged_path\n        else:\n            if show_progress:\n                print(\"Resampling and merging bands...\")\n\n            # Determine target cell size if not provided\n            if cell_size is None and band_data_arrays:\n                # Use the resolution of the first band (usually 10m for B02, B03, B04, B08)\n                # Get the affine transform (containing resolution info)\n                first_band_data = band_data_arrays[0][1]\n                # Extract resolution from transform\n                cell_size = abs(first_band_data.rio.transform()[0])\n                if show_progress:\n                    print(f\"Using detected resolution: {cell_size}m\")\n            elif cell_size is None:\n                # Default to 10m if no bands are available\n                cell_size = 10\n                if show_progress:\n                    print(f\"Using default resolution: {cell_size}m\")\n\n            # Process bands in memory-efficient way\n            for i, (band_name, data_array) in enumerate(band_data_arrays):\n                if show_progress:\n                    print(f\"Processing band: {band_name}\")\n\n                # Get current resolution\n                current_res = abs(data_array.rio.transform()[0])\n\n                # Resample if needed\n                if (\n                    abs(current_res - cell_size) &gt; 0.01\n                ):  # Small tolerance for floating point comparison\n                    if show_progress:\n                        print(\n                            f\"Resampling {band_name} from {current_res}m to {cell_size}m\"\n                        )\n\n                    # Use bilinear for downsampling (higher to lower resolution)\n                    # Use nearest for upsampling (lower to higher resolution)\n                    resampling_method = (\n                        Resampling.bilinear\n                        if current_res &lt; cell_size\n                        else Resampling.nearest\n                    )\n\n                    resampled = data_array.rio.reproject(\n                        data_array.rio.crs,\n                        resolution=(cell_size, cell_size),\n                        resampling=resampling_method,\n                    )\n                    resampled_arrays.append(resampled)\n                else:\n                    resampled_arrays.append(data_array)\n\n            if show_progress:\n                print(\"Stacking bands...\")\n\n            # Concatenate all resampled arrays along the band dimension\n            try:\n                merged_data = xr.concat(resampled_arrays, dim=\"band\")\n\n                if show_progress:\n                    print(f\"Writing merged data to {merged_path}...\")\n\n                # Add description metadata\n                merged_data.attrs[\"description\"] = (\n                    f\"Multi-band image containing {', '.join(band_names)}\"\n                )\n\n                # Create a dictionary mapping band indices to band names\n                band_descriptions = {}\n                for i, name in enumerate(band_names):\n                    band_descriptions[i + 1] = name\n\n                # Write the merged data to file with band descriptions\n                merged_data.rio.to_raster(\n                    merged_path,\n                    tags={\"BAND_NAMES\": \",\".join(band_names)},\n                    descriptions=band_names,\n                )\n\n                result[\"merged\"] = merged_path\n\n                if show_progress:\n                    print(f\"Merged bands saved to: {merged_path}\")\n                    print(f\"Band order in merged file: {', '.join(band_names)}\")\n            except Exception as e:\n                if show_progress:\n                    print(f\"Error during merging: {str(e)}\")\n                    print(f\"Error details: {type(e).__name__}: {str(e)}\")\n                raise\n\n    return result\n</code></pre>"},{"location":"download/#geoai.download.download_with_progress","title":"<code>download_with_progress(url, output_path)</code>","text":"<p>Download a file with a progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of the file to download.</p> required <code>output_path</code> <code>str</code> <p>Path where the file will be saved.</p> required Source code in <code>geoai/download.py</code> <pre><code>def download_with_progress(url: str, output_path: str) -&gt; None:\n    \"\"\"Download a file with a progress bar.\n\n    Args:\n        url: URL of the file to download.\n        output_path: Path where the file will be saved.\n    \"\"\"\n    response = requests.get(url, stream=True)\n    total_size = int(response.headers.get(\"content-length\", 0))\n    block_size = 1024  # 1 Kibibyte\n\n    with (\n        open(output_path, \"wb\") as file,\n        tqdm(\n            desc=os.path.basename(output_path),\n            total=total_size,\n            unit=\"iB\",\n            unit_scale=True,\n            unit_divisor=1024,\n        ) as bar,\n    ):\n        for data in response.iter_content(block_size):\n            size = file.write(data)\n            bar.update(size)\n</code></pre>"},{"location":"download/#geoai.download.extract_building_stats","title":"<code>extract_building_stats(data)</code>","text":"<p>Extract statistics from the building data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>Path to the GeoJSON file or GeoDataFrame containing building data.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with statistics.</p> Source code in <code>geoai/download.py</code> <pre><code>def extract_building_stats(data: str) -&gt; Dict[str, Any]:\n    \"\"\"Extract statistics from the building data.\n\n    Args:\n        data: Path to the GeoJSON file or GeoDataFrame containing building data.\n\n    Returns:\n        Dictionary with statistics.\n    \"\"\"\n    try:\n        # Read the GeoJSON file\n\n        if isinstance(data, gpd.GeoDataFrame):\n            gdf = data\n        else:\n\n            gdf = gpd.read_file(data)\n\n        # Calculate statistics\n        bbox = gdf.total_bounds.tolist()\n        # Convert numpy values to Python native types\n        bbox = [float(x) for x in bbox]\n\n        stats = {\n            \"total_buildings\": int(len(gdf)),\n            \"has_height\": (\n                int(gdf[\"height\"].notna().sum()) if \"height\" in gdf.columns else 0\n            ),\n            \"has_name\": (\n                int(gdf[\"names.common.value\"].notna().sum())\n                if \"names.common.value\" in gdf.columns\n                else 0\n            ),\n            \"bbox\": bbox,\n        }\n\n        return stats\n\n    except Exception as e:\n        logger.error(f\"Error extracting statistics: {str(e)}\")\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"download/#geoai.download.get_all_overture_types","title":"<code>get_all_overture_types()</code>","text":"<p>Get a list of all available Overture Maps data types.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of available Overture Maps data types.</p> Source code in <code>geoai/download.py</code> <pre><code>def get_all_overture_types():\n    \"\"\"Get a list of all available Overture Maps data types.\n\n    Returns:\n        list: List of available Overture Maps data types.\n    \"\"\"\n    from overturemaps import core\n\n    return core.get_all_overture_types()\n</code></pre>"},{"location":"download/#geoai.download.get_overture_data","title":"<code>get_overture_data(overture_type, bbox=None, columns=None, output=None, **kwargs)</code>","text":"<p>Fetches overture data and returns it as a GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>overture_type</code> <code>str</code> <p>The type of overture data to fetch.It can be one of the following: address|building|building_part|division|division_area|division_boundary|place| segment|connector|infrastructure|land|land_cover|land_use|water</p> required <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>The bounding box to filter the data. Defaults to None.</p> <code>None</code> <code>columns</code> <code>List[str]</code> <p>The columns to include in the output. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>The file path to save the output GeoDataFrame. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>gpd.GeoDataFrame</code> <p>The fetched overture data as a GeoDataFrame.</p> <p>Exceptions:</p> Type Description <code>ImportError</code> <p>If the overture package is not installed.</p> Source code in <code>geoai/download.py</code> <pre><code>def get_overture_data(\n    overture_type: str,\n    bbox: Tuple[float, float, float, float] = None,\n    columns: List[str] = None,\n    output: str = None,\n    **kwargs: Any,\n) -&gt; \"gpd.GeoDataFrame\":\n    \"\"\"Fetches overture data and returns it as a GeoDataFrame.\n\n    Args:\n        overture_type (str): The type of overture data to fetch.It can be one of the following:\n            address|building|building_part|division|division_area|division_boundary|place|\n            segment|connector|infrastructure|land|land_cover|land_use|water\n        bbox (Tuple[float, float, float, float], optional): The bounding box to\n            filter the data. Defaults to None.\n        columns (List[str], optional): The columns to include in the output.\n            Defaults to None.\n        output (str, optional): The file path to save the output GeoDataFrame.\n            Defaults to None.\n\n    Returns:\n        gpd.GeoDataFrame: The fetched overture data as a GeoDataFrame.\n\n    Raises:\n        ImportError: If the overture package is not installed.\n    \"\"\"\n\n    try:\n        from overturemaps import core\n    except ImportError:\n        raise ImportError(\"The overturemaps package is required to use this function\")\n\n    gdf = core.geodataframe(overture_type, bbox=bbox)\n    if columns is not None:\n        gdf = gdf[columns]\n\n    gdf.crs = \"EPSG:4326\"\n\n    out_dir = os.path.dirname(os.path.abspath(output))\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    if output is not None:\n        gdf.to_file(output, **kwargs)\n\n    return gdf\n</code></pre>"},{"location":"download/#geoai.download.get_overture_latest_release","title":"<code>get_overture_latest_release(patch=True)</code>","text":"<p>Retrieves the value of the 'latest' key from the Overture Maps release JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>bool</code> <p>If True, returns the full version string (e.g., \"2025-02-19.0\").</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>The value of the 'latest' key from the releases.json file.</p> <p>Exceptions:</p> Type Description <code>requests.RequestException</code> <p>If there's an issue with the HTTP request.</p> <code>KeyError</code> <p>If the 'latest' key is not found in the JSON data.</p> <code>json.JSONDecodeError</code> <p>If the response cannot be parsed as JSON.</p> Source code in <code>geoai/download.py</code> <pre><code>def get_overture_latest_release(patch=True) -&gt; str:\n    \"\"\"\n    Retrieves the value of the 'latest' key from the Overture Maps release JSON file.\n\n    Args:\n        patch (bool): If True, returns the full version string (e.g., \"2025-02-19.0\").\n\n    Returns:\n        str: The value of the 'latest' key from the releases.json file.\n\n    Raises:\n        requests.RequestException: If there's an issue with the HTTP request.\n        KeyError: If the 'latest' key is not found in the JSON data.\n        json.JSONDecodeError: If the response cannot be parsed as JSON.\n    \"\"\"\n    url = \"https://labs.overturemaps.org/data/releases.json\"\n\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n\n        data = response.json()\n        if patch:\n            latest_release = data.get(\"latest\")\n        else:\n            latest_release = data.get(\"latest\").split(\".\")[\n                0\n            ]  # Extract the version number\n\n        if latest_release is None:\n            raise KeyError(\"The 'latest' key was not found in the releases.json file\")\n\n        return latest_release\n\n    except requests.RequestException as e:\n        print(f\"Error making the request: {e}\")\n        raise\n    except json.JSONDecodeError as e:\n        print(f\"Error parsing JSON response: {e}\")\n        raise\n    except KeyError as e:\n        print(f\"Key error: {e}\")\n        raise\n</code></pre>"},{"location":"download/#geoai.download.json_serializable","title":"<code>json_serializable(obj)</code>","text":"<p>Convert NumPy types to native Python types for JSON serialization.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Any object to convert.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>JSON serializable version of the object.</p> Source code in <code>geoai/download.py</code> <pre><code>def json_serializable(obj: Any) -&gt; Any:\n    \"\"\"Convert NumPy types to native Python types for JSON serialization.\n\n    Args:\n        obj: Any object to convert.\n\n    Returns:\n        JSON serializable version of the object.\n    \"\"\"\n    if isinstance(obj, np.integer):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    else:\n        return obj\n</code></pre>"},{"location":"download/#geoai.download.pc_collection_list","title":"<code>pc_collection_list(endpoint='https://planetarycomputer.microsoft.com/api/stac/v1', detailed=False, filter_by=None, sort_by='id')</code>","text":"<p>Retrieves and displays the list of available collections from Planetary Computer.</p> <p>This function connects to the Planetary Computer STAC API and retrieves the list of all available collections, with options to filter and sort the results.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>STAC API endpoint URL. Defaults to \"https://planetarycomputer.microsoft.com/api/stac/v1\".</p> <code>'https://planetarycomputer.microsoft.com/api/stac/v1'</code> <code>detailed</code> <code>bool</code> <p>Whether to return detailed information for each collection. If False, returns only basic info. Defaults to False.</p> <code>False</code> <code>filter_by</code> <code>dict</code> <p>Dictionary of field:value pairs to filter collections. For example, {\"license\": \"CC-BY-4.0\"}. Defaults to None.</p> <code>None</code> <code>sort_by</code> <code>str</code> <p>Field to sort the collections by. Defaults to \"id\".</p> <code>'id'</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>DataFrame containing collection information.</p> <p>Exceptions:</p> Type Description <code>ConnectionError</code> <p>If there's an issue connecting to the API.</p> Source code in <code>geoai/download.py</code> <pre><code>def pc_collection_list(\n    endpoint=\"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    detailed=False,\n    filter_by=None,\n    sort_by=\"id\",\n):\n    \"\"\"\n    Retrieves and displays the list of available collections from Planetary Computer.\n\n    This function connects to the Planetary Computer STAC API and retrieves the\n    list of all available collections, with options to filter and sort the results.\n\n    Args:\n        endpoint (str, optional): STAC API endpoint URL.\n            Defaults to \"https://planetarycomputer.microsoft.com/api/stac/v1\".\n        detailed (bool, optional): Whether to return detailed information for each\n            collection. If False, returns only basic info. Defaults to False.\n        filter_by (dict, optional): Dictionary of field:value pairs to filter\n            collections. For example, {\"license\": \"CC-BY-4.0\"}. Defaults to None.\n        sort_by (str, optional): Field to sort the collections by.\n            Defaults to \"id\".\n\n    Returns:\n        pandas.DataFrame: DataFrame containing collection information.\n\n    Raises:\n        ConnectionError: If there's an issue connecting to the API.\n    \"\"\"\n    # Initialize the STAC client\n    try:\n        catalog = Client.open(endpoint)\n    except Exception as e:\n        raise ConnectionError(f\"Failed to connect to STAC API at {endpoint}: {str(e)}\")\n\n    # Get all collections\n    try:\n        collections = list(catalog.get_collections())\n    except Exception as e:\n        raise Exception(f\"Error retrieving collections: {str(e)}\")\n\n    # Basic info to extract from all collections\n    collection_info = []\n\n    # Extract information based on detail level\n    for collection in collections:\n        # Basic information always included\n        info = {\n            \"id\": collection.id,\n            \"title\": collection.title or \"No title\",\n            \"description\": (\n                collection.description[:100] + \"...\"\n                if collection.description and len(collection.description) &gt; 100\n                else collection.description\n            ),\n        }\n\n        # Add detailed information if requested\n        if detailed:\n            # Get temporal extent if available\n            temporal_extent = \"Unknown\"\n            if collection.extent and collection.extent.temporal:\n                interval = (\n                    collection.extent.temporal.intervals[0]\n                    if collection.extent.temporal.intervals\n                    else None\n                )\n                if interval:\n                    start = interval[0] or \"Unknown Start\"\n                    end = interval[1] or \"Present\"\n                    if isinstance(start, datetime.datetime):\n                        start = start.strftime(\"%Y-%m-%d\")\n                    if isinstance(end, datetime.datetime):\n                        end = end.strftime(\"%Y-%m-%d\")\n                    temporal_extent = f\"{start} to {end}\"\n\n            # Add additional details\n            info.update(\n                {\n                    \"license\": collection.license or \"Unknown\",\n                    \"keywords\": (\n                        \", \".join(collection.keywords)\n                        if collection.keywords\n                        else \"None\"\n                    ),\n                    \"temporal_extent\": temporal_extent,\n                    \"asset_count\": len(collection.assets) if collection.assets else 0,\n                    \"providers\": (\n                        \", \".join([p.name for p in collection.providers])\n                        if collection.providers\n                        else \"Unknown\"\n                    ),\n                }\n            )\n\n            # Add spatial extent if available\n            if collection.extent and collection.extent.spatial:\n                info[\"bbox\"] = (\n                    str(collection.extent.spatial.bboxes[0])\n                    if collection.extent.spatial.bboxes\n                    else \"Unknown\"\n                )\n\n        collection_info.append(info)\n\n    # Convert to DataFrame for easier filtering and sorting\n    df = pd.DataFrame(collection_info)\n\n    # Apply filtering if specified\n    if filter_by:\n        for field, value in filter_by.items():\n            if field in df.columns:\n                df = df[df[field].astype(str).str.contains(value, case=False, na=False)]\n\n    # Apply sorting\n    if sort_by in df.columns:\n        df = df.sort_values(by=sort_by)\n\n    print(f\"Retrieved {len(df)} collections from Planetary Computer\")\n\n    # # Print a nicely formatted table\n    # if not df.empty:\n    #     print(\"\\nAvailable collections:\")\n    #     print(tabulate(df, headers=\"keys\", tablefmt=\"grid\", showindex=False))\n\n    return df\n</code></pre>"},{"location":"download/#geoai.download.pc_item_asset_list","title":"<code>pc_item_asset_list(item)</code>","text":"<p>Retrieve the list of asset keys from a STAC item in the Planetary Computer catalog.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>str</code> <p>The URL of the STAC item.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of asset keys available in the signed STAC item.</p> Source code in <code>geoai/download.py</code> <pre><code>def pc_item_asset_list(item):\n    \"\"\"\n    Retrieve the list of asset keys from a STAC item in the Planetary Computer catalog.\n\n    Args:\n        item (str): The URL of the STAC item.\n\n    Returns:\n        list: A list of asset keys available in the signed STAC item.\n    \"\"\"\n    if isinstance(item, str):\n        item = pystac.Item.from_file(item)\n\n    if not isinstance(item, pystac.Item):\n        raise ValueError(\"item_url must be a string (URL) or a pystac.Item object\")\n\n    return list(item.assets.keys())\n</code></pre>"},{"location":"download/#geoai.download.pc_stac_download","title":"<code>pc_stac_download(items, output_dir='.', assets=None, max_workers=4, skip_existing=True)</code>","text":"<p>Download assets from STAC items retrieved from the Planetary Computer.</p> <p>This function downloads specified assets from a list of STAC items to the specified output directory. It supports parallel downloads and can skip already downloaded files.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list or pystac.Item</code> <p>STAC Item object or list of STAC Item objects.</p> required <code>output_dir</code> <code>str</code> <p>Directory where assets will be saved. Defaults to current directory.</p> <code>'.'</code> <code>assets</code> <code>list</code> <p>List of asset keys to download. If None, downloads all available assets. Defaults to None.</p> <code>None</code> <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent download threads. Defaults to 4.</p> <code>4</code> <code>skip_existing</code> <code>bool</code> <p>Skip download if the file already exists. Defaults to True.</p> <code>True</code> <code>sign_urls</code> <code>bool</code> <p>Whether to sign URLs for authenticated access. Defaults to True.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary mapping STAC item IDs to dictionaries of their downloaded     assets {asset_key: file_path}.</p> <p>Exceptions:</p> Type Description <code>TypeError</code> <p>If items is not a STAC Item or list of STAC Items.</p> <code>IOError</code> <p>If there's an error writing the downloaded assets to disk.</p> Source code in <code>geoai/download.py</code> <pre><code>def pc_stac_download(\n    items,\n    output_dir=\".\",\n    assets=None,\n    max_workers=4,\n    skip_existing=True,\n):\n    \"\"\"\n    Download assets from STAC items retrieved from the Planetary Computer.\n\n    This function downloads specified assets from a list of STAC items to the\n    specified output directory. It supports parallel downloads and can skip\n    already downloaded files.\n\n    Args:\n        items (list or pystac.Item): STAC Item object or list of STAC Item objects.\n        output_dir (str, optional): Directory where assets will be saved.\n            Defaults to current directory.\n        assets (list, optional): List of asset keys to download. If None,\n            downloads all available assets. Defaults to None.\n        max_workers (int, optional): Maximum number of concurrent download threads.\n            Defaults to 4.\n        skip_existing (bool, optional): Skip download if the file already exists.\n            Defaults to True.\n        sign_urls (bool, optional): Whether to sign URLs for authenticated access.\n            Defaults to True.\n\n    Returns:\n        dict: Dictionary mapping STAC item IDs to dictionaries of their downloaded\n            assets {asset_key: file_path}.\n\n    Raises:\n        TypeError: If items is not a STAC Item or list of STAC Items.\n        IOError: If there's an error writing the downloaded assets to disk.\n    \"\"\"\n\n    from concurrent.futures import ThreadPoolExecutor, as_completed\n\n    # Handle single item case\n    if isinstance(items, pystac.Item) or isinstance(items, str):\n        items = [items]\n    elif not isinstance(items, list):\n        raise TypeError(\"items must be a STAC Item or list of STAC Items\")\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Function to download a single asset\n    def download_asset(item, asset_key, asset):\n        item = pc.sign(item)\n        item_id = item.id\n\n        # Get the asset URL and sign it if needed\n        asset_url = item.assets[asset_key].href\n        # Determine output filename\n        if asset.media_type:\n            # Use appropriate file extension based on media type\n            if \"tiff\" in asset.media_type or \"geotiff\" in asset.media_type:\n                ext = \".tif\"\n            elif \"jpeg\" in asset.media_type:\n                ext = \".jpg\"\n            elif \"png\" in asset.media_type:\n                ext = \".png\"\n            elif \"json\" in asset.media_type:\n                ext = \".json\"\n            else:\n                # Default extension based on the original URL\n                ext = os.path.splitext(asset_url.split(\"?\")[0])[1] or \".data\"\n        else:\n            # Default extension based on the original URL\n            ext = os.path.splitext(asset_url.split(\"?\")[0])[1] or \".data\"\n\n        output_path = os.path.join(output_dir, f\"{item_id}_{asset_key}{ext}\")\n\n        # Skip if file exists and skip_existing is True\n        if skip_existing and os.path.exists(output_path):\n            print(f\"Skipping existing asset: {asset_key} -&gt; {output_path}\")\n            return asset_key, output_path\n\n        try:\n            # Download the asset with progress bar\n            with requests.get(asset_url, stream=True) as r:\n                r.raise_for_status()\n                total_size = int(r.headers.get(\"content-length\", 0))\n                with open(output_path, \"wb\") as f:\n                    with tqdm(\n                        total=total_size,\n                        unit=\"B\",\n                        unit_scale=True,\n                        unit_divisor=1024,\n                        desc=f\"Downloading {item_id}_{asset_key}\",\n                        ncols=100,\n                    ) as pbar:\n                        for chunk in r.iter_content(chunk_size=8192):\n                            f.write(chunk)\n                            pbar.update(len(chunk))\n\n            return asset_key, output_path\n        except Exception as e:\n            print(f\"Error downloading {asset_key} for item {item_id}: {str(e)}\")\n            if os.path.exists(output_path):\n                os.remove(output_path)  # Clean up partial download\n            return asset_key, None\n\n    # Process all items and their assets\n    results = {}\n\n    for item in items:\n        item_assets = {}\n        if isinstance(item, str):\n            item = pystac.Item.from_file(item)\n        item_id = item.id\n        print(f\"Processing STAC item: {item_id}\")\n\n        # Determine which assets to download\n        if assets:\n            assets_to_download = {k: v for k, v in item.assets.items() if k in assets}\n            if not assets_to_download:\n                print(\n                    f\"Warning: None of the specified asset keys {assets} found in item {item_id}\"\n                )\n                print(f\"Available asset keys: {list(item.assets.keys())}\")\n                continue\n        else:\n            assets_to_download = item.assets\n\n        # Download assets concurrently\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            # Submit all download tasks\n            future_to_asset = {\n                executor.submit(download_asset, item, asset_key, asset): (\n                    asset_key,\n                    asset,\n                )\n                for asset_key, asset in assets_to_download.items()\n            }\n\n            # Process results as they complete\n            for future in as_completed(future_to_asset):\n                asset_key, asset = future_to_asset[future]\n                try:\n                    key, path = future.result()\n                    if path:\n                        item_assets[key] = path\n                except Exception as e:\n                    print(\n                        f\"Error processing asset {asset_key} for item {item_id}: {str(e)}\"\n                    )\n\n        results[item_id] = item_assets\n\n    # Count total downloaded assets\n    total_assets = sum(len(assets) for assets in results.values())\n    print(f\"\\nDownloaded {total_assets} assets for {len(results)} items\")\n\n    return results\n</code></pre>"},{"location":"download/#geoai.download.pc_stac_search","title":"<code>pc_stac_search(collection, bbox=None, time_range=None, query=None, limit=10, max_items=None, endpoint='https://planetarycomputer.microsoft.com/api/stac/v1')</code>","text":"<p>Search for STAC items in the Planetary Computer catalog.</p> <p>This function queries the Planetary Computer STAC API to find items matching the specified criteria, including collection, bounding box, time range, and additional query parameters.</p> <p>Parameters:</p> Name Type Description Default <code>collection</code> <code>str</code> <p>The STAC collection ID to search within.</p> required <code>bbox</code> <code>list</code> <p>Bounding box coordinates [west, south, east, north]. Defaults to None.</p> <code>None</code> <code>time_range</code> <code>str or tuple</code> <p>Time range as a string \"start/end\" or a tuple of (start, end) datetime objects. Defaults to None.</p> <code>None</code> <code>query</code> <code>dict</code> <p>Additional query parameters for filtering. Defaults to None.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Number of items to return per page. Defaults to 10.</p> <code>10</code> <code>max_items</code> <code>int</code> <p>Maximum total number of items to return. Defaults to None (returns all matching items).</p> <code>None</code> <code>endpoint</code> <code>str</code> <p>STAC API endpoint URL. Defaults to \"https://planetarycomputer.microsoft.com/api/stac/v1\".</p> <code>'https://planetarycomputer.microsoft.com/api/stac/v1'</code> <p>Returns:</p> Type Description <code>list</code> <p>List of STAC Item objects matching the search criteria.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If invalid parameters are provided.</p> <code>ConnectionError</code> <p>If there's an issue connecting to the API.</p> Source code in <code>geoai/download.py</code> <pre><code>def pc_stac_search(\n    collection,\n    bbox=None,\n    time_range=None,\n    query=None,\n    limit=10,\n    max_items=None,\n    endpoint=\"https://planetarycomputer.microsoft.com/api/stac/v1\",\n):\n    \"\"\"\n    Search for STAC items in the Planetary Computer catalog.\n\n    This function queries the Planetary Computer STAC API to find items matching\n    the specified criteria, including collection, bounding box, time range, and\n    additional query parameters.\n\n    Args:\n        collection (str): The STAC collection ID to search within.\n        bbox (list, optional): Bounding box coordinates [west, south, east, north].\n            Defaults to None.\n        time_range (str or tuple, optional): Time range as a string \"start/end\" or\n            a tuple of (start, end) datetime objects. Defaults to None.\n        query (dict, optional): Additional query parameters for filtering.\n            Defaults to None.\n        limit (int, optional): Number of items to return per page. Defaults to 10.\n        max_items (int, optional): Maximum total number of items to return.\n            Defaults to None (returns all matching items).\n        endpoint (str, optional): STAC API endpoint URL.\n            Defaults to \"https://planetarycomputer.microsoft.com/api/stac/v1\".\n\n    Returns:\n        list: List of STAC Item objects matching the search criteria.\n\n    Raises:\n        ValueError: If invalid parameters are provided.\n        ConnectionError: If there's an issue connecting to the API.\n    \"\"\"\n    import datetime\n\n    # Initialize the STAC client\n    try:\n        catalog = Client.open(endpoint)\n    except Exception as e:\n        raise ConnectionError(f\"Failed to connect to STAC API at {endpoint}: {str(e)}\")\n\n    # Process time_range if provided\n    if time_range:\n        if isinstance(time_range, tuple) and len(time_range) == 2:\n            # Convert datetime objects to ISO format strings\n            start, end = time_range\n            if isinstance(start, datetime.datetime):\n                start = start.isoformat()\n            if isinstance(end, datetime.datetime):\n                end = end.isoformat()\n            time_str = f\"{start}/{end}\"\n        elif isinstance(time_range, str):\n            time_str = time_range\n        else:\n            raise ValueError(\n                \"time_range must be a 'start/end' string or tuple of (start, end)\"\n            )\n    else:\n        time_str = None\n\n    # Create the search object\n    search = catalog.search(\n        collections=[collection], bbox=bbox, datetime=time_str, query=query, limit=limit\n    )\n\n    # Collect the items\n    items = []\n    try:\n        # Use max_items if specified, otherwise get all items\n        if max_items:\n            items_gen = search.get_items()\n            for item in items_gen:\n                items.append(item)\n                if len(items) &gt;= max_items:\n                    break\n        else:\n            items = list(search.get_items())\n    except Exception as e:\n        raise Exception(f\"Error retrieving search results: {str(e)}\")\n\n    print(f\"Found {len(items)} items matching search criteria\")\n\n    return items\n</code></pre>"},{"location":"download/#geoai.download.preview_raster","title":"<code>preview_raster(data, title=None)</code>","text":"<p>Display a preview of the downloaded imagery.</p> <p>This function creates a visualization of the downloaded NAIP imagery by converting it to an RGB array and displaying it with matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The raster data as a rioxarray object.</p> required <code>title</code> <code>str</code> <p>The title for the preview plot.</p> <code>None</code> Source code in <code>geoai/download.py</code> <pre><code>def preview_raster(data: Any, title: str = None) -&gt; None:\n    \"\"\"Display a preview of the downloaded imagery.\n\n    This function creates a visualization of the downloaded NAIP imagery\n    by converting it to an RGB array and displaying it with matplotlib.\n\n    Args:\n        data: The raster data as a rioxarray object.\n        title: The title for the preview plot.\n    \"\"\"\n    # Convert to 8-bit RGB for display\n    rgb_data = data.transpose(\"y\", \"x\", \"band\").values[:, :, 0:3]\n    rgb_data = np.where(rgb_data &gt; 255, 255, rgb_data).astype(np.uint8)\n\n    plt.figure(figsize=(10, 10))\n    plt.imshow(rgb_data)\n    if title is not None:\n        plt.title(title)\n    plt.axis(\"off\")\n    plt.show()\n</code></pre>"},{"location":"download/#geoai.download.read_pc_item_asset","title":"<code>read_pc_item_asset(item, asset, output=None, as_cog=True, **kwargs)</code>","text":"<p>Read a specific asset from a STAC item in the Planetary Computer catalog.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>str</code> <p>The URL of the STAC item.</p> required <code>asset</code> <code>str</code> <p>The key of the asset to read.</p> required <code>output</code> <code>str</code> <p>If specified, the path to save the asset as a raster file.</p> <code>None</code> <code>as_cog</code> <code>bool</code> <p>If True, save the asset as a Cloud Optimized GeoTIFF (COG).</p> <code>True</code> <p>Returns:</p> Type Description <code>xarray.DataArray</code> <p>The data array for the specified asset.</p> Source code in <code>geoai/download.py</code> <pre><code>def read_pc_item_asset(item, asset, output=None, as_cog=True, **kwargs):\n    \"\"\"\n    Read a specific asset from a STAC item in the Planetary Computer catalog.\n\n    Args:\n        item (str): The URL of the STAC item.\n        asset (str): The key of the asset to read.\n        output (str, optional): If specified, the path to save the asset as a raster file.\n        as_cog (bool, optional): If True, save the asset as a Cloud Optimized GeoTIFF (COG).\n\n    Returns:\n        xarray.DataArray: The data array for the specified asset.\n    \"\"\"\n    if isinstance(item, str):\n        item = pystac.Item.from_file(item)\n\n    if not isinstance(item, pystac.Item):\n        raise ValueError(\"item must be a string (URL) or a pystac.Item object\")\n\n    signed_item = pc.sign(item)\n\n    if asset not in signed_item.assets:\n        raise ValueError(\n            f\"Asset '{asset}' not found in item '{item.id}'. It has available assets: {list(signed_item.assets.keys())}\"\n        )\n\n    asset_url = signed_item.assets[asset].href\n    ds = rxr.open_rasterio(asset_url)\n\n    if as_cog:\n        kwargs[\"driver\"] = \"COG\"  # Ensure the output is a Cloud Optimized GeoTIFF\n\n    if output:\n        print(f\"Saving asset '{asset}' to {output}...\")\n        ds.rio.to_raster(output, **kwargs)\n        print(f\"Asset '{asset}' saved successfully.\")\n    return ds\n</code></pre>"},{"location":"extract/","title":"extract module","text":"<p>This module provides a dataset class for object extraction from raster data</p>"},{"location":"extract/#geoai.extract.AgricultureFieldDelineator","title":"<code> AgricultureFieldDelineator            (ObjectDetector)         </code>","text":"<p>Agricultural field boundary delineation using a pre-trained Mask R-CNN model.</p> <p>This class extends the ObjectDetector class to specifically handle Sentinel-2 imagery with 12 spectral bands for agricultural field boundary detection.</p> <p>Attributes:</p> Name Type Description <code>band_selection</code> <p>List of band indices to use for prediction (default: RGB)</p> <code>sentinel_band_stats</code> <p>Per-band statistics for Sentinel-2 data</p> <code>use_ndvi</code> <p>Whether to calculate and include NDVI as an additional channel</p> Source code in <code>geoai/extract.py</code> <pre><code>class AgricultureFieldDelineator(ObjectDetector):\n    \"\"\"\n    Agricultural field boundary delineation using a pre-trained Mask R-CNN model.\n\n    This class extends the ObjectDetector class to specifically handle Sentinel-2\n    imagery with 12 spectral bands for agricultural field boundary detection.\n\n    Attributes:\n        band_selection: List of band indices to use for prediction (default: RGB)\n        sentinel_band_stats: Per-band statistics for Sentinel-2 data\n        use_ndvi: Whether to calculate and include NDVI as an additional channel\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path=\"field_boundary_detector.pth\",\n        repo_id=None,\n        model=None,\n        device=None,\n        band_selection=None,\n        use_ndvi=False,\n    ):\n        \"\"\"\n        Initialize the field boundary delineator.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n            band_selection: List of Sentinel-2 band indices to use (None = adapt based on model)\n            use_ndvi: Whether to calculate and include NDVI as an additional channel\n        \"\"\"\n        # Save parameters before calling parent constructor\n        self.custom_band_selection = band_selection\n        self.use_ndvi = use_ndvi\n\n        # Set device (copied from parent init to ensure it's set before initialize_model)\n        if device is None:\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = torch.device(device)\n\n        # Initialize model differently for multi-spectral input\n        model = self.initialize_sentinel2_model(model)\n\n        # Call parent but with our custom model\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n\n        # Default Sentinel-2 band statistics (can be overridden with actual stats)\n        # Band order: [B1, B2, B3, B4, B5, B6, B7, B8, B8A, B9, B10, B11, B12]\n        self.sentinel_band_stats = {\n            \"means\": [\n                0.0975,\n                0.0476,\n                0.0598,\n                0.0697,\n                0.1077,\n                0.1859,\n                0.2378,\n                0.2061,\n                0.2598,\n                0.4120,\n                0.1956,\n                0.1410,\n            ],\n            \"stds\": [\n                0.0551,\n                0.0290,\n                0.0298,\n                0.0479,\n                0.0506,\n                0.0505,\n                0.0747,\n                0.0642,\n                0.0782,\n                0.1187,\n                0.0651,\n                0.0679,\n            ],\n        }\n\n        # Set default band selection (RGB - typically B4, B3, B2 for Sentinel-2)\n        self.band_selection = (\n            self.custom_band_selection\n            if self.custom_band_selection is not None\n            else [3, 2, 1]\n        )  # R, G, B bands\n\n        # Customize parameters for field delineation\n        self.confidence_threshold = 0.5  # Default confidence threshold\n        self.overlap = 0.5  # Higher overlap for field boundary detection\n        self.min_object_area = 1000  # Minimum area in pixels for field detection\n        self.simplify_tolerance = 2.0  # Higher tolerance for field boundaries\n\n    def initialize_sentinel2_model(self, model=None):\n        \"\"\"\n        Initialize a Mask R-CNN model with a modified first layer to accept Sentinel-2 data.\n\n        Args:\n            model: Pre-initialized model (optional)\n\n        Returns:\n            Modified model with appropriate input channels\n        \"\"\"\n        import torchvision\n        from torchvision.models.detection import maskrcnn_resnet50_fpn\n        from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n\n        if model is not None:\n            return model\n\n        # Determine number of input channels based on band selection and NDVI\n        num_input_channels = (\n            len(self.custom_band_selection)\n            if self.custom_band_selection is not None\n            else 3\n        )\n        if self.use_ndvi:\n            num_input_channels += 1\n\n        print(f\"Initializing Mask R-CNN model with {num_input_channels} input channels\")\n\n        # Create a ResNet50 backbone with modified input channels\n        backbone = resnet_fpn_backbone(\"resnet50\", weights=None)\n\n        # Replace the first conv layer to accept multi-spectral input\n        original_conv = backbone.body.conv1\n        backbone.body.conv1 = torch.nn.Conv2d(\n            num_input_channels,\n            original_conv.out_channels,\n            kernel_size=original_conv.kernel_size,\n            stride=original_conv.stride,\n            padding=original_conv.padding,\n            bias=original_conv.bias is not None,\n        )\n\n        # Create Mask R-CNN with our modified backbone\n        model = maskrcnn_resnet50_fpn(\n            backbone=backbone,\n            num_classes=2,  # Background + field\n            image_mean=[0.485] * num_input_channels,  # Extend mean to all channels\n            image_std=[0.229] * num_input_channels,  # Extend std to all channels\n        )\n\n        model.to(self.device)\n        return model\n\n    def preprocess_sentinel_bands(self, image_data, band_selection=None, use_ndvi=None):\n        \"\"\"\n        Preprocess Sentinel-2 band data for model input.\n\n        Args:\n            image_data: Raw Sentinel-2 image data as numpy array [bands, height, width]\n            band_selection: List of band indices to use (overrides instance default if provided)\n            use_ndvi: Whether to include NDVI (overrides instance default if provided)\n\n        Returns:\n            Processed tensor ready for model input\n        \"\"\"\n        # Use instance defaults if not specified\n        band_selection = (\n            band_selection if band_selection is not None else self.band_selection\n        )\n        use_ndvi = use_ndvi if use_ndvi is not None else self.use_ndvi\n\n        # Select bands\n        selected_bands = image_data[band_selection]\n\n        # Calculate NDVI if requested (using B8 and B4 which are indices 7 and 3)\n        if (\n            use_ndvi\n            and 7 in range(image_data.shape[0])\n            and 3 in range(image_data.shape[0])\n        ):\n            nir = image_data[7].astype(np.float32)  # B8 (NIR)\n            red = image_data[3].astype(np.float32)  # B4 (Red)\n\n            # Avoid division by zero\n            denominator = nir + red\n            ndvi = np.zeros_like(nir)\n            valid_mask = denominator &gt; 0\n            ndvi[valid_mask] = (nir[valid_mask] - red[valid_mask]) / denominator[\n                valid_mask\n            ]\n\n            # Rescale NDVI from [-1, 1] to [0, 1]\n            ndvi = (ndvi + 1) / 2\n\n            # Add NDVI as an additional channel\n            selected_bands = np.vstack([selected_bands, ndvi[np.newaxis, :, :]])\n\n        # Convert to tensor\n        image_tensor = torch.from_numpy(selected_bands).float()\n\n        # Normalize using band statistics\n        for i, band_idx in enumerate(band_selection):\n            # Make sure band_idx is within range of our statistics\n            if band_idx &lt; len(self.sentinel_band_stats[\"means\"]):\n                mean = self.sentinel_band_stats[\"means\"][band_idx]\n                std = self.sentinel_band_stats[\"stds\"][band_idx]\n                image_tensor[i] = (image_tensor[i] - mean) / std\n\n        # If NDVI was added, normalize it too (last channel)\n        if use_ndvi:\n            # NDVI is already roughly in [0,1] range, just standardize it slightly\n            image_tensor[-1] = (image_tensor[-1] - 0.5) / 0.5\n\n        return image_tensor\n\n    def update_band_stats(self, raster_path, band_selection=None, sample_size=1000):\n        \"\"\"\n        Update band statistics from the input Sentinel-2 raster.\n\n        Args:\n            raster_path: Path to the Sentinel-2 raster file\n            band_selection: Specific bands to update (None = update all available)\n            sample_size: Number of random pixels to sample for statistics calculation\n\n        Returns:\n            Updated band statistics dictionary\n        \"\"\"\n        with rasterio.open(raster_path) as src:\n            # Check if this is likely a Sentinel-2 product\n            band_count = src.count\n            if band_count &lt; 3:\n                print(\n                    f\"Warning: Raster has only {band_count} bands, may not be Sentinel-2 data\"\n                )\n\n            # Get dimensions\n            height, width = src.height, src.width\n\n            # Determine which bands to analyze\n            if band_selection is None:\n                band_selection = list(range(1, band_count + 1))  # 1-indexed\n\n            # Initialize arrays for band statistics\n            means = []\n            stds = []\n\n            # Sample random pixels\n            np.random.seed(42)  # For reproducibility\n            sample_rows = np.random.randint(0, height, sample_size)\n            sample_cols = np.random.randint(0, width, sample_size)\n\n            # Calculate statistics for each band\n            for band in band_selection:\n                # Read band data\n                band_data = src.read(band)\n\n                # Sample values\n                sample_values = band_data[sample_rows, sample_cols]\n\n                # Remove invalid values (e.g., nodata)\n                valid_samples = sample_values[np.isfinite(sample_values)]\n\n                # Calculate statistics\n                mean = float(np.mean(valid_samples))\n                std = float(np.std(valid_samples))\n\n                # Store results\n                means.append(mean)\n                stds.append(std)\n\n                print(f\"Band {band}: mean={mean:.4f}, std={std:.4f}\")\n\n            # Update instance variables\n            self.sentinel_band_stats = {\"means\": means, \"stds\": stds}\n\n            return self.sentinel_band_stats\n\n    def process_sentinel_raster(\n        self,\n        raster_path,\n        output_path=None,\n        batch_size=4,\n        band_selection=None,\n        use_ndvi=None,\n        filter_edges=True,\n        edge_buffer=20,\n        **kwargs,\n    ):\n        \"\"\"\n        Process a Sentinel-2 raster to extract field boundaries.\n\n        Args:\n            raster_path: Path to Sentinel-2 raster file\n            output_path: Path to output GeoJSON or Parquet file (optional)\n            batch_size: Batch size for processing\n            band_selection: List of bands to use (None = use instance default)\n            use_ndvi: Whether to include NDVI (None = use instance default)\n            filter_edges: Whether to filter out objects at the edges of the image\n            edge_buffer: Size of edge buffer in pixels to filter out objects\n            **kwargs: Additional parameters for processing\n\n        Returns:\n            GeoDataFrame with field boundaries\n        \"\"\"\n        # Use instance defaults if not specified\n        band_selection = (\n            band_selection if band_selection is not None else self.band_selection\n        )\n        use_ndvi = use_ndvi if use_ndvi is not None else self.use_ndvi\n\n        # Get parameters from kwargs or use instance defaults\n        confidence_threshold = kwargs.get(\n            \"confidence_threshold\", self.confidence_threshold\n        )\n        overlap = kwargs.get(\"overlap\", self.overlap)\n        chip_size = kwargs.get(\"chip_size\", self.chip_size)\n        nms_iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n        mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n        min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n        simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n\n        # Update band statistics if not already done\n        if kwargs.get(\"update_stats\", True):\n            self.update_band_stats(raster_path, band_selection)\n\n        print(f\"Processing with parameters:\")\n        print(f\"- Using bands: {band_selection}\")\n        print(f\"- Include NDVI: {use_ndvi}\")\n        print(f\"- Confidence threshold: {confidence_threshold}\")\n        print(f\"- Tile overlap: {overlap}\")\n        print(f\"- Chip size: {chip_size}\")\n        print(f\"- Filter edge objects: {filter_edges}\")\n\n        # Create a custom Sentinel-2 dataset class\n        class Sentinel2Dataset(torch.utils.data.Dataset):\n            def __init__(\n                self,\n                raster_path,\n                chip_size,\n                stride_x,\n                stride_y,\n                band_selection,\n                use_ndvi,\n                field_delineator,\n            ):\n                self.raster_path = raster_path\n                self.chip_size = chip_size\n                self.stride_x = stride_x\n                self.stride_y = stride_y\n                self.band_selection = band_selection\n                self.use_ndvi = use_ndvi\n                self.field_delineator = field_delineator\n\n                with rasterio.open(self.raster_path) as src:\n                    self.height = src.height\n                    self.width = src.width\n                    self.count = src.count\n                    self.crs = src.crs\n                    self.transform = src.transform\n\n                    # Calculate row_starts and col_starts\n                    self.row_starts = []\n                    self.col_starts = []\n\n                    # Normal row starts using stride\n                    for r in range((self.height - 1) // self.stride_y):\n                        self.row_starts.append(r * self.stride_y)\n\n                    # Add a special last row that ensures we reach the bottom edge\n                    if self.height &gt; self.chip_size[0]:\n                        self.row_starts.append(max(0, self.height - self.chip_size[0]))\n                    else:\n                        # If the image is smaller than chip size, just start at 0\n                        if not self.row_starts:\n                            self.row_starts.append(0)\n\n                    # Normal column starts using stride\n                    for c in range((self.width - 1) // self.stride_x):\n                        self.col_starts.append(c * self.stride_x)\n\n                    # Add a special last column that ensures we reach the right edge\n                    if self.width &gt; self.chip_size[1]:\n                        self.col_starts.append(max(0, self.width - self.chip_size[1]))\n                    else:\n                        # If the image is smaller than chip size, just start at 0\n                        if not self.col_starts:\n                            self.col_starts.append(0)\n\n                # Calculate number of tiles\n                self.rows = len(self.row_starts)\n                self.cols = len(self.col_starts)\n\n                print(\n                    f\"Dataset initialized with {self.rows} rows and {self.cols} columns of chips\"\n                )\n                print(f\"Image dimensions: {self.width} x {self.height} pixels\")\n                print(f\"Chip size: {self.chip_size[1]} x {self.chip_size[0]} pixels\")\n\n            def __len__(self):\n                return self.rows * self.cols\n\n            def __getitem__(self, idx):\n                # Convert flat index to grid position\n                row = idx // self.cols\n                col = idx % self.cols\n\n                # Get pre-calculated starting positions\n                j = self.row_starts[row]\n                i = self.col_starts[col]\n\n                # Read window from raster\n                with rasterio.open(self.raster_path) as src:\n                    # Make sure we don't read outside the image\n                    width = min(self.chip_size[1], self.width - i)\n                    height = min(self.chip_size[0], self.height - j)\n\n                    window = Window(i, j, width, height)\n\n                    # Read all bands\n                    image = src.read(window=window)\n\n                    # Handle partial windows at edges by padding\n                    if (\n                        image.shape[1] != self.chip_size[0]\n                        or image.shape[2] != self.chip_size[1]\n                    ):\n                        temp = np.zeros(\n                            (image.shape[0], self.chip_size[0], self.chip_size[1]),\n                            dtype=image.dtype,\n                        )\n                        temp[:, : image.shape[1], : image.shape[2]] = image\n                        image = temp\n\n                # Preprocess bands for the model\n                image_tensor = self.field_delineator.preprocess_sentinel_bands(\n                    image, self.band_selection, self.use_ndvi\n                )\n\n                # Get geographic bounds for the window\n                with rasterio.open(self.raster_path) as src:\n                    window_transform = src.window_transform(window)\n                    minx, miny = window_transform * (0, height)\n                    maxx, maxy = window_transform * (width, 0)\n                    bbox = [minx, miny, maxx, maxy]\n\n                return {\n                    \"image\": image_tensor,\n                    \"bbox\": bbox,\n                    \"coords\": torch.tensor([i, j], dtype=torch.long),\n                    \"window_size\": torch.tensor([width, height], dtype=torch.long),\n                }\n\n        # Calculate stride based on overlap\n        stride_x = int(chip_size[1] * (1 - overlap))\n        stride_y = int(chip_size[0] * (1 - overlap))\n\n        # Create dataset\n        dataset = Sentinel2Dataset(\n            raster_path=raster_path,\n            chip_size=chip_size,\n            stride_x=stride_x,\n            stride_y=stride_y,\n            band_selection=band_selection,\n            use_ndvi=use_ndvi,\n            field_delineator=self,\n        )\n\n        # Define custom collate function\n        def custom_collate(batch):\n            elem = batch[0]\n            if isinstance(elem, dict):\n                result = {}\n                for key in elem:\n                    if key == \"bbox\":\n                        # Don't collate bbox objects, keep as list\n                        result[key] = [d[key] for d in batch]\n                    else:\n                        # For tensors and other collatable types\n                        try:\n                            result[key] = (\n                                torch.utils.data._utils.collate.default_collate(\n                                    [d[key] for d in batch]\n                                )\n                            )\n                        except TypeError:\n                            # Fall back to list for non-collatable types\n                            result[key] = [d[key] for d in batch]\n                return result\n            else:\n                # Default collate for non-dict types\n                return torch.utils.data._utils.collate.default_collate(batch)\n\n        # Create dataloader\n        dataloader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=0,\n            collate_fn=custom_collate,\n        )\n\n        # Process batches (call the parent class's process_raster method)\n        # We'll adapt the process_raster method to work with our Sentinel2Dataset\n        results = super().process_raster(\n            raster_path=raster_path,\n            output_path=output_path,\n            batch_size=batch_size,\n            filter_edges=filter_edges,\n            edge_buffer=edge_buffer,\n            confidence_threshold=confidence_threshold,\n            overlap=overlap,\n            chip_size=chip_size,\n            nms_iou_threshold=nms_iou_threshold,\n            mask_threshold=mask_threshold,\n            min_object_area=min_object_area,\n            simplify_tolerance=simplify_tolerance,\n        )\n\n        return results\n</code></pre>"},{"location":"extract/#geoai.extract.AgricultureFieldDelineator.__init__","title":"<code>__init__(self, model_path='field_boundary_detector.pth', repo_id=None, model=None, device=None, band_selection=None, use_ndvi=False)</code>  <code>special</code>","text":"<p>Initialize the field boundary delineator.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <p>Path to the .pth model file.</p> <code>'field_boundary_detector.pth'</code> <code>repo_id</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> <code>band_selection</code> <p>List of Sentinel-2 band indices to use (None = adapt based on model)</p> <code>None</code> <code>use_ndvi</code> <p>Whether to calculate and include NDVI as an additional channel</p> <code>False</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path=\"field_boundary_detector.pth\",\n    repo_id=None,\n    model=None,\n    device=None,\n    band_selection=None,\n    use_ndvi=False,\n):\n    \"\"\"\n    Initialize the field boundary delineator.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        band_selection: List of Sentinel-2 band indices to use (None = adapt based on model)\n        use_ndvi: Whether to calculate and include NDVI as an additional channel\n    \"\"\"\n    # Save parameters before calling parent constructor\n    self.custom_band_selection = band_selection\n    self.use_ndvi = use_ndvi\n\n    # Set device (copied from parent init to ensure it's set before initialize_model)\n    if device is None:\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    else:\n        self.device = torch.device(device)\n\n    # Initialize model differently for multi-spectral input\n    model = self.initialize_sentinel2_model(model)\n\n    # Call parent but with our custom model\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n\n    # Default Sentinel-2 band statistics (can be overridden with actual stats)\n    # Band order: [B1, B2, B3, B4, B5, B6, B7, B8, B8A, B9, B10, B11, B12]\n    self.sentinel_band_stats = {\n        \"means\": [\n            0.0975,\n            0.0476,\n            0.0598,\n            0.0697,\n            0.1077,\n            0.1859,\n            0.2378,\n            0.2061,\n            0.2598,\n            0.4120,\n            0.1956,\n            0.1410,\n        ],\n        \"stds\": [\n            0.0551,\n            0.0290,\n            0.0298,\n            0.0479,\n            0.0506,\n            0.0505,\n            0.0747,\n            0.0642,\n            0.0782,\n            0.1187,\n            0.0651,\n            0.0679,\n        ],\n    }\n\n    # Set default band selection (RGB - typically B4, B3, B2 for Sentinel-2)\n    self.band_selection = (\n        self.custom_band_selection\n        if self.custom_band_selection is not None\n        else [3, 2, 1]\n    )  # R, G, B bands\n\n    # Customize parameters for field delineation\n    self.confidence_threshold = 0.5  # Default confidence threshold\n    self.overlap = 0.5  # Higher overlap for field boundary detection\n    self.min_object_area = 1000  # Minimum area in pixels for field detection\n    self.simplify_tolerance = 2.0  # Higher tolerance for field boundaries\n</code></pre>"},{"location":"extract/#geoai.extract.AgricultureFieldDelineator.initialize_sentinel2_model","title":"<code>initialize_sentinel2_model(self, model=None)</code>","text":"<p>Initialize a Mask R-CNN model with a modified first layer to accept Sentinel-2 data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Pre-initialized model (optional)</p> <code>None</code> <p>Returns:</p> Type Description <p>Modified model with appropriate input channels</p> Source code in <code>geoai/extract.py</code> <pre><code>def initialize_sentinel2_model(self, model=None):\n    \"\"\"\n    Initialize a Mask R-CNN model with a modified first layer to accept Sentinel-2 data.\n\n    Args:\n        model: Pre-initialized model (optional)\n\n    Returns:\n        Modified model with appropriate input channels\n    \"\"\"\n    import torchvision\n    from torchvision.models.detection import maskrcnn_resnet50_fpn\n    from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n\n    if model is not None:\n        return model\n\n    # Determine number of input channels based on band selection and NDVI\n    num_input_channels = (\n        len(self.custom_band_selection)\n        if self.custom_band_selection is not None\n        else 3\n    )\n    if self.use_ndvi:\n        num_input_channels += 1\n\n    print(f\"Initializing Mask R-CNN model with {num_input_channels} input channels\")\n\n    # Create a ResNet50 backbone with modified input channels\n    backbone = resnet_fpn_backbone(\"resnet50\", weights=None)\n\n    # Replace the first conv layer to accept multi-spectral input\n    original_conv = backbone.body.conv1\n    backbone.body.conv1 = torch.nn.Conv2d(\n        num_input_channels,\n        original_conv.out_channels,\n        kernel_size=original_conv.kernel_size,\n        stride=original_conv.stride,\n        padding=original_conv.padding,\n        bias=original_conv.bias is not None,\n    )\n\n    # Create Mask R-CNN with our modified backbone\n    model = maskrcnn_resnet50_fpn(\n        backbone=backbone,\n        num_classes=2,  # Background + field\n        image_mean=[0.485] * num_input_channels,  # Extend mean to all channels\n        image_std=[0.229] * num_input_channels,  # Extend std to all channels\n    )\n\n    model.to(self.device)\n    return model\n</code></pre>"},{"location":"extract/#geoai.extract.AgricultureFieldDelineator.preprocess_sentinel_bands","title":"<code>preprocess_sentinel_bands(self, image_data, band_selection=None, use_ndvi=None)</code>","text":"<p>Preprocess Sentinel-2 band data for model input.</p> <p>Parameters:</p> Name Type Description Default <code>image_data</code> <p>Raw Sentinel-2 image data as numpy array [bands, height, width]</p> required <code>band_selection</code> <p>List of band indices to use (overrides instance default if provided)</p> <code>None</code> <code>use_ndvi</code> <p>Whether to include NDVI (overrides instance default if provided)</p> <code>None</code> <p>Returns:</p> Type Description <p>Processed tensor ready for model input</p> Source code in <code>geoai/extract.py</code> <pre><code>def preprocess_sentinel_bands(self, image_data, band_selection=None, use_ndvi=None):\n    \"\"\"\n    Preprocess Sentinel-2 band data for model input.\n\n    Args:\n        image_data: Raw Sentinel-2 image data as numpy array [bands, height, width]\n        band_selection: List of band indices to use (overrides instance default if provided)\n        use_ndvi: Whether to include NDVI (overrides instance default if provided)\n\n    Returns:\n        Processed tensor ready for model input\n    \"\"\"\n    # Use instance defaults if not specified\n    band_selection = (\n        band_selection if band_selection is not None else self.band_selection\n    )\n    use_ndvi = use_ndvi if use_ndvi is not None else self.use_ndvi\n\n    # Select bands\n    selected_bands = image_data[band_selection]\n\n    # Calculate NDVI if requested (using B8 and B4 which are indices 7 and 3)\n    if (\n        use_ndvi\n        and 7 in range(image_data.shape[0])\n        and 3 in range(image_data.shape[0])\n    ):\n        nir = image_data[7].astype(np.float32)  # B8 (NIR)\n        red = image_data[3].astype(np.float32)  # B4 (Red)\n\n        # Avoid division by zero\n        denominator = nir + red\n        ndvi = np.zeros_like(nir)\n        valid_mask = denominator &gt; 0\n        ndvi[valid_mask] = (nir[valid_mask] - red[valid_mask]) / denominator[\n            valid_mask\n        ]\n\n        # Rescale NDVI from [-1, 1] to [0, 1]\n        ndvi = (ndvi + 1) / 2\n\n        # Add NDVI as an additional channel\n        selected_bands = np.vstack([selected_bands, ndvi[np.newaxis, :, :]])\n\n    # Convert to tensor\n    image_tensor = torch.from_numpy(selected_bands).float()\n\n    # Normalize using band statistics\n    for i, band_idx in enumerate(band_selection):\n        # Make sure band_idx is within range of our statistics\n        if band_idx &lt; len(self.sentinel_band_stats[\"means\"]):\n            mean = self.sentinel_band_stats[\"means\"][band_idx]\n            std = self.sentinel_band_stats[\"stds\"][band_idx]\n            image_tensor[i] = (image_tensor[i] - mean) / std\n\n    # If NDVI was added, normalize it too (last channel)\n    if use_ndvi:\n        # NDVI is already roughly in [0,1] range, just standardize it slightly\n        image_tensor[-1] = (image_tensor[-1] - 0.5) / 0.5\n\n    return image_tensor\n</code></pre>"},{"location":"extract/#geoai.extract.AgricultureFieldDelineator.process_sentinel_raster","title":"<code>process_sentinel_raster(self, raster_path, output_path=None, batch_size=4, band_selection=None, use_ndvi=None, filter_edges=True, edge_buffer=20, **kwargs)</code>","text":"<p>Process a Sentinel-2 raster to extract field boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <p>Path to Sentinel-2 raster file</p> required <code>output_path</code> <p>Path to output GeoJSON or Parquet file (optional)</p> <code>None</code> <code>batch_size</code> <p>Batch size for processing</p> <code>4</code> <code>band_selection</code> <p>List of bands to use (None = use instance default)</p> <code>None</code> <code>use_ndvi</code> <p>Whether to include NDVI (None = use instance default)</p> <code>None</code> <code>filter_edges</code> <p>Whether to filter out objects at the edges of the image</p> <code>True</code> <code>edge_buffer</code> <p>Size of edge buffer in pixels to filter out objects</p> <code>20</code> <code>**kwargs</code> <p>Additional parameters for processing</p> <code>{}</code> <p>Returns:</p> Type Description <p>GeoDataFrame with field boundaries</p> Source code in <code>geoai/extract.py</code> <pre><code>def process_sentinel_raster(\n    self,\n    raster_path,\n    output_path=None,\n    batch_size=4,\n    band_selection=None,\n    use_ndvi=None,\n    filter_edges=True,\n    edge_buffer=20,\n    **kwargs,\n):\n    \"\"\"\n    Process a Sentinel-2 raster to extract field boundaries.\n\n    Args:\n        raster_path: Path to Sentinel-2 raster file\n        output_path: Path to output GeoJSON or Parquet file (optional)\n        batch_size: Batch size for processing\n        band_selection: List of bands to use (None = use instance default)\n        use_ndvi: Whether to include NDVI (None = use instance default)\n        filter_edges: Whether to filter out objects at the edges of the image\n        edge_buffer: Size of edge buffer in pixels to filter out objects\n        **kwargs: Additional parameters for processing\n\n    Returns:\n        GeoDataFrame with field boundaries\n    \"\"\"\n    # Use instance defaults if not specified\n    band_selection = (\n        band_selection if band_selection is not None else self.band_selection\n    )\n    use_ndvi = use_ndvi if use_ndvi is not None else self.use_ndvi\n\n    # Get parameters from kwargs or use instance defaults\n    confidence_threshold = kwargs.get(\n        \"confidence_threshold\", self.confidence_threshold\n    )\n    overlap = kwargs.get(\"overlap\", self.overlap)\n    chip_size = kwargs.get(\"chip_size\", self.chip_size)\n    nms_iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n    mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n    min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n    simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n\n    # Update band statistics if not already done\n    if kwargs.get(\"update_stats\", True):\n        self.update_band_stats(raster_path, band_selection)\n\n    print(f\"Processing with parameters:\")\n    print(f\"- Using bands: {band_selection}\")\n    print(f\"- Include NDVI: {use_ndvi}\")\n    print(f\"- Confidence threshold: {confidence_threshold}\")\n    print(f\"- Tile overlap: {overlap}\")\n    print(f\"- Chip size: {chip_size}\")\n    print(f\"- Filter edge objects: {filter_edges}\")\n\n    # Create a custom Sentinel-2 dataset class\n    class Sentinel2Dataset(torch.utils.data.Dataset):\n        def __init__(\n            self,\n            raster_path,\n            chip_size,\n            stride_x,\n            stride_y,\n            band_selection,\n            use_ndvi,\n            field_delineator,\n        ):\n            self.raster_path = raster_path\n            self.chip_size = chip_size\n            self.stride_x = stride_x\n            self.stride_y = stride_y\n            self.band_selection = band_selection\n            self.use_ndvi = use_ndvi\n            self.field_delineator = field_delineator\n\n            with rasterio.open(self.raster_path) as src:\n                self.height = src.height\n                self.width = src.width\n                self.count = src.count\n                self.crs = src.crs\n                self.transform = src.transform\n\n                # Calculate row_starts and col_starts\n                self.row_starts = []\n                self.col_starts = []\n\n                # Normal row starts using stride\n                for r in range((self.height - 1) // self.stride_y):\n                    self.row_starts.append(r * self.stride_y)\n\n                # Add a special last row that ensures we reach the bottom edge\n                if self.height &gt; self.chip_size[0]:\n                    self.row_starts.append(max(0, self.height - self.chip_size[0]))\n                else:\n                    # If the image is smaller than chip size, just start at 0\n                    if not self.row_starts:\n                        self.row_starts.append(0)\n\n                # Normal column starts using stride\n                for c in range((self.width - 1) // self.stride_x):\n                    self.col_starts.append(c * self.stride_x)\n\n                # Add a special last column that ensures we reach the right edge\n                if self.width &gt; self.chip_size[1]:\n                    self.col_starts.append(max(0, self.width - self.chip_size[1]))\n                else:\n                    # If the image is smaller than chip size, just start at 0\n                    if not self.col_starts:\n                        self.col_starts.append(0)\n\n            # Calculate number of tiles\n            self.rows = len(self.row_starts)\n            self.cols = len(self.col_starts)\n\n            print(\n                f\"Dataset initialized with {self.rows} rows and {self.cols} columns of chips\"\n            )\n            print(f\"Image dimensions: {self.width} x {self.height} pixels\")\n            print(f\"Chip size: {self.chip_size[1]} x {self.chip_size[0]} pixels\")\n\n        def __len__(self):\n            return self.rows * self.cols\n\n        def __getitem__(self, idx):\n            # Convert flat index to grid position\n            row = idx // self.cols\n            col = idx % self.cols\n\n            # Get pre-calculated starting positions\n            j = self.row_starts[row]\n            i = self.col_starts[col]\n\n            # Read window from raster\n            with rasterio.open(self.raster_path) as src:\n                # Make sure we don't read outside the image\n                width = min(self.chip_size[1], self.width - i)\n                height = min(self.chip_size[0], self.height - j)\n\n                window = Window(i, j, width, height)\n\n                # Read all bands\n                image = src.read(window=window)\n\n                # Handle partial windows at edges by padding\n                if (\n                    image.shape[1] != self.chip_size[0]\n                    or image.shape[2] != self.chip_size[1]\n                ):\n                    temp = np.zeros(\n                        (image.shape[0], self.chip_size[0], self.chip_size[1]),\n                        dtype=image.dtype,\n                    )\n                    temp[:, : image.shape[1], : image.shape[2]] = image\n                    image = temp\n\n            # Preprocess bands for the model\n            image_tensor = self.field_delineator.preprocess_sentinel_bands(\n                image, self.band_selection, self.use_ndvi\n            )\n\n            # Get geographic bounds for the window\n            with rasterio.open(self.raster_path) as src:\n                window_transform = src.window_transform(window)\n                minx, miny = window_transform * (0, height)\n                maxx, maxy = window_transform * (width, 0)\n                bbox = [minx, miny, maxx, maxy]\n\n            return {\n                \"image\": image_tensor,\n                \"bbox\": bbox,\n                \"coords\": torch.tensor([i, j], dtype=torch.long),\n                \"window_size\": torch.tensor([width, height], dtype=torch.long),\n            }\n\n    # Calculate stride based on overlap\n    stride_x = int(chip_size[1] * (1 - overlap))\n    stride_y = int(chip_size[0] * (1 - overlap))\n\n    # Create dataset\n    dataset = Sentinel2Dataset(\n        raster_path=raster_path,\n        chip_size=chip_size,\n        stride_x=stride_x,\n        stride_y=stride_y,\n        band_selection=band_selection,\n        use_ndvi=use_ndvi,\n        field_delineator=self,\n    )\n\n    # Define custom collate function\n    def custom_collate(batch):\n        elem = batch[0]\n        if isinstance(elem, dict):\n            result = {}\n            for key in elem:\n                if key == \"bbox\":\n                    # Don't collate bbox objects, keep as list\n                    result[key] = [d[key] for d in batch]\n                else:\n                    # For tensors and other collatable types\n                    try:\n                        result[key] = (\n                            torch.utils.data._utils.collate.default_collate(\n                                [d[key] for d in batch]\n                            )\n                        )\n                    except TypeError:\n                        # Fall back to list for non-collatable types\n                        result[key] = [d[key] for d in batch]\n            return result\n        else:\n            # Default collate for non-dict types\n            return torch.utils.data._utils.collate.default_collate(batch)\n\n    # Create dataloader\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=0,\n        collate_fn=custom_collate,\n    )\n\n    # Process batches (call the parent class's process_raster method)\n    # We'll adapt the process_raster method to work with our Sentinel2Dataset\n    results = super().process_raster(\n        raster_path=raster_path,\n        output_path=output_path,\n        batch_size=batch_size,\n        filter_edges=filter_edges,\n        edge_buffer=edge_buffer,\n        confidence_threshold=confidence_threshold,\n        overlap=overlap,\n        chip_size=chip_size,\n        nms_iou_threshold=nms_iou_threshold,\n        mask_threshold=mask_threshold,\n        min_object_area=min_object_area,\n        simplify_tolerance=simplify_tolerance,\n    )\n\n    return results\n</code></pre>"},{"location":"extract/#geoai.extract.AgricultureFieldDelineator.update_band_stats","title":"<code>update_band_stats(self, raster_path, band_selection=None, sample_size=1000)</code>","text":"<p>Update band statistics from the input Sentinel-2 raster.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <p>Path to the Sentinel-2 raster file</p> required <code>band_selection</code> <p>Specific bands to update (None = update all available)</p> <code>None</code> <code>sample_size</code> <p>Number of random pixels to sample for statistics calculation</p> <code>1000</code> <p>Returns:</p> Type Description <p>Updated band statistics dictionary</p> Source code in <code>geoai/extract.py</code> <pre><code>def update_band_stats(self, raster_path, band_selection=None, sample_size=1000):\n    \"\"\"\n    Update band statistics from the input Sentinel-2 raster.\n\n    Args:\n        raster_path: Path to the Sentinel-2 raster file\n        band_selection: Specific bands to update (None = update all available)\n        sample_size: Number of random pixels to sample for statistics calculation\n\n    Returns:\n        Updated band statistics dictionary\n    \"\"\"\n    with rasterio.open(raster_path) as src:\n        # Check if this is likely a Sentinel-2 product\n        band_count = src.count\n        if band_count &lt; 3:\n            print(\n                f\"Warning: Raster has only {band_count} bands, may not be Sentinel-2 data\"\n            )\n\n        # Get dimensions\n        height, width = src.height, src.width\n\n        # Determine which bands to analyze\n        if band_selection is None:\n            band_selection = list(range(1, band_count + 1))  # 1-indexed\n\n        # Initialize arrays for band statistics\n        means = []\n        stds = []\n\n        # Sample random pixels\n        np.random.seed(42)  # For reproducibility\n        sample_rows = np.random.randint(0, height, sample_size)\n        sample_cols = np.random.randint(0, width, sample_size)\n\n        # Calculate statistics for each band\n        for band in band_selection:\n            # Read band data\n            band_data = src.read(band)\n\n            # Sample values\n            sample_values = band_data[sample_rows, sample_cols]\n\n            # Remove invalid values (e.g., nodata)\n            valid_samples = sample_values[np.isfinite(sample_values)]\n\n            # Calculate statistics\n            mean = float(np.mean(valid_samples))\n            std = float(np.std(valid_samples))\n\n            # Store results\n            means.append(mean)\n            stds.append(std)\n\n            print(f\"Band {band}: mean={mean:.4f}, std={std:.4f}\")\n\n        # Update instance variables\n        self.sentinel_band_stats = {\"means\": means, \"stds\": stds}\n\n        return self.sentinel_band_stats\n</code></pre>"},{"location":"extract/#geoai.extract.BuildingFootprintExtractor","title":"<code> BuildingFootprintExtractor            (ObjectDetector)         </code>","text":"<p>Building footprint extraction using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for building footprint extraction.\"</p> Source code in <code>geoai/extract.py</code> <pre><code>class BuildingFootprintExtractor(ObjectDetector):\n    \"\"\"\n    Building footprint extraction using a pre-trained Mask R-CNN model.\n\n    This class extends the\n    `ObjectDetector` class with additional methods for building footprint extraction.\"\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path=\"building_footprints_usa.pth\",\n        repo_id=None,\n        model=None,\n        device=None,\n    ):\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n\n    def regularize_buildings(\n        self,\n        gdf,\n        min_area=10,\n        angle_threshold=15,\n        orthogonality_threshold=0.3,\n        rectangularity_threshold=0.7,\n    ):\n        \"\"\"\n        Regularize building footprints to enforce right angles and rectangular shapes.\n\n        Args:\n            gdf: GeoDataFrame with building footprints\n            min_area: Minimum area in square units to keep a building\n            angle_threshold: Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)\n            orthogonality_threshold: Percentage of angles that must be orthogonal for a building to be regularized\n            rectangularity_threshold: Minimum area ratio to building's oriented bounding box for rectangular simplification\n\n        Returns:\n            GeoDataFrame with regularized building footprints\n        \"\"\"\n        return self.regularize_objects(\n            gdf,\n            min_area=min_area,\n            angle_threshold=angle_threshold,\n            orthogonality_threshold=orthogonality_threshold,\n            rectangularity_threshold=rectangularity_threshold,\n        )\n</code></pre>"},{"location":"extract/#geoai.extract.BuildingFootprintExtractor.__init__","title":"<code>__init__(self, model_path='building_footprints_usa.pth', repo_id=None, model=None, device=None)</code>  <code>special</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <p>Path to the .pth model file.</p> <code>'building_footprints_usa.pth'</code> <code>repo_id</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path=\"building_footprints_usa.pth\",\n    repo_id=None,\n    model=None,\n    device=None,\n):\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n</code></pre>"},{"location":"extract/#geoai.extract.BuildingFootprintExtractor.regularize_buildings","title":"<code>regularize_buildings(self, gdf, min_area=10, angle_threshold=15, orthogonality_threshold=0.3, rectangularity_threshold=0.7)</code>","text":"<p>Regularize building footprints to enforce right angles and rectangular shapes.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <p>GeoDataFrame with building footprints</p> required <code>min_area</code> <p>Minimum area in square units to keep a building</p> <code>10</code> <code>angle_threshold</code> <p>Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)</p> <code>15</code> <code>orthogonality_threshold</code> <p>Percentage of angles that must be orthogonal for a building to be regularized</p> <code>0.3</code> <code>rectangularity_threshold</code> <p>Minimum area ratio to building's oriented bounding box for rectangular simplification</p> <code>0.7</code> <p>Returns:</p> Type Description <p>GeoDataFrame with regularized building footprints</p> Source code in <code>geoai/extract.py</code> <pre><code>def regularize_buildings(\n    self,\n    gdf,\n    min_area=10,\n    angle_threshold=15,\n    orthogonality_threshold=0.3,\n    rectangularity_threshold=0.7,\n):\n    \"\"\"\n    Regularize building footprints to enforce right angles and rectangular shapes.\n\n    Args:\n        gdf: GeoDataFrame with building footprints\n        min_area: Minimum area in square units to keep a building\n        angle_threshold: Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)\n        orthogonality_threshold: Percentage of angles that must be orthogonal for a building to be regularized\n        rectangularity_threshold: Minimum area ratio to building's oriented bounding box for rectangular simplification\n\n    Returns:\n        GeoDataFrame with regularized building footprints\n    \"\"\"\n    return self.regularize_objects(\n        gdf,\n        min_area=min_area,\n        angle_threshold=angle_threshold,\n        orthogonality_threshold=orthogonality_threshold,\n        rectangularity_threshold=rectangularity_threshold,\n    )\n</code></pre>"},{"location":"extract/#geoai.extract.CarDetector","title":"<code> CarDetector            (ObjectDetector)         </code>","text":"<p>Car detection using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for car detection.</p> Source code in <code>geoai/extract.py</code> <pre><code>class CarDetector(ObjectDetector):\n    \"\"\"\n    Car detection using a pre-trained Mask R-CNN model.\n\n    This class extends the `ObjectDetector` class with additional methods for car detection.\n    \"\"\"\n\n    def __init__(\n        self, model_path=\"car_detection_usa.pth\", repo_id=None, model=None, device=None\n    ):\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n</code></pre>"},{"location":"extract/#geoai.extract.CarDetector.__init__","title":"<code>__init__(self, model_path='car_detection_usa.pth', repo_id=None, model=None, device=None)</code>  <code>special</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <p>Path to the .pth model file.</p> <code>'car_detection_usa.pth'</code> <code>repo_id</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self, model_path=\"car_detection_usa.pth\", repo_id=None, model=None, device=None\n):\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n</code></pre>"},{"location":"extract/#geoai.extract.CustomDataset","title":"<code> CustomDataset            (NonGeoDataset)         </code>","text":"<p>A TorchGeo dataset for object extraction with overlapping tiles support.</p> <p>This dataset class creates overlapping image tiles for object detection, ensuring complete coverage of the input raster including right and bottom edges. It inherits from NonGeoDataset to avoid spatial indexing issues.</p> <p>Attributes:</p> Name Type Description <code>raster_path</code> <p>Path to the input raster file.</p> <code>chip_size</code> <p>Size of image chips to extract (height, width).</p> <code>overlap</code> <p>Amount of overlap between adjacent tiles (0.0-1.0).</p> <code>transforms</code> <p>Transforms to apply to the image.</p> <code>verbose</code> <p>Whether to print detailed processing information.</p> <code>stride_x</code> <p>Horizontal stride between tiles based on overlap.</p> <code>stride_y</code> <p>Vertical stride between tiles based on overlap.</p> <code>row_starts</code> <p>Starting Y positions for each row of tiles.</p> <code>col_starts</code> <p>Starting X positions for each column of tiles.</p> <code>crs</code> <p>Coordinate reference system of the raster.</p> <code>transform</code> <p>Affine transform of the raster.</p> <code>height</code> <p>Height of the raster in pixels.</p> <code>width</code> <p>Width of the raster in pixels.</p> <code>count</code> <p>Number of bands in the raster.</p> <code>bounds</code> <p>Geographic bounds of the raster (west, south, east, north).</p> <code>roi</code> <p>Shapely box representing the region of interest.</p> <code>rows</code> <p>Number of rows of tiles.</p> <code>cols</code> <p>Number of columns of tiles.</p> <code>raster_stats</code> <p>Statistics of the raster.</p> Source code in <code>geoai/extract.py</code> <pre><code>class CustomDataset(NonGeoDataset):\n    \"\"\"\n    A TorchGeo dataset for object extraction with overlapping tiles support.\n\n    This dataset class creates overlapping image tiles for object detection,\n    ensuring complete coverage of the input raster including right and bottom edges.\n    It inherits from NonGeoDataset to avoid spatial indexing issues.\n\n    Attributes:\n        raster_path: Path to the input raster file.\n        chip_size: Size of image chips to extract (height, width).\n        overlap: Amount of overlap between adjacent tiles (0.0-1.0).\n        transforms: Transforms to apply to the image.\n        verbose: Whether to print detailed processing information.\n        stride_x: Horizontal stride between tiles based on overlap.\n        stride_y: Vertical stride between tiles based on overlap.\n        row_starts: Starting Y positions for each row of tiles.\n        col_starts: Starting X positions for each column of tiles.\n        crs: Coordinate reference system of the raster.\n        transform: Affine transform of the raster.\n        height: Height of the raster in pixels.\n        width: Width of the raster in pixels.\n        count: Number of bands in the raster.\n        bounds: Geographic bounds of the raster (west, south, east, north).\n        roi: Shapely box representing the region of interest.\n        rows: Number of rows of tiles.\n        cols: Number of columns of tiles.\n        raster_stats: Statistics of the raster.\n    \"\"\"\n\n    def __init__(\n        self,\n        raster_path,\n        chip_size=(512, 512),\n        overlap=0.5,\n        transforms=None,\n        band_indexes=None,\n        verbose=False,\n    ):\n        \"\"\"\n        Initialize the dataset with overlapping tiles.\n\n        Args:\n            raster_path: Path to the input raster file.\n            chip_size: Size of image chips to extract (height, width). Default is (512, 512).\n            overlap: Amount of overlap between adjacent tiles (0.0-1.0). Default is 0.5 (50%).\n            transforms: Transforms to apply to the image. Default is None.\n            band_indexes: List of band indexes to use. Default is None (use all bands).\n            verbose: Whether to print detailed processing information. Default is False.\n\n        Raises:\n            ValueError: If overlap is too high resulting in non-positive stride.\n        \"\"\"\n        super().__init__()\n\n        # Initialize parameters\n        self.raster_path = raster_path\n        self.chip_size = chip_size\n        self.overlap = overlap\n        self.transforms = transforms\n        self.band_indexes = band_indexes\n        self.verbose = verbose\n        self.warned_about_bands = False\n\n        # Calculate stride based on overlap\n        self.stride_x = int(chip_size[1] * (1 - overlap))\n        self.stride_y = int(chip_size[0] * (1 - overlap))\n\n        if self.stride_x &lt;= 0 or self.stride_y &lt;= 0:\n            raise ValueError(\n                f\"Overlap {overlap} is too high, resulting in non-positive stride\"\n            )\n\n        with rasterio.open(self.raster_path) as src:\n            self.crs = src.crs\n            self.transform = src.transform\n            self.height = src.height\n            self.width = src.width\n            self.count = src.count\n\n            # Define the bounds of the dataset\n            west, south, east, north = src.bounds\n            self.bounds = (west, south, east, north)\n            self.roi = box(*self.bounds)\n\n            # Calculate starting positions for each tile\n            self.row_starts = []\n            self.col_starts = []\n\n            # Normal row starts using stride\n            for r in range((self.height - 1) // self.stride_y):\n                self.row_starts.append(r * self.stride_y)\n\n            # Add a special last row that ensures we reach the bottom edge\n            if self.height &gt; self.chip_size[0]:\n                self.row_starts.append(max(0, self.height - self.chip_size[0]))\n            else:\n                # If the image is smaller than chip size, just start at 0\n                if not self.row_starts:\n                    self.row_starts.append(0)\n\n            # Normal column starts using stride\n            for c in range((self.width - 1) // self.stride_x):\n                self.col_starts.append(c * self.stride_x)\n\n            # Add a special last column that ensures we reach the right edge\n            if self.width &gt; self.chip_size[1]:\n                self.col_starts.append(max(0, self.width - self.chip_size[1]))\n            else:\n                # If the image is smaller than chip size, just start at 0\n                if not self.col_starts:\n                    self.col_starts.append(0)\n\n            # Update rows and cols based on actual starting positions\n            self.rows = len(self.row_starts)\n            self.cols = len(self.col_starts)\n\n            print(\n                f\"Dataset initialized with {self.rows} rows and {self.cols} columns of chips\"\n            )\n            print(f\"Image dimensions: {self.width} x {self.height} pixels\")\n            print(f\"Chip size: {self.chip_size[1]} x {self.chip_size[0]} pixels\")\n            print(\n                f\"Overlap: {overlap*100}% (stride_x={self.stride_x}, stride_y={self.stride_y})\"\n            )\n            if src.crs:\n                print(f\"CRS: {src.crs}\")\n\n        # Get raster stats\n        self.raster_stats = get_raster_stats(raster_path, divide_by=255)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Get an image chip from the dataset by index.\n\n        Retrieves an image tile with the specified overlap pattern, ensuring\n        proper coverage of the entire raster including edges.\n\n        Args:\n            idx: Index of the chip to retrieve.\n\n        Returns:\n            dict: Dictionary containing:\n                - image: Image tensor.\n                - bbox: Geographic bounding box for the window.\n                - coords: Pixel coordinates as tensor [i, j].\n                - window_size: Window size as tensor [width, height].\n        \"\"\"\n        # Convert flat index to grid position\n        row = idx // self.cols\n        col = idx % self.cols\n\n        # Get pre-calculated starting positions\n        j = self.row_starts[row]\n        i = self.col_starts[col]\n\n        # Read window from raster\n        with rasterio.open(self.raster_path) as src:\n            # Make sure we don't read outside the image\n            width = min(self.chip_size[1], self.width - i)\n            height = min(self.chip_size[0], self.height - j)\n\n            window = Window(i, j, width, height)\n            image = src.read(window=window)\n\n            # Handle RGBA or multispectral images - keep only first 3 bands\n            if image.shape[0] &gt; 3:\n                if not self.warned_about_bands and self.verbose:\n                    print(f\"Image has {image.shape[0]} bands, using first 3 bands only\")\n                    self.warned_about_bands = True\n                if self.band_indexes is not None:\n                    image = image[self.band_indexes]\n                else:\n                    image = image[:3]\n            elif image.shape[0] &lt; 3:\n                # If image has fewer than 3 bands, duplicate the last band to make 3\n                if not self.warned_about_bands and self.verbose:\n                    print(\n                        f\"Image has {image.shape[0]} bands, duplicating bands to make 3\"\n                    )\n                    self.warned_about_bands = True\n                temp = np.zeros((3, image.shape[1], image.shape[2]), dtype=image.dtype)\n                for c in range(3):\n                    temp[c] = image[min(c, image.shape[0] - 1)]\n                image = temp\n\n            # Handle partial windows at edges by padding\n            if (\n                image.shape[1] != self.chip_size[0]\n                or image.shape[2] != self.chip_size[1]\n            ):\n                temp = np.zeros(\n                    (image.shape[0], self.chip_size[0], self.chip_size[1]),\n                    dtype=image.dtype,\n                )\n                temp[:, : image.shape[1], : image.shape[2]] = image\n                image = temp\n\n        # Convert to format expected by model (C,H,W)\n        image = torch.from_numpy(image).float()\n\n        # Normalize to [0, 1]\n        if image.max() &gt; 1:\n            image = image / 255.0\n\n        # Apply transforms if any\n        if self.transforms is not None:\n            image = self.transforms(image)\n\n        # Create geographic bounding box for the window\n        minx, miny = self.transform * (i, j + height)\n        maxx, maxy = self.transform * (i + width, j)\n        bbox = box(minx, miny, maxx, maxy)\n\n        return {\n            \"image\": image,\n            \"bbox\": bbox,\n            \"coords\": torch.tensor([i, j], dtype=torch.long),  # Consistent format\n            \"window_size\": torch.tensor(\n                [width, height], dtype=torch.long\n            ),  # Consistent format\n        }\n\n    def __len__(self):\n        \"\"\"\n        Return the number of samples in the dataset.\n\n        Returns:\n            int: Total number of tiles in the dataset.\n        \"\"\"\n        return self.rows * self.cols\n</code></pre>"},{"location":"extract/#geoai.extract.CustomDataset.__getitem__","title":"<code>__getitem__(self, idx)</code>  <code>special</code>","text":"<p>Get an image chip from the dataset by index.</p> <p>Retrieves an image tile with the specified overlap pattern, ensuring proper coverage of the entire raster including edges.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <p>Index of the chip to retrieve.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing:     - image: Image tensor.     - bbox: Geographic bounding box for the window.     - coords: Pixel coordinates as tensor [i, j].     - window_size: Window size as tensor [width, height].</p> Source code in <code>geoai/extract.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Get an image chip from the dataset by index.\n\n    Retrieves an image tile with the specified overlap pattern, ensuring\n    proper coverage of the entire raster including edges.\n\n    Args:\n        idx: Index of the chip to retrieve.\n\n    Returns:\n        dict: Dictionary containing:\n            - image: Image tensor.\n            - bbox: Geographic bounding box for the window.\n            - coords: Pixel coordinates as tensor [i, j].\n            - window_size: Window size as tensor [width, height].\n    \"\"\"\n    # Convert flat index to grid position\n    row = idx // self.cols\n    col = idx % self.cols\n\n    # Get pre-calculated starting positions\n    j = self.row_starts[row]\n    i = self.col_starts[col]\n\n    # Read window from raster\n    with rasterio.open(self.raster_path) as src:\n        # Make sure we don't read outside the image\n        width = min(self.chip_size[1], self.width - i)\n        height = min(self.chip_size[0], self.height - j)\n\n        window = Window(i, j, width, height)\n        image = src.read(window=window)\n\n        # Handle RGBA or multispectral images - keep only first 3 bands\n        if image.shape[0] &gt; 3:\n            if not self.warned_about_bands and self.verbose:\n                print(f\"Image has {image.shape[0]} bands, using first 3 bands only\")\n                self.warned_about_bands = True\n            if self.band_indexes is not None:\n                image = image[self.band_indexes]\n            else:\n                image = image[:3]\n        elif image.shape[0] &lt; 3:\n            # If image has fewer than 3 bands, duplicate the last band to make 3\n            if not self.warned_about_bands and self.verbose:\n                print(\n                    f\"Image has {image.shape[0]} bands, duplicating bands to make 3\"\n                )\n                self.warned_about_bands = True\n            temp = np.zeros((3, image.shape[1], image.shape[2]), dtype=image.dtype)\n            for c in range(3):\n                temp[c] = image[min(c, image.shape[0] - 1)]\n            image = temp\n\n        # Handle partial windows at edges by padding\n        if (\n            image.shape[1] != self.chip_size[0]\n            or image.shape[2] != self.chip_size[1]\n        ):\n            temp = np.zeros(\n                (image.shape[0], self.chip_size[0], self.chip_size[1]),\n                dtype=image.dtype,\n            )\n            temp[:, : image.shape[1], : image.shape[2]] = image\n            image = temp\n\n    # Convert to format expected by model (C,H,W)\n    image = torch.from_numpy(image).float()\n\n    # Normalize to [0, 1]\n    if image.max() &gt; 1:\n        image = image / 255.0\n\n    # Apply transforms if any\n    if self.transforms is not None:\n        image = self.transforms(image)\n\n    # Create geographic bounding box for the window\n    minx, miny = self.transform * (i, j + height)\n    maxx, maxy = self.transform * (i + width, j)\n    bbox = box(minx, miny, maxx, maxy)\n\n    return {\n        \"image\": image,\n        \"bbox\": bbox,\n        \"coords\": torch.tensor([i, j], dtype=torch.long),  # Consistent format\n        \"window_size\": torch.tensor(\n            [width, height], dtype=torch.long\n        ),  # Consistent format\n    }\n</code></pre>"},{"location":"extract/#geoai.extract.CustomDataset.__init__","title":"<code>__init__(self, raster_path, chip_size=(512, 512), overlap=0.5, transforms=None, band_indexes=None, verbose=False)</code>  <code>special</code>","text":"<p>Initialize the dataset with overlapping tiles.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <p>Path to the input raster file.</p> required <code>chip_size</code> <p>Size of image chips to extract (height, width). Default is (512, 512).</p> <code>(512, 512)</code> <code>overlap</code> <p>Amount of overlap between adjacent tiles (0.0-1.0). Default is 0.5 (50%).</p> <code>0.5</code> <code>transforms</code> <p>Transforms to apply to the image. Default is None.</p> <code>None</code> <code>band_indexes</code> <p>List of band indexes to use. Default is None (use all bands).</p> <code>None</code> <code>verbose</code> <p>Whether to print detailed processing information. Default is False.</p> <code>False</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If overlap is too high resulting in non-positive stride.</p> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    raster_path,\n    chip_size=(512, 512),\n    overlap=0.5,\n    transforms=None,\n    band_indexes=None,\n    verbose=False,\n):\n    \"\"\"\n    Initialize the dataset with overlapping tiles.\n\n    Args:\n        raster_path: Path to the input raster file.\n        chip_size: Size of image chips to extract (height, width). Default is (512, 512).\n        overlap: Amount of overlap between adjacent tiles (0.0-1.0). Default is 0.5 (50%).\n        transforms: Transforms to apply to the image. Default is None.\n        band_indexes: List of band indexes to use. Default is None (use all bands).\n        verbose: Whether to print detailed processing information. Default is False.\n\n    Raises:\n        ValueError: If overlap is too high resulting in non-positive stride.\n    \"\"\"\n    super().__init__()\n\n    # Initialize parameters\n    self.raster_path = raster_path\n    self.chip_size = chip_size\n    self.overlap = overlap\n    self.transforms = transforms\n    self.band_indexes = band_indexes\n    self.verbose = verbose\n    self.warned_about_bands = False\n\n    # Calculate stride based on overlap\n    self.stride_x = int(chip_size[1] * (1 - overlap))\n    self.stride_y = int(chip_size[0] * (1 - overlap))\n\n    if self.stride_x &lt;= 0 or self.stride_y &lt;= 0:\n        raise ValueError(\n            f\"Overlap {overlap} is too high, resulting in non-positive stride\"\n        )\n\n    with rasterio.open(self.raster_path) as src:\n        self.crs = src.crs\n        self.transform = src.transform\n        self.height = src.height\n        self.width = src.width\n        self.count = src.count\n\n        # Define the bounds of the dataset\n        west, south, east, north = src.bounds\n        self.bounds = (west, south, east, north)\n        self.roi = box(*self.bounds)\n\n        # Calculate starting positions for each tile\n        self.row_starts = []\n        self.col_starts = []\n\n        # Normal row starts using stride\n        for r in range((self.height - 1) // self.stride_y):\n            self.row_starts.append(r * self.stride_y)\n\n        # Add a special last row that ensures we reach the bottom edge\n        if self.height &gt; self.chip_size[0]:\n            self.row_starts.append(max(0, self.height - self.chip_size[0]))\n        else:\n            # If the image is smaller than chip size, just start at 0\n            if not self.row_starts:\n                self.row_starts.append(0)\n\n        # Normal column starts using stride\n        for c in range((self.width - 1) // self.stride_x):\n            self.col_starts.append(c * self.stride_x)\n\n        # Add a special last column that ensures we reach the right edge\n        if self.width &gt; self.chip_size[1]:\n            self.col_starts.append(max(0, self.width - self.chip_size[1]))\n        else:\n            # If the image is smaller than chip size, just start at 0\n            if not self.col_starts:\n                self.col_starts.append(0)\n\n        # Update rows and cols based on actual starting positions\n        self.rows = len(self.row_starts)\n        self.cols = len(self.col_starts)\n\n        print(\n            f\"Dataset initialized with {self.rows} rows and {self.cols} columns of chips\"\n        )\n        print(f\"Image dimensions: {self.width} x {self.height} pixels\")\n        print(f\"Chip size: {self.chip_size[1]} x {self.chip_size[0]} pixels\")\n        print(\n            f\"Overlap: {overlap*100}% (stride_x={self.stride_x}, stride_y={self.stride_y})\"\n        )\n        if src.crs:\n            print(f\"CRS: {src.crs}\")\n\n    # Get raster stats\n    self.raster_stats = get_raster_stats(raster_path, divide_by=255)\n</code></pre>"},{"location":"extract/#geoai.extract.CustomDataset.__len__","title":"<code>__len__(self)</code>  <code>special</code>","text":"<p>Return the number of samples in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total number of tiles in the dataset.</p> Source code in <code>geoai/extract.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Return the number of samples in the dataset.\n\n    Returns:\n        int: Total number of tiles in the dataset.\n    \"\"\"\n    return self.rows * self.cols\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector","title":"<code> ObjectDetector        </code>","text":"<p>Object extraction using Mask R-CNN with TorchGeo.</p> Source code in <code>geoai/extract.py</code> <pre><code>class ObjectDetector:\n    \"\"\"\n    Object extraction using Mask R-CNN with TorchGeo.\n    \"\"\"\n\n    def __init__(\n        self, model_path=None, repo_id=None, model=None, num_classes=2, device=None\n    ):\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Hugging Face repository ID for model download.\n            model: Pre-initialized model object (optional).\n            num_classes: Number of classes for detection (default: 2).\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        # Set device\n        if device is None:\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = torch.device(device)\n\n        # Default parameters for object detection - these can be overridden in process_raster\n        self.chip_size = (512, 512)  # Size of image chips for processing\n        self.overlap = 0.25  # Default overlap between tiles\n        self.confidence_threshold = 0.5  # Default confidence threshold\n        self.nms_iou_threshold = 0.5  # IoU threshold for non-maximum suppression\n        self.min_object_area = 100  # Minimum area in pixels to keep an object\n        self.max_object_area = None  # Maximum area in pixels to keep an object\n        self.mask_threshold = 0.5  # Threshold for mask binarization\n        self.simplify_tolerance = 1.0  # Tolerance for polygon simplification\n\n        # Initialize model\n        self.model = self.initialize_model(model, num_classes=num_classes)\n\n        # Download model if needed\n        if model_path is None or (not os.path.exists(model_path)):\n            model_path = self.download_model_from_hf(model_path, repo_id)\n\n        # Load model weights\n        self.load_weights(model_path)\n\n        # Set model to evaluation mode\n        self.model.eval()\n\n    def download_model_from_hf(self, model_path=None, repo_id=None):\n        \"\"\"\n        Download the object detection model from Hugging Face.\n\n        Args:\n            model_path: Path to the model file.\n            repo_id: Hugging Face repository ID.\n\n        Returns:\n            Path to the downloaded model file\n        \"\"\"\n        try:\n\n            print(\"Model path not specified, downloading from Hugging Face...\")\n\n            # Define the repository ID and model filename\n            if repo_id is None:\n                repo_id = \"giswqs/geoai\"\n\n            if model_path is None:\n                model_path = \"building_footprints_usa.pth\"\n\n            # Download the model\n            model_path = hf_hub_download(repo_id=repo_id, filename=model_path)\n            print(f\"Model downloaded to: {model_path}\")\n\n            return model_path\n\n        except Exception as e:\n            print(f\"Error downloading model from Hugging Face: {e}\")\n            print(\"Please specify a local model path or ensure internet connectivity.\")\n            raise\n\n    def initialize_model(self, model, num_classes=2):\n        \"\"\"Initialize a deep learning model for object detection.\n\n        Args:\n            model (torch.nn.Module): A pre-initialized model object.\n            num_classes (int): Number of classes for detection.\n\n        Returns:\n            torch.nn.Module: A deep learning model for object detection.\n        \"\"\"\n\n        if model is None:  # Initialize Mask R-CNN model with ResNet50 backbone.\n            # Standard image mean and std for pre-trained models\n            image_mean = [0.485, 0.456, 0.406]\n            image_std = [0.229, 0.224, 0.225]\n\n            # Create model with explicit normalization parameters\n            model = maskrcnn_resnet50_fpn(\n                weights=None,\n                progress=False,\n                num_classes=num_classes,  # Background + object\n                weights_backbone=None,\n                # These parameters ensure consistent normalization\n                image_mean=image_mean,\n                image_std=image_std,\n            )\n\n        model.to(self.device)\n        return model\n\n    def load_weights(self, model_path):\n        \"\"\"\n        Load weights from file with error handling for different formats.\n\n        Args:\n            model_path: Path to model weights\n        \"\"\"\n        if not os.path.exists(model_path):\n            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n\n        try:\n            state_dict = torch.load(model_path, map_location=self.device)\n\n            # Handle different state dict formats\n            if isinstance(state_dict, dict):\n                if \"model\" in state_dict:\n                    state_dict = state_dict[\"model\"]\n                elif \"state_dict\" in state_dict:\n                    state_dict = state_dict[\"state_dict\"]\n\n            # Try to load state dict\n            try:\n                self.model.load_state_dict(state_dict)\n                print(\"Model loaded successfully\")\n            except Exception as e:\n                print(f\"Error loading model: {e}\")\n                print(\"Attempting to fix state_dict keys...\")\n\n                # Try to fix state_dict keys (remove module prefix if needed)\n                new_state_dict = {}\n                for k, v in state_dict.items():\n                    if k.startswith(\"module.\"):\n                        new_state_dict[k[7:]] = v\n                    else:\n                        new_state_dict[k] = v\n\n                self.model.load_state_dict(new_state_dict)\n                print(\"Model loaded successfully after key fixing\")\n\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load model: {e}\")\n\n    def mask_to_polygons(self, mask, **kwargs):\n        \"\"\"\n        Convert binary mask to polygon contours using OpenCV.\n\n        Args:\n            mask: Binary mask as numpy array\n            **kwargs: Optional parameters:\n                simplify_tolerance: Tolerance for polygon simplification\n                mask_threshold: Threshold for mask binarization\n                min_object_area: Minimum area in pixels to keep an object\n                max_object_area: Maximum area in pixels to keep an object\n\n        Returns:\n            List of polygons as lists of (x, y) coordinates\n        \"\"\"\n\n        # Get parameters from kwargs or use instance defaults\n        simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n        mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n        min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n        max_object_area = kwargs.get(\"max_object_area\", self.max_object_area)\n\n        # Ensure binary mask\n        mask = (mask &gt; mask_threshold).astype(np.uint8)\n\n        # Optional: apply morphological operations to improve mask quality\n        kernel = np.ones((3, 3), np.uint8)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Convert to list of [x, y] coordinates\n        polygons = []\n        for contour in contours:\n            # Filter out too small contours\n            if contour.shape[0] &lt; 3 or cv2.contourArea(contour) &lt; min_object_area:\n                continue\n\n            # Filter out too large contours\n            if (\n                max_object_area is not None\n                and cv2.contourArea(contour) &gt; max_object_area\n            ):\n                continue\n\n            # Simplify contour if it has many points\n            if contour.shape[0] &gt; 50:\n                epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                contour = cv2.approxPolyDP(contour, epsilon, True)\n\n            # Convert to list of [x, y] coordinates\n            polygon = contour.reshape(-1, 2).tolist()\n            polygons.append(polygon)\n\n        return polygons\n\n    def filter_overlapping_polygons(self, gdf, **kwargs):\n        \"\"\"\n        Filter overlapping polygons using non-maximum suppression.\n\n        Args:\n            gdf: GeoDataFrame with polygons\n            **kwargs: Optional parameters:\n                nms_iou_threshold: IoU threshold for filtering\n\n        Returns:\n            Filtered GeoDataFrame\n        \"\"\"\n        if len(gdf) &lt;= 1:\n            return gdf\n\n        # Get parameters from kwargs or use instance defaults\n        iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n\n        # Sort by confidence\n        gdf = gdf.sort_values(\"confidence\", ascending=False)\n\n        # Fix any invalid geometries\n        gdf[\"geometry\"] = gdf[\"geometry\"].apply(\n            lambda geom: geom.buffer(0) if not geom.is_valid else geom\n        )\n\n        keep_indices = []\n        polygons = gdf.geometry.values\n\n        for i in range(len(polygons)):\n            if i in keep_indices:\n                continue\n\n            keep = True\n            for j in keep_indices:\n                # Skip invalid geometries\n                if not polygons[i].is_valid or not polygons[j].is_valid:\n                    continue\n\n                # Calculate IoU\n                try:\n                    intersection = polygons[i].intersection(polygons[j]).area\n                    union = polygons[i].area + polygons[j].area - intersection\n                    iou = intersection / union if union &gt; 0 else 0\n\n                    if iou &gt; iou_threshold:\n                        keep = False\n                        break\n                except Exception:\n                    # Skip on topology exceptions\n                    continue\n\n            if keep:\n                keep_indices.append(i)\n\n        return gdf.iloc[keep_indices]\n\n    def filter_edge_objects(self, gdf, raster_path, edge_buffer=10):\n        \"\"\"\n        Filter out object detections that fall in padding/edge areas of the image.\n\n        Args:\n            gdf: GeoDataFrame with object detections\n            raster_path: Path to the original raster file\n            edge_buffer: Buffer in pixels to consider as edge region\n\n        Returns:\n            GeoDataFrame with filtered objects\n        \"\"\"\n        import rasterio\n        from shapely.geometry import box\n\n        # If no objects detected, return empty GeoDataFrame\n        if gdf is None or len(gdf) == 0:\n            return gdf\n\n        print(f\"Objects before filtering: {len(gdf)}\")\n\n        with rasterio.open(raster_path) as src:\n            # Get raster bounds\n            raster_bounds = src.bounds\n            raster_width = src.width\n            raster_height = src.height\n\n            # Convert edge buffer from pixels to geographic units\n            # We need the smallest dimension of a pixel in geographic units\n            pixel_width = (raster_bounds[2] - raster_bounds[0]) / raster_width\n            pixel_height = (raster_bounds[3] - raster_bounds[1]) / raster_height\n            buffer_size = min(pixel_width, pixel_height) * edge_buffer\n\n            # Create a slightly smaller bounding box to exclude edge regions\n            inner_bounds = (\n                raster_bounds[0] + buffer_size,  # min x (west)\n                raster_bounds[1] + buffer_size,  # min y (south)\n                raster_bounds[2] - buffer_size,  # max x (east)\n                raster_bounds[3] - buffer_size,  # max y (north)\n            )\n\n            # Check that inner bounds are valid\n            if inner_bounds[0] &gt;= inner_bounds[2] or inner_bounds[1] &gt;= inner_bounds[3]:\n                print(\"Warning: Edge buffer too large, using original bounds\")\n                inner_box = box(*raster_bounds)\n            else:\n                inner_box = box(*inner_bounds)\n\n            # Filter out objects that intersect with the edge of the image\n            filtered_gdf = gdf[gdf.intersects(inner_box)]\n\n            # Additional check for objects that have &gt;50% of their area outside the valid region\n            valid_objects = []\n            for idx, row in filtered_gdf.iterrows():\n                if row.geometry.intersection(inner_box).area &gt;= 0.5 * row.geometry.area:\n                    valid_objects.append(idx)\n\n            filtered_gdf = filtered_gdf.loc[valid_objects]\n\n            print(f\"Objects after filtering: {len(filtered_gdf)}\")\n\n            return filtered_gdf\n\n    def masks_to_vector(\n        self,\n        mask_path,\n        output_path=None,\n        simplify_tolerance=None,\n        mask_threshold=None,\n        min_object_area=None,\n        max_object_area=None,\n        nms_iou_threshold=None,\n        regularize=True,\n        angle_threshold=15,\n        rectangularity_threshold=0.7,\n    ):\n        \"\"\"\n        Convert an object mask GeoTIFF to vector polygons and save as GeoJSON.\n\n        Args:\n            mask_path: Path to the object masks GeoTIFF\n            output_path: Path to save the output GeoJSON or Parquet file (default: mask_path with .geojson extension)\n            simplify_tolerance: Tolerance for polygon simplification (default: self.simplify_tolerance)\n            mask_threshold: Threshold for mask binarization (default: self.mask_threshold)\n            min_object_area: Minimum area in pixels to keep an object (default: self.min_object_area)\n            max_object_area: Minimum area in pixels to keep an object (default: self.max_object_area)\n            nms_iou_threshold: IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)\n            regularize: Whether to regularize objects to right angles (default: True)\n            angle_threshold: Maximum deviation from 90 degrees for regularization (default: 15)\n            rectangularity_threshold: Threshold for rectangle simplification (default: 0.7)\n\n        Returns:\n            GeoDataFrame with objects\n        \"\"\"\n        # Use class defaults if parameters not provided\n        simplify_tolerance = (\n            simplify_tolerance\n            if simplify_tolerance is not None\n            else self.simplify_tolerance\n        )\n        mask_threshold = (\n            mask_threshold if mask_threshold is not None else self.mask_threshold\n        )\n        min_object_area = (\n            min_object_area if min_object_area is not None else self.min_object_area\n        )\n        max_object_area = (\n            max_object_area if max_object_area is not None else self.max_object_area\n        )\n        nms_iou_threshold = (\n            nms_iou_threshold\n            if nms_iou_threshold is not None\n            else self.nms_iou_threshold\n        )\n\n        # Set default output path if not provided\n        # if output_path is None:\n        #     output_path = os.path.splitext(mask_path)[0] + \".geojson\"\n\n        print(f\"Converting mask to GeoJSON with parameters:\")\n        print(f\"- Mask threshold: {mask_threshold}\")\n        print(f\"- Min object area: {min_object_area}\")\n        print(f\"- Max object area: {max_object_area}\")\n        print(f\"- Simplify tolerance: {simplify_tolerance}\")\n        print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n        print(f\"- Regularize objects: {regularize}\")\n        if regularize:\n            print(f\"- Angle threshold: {angle_threshold}\u00b0 from 90\u00b0\")\n            print(f\"- Rectangularity threshold: {rectangularity_threshold*100}%\")\n\n        # Open the mask raster\n        with rasterio.open(mask_path) as src:\n            # Read the mask data\n            mask_data = src.read(1)\n            transform = src.transform\n            crs = src.crs\n\n            # Print mask statistics\n            print(f\"Mask dimensions: {mask_data.shape}\")\n            print(f\"Mask value range: {mask_data.min()} to {mask_data.max()}\")\n\n            # Prepare for connected component analysis\n            # Binarize the mask based on threshold\n            binary_mask = (mask_data &gt; (mask_threshold * 255)).astype(np.uint8)\n\n            # Apply morphological operations for better results (optional)\n            kernel = np.ones((3, 3), np.uint8)\n            binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n\n            # Find connected components\n            num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n                binary_mask, connectivity=8\n            )\n\n            print(\n                f\"Found {num_labels-1} potential objects\"\n            )  # Subtract 1 for background\n\n            # Create list to store polygons and confidence values\n            all_polygons = []\n            all_confidences = []\n\n            # Process each component (skip the first one which is background)\n            for i in tqdm(range(1, num_labels)):\n                # Extract this object\n                area = stats[i, cv2.CC_STAT_AREA]\n\n                # Skip if too small\n                if area &lt; min_object_area:\n                    continue\n\n                # Skip if too large\n                if max_object_area is not None and area &gt; max_object_area:\n                    continue\n\n                # Create a mask for this object\n                object_mask = (labels == i).astype(np.uint8)\n\n                # Find contours\n                contours, _ = cv2.findContours(\n                    object_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n                )\n\n                # Process each contour\n                for contour in contours:\n                    # Skip if too few points\n                    if contour.shape[0] &lt; 3:\n                        continue\n\n                    # Simplify contour if it has many points\n                    if contour.shape[0] &gt; 50 and simplify_tolerance &gt; 0:\n                        epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                        contour = cv2.approxPolyDP(contour, epsilon, True)\n\n                    # Convert to list of (x, y) coordinates\n                    polygon_points = contour.reshape(-1, 2)\n\n                    # Convert pixel coordinates to geographic coordinates\n                    geo_points = []\n                    for x, y in polygon_points:\n                        gx, gy = transform * (x, y)\n                        geo_points.append((gx, gy))\n\n                    # Create Shapely polygon\n                    if len(geo_points) &gt;= 3:\n                        try:\n                            shapely_poly = Polygon(geo_points)\n                            if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                                all_polygons.append(shapely_poly)\n\n                                # Calculate \"confidence\" as normalized size\n                                # This is a proxy since we don't have model confidence scores\n                                normalized_size = min(1.0, area / 1000)  # Cap at 1.0\n                                all_confidences.append(normalized_size)\n                        except Exception as e:\n                            print(f\"Error creating polygon: {e}\")\n\n            print(f\"Created {len(all_polygons)} valid polygons\")\n\n            # Create GeoDataFrame\n            if not all_polygons:\n                print(\"No valid polygons found\")\n                return None\n\n            gdf = gpd.GeoDataFrame(\n                {\n                    \"geometry\": all_polygons,\n                    \"confidence\": all_confidences,\n                    \"class\": 1,  # Object class\n                },\n                crs=crs,\n            )\n\n            # Apply non-maximum suppression to remove overlapping polygons\n            gdf = self.filter_overlapping_polygons(\n                gdf, nms_iou_threshold=nms_iou_threshold\n            )\n\n            print(f\"Object count after NMS filtering: {len(gdf)}\")\n\n            # Apply regularization if requested\n            if regularize and len(gdf) &gt; 0:\n                # Convert pixel area to geographic units for min_area parameter\n                # Estimate pixel size in geographic units\n                with rasterio.open(mask_path) as src:\n                    pixel_size_x = src.transform[\n                        0\n                    ]  # width of a pixel in geographic units\n                    pixel_size_y = abs(\n                        src.transform[4]\n                    )  # height of a pixel in geographic units\n                    avg_pixel_area = pixel_size_x * pixel_size_y\n\n                # Use 10 pixels as minimum area in geographic units\n                min_geo_area = 10 * avg_pixel_area\n\n                # Regularize objects\n                gdf = self.regularize_objects(\n                    gdf,\n                    min_area=min_geo_area,\n                    angle_threshold=angle_threshold,\n                    rectangularity_threshold=rectangularity_threshold,\n                )\n\n            # Save to file\n            if output_path:\n                if output_path.endswith(\".parquet\"):\n                    gdf.to_parquet(output_path)\n                else:\n                    gdf.to_file(output_path)\n                print(f\"Saved {len(gdf)} objects to {output_path}\")\n\n            return gdf\n\n    @torch.no_grad()\n    def process_raster(\n        self,\n        raster_path,\n        output_path=None,\n        batch_size=4,\n        filter_edges=True,\n        edge_buffer=20,\n        band_indexes=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Process a raster file to extract objects with customizable parameters.\n\n        Args:\n            raster_path: Path to input raster file\n            output_path: Path to output GeoJSON or Parquet file (optional)\n            batch_size: Batch size for processing\n            filter_edges: Whether to filter out objects at the edges of the image\n            edge_buffer: Size of edge buffer in pixels to filter out objects (if filter_edges=True)\n            band_indexes: List of band indexes to use (if None, use all bands)\n            **kwargs: Additional parameters:\n                confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n                overlap: Overlap between adjacent tiles (0.0-1.0)\n                chip_size: Size of image chips for processing (height, width)\n                nms_iou_threshold: IoU threshold for non-maximum suppression (0.0-1.0)\n                mask_threshold: Threshold for mask binarization (0.0-1.0)\n                min_object_area: Minimum area in pixels to keep an object\n                simplify_tolerance: Tolerance for polygon simplification\n\n        Returns:\n            GeoDataFrame with objects\n        \"\"\"\n        # Get parameters from kwargs or use instance defaults\n        confidence_threshold = kwargs.get(\n            \"confidence_threshold\", self.confidence_threshold\n        )\n        overlap = kwargs.get(\"overlap\", self.overlap)\n        chip_size = kwargs.get(\"chip_size\", self.chip_size)\n        nms_iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n        mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n        min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n        max_object_area = kwargs.get(\"max_object_area\", self.max_object_area)\n        simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n\n        # Print parameters being used\n        print(f\"Processing with parameters:\")\n        print(f\"- Confidence threshold: {confidence_threshold}\")\n        print(f\"- Tile overlap: {overlap}\")\n        print(f\"- Chip size: {chip_size}\")\n        print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n        print(f\"- Mask threshold: {mask_threshold}\")\n        print(f\"- Min object area: {min_object_area}\")\n        print(f\"- Max object area: {max_object_area}\")\n        print(f\"- Simplify tolerance: {simplify_tolerance}\")\n        print(f\"- Filter edge objects: {filter_edges}\")\n        if filter_edges:\n            print(f\"- Edge buffer size: {edge_buffer} pixels\")\n\n        # Create dataset\n        dataset = CustomDataset(\n            raster_path=raster_path,\n            chip_size=chip_size,\n            overlap=overlap,\n            band_indexes=band_indexes,\n        )\n        self.raster_stats = dataset.raster_stats\n\n        # Custom collate function to handle Shapely objects\n        def custom_collate(batch):\n            \"\"\"\n            Custom collate function that handles Shapely geometries\n            by keeping them as Python objects rather than trying to collate them.\n            \"\"\"\n            elem = batch[0]\n            if isinstance(elem, dict):\n                result = {}\n                for key in elem:\n                    if key == \"bbox\":\n                        # Don't collate shapely objects, keep as list\n                        result[key] = [d[key] for d in batch]\n                    else:\n                        # For tensors and other collatable types\n                        try:\n                            result[key] = (\n                                torch.utils.data._utils.collate.default_collate(\n                                    [d[key] for d in batch]\n                                )\n                            )\n                        except TypeError:\n                            # Fall back to list for non-collatable types\n                            result[key] = [d[key] for d in batch]\n                return result\n            else:\n                # Default collate for non-dict types\n                return torch.utils.data._utils.collate.default_collate(batch)\n\n        # Create dataloader with simple indexing and custom collate\n        dataloader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=0,\n            collate_fn=custom_collate,\n        )\n\n        # Process batches\n        all_polygons = []\n        all_scores = []\n\n        print(f\"Processing raster with {len(dataloader)} batches\")\n        for batch in tqdm(dataloader):\n            # Move images to device\n            images = batch[\"image\"].to(self.device)\n            coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n            bboxes = batch[\n                \"bbox\"\n            ]  # Geographic bounding boxes - now a list, not a tensor\n\n            # Run inference\n            predictions = self.model(images)\n\n            # Process predictions\n            for idx, prediction in enumerate(predictions):\n                masks = prediction[\"masks\"].cpu().numpy()\n                scores = prediction[\"scores\"].cpu().numpy()\n                labels = prediction[\"labels\"].cpu().numpy()\n\n                # Skip if no predictions\n                if len(scores) == 0:\n                    continue\n\n                # Filter by confidence threshold\n                valid_indices = scores &gt;= confidence_threshold\n                masks = masks[valid_indices]\n                scores = scores[valid_indices]\n                labels = labels[valid_indices]\n\n                # Skip if no valid predictions\n                if len(scores) == 0:\n                    continue\n\n                # Get window coordinates\n                # The coords might be in different formats depending on batch handling\n                if isinstance(coords, list):\n                    # If coords is a list of tuples\n                    coord_item = coords[idx]\n                    if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                        i, j = coord_item\n                    elif isinstance(coord_item, torch.Tensor):\n                        i, j = coord_item.cpu().numpy().tolist()\n                    else:\n                        print(f\"Unexpected coords format: {type(coord_item)}\")\n                        continue\n                elif isinstance(coords, torch.Tensor):\n                    # If coords is a tensor of shape [batch_size, 2]\n                    i, j = coords[idx].cpu().numpy().tolist()\n                else:\n                    print(f\"Unexpected coords type: {type(coords)}\")\n                    continue\n\n                # Get window size\n                if isinstance(batch[\"window_size\"], list):\n                    window_item = batch[\"window_size\"][idx]\n                    if isinstance(window_item, tuple) and len(window_item) == 2:\n                        window_width, window_height = window_item\n                    elif isinstance(window_item, torch.Tensor):\n                        window_width, window_height = window_item.cpu().numpy().tolist()\n                    else:\n                        print(f\"Unexpected window_size format: {type(window_item)}\")\n                        continue\n                elif isinstance(batch[\"window_size\"], torch.Tensor):\n                    window_width, window_height = (\n                        batch[\"window_size\"][idx].cpu().numpy().tolist()\n                    )\n                else:\n                    print(f\"Unexpected window_size type: {type(batch['window_size'])}\")\n                    continue\n\n                # Process masks to polygons\n                for mask_idx, mask in enumerate(masks):\n                    # Get binary mask\n                    binary_mask = mask[0]  # Get binary mask\n\n                    # Convert mask to polygon with custom parameters\n                    contours = self.mask_to_polygons(\n                        binary_mask,\n                        simplify_tolerance=simplify_tolerance,\n                        mask_threshold=mask_threshold,\n                        min_object_area=min_object_area,\n                        max_object_area=max_object_area,\n                    )\n\n                    # Skip if no valid polygons\n                    if not contours:\n                        continue\n\n                    # Transform polygons to geographic coordinates\n                    with rasterio.open(raster_path) as src:\n                        transform = src.transform\n\n                        for contour in contours:\n                            # Convert polygon to global coordinates\n                            global_polygon = []\n                            for x, y in contour:\n                                # Adjust coordinates based on window position\n                                gx, gy = transform * (i + x, j + y)\n                                global_polygon.append((gx, gy))\n\n                            # Create Shapely polygon\n                            if len(global_polygon) &gt;= 3:\n                                try:\n                                    shapely_poly = Polygon(global_polygon)\n                                    if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                                        all_polygons.append(shapely_poly)\n                                        all_scores.append(float(scores[mask_idx]))\n                                except Exception as e:\n                                    print(f\"Error creating polygon: {e}\")\n\n        # Create GeoDataFrame\n        if not all_polygons:\n            print(\"No valid polygons found\")\n            return None\n\n        gdf = gpd.GeoDataFrame(\n            {\n                \"geometry\": all_polygons,\n                \"confidence\": all_scores,\n                \"class\": 1,  # Object class\n            },\n            crs=dataset.crs,\n        )\n\n        # Remove overlapping polygons with custom threshold\n        gdf = self.filter_overlapping_polygons(gdf, nms_iou_threshold=nms_iou_threshold)\n\n        # Filter edge objects if requested\n        if filter_edges:\n            gdf = self.filter_edge_objects(gdf, raster_path, edge_buffer=edge_buffer)\n\n        # Save to file if requested\n        if output_path:\n            if output_path.endswith(\".parquet\"):\n                gdf.to_parquet(output_path)\n            else:\n                gdf.to_file(output_path, driver=\"GeoJSON\")\n            print(f\"Saved {len(gdf)} objects to {output_path}\")\n\n        return gdf\n\n    def save_masks_as_geotiff(\n        self, raster_path, output_path=None, batch_size=4, verbose=False, **kwargs\n    ):\n        \"\"\"\n        Process a raster file to extract object masks and save as GeoTIFF.\n\n        Args:\n            raster_path: Path to input raster file\n            output_path: Path to output GeoTIFF file (optional, default: input_masks.tif)\n            batch_size: Batch size for processing\n            verbose: Whether to print detailed processing information\n            **kwargs: Additional parameters:\n                confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n                chip_size: Size of image chips for processing (height, width)\n                mask_threshold: Threshold for mask binarization (0.0-1.0)\n\n        Returns:\n            Path to the saved GeoTIFF file\n        \"\"\"\n\n        # Get parameters from kwargs or use instance defaults\n        confidence_threshold = kwargs.get(\n            \"confidence_threshold\", self.confidence_threshold\n        )\n        chip_size = kwargs.get(\"chip_size\", self.chip_size)\n        mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n        overlap = kwargs.get(\"overlap\", self.overlap)\n\n        # Set default output path if not provided\n        if output_path is None:\n            output_path = os.path.splitext(raster_path)[0] + \"_masks.tif\"\n\n        # Print parameters being used\n        print(f\"Processing masks with parameters:\")\n        print(f\"- Confidence threshold: {confidence_threshold}\")\n        print(f\"- Chip size: {chip_size}\")\n        print(f\"- Mask threshold: {mask_threshold}\")\n\n        # Create dataset\n        dataset = CustomDataset(\n            raster_path=raster_path,\n            chip_size=chip_size,\n            overlap=overlap,\n            verbose=verbose,\n        )\n\n        # Store a flag to avoid repetitive messages\n        self.raster_stats = dataset.raster_stats\n        seen_warnings = {\n            \"bands\": False,\n            \"resize\": {},  # Dictionary to track resize warnings by shape\n        }\n\n        # Open original raster to get metadata\n        with rasterio.open(raster_path) as src:\n            # Create output binary mask raster with same dimensions as input\n            output_profile = src.profile.copy()\n            output_profile.update(\n                dtype=rasterio.uint8,\n                count=1,  # Single band for object mask\n                compress=\"lzw\",\n                nodata=0,\n            )\n\n            # Create output mask raster\n            with rasterio.open(output_path, \"w\", **output_profile) as dst:\n                # Initialize mask with zeros\n                mask_array = np.zeros((src.height, src.width), dtype=np.uint8)\n\n                # Custom collate function to handle Shapely objects\n                def custom_collate(batch):\n                    \"\"\"Custom collate function for DataLoader\"\"\"\n                    elem = batch[0]\n                    if isinstance(elem, dict):\n                        result = {}\n                        for key in elem:\n                            if key == \"bbox\":\n                                # Don't collate shapely objects, keep as list\n                                result[key] = [d[key] for d in batch]\n                            else:\n                                # For tensors and other collatable types\n                                try:\n                                    result[key] = (\n                                        torch.utils.data._utils.collate.default_collate(\n                                            [d[key] for d in batch]\n                                        )\n                                    )\n                                except TypeError:\n                                    # Fall back to list for non-collatable types\n                                    result[key] = [d[key] for d in batch]\n                        return result\n                    else:\n                        # Default collate for non-dict types\n                        return torch.utils.data._utils.collate.default_collate(batch)\n\n                # Create dataloader\n                dataloader = torch.utils.data.DataLoader(\n                    dataset,\n                    batch_size=batch_size,\n                    shuffle=False,\n                    num_workers=0,\n                    collate_fn=custom_collate,\n                )\n\n                # Process batches\n                print(f\"Processing raster with {len(dataloader)} batches\")\n                for batch in tqdm(dataloader):\n                    # Move images to device\n                    images = batch[\"image\"].to(self.device)\n                    coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n\n                    # Run inference\n                    with torch.no_grad():\n                        predictions = self.model(images)\n\n                    # Process predictions\n                    for idx, prediction in enumerate(predictions):\n                        masks = prediction[\"masks\"].cpu().numpy()\n                        scores = prediction[\"scores\"].cpu().numpy()\n\n                        # Skip if no predictions\n                        if len(scores) == 0:\n                            continue\n\n                        # Filter by confidence threshold\n                        valid_indices = scores &gt;= confidence_threshold\n                        masks = masks[valid_indices]\n                        scores = scores[valid_indices]\n\n                        # Skip if no valid predictions\n                        if len(scores) == 0:\n                            continue\n\n                        # Get window coordinates\n                        if isinstance(coords, list):\n                            coord_item = coords[idx]\n                            if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                                i, j = coord_item\n                            elif isinstance(coord_item, torch.Tensor):\n                                i, j = coord_item.cpu().numpy().tolist()\n                            else:\n                                print(f\"Unexpected coords format: {type(coord_item)}\")\n                                continue\n                        elif isinstance(coords, torch.Tensor):\n                            i, j = coords[idx].cpu().numpy().tolist()\n                        else:\n                            print(f\"Unexpected coords type: {type(coords)}\")\n                            continue\n\n                        # Get window size\n                        if isinstance(batch[\"window_size\"], list):\n                            window_item = batch[\"window_size\"][idx]\n                            if isinstance(window_item, tuple) and len(window_item) == 2:\n                                window_width, window_height = window_item\n                            elif isinstance(window_item, torch.Tensor):\n                                window_width, window_height = (\n                                    window_item.cpu().numpy().tolist()\n                                )\n                            else:\n                                print(\n                                    f\"Unexpected window_size format: {type(window_item)}\"\n                                )\n                                continue\n                        elif isinstance(batch[\"window_size\"], torch.Tensor):\n                            window_width, window_height = (\n                                batch[\"window_size\"][idx].cpu().numpy().tolist()\n                            )\n                        else:\n                            print(\n                                f\"Unexpected window_size type: {type(batch['window_size'])}\"\n                            )\n                            continue\n\n                        # Combine all masks for this window\n                        combined_mask = np.zeros(\n                            (window_height, window_width), dtype=np.uint8\n                        )\n\n                        for mask in masks:\n                            # Get the binary mask\n                            binary_mask = (mask[0] &gt; mask_threshold).astype(\n                                np.uint8\n                            ) * 255\n\n                            # Handle size mismatch - resize binary_mask if needed\n                            mask_h, mask_w = binary_mask.shape\n                            if mask_h != window_height or mask_w != window_width:\n                                resize_key = f\"{(mask_h, mask_w)}-&gt;{(window_height, window_width)}\"\n                                if resize_key not in seen_warnings[\"resize\"]:\n                                    if verbose:\n                                        print(\n                                            f\"Resizing mask from {binary_mask.shape} to {(window_height, window_width)}\"\n                                        )\n                                    else:\n                                        if not seen_warnings[\n                                            \"resize\"\n                                        ]:  # If this is the first resize warning\n                                            print(\n                                                f\"Resizing masks at image edges (set verbose=True for details)\"\n                                            )\n                                    seen_warnings[\"resize\"][resize_key] = True\n\n                                # Crop or pad the binary mask to match window size\n                                resized_mask = np.zeros(\n                                    (window_height, window_width), dtype=np.uint8\n                                )\n                                copy_h = min(mask_h, window_height)\n                                copy_w = min(mask_w, window_width)\n                                resized_mask[:copy_h, :copy_w] = binary_mask[\n                                    :copy_h, :copy_w\n                                ]\n                                binary_mask = resized_mask\n\n                            # Update combined mask (taking maximum where masks overlap)\n                            combined_mask = np.maximum(combined_mask, binary_mask)\n\n                        # Write combined mask to output array\n                        # Handle edge cases where window might be smaller than chip size\n                        h, w = combined_mask.shape\n                        valid_h = min(h, src.height - j)\n                        valid_w = min(w, src.width - i)\n\n                        if valid_h &gt; 0 and valid_w &gt; 0:\n                            mask_array[j : j + valid_h, i : i + valid_w] = np.maximum(\n                                mask_array[j : j + valid_h, i : i + valid_w],\n                                combined_mask[:valid_h, :valid_w],\n                            )\n\n                # Write the final mask to the output file\n                dst.write(mask_array, 1)\n\n        print(f\"Object masks saved to {output_path}\")\n        return output_path\n\n    def regularize_objects(\n        self,\n        gdf,\n        min_area=10,\n        angle_threshold=15,\n        orthogonality_threshold=0.3,\n        rectangularity_threshold=0.7,\n    ):\n        \"\"\"\n        Regularize objects to enforce right angles and rectangular shapes.\n\n        Args:\n            gdf: GeoDataFrame with objects\n            min_area: Minimum area in square units to keep an object\n            angle_threshold: Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)\n            orthogonality_threshold: Percentage of angles that must be orthogonal for an object to be regularized\n            rectangularity_threshold: Minimum area ratio to Object's oriented bounding box for rectangular simplification\n\n        Returns:\n            GeoDataFrame with regularized objects\n        \"\"\"\n        import math\n\n        import cv2\n        import geopandas as gpd\n        import numpy as np\n        from shapely.affinity import rotate, translate\n        from shapely.geometry import MultiPolygon, Polygon, box\n        from tqdm import tqdm\n\n        def get_angle(p1, p2, p3):\n            \"\"\"Calculate angle between three points in degrees (0-180)\"\"\"\n            a = np.array(p1)\n            b = np.array(p2)\n            c = np.array(p3)\n\n            ba = a - b\n            bc = c - b\n\n            cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n            # Handle numerical errors that could push cosine outside [-1, 1]\n            cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n            angle = np.degrees(np.arccos(cosine_angle))\n\n            return angle\n\n        def is_orthogonal(angle, threshold=angle_threshold):\n            \"\"\"Check if angle is close to 90 degrees\"\"\"\n            return abs(angle - 90) &lt;= threshold\n\n        def calculate_dominant_direction(polygon):\n            \"\"\"Find the dominant direction of a polygon using PCA\"\"\"\n            # Extract coordinates\n            coords = np.array(polygon.exterior.coords)\n\n            # Mean center the coordinates\n            mean = np.mean(coords, axis=0)\n            centered_coords = coords - mean\n\n            # Calculate covariance matrix and its eigenvalues/eigenvectors\n            cov_matrix = np.cov(centered_coords.T)\n            eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n            # Get the index of the largest eigenvalue\n            largest_idx = np.argmax(eigenvalues)\n\n            # Get the corresponding eigenvector (principal axis)\n            principal_axis = eigenvectors[:, largest_idx]\n\n            # Calculate the angle in degrees\n            angle_rad = np.arctan2(principal_axis[1], principal_axis[0])\n            angle_deg = np.degrees(angle_rad)\n\n            # Normalize to range 0-180\n            if angle_deg &lt; 0:\n                angle_deg += 180\n\n            return angle_deg\n\n        def create_oriented_envelope(polygon, angle_deg):\n            \"\"\"Create an oriented minimum area rectangle for the polygon\"\"\"\n            # Create a rotated rectangle using OpenCV method (more robust than Shapely methods)\n            coords = np.array(polygon.exterior.coords)[:-1].astype(\n                np.float32\n            )  # Skip the last point (same as first)\n\n            # Use OpenCV's minAreaRect\n            rect = cv2.minAreaRect(coords)\n            box_points = cv2.boxPoints(rect)\n\n            # Convert to shapely polygon\n            oriented_box = Polygon(box_points)\n\n            return oriented_box\n\n        def get_rectangularity(polygon, oriented_box):\n            \"\"\"Calculate the rectangularity (area ratio to its oriented bounding box)\"\"\"\n            if oriented_box.area == 0:\n                return 0\n            return polygon.area / oriented_box.area\n\n        def check_orthogonality(polygon):\n            \"\"\"Check what percentage of angles in the polygon are orthogonal\"\"\"\n            coords = list(polygon.exterior.coords)\n            if len(coords) &lt;= 4:  # Triangle or point\n                return 0\n\n            # Remove last point (same as first)\n            coords = coords[:-1]\n\n            orthogonal_count = 0\n            total_angles = len(coords)\n\n            for i in range(total_angles):\n                p1 = coords[i]\n                p2 = coords[(i + 1) % total_angles]\n                p3 = coords[(i + 2) % total_angles]\n\n                angle = get_angle(p1, p2, p3)\n                if is_orthogonal(angle):\n                    orthogonal_count += 1\n\n            return orthogonal_count / total_angles\n\n        def simplify_to_rectangle(polygon):\n            \"\"\"Simplify a polygon to a rectangle using its oriented bounding box\"\"\"\n            # Get dominant direction\n            angle = calculate_dominant_direction(polygon)\n\n            # Create oriented envelope\n            rect = create_oriented_envelope(polygon, angle)\n\n            return rect\n\n        if gdf is None or len(gdf) == 0:\n            print(\"No Objects to regularize\")\n            return gdf\n\n        print(f\"Regularizing {len(gdf)} objects...\")\n        print(f\"- Angle threshold: {angle_threshold}\u00b0 from 90\u00b0\")\n        print(f\"- Min orthogonality: {orthogonality_threshold*100}% of angles\")\n        print(\n            f\"- Min rectangularity: {rectangularity_threshold*100}% of bounding box area\"\n        )\n\n        # Create a copy to avoid modifying the original\n        result_gdf = gdf.copy()\n\n        # Track statistics\n        total_objects = len(gdf)\n        regularized_count = 0\n        rectangularized_count = 0\n\n        # Process each Object\n        for idx, row in tqdm(gdf.iterrows(), total=len(gdf)):\n            geom = row.geometry\n\n            # Skip invalid or empty geometries\n            if geom is None or geom.is_empty:\n                continue\n\n            # Handle MultiPolygons by processing the largest part\n            if isinstance(geom, MultiPolygon):\n                areas = [p.area for p in geom.geoms]\n                if not areas:\n                    continue\n                geom = list(geom.geoms)[np.argmax(areas)]\n\n            # Filter out tiny Objects\n            if geom.area &lt; min_area:\n                continue\n\n            # Check orthogonality\n            orthogonality = check_orthogonality(geom)\n\n            # Create oriented envelope\n            oriented_box = create_oriented_envelope(\n                geom, calculate_dominant_direction(geom)\n            )\n\n            # Check rectangularity\n            rectangularity = get_rectangularity(geom, oriented_box)\n\n            # Decide how to regularize\n            if rectangularity &gt;= rectangularity_threshold:\n                # Object is already quite rectangular, simplify to a rectangle\n                result_gdf.at[idx, \"geometry\"] = oriented_box\n                result_gdf.at[idx, \"regularized\"] = \"rectangle\"\n                rectangularized_count += 1\n            elif orthogonality &gt;= orthogonality_threshold:\n                # Object has many orthogonal angles but isn't rectangular\n                # Could implement more sophisticated regularization here\n                # For now, we'll still use the oriented rectangle\n                result_gdf.at[idx, \"geometry\"] = oriented_box\n                result_gdf.at[idx, \"regularized\"] = \"orthogonal\"\n                regularized_count += 1\n            else:\n                # Object doesn't have clear orthogonal structure\n                # Keep original but flag as unmodified\n                result_gdf.at[idx, \"regularized\"] = \"original\"\n\n        # Report statistics\n        print(f\"Regularization completed:\")\n        print(f\"- Total objects: {total_objects}\")\n        print(\n            f\"- Rectangular objects: {rectangularized_count} ({rectangularized_count/total_objects*100:.1f}%)\"\n        )\n        print(\n            f\"- Other regularized objects: {regularized_count} ({regularized_count/total_objects*100:.1f}%)\"\n        )\n        print(\n            f\"- Unmodified objects: {total_objects-rectangularized_count-regularized_count} ({(total_objects-rectangularized_count-regularized_count)/total_objects*100:.1f}%)\"\n        )\n\n        return result_gdf\n\n    def visualize_results(\n        self, raster_path, gdf=None, output_path=None, figsize=(12, 12)\n    ):\n        \"\"\"\n        Visualize object detection results with proper coordinate transformation.\n\n        This function displays objects on top of the raster image,\n        ensuring proper alignment between the GeoDataFrame polygons and the image.\n\n        Args:\n            raster_path: Path to input raster\n            gdf: GeoDataFrame with object polygons (optional)\n            output_path: Path to save visualization (optional)\n            figsize: Figure size (width, height) in inches\n\n        Returns:\n            bool: True if visualization was successful\n        \"\"\"\n        # Check if raster file exists\n        if not os.path.exists(raster_path):\n            print(f\"Error: Raster file '{raster_path}' not found.\")\n            return False\n\n        # Process raster if GeoDataFrame not provided\n        if gdf is None:\n            gdf = self.process_raster(raster_path)\n\n        if gdf is None or len(gdf) == 0:\n            print(\"No objects to visualize\")\n            return False\n\n        # Check if confidence column exists in the GeoDataFrame\n        has_confidence = False\n        if hasattr(gdf, \"columns\") and \"confidence\" in gdf.columns:\n            # Try to access a confidence value to confirm it works\n            try:\n                if len(gdf) &gt; 0:\n                    # Try getitem access\n                    conf_val = gdf[\"confidence\"].iloc[0]\n                    has_confidence = True\n                    print(\n                        f\"Using confidence values (range: {gdf['confidence'].min():.2f} - {gdf['confidence'].max():.2f})\"\n                    )\n            except Exception as e:\n                print(f\"Confidence column exists but couldn't access values: {e}\")\n                has_confidence = False\n        else:\n            print(\"No confidence column found in GeoDataFrame\")\n            has_confidence = False\n\n        # Read raster for visualization\n        with rasterio.open(raster_path) as src:\n            # Read the entire image or a subset if it's very large\n            if src.height &gt; 2000 or src.width &gt; 2000:\n                # Calculate scale factor to reduce size\n                scale = min(2000 / src.height, 2000 / src.width)\n                out_shape = (\n                    int(src.count),\n                    int(src.height * scale),\n                    int(src.width * scale),\n                )\n\n                # Read and resample\n                image = src.read(\n                    out_shape=out_shape, resampling=rasterio.enums.Resampling.bilinear\n                )\n\n                # Create a scaled transform for the resampled image\n                # Calculate scaling factors\n                x_scale = src.width / out_shape[2]\n                y_scale = src.height / out_shape[1]\n\n                # Get the original transform\n                orig_transform = src.transform\n\n                # Create a scaled transform\n                scaled_transform = rasterio.transform.Affine(\n                    orig_transform.a * x_scale,\n                    orig_transform.b,\n                    orig_transform.c,\n                    orig_transform.d,\n                    orig_transform.e * y_scale,\n                    orig_transform.f,\n                )\n            else:\n                image = src.read()\n                scaled_transform = src.transform\n\n            # Convert to RGB for display\n            if image.shape[0] &gt; 3:\n                image = image[:3]\n            elif image.shape[0] == 1:\n                image = np.repeat(image, 3, axis=0)\n\n            # Normalize image for display\n            image = image.transpose(1, 2, 0)  # CHW to HWC\n            image = image.astype(np.float32)\n\n            if image.max() &gt; 10:  # Likely 0-255 range\n                image = image / 255.0\n\n            image = np.clip(image, 0, 1)\n\n            # Get image bounds\n            bounds = src.bounds\n            crs = src.crs\n\n        # Create figure with appropriate aspect ratio\n        aspect_ratio = image.shape[1] / image.shape[0]  # width / height\n        plt.figure(figsize=(figsize[0], figsize[0] / aspect_ratio))\n        ax = plt.gca()\n\n        # Display image\n        ax.imshow(image)\n\n        # Make sure the GeoDataFrame has the same CRS as the raster\n        if gdf.crs != crs:\n            print(f\"Reprojecting GeoDataFrame from {gdf.crs} to {crs}\")\n            gdf = gdf.to_crs(crs)\n\n        # Set up colors for confidence visualization\n        if has_confidence:\n            try:\n                import matplotlib.cm as cm\n                from matplotlib.colors import Normalize\n\n                # Get min/max confidence values\n                min_conf = gdf[\"confidence\"].min()\n                max_conf = gdf[\"confidence\"].max()\n\n                # Set up normalization and colormap\n                norm = Normalize(vmin=min_conf, vmax=max_conf)\n                cmap = cm.viridis\n\n                # Create scalar mappable for colorbar\n                sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n                sm.set_array([])\n\n                # Add colorbar\n                cbar = plt.colorbar(\n                    sm, ax=ax, orientation=\"vertical\", shrink=0.7, pad=0.01\n                )\n                cbar.set_label(\"Confidence Score\")\n            except Exception as e:\n                print(f\"Error setting up confidence visualization: {e}\")\n                has_confidence = False\n\n        # Function to convert coordinates\n        def geo_to_pixel(geometry, transform):\n            \"\"\"Convert geometry to pixel coordinates using the provided transform.\"\"\"\n            if geometry.is_empty:\n                return None\n\n            if geometry.geom_type == \"Polygon\":\n                # Get exterior coordinates\n                exterior_coords = list(geometry.exterior.coords)\n\n                # Convert to pixel coordinates\n                pixel_coords = [~transform * (x, y) for x, y in exterior_coords]\n\n                # Split into x and y lists\n                pixel_x = [coord[0] for coord in pixel_coords]\n                pixel_y = [coord[1] for coord in pixel_coords]\n\n                return pixel_x, pixel_y\n            else:\n                print(f\"Unsupported geometry type: {geometry.geom_type}\")\n                return None\n\n        # Plot each object\n        for idx, row in gdf.iterrows():\n            try:\n                # Convert polygon to pixel coordinates\n                coords = geo_to_pixel(row.geometry, scaled_transform)\n\n                if coords:\n                    pixel_x, pixel_y = coords\n\n                    if has_confidence:\n                        try:\n                            # Get confidence value using different methods\n                            # Method 1: Try direct attribute access\n                            confidence = None\n                            try:\n                                confidence = row.confidence\n                            except:\n                                pass\n\n                            # Method 2: Try dictionary-style access\n                            if confidence is None:\n                                try:\n                                    confidence = row[\"confidence\"]\n                                except:\n                                    pass\n\n                            # Method 3: Try accessing by index from the GeoDataFrame\n                            if confidence is None:\n                                try:\n                                    confidence = gdf.iloc[idx][\"confidence\"]\n                                except:\n                                    pass\n\n                            if confidence is not None:\n                                color = cmap(norm(confidence))\n                                # Fill polygon with semi-transparent color\n                                ax.fill(pixel_x, pixel_y, color=color, alpha=0.5)\n                                # Draw border\n                                ax.plot(\n                                    pixel_x,\n                                    pixel_y,\n                                    color=color,\n                                    linewidth=1,\n                                    alpha=0.8,\n                                )\n                            else:\n                                # Fall back to red if confidence value couldn't be accessed\n                                ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n                        except Exception as e:\n                            print(\n                                f\"Error using confidence value for polygon {idx}: {e}\"\n                            )\n                            ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n                    else:\n                        # No confidence data, just plot outlines in red\n                        ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n            except Exception as e:\n                print(f\"Error plotting polygon {idx}: {e}\")\n\n        # Remove axes\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"objects (Found: {len(gdf)})\")\n\n        # Save if requested\n        if output_path:\n            plt.tight_layout()\n            plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n            print(f\"Visualization saved to {output_path}\")\n\n        plt.close()\n\n        # Create a simpler visualization focused just on a subset of objects\n        if len(gdf) &gt; 0:\n            plt.figure(figsize=figsize)\n            ax = plt.gca()\n\n            # Choose a subset of the image to show\n            with rasterio.open(raster_path) as src:\n                # Get centroid of first object\n                sample_geom = gdf.iloc[0].geometry\n                centroid = sample_geom.centroid\n\n                # Convert to pixel coordinates\n                center_x, center_y = ~src.transform * (centroid.x, centroid.y)\n\n                # Define a window around this object\n                window_size = 500  # pixels\n                window = rasterio.windows.Window(\n                    max(0, int(center_x - window_size / 2)),\n                    max(0, int(center_y - window_size / 2)),\n                    min(window_size, src.width - int(center_x - window_size / 2)),\n                    min(window_size, src.height - int(center_y - window_size / 2)),\n                )\n\n                # Read this window\n                sample_image = src.read(window=window)\n\n                # Convert to RGB for display\n                if sample_image.shape[0] &gt; 3:\n                    sample_image = sample_image[:3]\n                elif sample_image.shape[0] == 1:\n                    sample_image = np.repeat(sample_image, 3, axis=0)\n\n                # Normalize image for display\n                sample_image = sample_image.transpose(1, 2, 0)  # CHW to HWC\n                sample_image = sample_image.astype(np.float32)\n\n                if sample_image.max() &gt; 10:  # Likely 0-255 range\n                    sample_image = sample_image / 255.0\n\n                sample_image = np.clip(sample_image, 0, 1)\n\n                # Display sample image\n                ax.imshow(sample_image, extent=[0, window.width, window.height, 0])\n\n                # Get the correct transform for this window\n                window_transform = src.window_transform(window)\n\n                # Calculate bounds of the window\n                window_bounds = rasterio.windows.bounds(window, src.transform)\n                window_box = box(*window_bounds)\n\n                # Filter objects that intersect with this window\n                visible_gdf = gdf[gdf.intersects(window_box)]\n\n                # Set up colors for sample view if confidence data exists\n                if has_confidence:\n                    try:\n                        # Reuse the same normalization and colormap from main view\n                        sample_sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n                        sample_sm.set_array([])\n\n                        # Add colorbar to sample view\n                        sample_cbar = plt.colorbar(\n                            sample_sm,\n                            ax=ax,\n                            orientation=\"vertical\",\n                            shrink=0.7,\n                            pad=0.01,\n                        )\n                        sample_cbar.set_label(\"Confidence Score\")\n                    except Exception as e:\n                        print(f\"Error setting up sample confidence visualization: {e}\")\n\n                # Plot objects in sample view\n                for idx, row in visible_gdf.iterrows():\n                    try:\n                        # Get window-relative pixel coordinates\n                        geom = row.geometry\n\n                        # Skip empty geometries\n                        if geom.is_empty:\n                            continue\n\n                        # Get exterior coordinates\n                        exterior_coords = list(geom.exterior.coords)\n\n                        # Convert to pixel coordinates relative to window origin\n                        pixel_coords = []\n                        for x, y in exterior_coords:\n                            px, py = ~src.transform * (x, y)  # Convert to image pixels\n                            # Make coordinates relative to window\n                            px = px - window.col_off\n                            py = py - window.row_off\n                            pixel_coords.append((px, py))\n\n                        # Extract x and y coordinates\n                        pixel_x = [coord[0] for coord in pixel_coords]\n                        pixel_y = [coord[1] for coord in pixel_coords]\n\n                        # Use confidence colors if available\n                        if has_confidence:\n                            try:\n                                # Try different methods to access confidence\n                                confidence = None\n                                try:\n                                    confidence = row.confidence\n                                except:\n                                    pass\n\n                                if confidence is None:\n                                    try:\n                                        confidence = row[\"confidence\"]\n                                    except:\n                                        pass\n\n                                if confidence is None:\n                                    try:\n                                        confidence = visible_gdf.iloc[idx][\"confidence\"]\n                                    except:\n                                        pass\n\n                                if confidence is not None:\n                                    color = cmap(norm(confidence))\n                                    # Fill polygon with semi-transparent color\n                                    ax.fill(pixel_x, pixel_y, color=color, alpha=0.5)\n                                    # Draw border\n                                    ax.plot(\n                                        pixel_x,\n                                        pixel_y,\n                                        color=color,\n                                        linewidth=1.5,\n                                        alpha=0.8,\n                                    )\n                                else:\n                                    ax.plot(\n                                        pixel_x, pixel_y, color=\"red\", linewidth=1.5\n                                    )\n                            except Exception as e:\n                                print(\n                                    f\"Error using confidence in sample view for polygon {idx}: {e}\"\n                                )\n                                ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                        else:\n                            ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                    except Exception as e:\n                        print(f\"Error plotting polygon in sample view: {e}\")\n\n                # Set title\n                ax.set_title(f\"Sample Area - objects (Showing: {len(visible_gdf)})\")\n\n                # Remove axes\n                ax.set_xticks([])\n                ax.set_yticks([])\n\n                # Save if requested\n                if output_path:\n                    sample_output = (\n                        os.path.splitext(output_path)[0]\n                        + \"_sample\"\n                        + os.path.splitext(output_path)[1]\n                    )\n                    plt.tight_layout()\n                    plt.savefig(sample_output, dpi=300, bbox_inches=\"tight\")\n                    print(f\"Sample visualization saved to {sample_output}\")\n\n    def generate_masks(\n        self,\n        raster_path,\n        output_path=None,\n        confidence_threshold=None,\n        mask_threshold=None,\n        min_object_area=10,\n        max_object_area=float(\"inf\"),\n        overlap=0.25,\n        batch_size=4,\n        band_indexes=None,\n        verbose=False,\n        **kwargs,\n    ):\n        \"\"\"\n        Save masks with confidence values as a multi-band GeoTIFF.\n\n        Objects with area smaller than min_object_area or larger than max_object_area\n        will be filtered out.\n\n        Args:\n            raster_path: Path to input raster\n            output_path: Path for output GeoTIFF\n            confidence_threshold: Minimum confidence score (0.0-1.0)\n            mask_threshold: Threshold for mask binarization (0.0-1.0)\n            min_object_area: Minimum area (in pixels) for an object to be included\n            max_object_area: Maximum area (in pixels) for an object to be included\n            overlap: Overlap between tiles (0.0-1.0)\n            batch_size: Batch size for processing\n            band_indexes: List of band indexes to use (default: all bands)\n            verbose: Whether to print detailed processing information\n\n        Returns:\n            Path to the saved GeoTIFF\n        \"\"\"\n        # Use provided thresholds or fall back to instance defaults\n        if confidence_threshold is None:\n            confidence_threshold = self.confidence_threshold\n        if mask_threshold is None:\n            mask_threshold = self.mask_threshold\n\n        chip_size = kwargs.get(\"chip_size\", self.chip_size)\n\n        # Default output path\n        if output_path is None:\n            output_path = os.path.splitext(raster_path)[0] + \"_masks_conf.tif\"\n\n        # Process the raster to get individual masks with confidence\n        with rasterio.open(raster_path) as src:\n            # Create dataset with the specified overlap\n            dataset = CustomDataset(\n                raster_path=raster_path,\n                chip_size=chip_size,\n                overlap=overlap,\n                band_indexes=band_indexes,\n                verbose=verbose,\n            )\n\n            # Create output profile\n            output_profile = src.profile.copy()\n            output_profile.update(\n                dtype=rasterio.uint8,\n                count=2,  # Two bands: mask and confidence\n                compress=\"lzw\",\n                nodata=0,\n            )\n\n            # Initialize mask and confidence arrays\n            mask_array = np.zeros((src.height, src.width), dtype=np.uint8)\n            conf_array = np.zeros((src.height, src.width), dtype=np.uint8)\n\n            # Define custom collate function to handle Shapely objects\n            def custom_collate(batch):\n                \"\"\"\n                Custom collate function that handles Shapely geometries\n                by keeping them as Python objects rather than trying to collate them.\n                \"\"\"\n                elem = batch[0]\n                if isinstance(elem, dict):\n                    result = {}\n                    for key in elem:\n                        if key == \"bbox\":\n                            # Don't collate shapely objects, keep as list\n                            result[key] = [d[key] for d in batch]\n                        else:\n                            # For tensors and other collatable types\n                            try:\n                                result[key] = (\n                                    torch.utils.data._utils.collate.default_collate(\n                                        [d[key] for d in batch]\n                                    )\n                                )\n                            except TypeError:\n                                # Fall back to list for non-collatable types\n                                result[key] = [d[key] for d in batch]\n                    return result\n                else:\n                    # Default collate for non-dict types\n                    return torch.utils.data._utils.collate.default_collate(batch)\n\n            # Create dataloader with custom collate function\n            dataloader = torch.utils.data.DataLoader(\n                dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=0,\n                collate_fn=custom_collate,\n            )\n\n            # Process batches\n            print(f\"Processing raster with {len(dataloader)} batches\")\n            for batch in tqdm(dataloader):\n                # Move images to device\n                images = batch[\"image\"].to(self.device)\n                coords = batch[\"coords\"]  # Tensor of shape [batch_size, 2]\n\n                # Run inference\n                with torch.no_grad():\n                    predictions = self.model(images)\n\n                # Process predictions\n                for idx, prediction in enumerate(predictions):\n                    masks = prediction[\"masks\"].cpu().numpy()\n                    scores = prediction[\"scores\"].cpu().numpy()\n\n                    # Filter by confidence threshold\n                    valid_indices = scores &gt;= confidence_threshold\n                    masks = masks[valid_indices]\n                    scores = scores[valid_indices]\n\n                    # Skip if no valid predictions\n                    if len(masks) == 0:\n                        continue\n\n                    # Get window coordinates\n                    i, j = coords[idx].cpu().numpy()\n\n                    # Process each mask\n                    for mask_idx, mask in enumerate(masks):\n                        # Convert to binary mask\n                        binary_mask = (mask[0] &gt; mask_threshold).astype(np.uint8) * 255\n\n                        # Check object area - calculate number of pixels in the mask\n                        object_area = np.sum(binary_mask &gt; 0)\n\n                        # Skip objects that don't meet area criteria\n                        if (\n                            object_area &lt; min_object_area\n                            or object_area &gt; max_object_area\n                        ):\n                            if verbose:\n                                print(\n                                    f\"Filtering out object with area {object_area} pixels\"\n                                )\n                            continue\n\n                        conf_value = int(scores[mask_idx] * 255)  # Scale to 0-255\n\n                        # Update the mask and confidence arrays\n                        h, w = binary_mask.shape\n                        valid_h = min(h, src.height - j)\n                        valid_w = min(w, src.width - i)\n\n                        if valid_h &gt; 0 and valid_w &gt; 0:\n                            # Use maximum for overlapping regions in the mask\n                            mask_array[j : j + valid_h, i : i + valid_w] = np.maximum(\n                                mask_array[j : j + valid_h, i : i + valid_w],\n                                binary_mask[:valid_h, :valid_w],\n                            )\n\n                            # For confidence, only update where mask is positive\n                            # and confidence is higher than existing\n                            mask_region = binary_mask[:valid_h, :valid_w] &gt; 0\n                            if np.any(mask_region):\n                                # Only update where mask is positive and new confidence is higher\n                                current_conf = conf_array[\n                                    j : j + valid_h, i : i + valid_w\n                                ]\n\n                                # Where to update confidence (mask positive &amp; higher confidence)\n                                update_mask = np.logical_and(\n                                    mask_region,\n                                    np.logical_or(\n                                        current_conf == 0, current_conf &lt; conf_value\n                                    ),\n                                )\n\n                                if np.any(update_mask):\n                                    conf_array[j : j + valid_h, i : i + valid_w][\n                                        update_mask\n                                    ] = conf_value\n\n            # Write to GeoTIFF\n            with rasterio.open(output_path, \"w\", **output_profile) as dst:\n                dst.write(mask_array, 1)\n                dst.write(conf_array, 2)\n\n            print(f\"Masks with confidence values saved to {output_path}\")\n            return output_path\n\n    def vectorize_masks(\n        self,\n        masks_path,\n        output_path=None,\n        confidence_threshold=0.5,\n        min_object_area=100,\n        max_object_area=None,\n        n_workers=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Convert masks with confidence to vector polygons.\n\n        Args:\n            masks_path: Path to masks GeoTIFF with confidence band.\n            output_path: Path for output GeoJSON.\n            confidence_threshold: Minimum confidence score (0.0-1.0). Default: 0.5\n            min_object_area: Minimum area in pixels to keep an object. Default: 100\n            max_object_area: Maximum area in pixels to keep an object. Default: None\n            n_workers: int, default=None\n                The number of worker threads to use.\n                \"None\" means single-threaded processing.\n                \"-1\"   means using all available CPU processors.\n                Positive integer means using that specific number of threads.\n            **kwargs: Additional parameters\n\n        Returns:\n            GeoDataFrame with car detections and confidence values\n        \"\"\"\n\n        def _process_single_component(\n            component_mask,\n            conf_data,\n            transform,\n            confidence_threshold,\n            min_object_area,\n            max_object_area,\n        ):\n            # Get confidence value\n            conf_region = conf_data[component_mask &gt; 0]\n            if len(conf_region) &gt; 0:\n                confidence = np.mean(conf_region) / 255.0\n            else:\n                confidence = 0.0\n\n            # Skip if confidence is below threshold\n            if confidence &lt; confidence_threshold:\n                return None\n\n            # Find contours\n            contours, _ = cv2.findContours(\n                component_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n            )\n\n            results = []\n\n            for contour in contours:\n                # Filter by size\n                area = cv2.contourArea(contour)\n                if area &lt; min_object_area:\n                    continue\n\n                if max_object_area is not None and area &gt; max_object_area:\n                    continue\n\n                # Get minimum area rectangle\n                rect = cv2.minAreaRect(contour)\n                box_points = cv2.boxPoints(rect)\n\n                # Convert to geographic coordinates\n                geo_points = []\n                for x, y in box_points:\n                    gx, gy = transform * (x, y)\n                    geo_points.append((gx, gy))\n\n                # Create polygon\n                poly = Polygon(geo_points)\n                results.append((poly, confidence, area))\n\n            return results\n\n        import concurrent.futures\n        from functools import partial\n\n        def process_component(args):\n            \"\"\"\n            Helper function to process a single component\n            \"\"\"\n            (\n                label,\n                labeled_mask,\n                conf_data,\n                transform,\n                confidence_threshold,\n                min_object_area,\n                max_object_area,\n            ) = args\n\n            # Create mask for this component\n            component_mask = (labeled_mask == label).astype(np.uint8)\n\n            return _process_single_component(\n                component_mask,\n                conf_data,\n                transform,\n                confidence_threshold,\n                min_object_area,\n                max_object_area,\n            )\n\n        start_time = time.time()\n        print(f\"Processing masks from: {masks_path}\")\n\n        if n_workers == -1:\n            n_workers = os.cpu_count()\n\n        with rasterio.open(masks_path) as src:\n            # Read mask and confidence bands\n            mask_data = src.read(1)\n            conf_data = src.read(2)\n            transform = src.transform\n            crs = src.crs\n\n            # Convert to binary mask\n            binary_mask = mask_data &gt; 0\n\n            # Find connected components\n            labeled_mask, num_features = ndimage.label(binary_mask)\n            print(f\"Found {num_features} connected components\")\n\n            # Process each component\n            polygons = []\n            confidences = []\n            pixels = []\n\n            if n_workers is None or n_workers == 1:\n                print(\n                    \"Using single-threaded processing, you can speed up processing by setting n_workers &gt; 1\"\n                )\n                # Add progress bar\n                for label in tqdm(\n                    range(1, num_features + 1), desc=\"Processing components\"\n                ):\n                    # Create mask for this component\n                    component_mask = (labeled_mask == label).astype(np.uint8)\n\n                    result = _process_single_component(\n                        component_mask,\n                        conf_data,\n                        transform,\n                        confidence_threshold,\n                        min_object_area,\n                        max_object_area,\n                    )\n\n                    if result:\n                        for poly, confidence, area in result:\n                            # Add to lists\n                            polygons.append(poly)\n                            confidences.append(confidence)\n                            pixels.append(area)\n\n            else:\n                # Process components in parallel\n                print(f\"Using {n_workers} workers for parallel processing\")\n\n                process_args = [\n                    (\n                        label,\n                        labeled_mask,\n                        conf_data,\n                        transform,\n                        confidence_threshold,\n                        min_object_area,\n                        max_object_area,\n                    )\n                    for label in range(1, num_features + 1)\n                ]\n\n                with concurrent.futures.ThreadPoolExecutor(\n                    max_workers=n_workers\n                ) as executor:\n                    results = list(\n                        tqdm(\n                            executor.map(process_component, process_args),\n                            total=num_features,\n                            desc=\"Processing components\",\n                        )\n                    )\n\n                    for result in results:\n                        if result:\n                            for poly, confidence, area in result:\n                                # Add to lists\n                                polygons.append(poly)\n                                confidences.append(confidence)\n                                pixels.append(area)\n\n            # Create GeoDataFrame\n            if polygons:\n                gdf = gpd.GeoDataFrame(\n                    {\n                        \"geometry\": polygons,\n                        \"confidence\": confidences,\n                        \"class\": [1] * len(polygons),\n                        \"pixels\": pixels,\n                    },\n                    crs=crs,\n                )\n\n                # Save to file if requested\n                if output_path:\n                    gdf.to_file(output_path, driver=\"GeoJSON\")\n                    print(f\"Saved {len(gdf)} objects with confidence to {output_path}\")\n\n                end_time = time.time()\n                print(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n                return gdf\n            else:\n                end_time = time.time()\n                print(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n                print(\"No valid polygons found\")\n                return None\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.__init__","title":"<code>__init__(self, model_path=None, repo_id=None, model=None, num_classes=2, device=None)</code>  <code>special</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <p>Path to the .pth model file.</p> <code>None</code> <code>repo_id</code> <p>Hugging Face repository ID for model download.</p> <code>None</code> <code>model</code> <p>Pre-initialized model object (optional).</p> <code>None</code> <code>num_classes</code> <p>Number of classes for detection (default: 2).</p> <code>2</code> <code>device</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self, model_path=None, repo_id=None, model=None, num_classes=2, device=None\n):\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Hugging Face repository ID for model download.\n        model: Pre-initialized model object (optional).\n        num_classes: Number of classes for detection (default: 2).\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    # Set device\n    if device is None:\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    else:\n        self.device = torch.device(device)\n\n    # Default parameters for object detection - these can be overridden in process_raster\n    self.chip_size = (512, 512)  # Size of image chips for processing\n    self.overlap = 0.25  # Default overlap between tiles\n    self.confidence_threshold = 0.5  # Default confidence threshold\n    self.nms_iou_threshold = 0.5  # IoU threshold for non-maximum suppression\n    self.min_object_area = 100  # Minimum area in pixels to keep an object\n    self.max_object_area = None  # Maximum area in pixels to keep an object\n    self.mask_threshold = 0.5  # Threshold for mask binarization\n    self.simplify_tolerance = 1.0  # Tolerance for polygon simplification\n\n    # Initialize model\n    self.model = self.initialize_model(model, num_classes=num_classes)\n\n    # Download model if needed\n    if model_path is None or (not os.path.exists(model_path)):\n        model_path = self.download_model_from_hf(model_path, repo_id)\n\n    # Load model weights\n    self.load_weights(model_path)\n\n    # Set model to evaluation mode\n    self.model.eval()\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.download_model_from_hf","title":"<code>download_model_from_hf(self, model_path=None, repo_id=None)</code>","text":"<p>Download the object detection model from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <p>Path to the model file.</p> <code>None</code> <code>repo_id</code> <p>Hugging Face repository ID.</p> <code>None</code> <p>Returns:</p> Type Description <p>Path to the downloaded model file</p> Source code in <code>geoai/extract.py</code> <pre><code>def download_model_from_hf(self, model_path=None, repo_id=None):\n    \"\"\"\n    Download the object detection model from Hugging Face.\n\n    Args:\n        model_path: Path to the model file.\n        repo_id: Hugging Face repository ID.\n\n    Returns:\n        Path to the downloaded model file\n    \"\"\"\n    try:\n\n        print(\"Model path not specified, downloading from Hugging Face...\")\n\n        # Define the repository ID and model filename\n        if repo_id is None:\n            repo_id = \"giswqs/geoai\"\n\n        if model_path is None:\n            model_path = \"building_footprints_usa.pth\"\n\n        # Download the model\n        model_path = hf_hub_download(repo_id=repo_id, filename=model_path)\n        print(f\"Model downloaded to: {model_path}\")\n\n        return model_path\n\n    except Exception as e:\n        print(f\"Error downloading model from Hugging Face: {e}\")\n        print(\"Please specify a local model path or ensure internet connectivity.\")\n        raise\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.filter_edge_objects","title":"<code>filter_edge_objects(self, gdf, raster_path, edge_buffer=10)</code>","text":"<p>Filter out object detections that fall in padding/edge areas of the image.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <p>GeoDataFrame with object detections</p> required <code>raster_path</code> <p>Path to the original raster file</p> required <code>edge_buffer</code> <p>Buffer in pixels to consider as edge region</p> <code>10</code> <p>Returns:</p> Type Description <p>GeoDataFrame with filtered objects</p> Source code in <code>geoai/extract.py</code> <pre><code>def filter_edge_objects(self, gdf, raster_path, edge_buffer=10):\n    \"\"\"\n    Filter out object detections that fall in padding/edge areas of the image.\n\n    Args:\n        gdf: GeoDataFrame with object detections\n        raster_path: Path to the original raster file\n        edge_buffer: Buffer in pixels to consider as edge region\n\n    Returns:\n        GeoDataFrame with filtered objects\n    \"\"\"\n    import rasterio\n    from shapely.geometry import box\n\n    # If no objects detected, return empty GeoDataFrame\n    if gdf is None or len(gdf) == 0:\n        return gdf\n\n    print(f\"Objects before filtering: {len(gdf)}\")\n\n    with rasterio.open(raster_path) as src:\n        # Get raster bounds\n        raster_bounds = src.bounds\n        raster_width = src.width\n        raster_height = src.height\n\n        # Convert edge buffer from pixels to geographic units\n        # We need the smallest dimension of a pixel in geographic units\n        pixel_width = (raster_bounds[2] - raster_bounds[0]) / raster_width\n        pixel_height = (raster_bounds[3] - raster_bounds[1]) / raster_height\n        buffer_size = min(pixel_width, pixel_height) * edge_buffer\n\n        # Create a slightly smaller bounding box to exclude edge regions\n        inner_bounds = (\n            raster_bounds[0] + buffer_size,  # min x (west)\n            raster_bounds[1] + buffer_size,  # min y (south)\n            raster_bounds[2] - buffer_size,  # max x (east)\n            raster_bounds[3] - buffer_size,  # max y (north)\n        )\n\n        # Check that inner bounds are valid\n        if inner_bounds[0] &gt;= inner_bounds[2] or inner_bounds[1] &gt;= inner_bounds[3]:\n            print(\"Warning: Edge buffer too large, using original bounds\")\n            inner_box = box(*raster_bounds)\n        else:\n            inner_box = box(*inner_bounds)\n\n        # Filter out objects that intersect with the edge of the image\n        filtered_gdf = gdf[gdf.intersects(inner_box)]\n\n        # Additional check for objects that have &gt;50% of their area outside the valid region\n        valid_objects = []\n        for idx, row in filtered_gdf.iterrows():\n            if row.geometry.intersection(inner_box).area &gt;= 0.5 * row.geometry.area:\n                valid_objects.append(idx)\n\n        filtered_gdf = filtered_gdf.loc[valid_objects]\n\n        print(f\"Objects after filtering: {len(filtered_gdf)}\")\n\n        return filtered_gdf\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.filter_overlapping_polygons","title":"<code>filter_overlapping_polygons(self, gdf, **kwargs)</code>","text":"<p>Filter overlapping polygons using non-maximum suppression.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <p>GeoDataFrame with polygons</p> required <code>**kwargs</code> <p>Optional parameters: nms_iou_threshold: IoU threshold for filtering</p> <code>{}</code> <p>Returns:</p> Type Description <p>Filtered GeoDataFrame</p> Source code in <code>geoai/extract.py</code> <pre><code>def filter_overlapping_polygons(self, gdf, **kwargs):\n    \"\"\"\n    Filter overlapping polygons using non-maximum suppression.\n\n    Args:\n        gdf: GeoDataFrame with polygons\n        **kwargs: Optional parameters:\n            nms_iou_threshold: IoU threshold for filtering\n\n    Returns:\n        Filtered GeoDataFrame\n    \"\"\"\n    if len(gdf) &lt;= 1:\n        return gdf\n\n    # Get parameters from kwargs or use instance defaults\n    iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n\n    # Sort by confidence\n    gdf = gdf.sort_values(\"confidence\", ascending=False)\n\n    # Fix any invalid geometries\n    gdf[\"geometry\"] = gdf[\"geometry\"].apply(\n        lambda geom: geom.buffer(0) if not geom.is_valid else geom\n    )\n\n    keep_indices = []\n    polygons = gdf.geometry.values\n\n    for i in range(len(polygons)):\n        if i in keep_indices:\n            continue\n\n        keep = True\n        for j in keep_indices:\n            # Skip invalid geometries\n            if not polygons[i].is_valid or not polygons[j].is_valid:\n                continue\n\n            # Calculate IoU\n            try:\n                intersection = polygons[i].intersection(polygons[j]).area\n                union = polygons[i].area + polygons[j].area - intersection\n                iou = intersection / union if union &gt; 0 else 0\n\n                if iou &gt; iou_threshold:\n                    keep = False\n                    break\n            except Exception:\n                # Skip on topology exceptions\n                continue\n\n        if keep:\n            keep_indices.append(i)\n\n    return gdf.iloc[keep_indices]\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.generate_masks","title":"<code>generate_masks(self, raster_path, output_path=None, confidence_threshold=None, mask_threshold=None, min_object_area=10, max_object_area=inf, overlap=0.25, batch_size=4, band_indexes=None, verbose=False, **kwargs)</code>","text":"<p>Save masks with confidence values as a multi-band GeoTIFF.</p> <p>Objects with area smaller than min_object_area or larger than max_object_area will be filtered out.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <p>Path to input raster</p> required <code>output_path</code> <p>Path for output GeoTIFF</p> <code>None</code> <code>confidence_threshold</code> <p>Minimum confidence score (0.0-1.0)</p> <code>None</code> <code>mask_threshold</code> <p>Threshold for mask binarization (0.0-1.0)</p> <code>None</code> <code>min_object_area</code> <p>Minimum area (in pixels) for an object to be included</p> <code>10</code> <code>max_object_area</code> <p>Maximum area (in pixels) for an object to be included</p> <code>inf</code> <code>overlap</code> <p>Overlap between tiles (0.0-1.0)</p> <code>0.25</code> <code>batch_size</code> <p>Batch size for processing</p> <code>4</code> <code>band_indexes</code> <p>List of band indexes to use (default: all bands)</p> <code>None</code> <code>verbose</code> <p>Whether to print detailed processing information</p> <code>False</code> <p>Returns:</p> Type Description <p>Path to the saved GeoTIFF</p> Source code in <code>geoai/extract.py</code> <pre><code>def generate_masks(\n    self,\n    raster_path,\n    output_path=None,\n    confidence_threshold=None,\n    mask_threshold=None,\n    min_object_area=10,\n    max_object_area=float(\"inf\"),\n    overlap=0.25,\n    batch_size=4,\n    band_indexes=None,\n    verbose=False,\n    **kwargs,\n):\n    \"\"\"\n    Save masks with confidence values as a multi-band GeoTIFF.\n\n    Objects with area smaller than min_object_area or larger than max_object_area\n    will be filtered out.\n\n    Args:\n        raster_path: Path to input raster\n        output_path: Path for output GeoTIFF\n        confidence_threshold: Minimum confidence score (0.0-1.0)\n        mask_threshold: Threshold for mask binarization (0.0-1.0)\n        min_object_area: Minimum area (in pixels) for an object to be included\n        max_object_area: Maximum area (in pixels) for an object to be included\n        overlap: Overlap between tiles (0.0-1.0)\n        batch_size: Batch size for processing\n        band_indexes: List of band indexes to use (default: all bands)\n        verbose: Whether to print detailed processing information\n\n    Returns:\n        Path to the saved GeoTIFF\n    \"\"\"\n    # Use provided thresholds or fall back to instance defaults\n    if confidence_threshold is None:\n        confidence_threshold = self.confidence_threshold\n    if mask_threshold is None:\n        mask_threshold = self.mask_threshold\n\n    chip_size = kwargs.get(\"chip_size\", self.chip_size)\n\n    # Default output path\n    if output_path is None:\n        output_path = os.path.splitext(raster_path)[0] + \"_masks_conf.tif\"\n\n    # Process the raster to get individual masks with confidence\n    with rasterio.open(raster_path) as src:\n        # Create dataset with the specified overlap\n        dataset = CustomDataset(\n            raster_path=raster_path,\n            chip_size=chip_size,\n            overlap=overlap,\n            band_indexes=band_indexes,\n            verbose=verbose,\n        )\n\n        # Create output profile\n        output_profile = src.profile.copy()\n        output_profile.update(\n            dtype=rasterio.uint8,\n            count=2,  # Two bands: mask and confidence\n            compress=\"lzw\",\n            nodata=0,\n        )\n\n        # Initialize mask and confidence arrays\n        mask_array = np.zeros((src.height, src.width), dtype=np.uint8)\n        conf_array = np.zeros((src.height, src.width), dtype=np.uint8)\n\n        # Define custom collate function to handle Shapely objects\n        def custom_collate(batch):\n            \"\"\"\n            Custom collate function that handles Shapely geometries\n            by keeping them as Python objects rather than trying to collate them.\n            \"\"\"\n            elem = batch[0]\n            if isinstance(elem, dict):\n                result = {}\n                for key in elem:\n                    if key == \"bbox\":\n                        # Don't collate shapely objects, keep as list\n                        result[key] = [d[key] for d in batch]\n                    else:\n                        # For tensors and other collatable types\n                        try:\n                            result[key] = (\n                                torch.utils.data._utils.collate.default_collate(\n                                    [d[key] for d in batch]\n                                )\n                            )\n                        except TypeError:\n                            # Fall back to list for non-collatable types\n                            result[key] = [d[key] for d in batch]\n                return result\n            else:\n                # Default collate for non-dict types\n                return torch.utils.data._utils.collate.default_collate(batch)\n\n        # Create dataloader with custom collate function\n        dataloader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=0,\n            collate_fn=custom_collate,\n        )\n\n        # Process batches\n        print(f\"Processing raster with {len(dataloader)} batches\")\n        for batch in tqdm(dataloader):\n            # Move images to device\n            images = batch[\"image\"].to(self.device)\n            coords = batch[\"coords\"]  # Tensor of shape [batch_size, 2]\n\n            # Run inference\n            with torch.no_grad():\n                predictions = self.model(images)\n\n            # Process predictions\n            for idx, prediction in enumerate(predictions):\n                masks = prediction[\"masks\"].cpu().numpy()\n                scores = prediction[\"scores\"].cpu().numpy()\n\n                # Filter by confidence threshold\n                valid_indices = scores &gt;= confidence_threshold\n                masks = masks[valid_indices]\n                scores = scores[valid_indices]\n\n                # Skip if no valid predictions\n                if len(masks) == 0:\n                    continue\n\n                # Get window coordinates\n                i, j = coords[idx].cpu().numpy()\n\n                # Process each mask\n                for mask_idx, mask in enumerate(masks):\n                    # Convert to binary mask\n                    binary_mask = (mask[0] &gt; mask_threshold).astype(np.uint8) * 255\n\n                    # Check object area - calculate number of pixels in the mask\n                    object_area = np.sum(binary_mask &gt; 0)\n\n                    # Skip objects that don't meet area criteria\n                    if (\n                        object_area &lt; min_object_area\n                        or object_area &gt; max_object_area\n                    ):\n                        if verbose:\n                            print(\n                                f\"Filtering out object with area {object_area} pixels\"\n                            )\n                        continue\n\n                    conf_value = int(scores[mask_idx] * 255)  # Scale to 0-255\n\n                    # Update the mask and confidence arrays\n                    h, w = binary_mask.shape\n                    valid_h = min(h, src.height - j)\n                    valid_w = min(w, src.width - i)\n\n                    if valid_h &gt; 0 and valid_w &gt; 0:\n                        # Use maximum for overlapping regions in the mask\n                        mask_array[j : j + valid_h, i : i + valid_w] = np.maximum(\n                            mask_array[j : j + valid_h, i : i + valid_w],\n                            binary_mask[:valid_h, :valid_w],\n                        )\n\n                        # For confidence, only update where mask is positive\n                        # and confidence is higher than existing\n                        mask_region = binary_mask[:valid_h, :valid_w] &gt; 0\n                        if np.any(mask_region):\n                            # Only update where mask is positive and new confidence is higher\n                            current_conf = conf_array[\n                                j : j + valid_h, i : i + valid_w\n                            ]\n\n                            # Where to update confidence (mask positive &amp; higher confidence)\n                            update_mask = np.logical_and(\n                                mask_region,\n                                np.logical_or(\n                                    current_conf == 0, current_conf &lt; conf_value\n                                ),\n                            )\n\n                            if np.any(update_mask):\n                                conf_array[j : j + valid_h, i : i + valid_w][\n                                    update_mask\n                                ] = conf_value\n\n        # Write to GeoTIFF\n        with rasterio.open(output_path, \"w\", **output_profile) as dst:\n            dst.write(mask_array, 1)\n            dst.write(conf_array, 2)\n\n        print(f\"Masks with confidence values saved to {output_path}\")\n        return output_path\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.initialize_model","title":"<code>initialize_model(self, model, num_classes=2)</code>","text":"<p>Initialize a deep learning model for object detection.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>A pre-initialized model object.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes for detection.</p> <code>2</code> <p>Returns:</p> Type Description <code>torch.nn.Module</code> <p>A deep learning model for object detection.</p> Source code in <code>geoai/extract.py</code> <pre><code>def initialize_model(self, model, num_classes=2):\n    \"\"\"Initialize a deep learning model for object detection.\n\n    Args:\n        model (torch.nn.Module): A pre-initialized model object.\n        num_classes (int): Number of classes for detection.\n\n    Returns:\n        torch.nn.Module: A deep learning model for object detection.\n    \"\"\"\n\n    if model is None:  # Initialize Mask R-CNN model with ResNet50 backbone.\n        # Standard image mean and std for pre-trained models\n        image_mean = [0.485, 0.456, 0.406]\n        image_std = [0.229, 0.224, 0.225]\n\n        # Create model with explicit normalization parameters\n        model = maskrcnn_resnet50_fpn(\n            weights=None,\n            progress=False,\n            num_classes=num_classes,  # Background + object\n            weights_backbone=None,\n            # These parameters ensure consistent normalization\n            image_mean=image_mean,\n            image_std=image_std,\n        )\n\n    model.to(self.device)\n    return model\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.load_weights","title":"<code>load_weights(self, model_path)</code>","text":"<p>Load weights from file with error handling for different formats.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <p>Path to model weights</p> required Source code in <code>geoai/extract.py</code> <pre><code>def load_weights(self, model_path):\n    \"\"\"\n    Load weights from file with error handling for different formats.\n\n    Args:\n        model_path: Path to model weights\n    \"\"\"\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n\n    try:\n        state_dict = torch.load(model_path, map_location=self.device)\n\n        # Handle different state dict formats\n        if isinstance(state_dict, dict):\n            if \"model\" in state_dict:\n                state_dict = state_dict[\"model\"]\n            elif \"state_dict\" in state_dict:\n                state_dict = state_dict[\"state_dict\"]\n\n        # Try to load state dict\n        try:\n            self.model.load_state_dict(state_dict)\n            print(\"Model loaded successfully\")\n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n            print(\"Attempting to fix state_dict keys...\")\n\n            # Try to fix state_dict keys (remove module prefix if needed)\n            new_state_dict = {}\n            for k, v in state_dict.items():\n                if k.startswith(\"module.\"):\n                    new_state_dict[k[7:]] = v\n                else:\n                    new_state_dict[k] = v\n\n            self.model.load_state_dict(new_state_dict)\n            print(\"Model loaded successfully after key fixing\")\n\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load model: {e}\")\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.mask_to_polygons","title":"<code>mask_to_polygons(self, mask, **kwargs)</code>","text":"<p>Convert binary mask to polygon contours using OpenCV.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <p>Binary mask as numpy array</p> required <code>**kwargs</code> <p>Optional parameters: simplify_tolerance: Tolerance for polygon simplification mask_threshold: Threshold for mask binarization min_object_area: Minimum area in pixels to keep an object max_object_area: Maximum area in pixels to keep an object</p> <code>{}</code> <p>Returns:</p> Type Description <p>List of polygons as lists of (x, y) coordinates</p> Source code in <code>geoai/extract.py</code> <pre><code>def mask_to_polygons(self, mask, **kwargs):\n    \"\"\"\n    Convert binary mask to polygon contours using OpenCV.\n\n    Args:\n        mask: Binary mask as numpy array\n        **kwargs: Optional parameters:\n            simplify_tolerance: Tolerance for polygon simplification\n            mask_threshold: Threshold for mask binarization\n            min_object_area: Minimum area in pixels to keep an object\n            max_object_area: Maximum area in pixels to keep an object\n\n    Returns:\n        List of polygons as lists of (x, y) coordinates\n    \"\"\"\n\n    # Get parameters from kwargs or use instance defaults\n    simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n    mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n    min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n    max_object_area = kwargs.get(\"max_object_area\", self.max_object_area)\n\n    # Ensure binary mask\n    mask = (mask &gt; mask_threshold).astype(np.uint8)\n\n    # Optional: apply morphological operations to improve mask quality\n    kernel = np.ones((3, 3), np.uint8)\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n    # Find contours\n    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Convert to list of [x, y] coordinates\n    polygons = []\n    for contour in contours:\n        # Filter out too small contours\n        if contour.shape[0] &lt; 3 or cv2.contourArea(contour) &lt; min_object_area:\n            continue\n\n        # Filter out too large contours\n        if (\n            max_object_area is not None\n            and cv2.contourArea(contour) &gt; max_object_area\n        ):\n            continue\n\n        # Simplify contour if it has many points\n        if contour.shape[0] &gt; 50:\n            epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n            contour = cv2.approxPolyDP(contour, epsilon, True)\n\n        # Convert to list of [x, y] coordinates\n        polygon = contour.reshape(-1, 2).tolist()\n        polygons.append(polygon)\n\n    return polygons\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.masks_to_vector","title":"<code>masks_to_vector(self, mask_path, output_path=None, simplify_tolerance=None, mask_threshold=None, min_object_area=None, max_object_area=None, nms_iou_threshold=None, regularize=True, angle_threshold=15, rectangularity_threshold=0.7)</code>","text":"<p>Convert an object mask GeoTIFF to vector polygons and save as GeoJSON.</p> <p>Parameters:</p> Name Type Description Default <code>mask_path</code> <p>Path to the object masks GeoTIFF</p> required <code>output_path</code> <p>Path to save the output GeoJSON or Parquet file (default: mask_path with .geojson extension)</p> <code>None</code> <code>simplify_tolerance</code> <p>Tolerance for polygon simplification (default: self.simplify_tolerance)</p> <code>None</code> <code>mask_threshold</code> <p>Threshold for mask binarization (default: self.mask_threshold)</p> <code>None</code> <code>min_object_area</code> <p>Minimum area in pixels to keep an object (default: self.min_object_area)</p> <code>None</code> <code>max_object_area</code> <p>Minimum area in pixels to keep an object (default: self.max_object_area)</p> <code>None</code> <code>nms_iou_threshold</code> <p>IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)</p> <code>None</code> <code>regularize</code> <p>Whether to regularize objects to right angles (default: True)</p> <code>True</code> <code>angle_threshold</code> <p>Maximum deviation from 90 degrees for regularization (default: 15)</p> <code>15</code> <code>rectangularity_threshold</code> <p>Threshold for rectangle simplification (default: 0.7)</p> <code>0.7</code> <p>Returns:</p> Type Description <p>GeoDataFrame with objects</p> Source code in <code>geoai/extract.py</code> <pre><code>def masks_to_vector(\n    self,\n    mask_path,\n    output_path=None,\n    simplify_tolerance=None,\n    mask_threshold=None,\n    min_object_area=None,\n    max_object_area=None,\n    nms_iou_threshold=None,\n    regularize=True,\n    angle_threshold=15,\n    rectangularity_threshold=0.7,\n):\n    \"\"\"\n    Convert an object mask GeoTIFF to vector polygons and save as GeoJSON.\n\n    Args:\n        mask_path: Path to the object masks GeoTIFF\n        output_path: Path to save the output GeoJSON or Parquet file (default: mask_path with .geojson extension)\n        simplify_tolerance: Tolerance for polygon simplification (default: self.simplify_tolerance)\n        mask_threshold: Threshold for mask binarization (default: self.mask_threshold)\n        min_object_area: Minimum area in pixels to keep an object (default: self.min_object_area)\n        max_object_area: Minimum area in pixels to keep an object (default: self.max_object_area)\n        nms_iou_threshold: IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)\n        regularize: Whether to regularize objects to right angles (default: True)\n        angle_threshold: Maximum deviation from 90 degrees for regularization (default: 15)\n        rectangularity_threshold: Threshold for rectangle simplification (default: 0.7)\n\n    Returns:\n        GeoDataFrame with objects\n    \"\"\"\n    # Use class defaults if parameters not provided\n    simplify_tolerance = (\n        simplify_tolerance\n        if simplify_tolerance is not None\n        else self.simplify_tolerance\n    )\n    mask_threshold = (\n        mask_threshold if mask_threshold is not None else self.mask_threshold\n    )\n    min_object_area = (\n        min_object_area if min_object_area is not None else self.min_object_area\n    )\n    max_object_area = (\n        max_object_area if max_object_area is not None else self.max_object_area\n    )\n    nms_iou_threshold = (\n        nms_iou_threshold\n        if nms_iou_threshold is not None\n        else self.nms_iou_threshold\n    )\n\n    # Set default output path if not provided\n    # if output_path is None:\n    #     output_path = os.path.splitext(mask_path)[0] + \".geojson\"\n\n    print(f\"Converting mask to GeoJSON with parameters:\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n    print(f\"- Min object area: {min_object_area}\")\n    print(f\"- Max object area: {max_object_area}\")\n    print(f\"- Simplify tolerance: {simplify_tolerance}\")\n    print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n    print(f\"- Regularize objects: {regularize}\")\n    if regularize:\n        print(f\"- Angle threshold: {angle_threshold}\u00b0 from 90\u00b0\")\n        print(f\"- Rectangularity threshold: {rectangularity_threshold*100}%\")\n\n    # Open the mask raster\n    with rasterio.open(mask_path) as src:\n        # Read the mask data\n        mask_data = src.read(1)\n        transform = src.transform\n        crs = src.crs\n\n        # Print mask statistics\n        print(f\"Mask dimensions: {mask_data.shape}\")\n        print(f\"Mask value range: {mask_data.min()} to {mask_data.max()}\")\n\n        # Prepare for connected component analysis\n        # Binarize the mask based on threshold\n        binary_mask = (mask_data &gt; (mask_threshold * 255)).astype(np.uint8)\n\n        # Apply morphological operations for better results (optional)\n        kernel = np.ones((3, 3), np.uint8)\n        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find connected components\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n            binary_mask, connectivity=8\n        )\n\n        print(\n            f\"Found {num_labels-1} potential objects\"\n        )  # Subtract 1 for background\n\n        # Create list to store polygons and confidence values\n        all_polygons = []\n        all_confidences = []\n\n        # Process each component (skip the first one which is background)\n        for i in tqdm(range(1, num_labels)):\n            # Extract this object\n            area = stats[i, cv2.CC_STAT_AREA]\n\n            # Skip if too small\n            if area &lt; min_object_area:\n                continue\n\n            # Skip if too large\n            if max_object_area is not None and area &gt; max_object_area:\n                continue\n\n            # Create a mask for this object\n            object_mask = (labels == i).astype(np.uint8)\n\n            # Find contours\n            contours, _ = cv2.findContours(\n                object_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n            )\n\n            # Process each contour\n            for contour in contours:\n                # Skip if too few points\n                if contour.shape[0] &lt; 3:\n                    continue\n\n                # Simplify contour if it has many points\n                if contour.shape[0] &gt; 50 and simplify_tolerance &gt; 0:\n                    epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                    contour = cv2.approxPolyDP(contour, epsilon, True)\n\n                # Convert to list of (x, y) coordinates\n                polygon_points = contour.reshape(-1, 2)\n\n                # Convert pixel coordinates to geographic coordinates\n                geo_points = []\n                for x, y in polygon_points:\n                    gx, gy = transform * (x, y)\n                    geo_points.append((gx, gy))\n\n                # Create Shapely polygon\n                if len(geo_points) &gt;= 3:\n                    try:\n                        shapely_poly = Polygon(geo_points)\n                        if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                            all_polygons.append(shapely_poly)\n\n                            # Calculate \"confidence\" as normalized size\n                            # This is a proxy since we don't have model confidence scores\n                            normalized_size = min(1.0, area / 1000)  # Cap at 1.0\n                            all_confidences.append(normalized_size)\n                    except Exception as e:\n                        print(f\"Error creating polygon: {e}\")\n\n        print(f\"Created {len(all_polygons)} valid polygons\")\n\n        # Create GeoDataFrame\n        if not all_polygons:\n            print(\"No valid polygons found\")\n            return None\n\n        gdf = gpd.GeoDataFrame(\n            {\n                \"geometry\": all_polygons,\n                \"confidence\": all_confidences,\n                \"class\": 1,  # Object class\n            },\n            crs=crs,\n        )\n\n        # Apply non-maximum suppression to remove overlapping polygons\n        gdf = self.filter_overlapping_polygons(\n            gdf, nms_iou_threshold=nms_iou_threshold\n        )\n\n        print(f\"Object count after NMS filtering: {len(gdf)}\")\n\n        # Apply regularization if requested\n        if regularize and len(gdf) &gt; 0:\n            # Convert pixel area to geographic units for min_area parameter\n            # Estimate pixel size in geographic units\n            with rasterio.open(mask_path) as src:\n                pixel_size_x = src.transform[\n                    0\n                ]  # width of a pixel in geographic units\n                pixel_size_y = abs(\n                    src.transform[4]\n                )  # height of a pixel in geographic units\n                avg_pixel_area = pixel_size_x * pixel_size_y\n\n            # Use 10 pixels as minimum area in geographic units\n            min_geo_area = 10 * avg_pixel_area\n\n            # Regularize objects\n            gdf = self.regularize_objects(\n                gdf,\n                min_area=min_geo_area,\n                angle_threshold=angle_threshold,\n                rectangularity_threshold=rectangularity_threshold,\n            )\n\n        # Save to file\n        if output_path:\n            if output_path.endswith(\".parquet\"):\n                gdf.to_parquet(output_path)\n            else:\n                gdf.to_file(output_path)\n            print(f\"Saved {len(gdf)} objects to {output_path}\")\n\n        return gdf\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.process_raster","title":"<code>process_raster(self, raster_path, output_path=None, batch_size=4, filter_edges=True, edge_buffer=20, band_indexes=None, **kwargs)</code>","text":"<p>Process a raster file to extract objects with customizable parameters.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <p>Path to input raster file</p> required <code>output_path</code> <p>Path to output GeoJSON or Parquet file (optional)</p> <code>None</code> <code>batch_size</code> <p>Batch size for processing</p> <code>4</code> <code>filter_edges</code> <p>Whether to filter out objects at the edges of the image</p> <code>True</code> <code>edge_buffer</code> <p>Size of edge buffer in pixels to filter out objects (if filter_edges=True)</p> <code>20</code> <code>band_indexes</code> <p>List of band indexes to use (if None, use all bands)</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters: confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0) overlap: Overlap between adjacent tiles (0.0-1.0) chip_size: Size of image chips for processing (height, width) nms_iou_threshold: IoU threshold for non-maximum suppression (0.0-1.0) mask_threshold: Threshold for mask binarization (0.0-1.0) min_object_area: Minimum area in pixels to keep an object simplify_tolerance: Tolerance for polygon simplification</p> <code>{}</code> <p>Returns:</p> Type Description <p>GeoDataFrame with objects</p> Source code in <code>geoai/extract.py</code> <pre><code>@torch.no_grad()\ndef process_raster(\n    self,\n    raster_path,\n    output_path=None,\n    batch_size=4,\n    filter_edges=True,\n    edge_buffer=20,\n    band_indexes=None,\n    **kwargs,\n):\n    \"\"\"\n    Process a raster file to extract objects with customizable parameters.\n\n    Args:\n        raster_path: Path to input raster file\n        output_path: Path to output GeoJSON or Parquet file (optional)\n        batch_size: Batch size for processing\n        filter_edges: Whether to filter out objects at the edges of the image\n        edge_buffer: Size of edge buffer in pixels to filter out objects (if filter_edges=True)\n        band_indexes: List of band indexes to use (if None, use all bands)\n        **kwargs: Additional parameters:\n            confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n            overlap: Overlap between adjacent tiles (0.0-1.0)\n            chip_size: Size of image chips for processing (height, width)\n            nms_iou_threshold: IoU threshold for non-maximum suppression (0.0-1.0)\n            mask_threshold: Threshold for mask binarization (0.0-1.0)\n            min_object_area: Minimum area in pixels to keep an object\n            simplify_tolerance: Tolerance for polygon simplification\n\n    Returns:\n        GeoDataFrame with objects\n    \"\"\"\n    # Get parameters from kwargs or use instance defaults\n    confidence_threshold = kwargs.get(\n        \"confidence_threshold\", self.confidence_threshold\n    )\n    overlap = kwargs.get(\"overlap\", self.overlap)\n    chip_size = kwargs.get(\"chip_size\", self.chip_size)\n    nms_iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n    mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n    min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n    max_object_area = kwargs.get(\"max_object_area\", self.max_object_area)\n    simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n\n    # Print parameters being used\n    print(f\"Processing with parameters:\")\n    print(f\"- Confidence threshold: {confidence_threshold}\")\n    print(f\"- Tile overlap: {overlap}\")\n    print(f\"- Chip size: {chip_size}\")\n    print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n    print(f\"- Min object area: {min_object_area}\")\n    print(f\"- Max object area: {max_object_area}\")\n    print(f\"- Simplify tolerance: {simplify_tolerance}\")\n    print(f\"- Filter edge objects: {filter_edges}\")\n    if filter_edges:\n        print(f\"- Edge buffer size: {edge_buffer} pixels\")\n\n    # Create dataset\n    dataset = CustomDataset(\n        raster_path=raster_path,\n        chip_size=chip_size,\n        overlap=overlap,\n        band_indexes=band_indexes,\n    )\n    self.raster_stats = dataset.raster_stats\n\n    # Custom collate function to handle Shapely objects\n    def custom_collate(batch):\n        \"\"\"\n        Custom collate function that handles Shapely geometries\n        by keeping them as Python objects rather than trying to collate them.\n        \"\"\"\n        elem = batch[0]\n        if isinstance(elem, dict):\n            result = {}\n            for key in elem:\n                if key == \"bbox\":\n                    # Don't collate shapely objects, keep as list\n                    result[key] = [d[key] for d in batch]\n                else:\n                    # For tensors and other collatable types\n                    try:\n                        result[key] = (\n                            torch.utils.data._utils.collate.default_collate(\n                                [d[key] for d in batch]\n                            )\n                        )\n                    except TypeError:\n                        # Fall back to list for non-collatable types\n                        result[key] = [d[key] for d in batch]\n            return result\n        else:\n            # Default collate for non-dict types\n            return torch.utils.data._utils.collate.default_collate(batch)\n\n    # Create dataloader with simple indexing and custom collate\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=0,\n        collate_fn=custom_collate,\n    )\n\n    # Process batches\n    all_polygons = []\n    all_scores = []\n\n    print(f\"Processing raster with {len(dataloader)} batches\")\n    for batch in tqdm(dataloader):\n        # Move images to device\n        images = batch[\"image\"].to(self.device)\n        coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n        bboxes = batch[\n            \"bbox\"\n        ]  # Geographic bounding boxes - now a list, not a tensor\n\n        # Run inference\n        predictions = self.model(images)\n\n        # Process predictions\n        for idx, prediction in enumerate(predictions):\n            masks = prediction[\"masks\"].cpu().numpy()\n            scores = prediction[\"scores\"].cpu().numpy()\n            labels = prediction[\"labels\"].cpu().numpy()\n\n            # Skip if no predictions\n            if len(scores) == 0:\n                continue\n\n            # Filter by confidence threshold\n            valid_indices = scores &gt;= confidence_threshold\n            masks = masks[valid_indices]\n            scores = scores[valid_indices]\n            labels = labels[valid_indices]\n\n            # Skip if no valid predictions\n            if len(scores) == 0:\n                continue\n\n            # Get window coordinates\n            # The coords might be in different formats depending on batch handling\n            if isinstance(coords, list):\n                # If coords is a list of tuples\n                coord_item = coords[idx]\n                if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                    i, j = coord_item\n                elif isinstance(coord_item, torch.Tensor):\n                    i, j = coord_item.cpu().numpy().tolist()\n                else:\n                    print(f\"Unexpected coords format: {type(coord_item)}\")\n                    continue\n            elif isinstance(coords, torch.Tensor):\n                # If coords is a tensor of shape [batch_size, 2]\n                i, j = coords[idx].cpu().numpy().tolist()\n            else:\n                print(f\"Unexpected coords type: {type(coords)}\")\n                continue\n\n            # Get window size\n            if isinstance(batch[\"window_size\"], list):\n                window_item = batch[\"window_size\"][idx]\n                if isinstance(window_item, tuple) and len(window_item) == 2:\n                    window_width, window_height = window_item\n                elif isinstance(window_item, torch.Tensor):\n                    window_width, window_height = window_item.cpu().numpy().tolist()\n                else:\n                    print(f\"Unexpected window_size format: {type(window_item)}\")\n                    continue\n            elif isinstance(batch[\"window_size\"], torch.Tensor):\n                window_width, window_height = (\n                    batch[\"window_size\"][idx].cpu().numpy().tolist()\n                )\n            else:\n                print(f\"Unexpected window_size type: {type(batch['window_size'])}\")\n                continue\n\n            # Process masks to polygons\n            for mask_idx, mask in enumerate(masks):\n                # Get binary mask\n                binary_mask = mask[0]  # Get binary mask\n\n                # Convert mask to polygon with custom parameters\n                contours = self.mask_to_polygons(\n                    binary_mask,\n                    simplify_tolerance=simplify_tolerance,\n                    mask_threshold=mask_threshold,\n                    min_object_area=min_object_area,\n                    max_object_area=max_object_area,\n                )\n\n                # Skip if no valid polygons\n                if not contours:\n                    continue\n\n                # Transform polygons to geographic coordinates\n                with rasterio.open(raster_path) as src:\n                    transform = src.transform\n\n                    for contour in contours:\n                        # Convert polygon to global coordinates\n                        global_polygon = []\n                        for x, y in contour:\n                            # Adjust coordinates based on window position\n                            gx, gy = transform * (i + x, j + y)\n                            global_polygon.append((gx, gy))\n\n                        # Create Shapely polygon\n                        if len(global_polygon) &gt;= 3:\n                            try:\n                                shapely_poly = Polygon(global_polygon)\n                                if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                                    all_polygons.append(shapely_poly)\n                                    all_scores.append(float(scores[mask_idx]))\n                            except Exception as e:\n                                print(f\"Error creating polygon: {e}\")\n\n    # Create GeoDataFrame\n    if not all_polygons:\n        print(\"No valid polygons found\")\n        return None\n\n    gdf = gpd.GeoDataFrame(\n        {\n            \"geometry\": all_polygons,\n            \"confidence\": all_scores,\n            \"class\": 1,  # Object class\n        },\n        crs=dataset.crs,\n    )\n\n    # Remove overlapping polygons with custom threshold\n    gdf = self.filter_overlapping_polygons(gdf, nms_iou_threshold=nms_iou_threshold)\n\n    # Filter edge objects if requested\n    if filter_edges:\n        gdf = self.filter_edge_objects(gdf, raster_path, edge_buffer=edge_buffer)\n\n    # Save to file if requested\n    if output_path:\n        if output_path.endswith(\".parquet\"):\n            gdf.to_parquet(output_path)\n        else:\n            gdf.to_file(output_path, driver=\"GeoJSON\")\n        print(f\"Saved {len(gdf)} objects to {output_path}\")\n\n    return gdf\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.regularize_objects","title":"<code>regularize_objects(self, gdf, min_area=10, angle_threshold=15, orthogonality_threshold=0.3, rectangularity_threshold=0.7)</code>","text":"<p>Regularize objects to enforce right angles and rectangular shapes.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <p>GeoDataFrame with objects</p> required <code>min_area</code> <p>Minimum area in square units to keep an object</p> <code>10</code> <code>angle_threshold</code> <p>Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)</p> <code>15</code> <code>orthogonality_threshold</code> <p>Percentage of angles that must be orthogonal for an object to be regularized</p> <code>0.3</code> <code>rectangularity_threshold</code> <p>Minimum area ratio to Object's oriented bounding box for rectangular simplification</p> <code>0.7</code> <p>Returns:</p> Type Description <p>GeoDataFrame with regularized objects</p> Source code in <code>geoai/extract.py</code> <pre><code>def regularize_objects(\n    self,\n    gdf,\n    min_area=10,\n    angle_threshold=15,\n    orthogonality_threshold=0.3,\n    rectangularity_threshold=0.7,\n):\n    \"\"\"\n    Regularize objects to enforce right angles and rectangular shapes.\n\n    Args:\n        gdf: GeoDataFrame with objects\n        min_area: Minimum area in square units to keep an object\n        angle_threshold: Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)\n        orthogonality_threshold: Percentage of angles that must be orthogonal for an object to be regularized\n        rectangularity_threshold: Minimum area ratio to Object's oriented bounding box for rectangular simplification\n\n    Returns:\n        GeoDataFrame with regularized objects\n    \"\"\"\n    import math\n\n    import cv2\n    import geopandas as gpd\n    import numpy as np\n    from shapely.affinity import rotate, translate\n    from shapely.geometry import MultiPolygon, Polygon, box\n    from tqdm import tqdm\n\n    def get_angle(p1, p2, p3):\n        \"\"\"Calculate angle between three points in degrees (0-180)\"\"\"\n        a = np.array(p1)\n        b = np.array(p2)\n        c = np.array(p3)\n\n        ba = a - b\n        bc = c - b\n\n        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n        # Handle numerical errors that could push cosine outside [-1, 1]\n        cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n        angle = np.degrees(np.arccos(cosine_angle))\n\n        return angle\n\n    def is_orthogonal(angle, threshold=angle_threshold):\n        \"\"\"Check if angle is close to 90 degrees\"\"\"\n        return abs(angle - 90) &lt;= threshold\n\n    def calculate_dominant_direction(polygon):\n        \"\"\"Find the dominant direction of a polygon using PCA\"\"\"\n        # Extract coordinates\n        coords = np.array(polygon.exterior.coords)\n\n        # Mean center the coordinates\n        mean = np.mean(coords, axis=0)\n        centered_coords = coords - mean\n\n        # Calculate covariance matrix and its eigenvalues/eigenvectors\n        cov_matrix = np.cov(centered_coords.T)\n        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n        # Get the index of the largest eigenvalue\n        largest_idx = np.argmax(eigenvalues)\n\n        # Get the corresponding eigenvector (principal axis)\n        principal_axis = eigenvectors[:, largest_idx]\n\n        # Calculate the angle in degrees\n        angle_rad = np.arctan2(principal_axis[1], principal_axis[0])\n        angle_deg = np.degrees(angle_rad)\n\n        # Normalize to range 0-180\n        if angle_deg &lt; 0:\n            angle_deg += 180\n\n        return angle_deg\n\n    def create_oriented_envelope(polygon, angle_deg):\n        \"\"\"Create an oriented minimum area rectangle for the polygon\"\"\"\n        # Create a rotated rectangle using OpenCV method (more robust than Shapely methods)\n        coords = np.array(polygon.exterior.coords)[:-1].astype(\n            np.float32\n        )  # Skip the last point (same as first)\n\n        # Use OpenCV's minAreaRect\n        rect = cv2.minAreaRect(coords)\n        box_points = cv2.boxPoints(rect)\n\n        # Convert to shapely polygon\n        oriented_box = Polygon(box_points)\n\n        return oriented_box\n\n    def get_rectangularity(polygon, oriented_box):\n        \"\"\"Calculate the rectangularity (area ratio to its oriented bounding box)\"\"\"\n        if oriented_box.area == 0:\n            return 0\n        return polygon.area / oriented_box.area\n\n    def check_orthogonality(polygon):\n        \"\"\"Check what percentage of angles in the polygon are orthogonal\"\"\"\n        coords = list(polygon.exterior.coords)\n        if len(coords) &lt;= 4:  # Triangle or point\n            return 0\n\n        # Remove last point (same as first)\n        coords = coords[:-1]\n\n        orthogonal_count = 0\n        total_angles = len(coords)\n\n        for i in range(total_angles):\n            p1 = coords[i]\n            p2 = coords[(i + 1) % total_angles]\n            p3 = coords[(i + 2) % total_angles]\n\n            angle = get_angle(p1, p2, p3)\n            if is_orthogonal(angle):\n                orthogonal_count += 1\n\n        return orthogonal_count / total_angles\n\n    def simplify_to_rectangle(polygon):\n        \"\"\"Simplify a polygon to a rectangle using its oriented bounding box\"\"\"\n        # Get dominant direction\n        angle = calculate_dominant_direction(polygon)\n\n        # Create oriented envelope\n        rect = create_oriented_envelope(polygon, angle)\n\n        return rect\n\n    if gdf is None or len(gdf) == 0:\n        print(\"No Objects to regularize\")\n        return gdf\n\n    print(f\"Regularizing {len(gdf)} objects...\")\n    print(f\"- Angle threshold: {angle_threshold}\u00b0 from 90\u00b0\")\n    print(f\"- Min orthogonality: {orthogonality_threshold*100}% of angles\")\n    print(\n        f\"- Min rectangularity: {rectangularity_threshold*100}% of bounding box area\"\n    )\n\n    # Create a copy to avoid modifying the original\n    result_gdf = gdf.copy()\n\n    # Track statistics\n    total_objects = len(gdf)\n    regularized_count = 0\n    rectangularized_count = 0\n\n    # Process each Object\n    for idx, row in tqdm(gdf.iterrows(), total=len(gdf)):\n        geom = row.geometry\n\n        # Skip invalid or empty geometries\n        if geom is None or geom.is_empty:\n            continue\n\n        # Handle MultiPolygons by processing the largest part\n        if isinstance(geom, MultiPolygon):\n            areas = [p.area for p in geom.geoms]\n            if not areas:\n                continue\n            geom = list(geom.geoms)[np.argmax(areas)]\n\n        # Filter out tiny Objects\n        if geom.area &lt; min_area:\n            continue\n\n        # Check orthogonality\n        orthogonality = check_orthogonality(geom)\n\n        # Create oriented envelope\n        oriented_box = create_oriented_envelope(\n            geom, calculate_dominant_direction(geom)\n        )\n\n        # Check rectangularity\n        rectangularity = get_rectangularity(geom, oriented_box)\n\n        # Decide how to regularize\n        if rectangularity &gt;= rectangularity_threshold:\n            # Object is already quite rectangular, simplify to a rectangle\n            result_gdf.at[idx, \"geometry\"] = oriented_box\n            result_gdf.at[idx, \"regularized\"] = \"rectangle\"\n            rectangularized_count += 1\n        elif orthogonality &gt;= orthogonality_threshold:\n            # Object has many orthogonal angles but isn't rectangular\n            # Could implement more sophisticated regularization here\n            # For now, we'll still use the oriented rectangle\n            result_gdf.at[idx, \"geometry\"] = oriented_box\n            result_gdf.at[idx, \"regularized\"] = \"orthogonal\"\n            regularized_count += 1\n        else:\n            # Object doesn't have clear orthogonal structure\n            # Keep original but flag as unmodified\n            result_gdf.at[idx, \"regularized\"] = \"original\"\n\n    # Report statistics\n    print(f\"Regularization completed:\")\n    print(f\"- Total objects: {total_objects}\")\n    print(\n        f\"- Rectangular objects: {rectangularized_count} ({rectangularized_count/total_objects*100:.1f}%)\"\n    )\n    print(\n        f\"- Other regularized objects: {regularized_count} ({regularized_count/total_objects*100:.1f}%)\"\n    )\n    print(\n        f\"- Unmodified objects: {total_objects-rectangularized_count-regularized_count} ({(total_objects-rectangularized_count-regularized_count)/total_objects*100:.1f}%)\"\n    )\n\n    return result_gdf\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.save_masks_as_geotiff","title":"<code>save_masks_as_geotiff(self, raster_path, output_path=None, batch_size=4, verbose=False, **kwargs)</code>","text":"<p>Process a raster file to extract object masks and save as GeoTIFF.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <p>Path to input raster file</p> required <code>output_path</code> <p>Path to output GeoTIFF file (optional, default: input_masks.tif)</p> <code>None</code> <code>batch_size</code> <p>Batch size for processing</p> <code>4</code> <code>verbose</code> <p>Whether to print detailed processing information</p> <code>False</code> <code>**kwargs</code> <p>Additional parameters: confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0) chip_size: Size of image chips for processing (height, width) mask_threshold: Threshold for mask binarization (0.0-1.0)</p> <code>{}</code> <p>Returns:</p> Type Description <p>Path to the saved GeoTIFF file</p> Source code in <code>geoai/extract.py</code> <pre><code>def save_masks_as_geotiff(\n    self, raster_path, output_path=None, batch_size=4, verbose=False, **kwargs\n):\n    \"\"\"\n    Process a raster file to extract object masks and save as GeoTIFF.\n\n    Args:\n        raster_path: Path to input raster file\n        output_path: Path to output GeoTIFF file (optional, default: input_masks.tif)\n        batch_size: Batch size for processing\n        verbose: Whether to print detailed processing information\n        **kwargs: Additional parameters:\n            confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n            chip_size: Size of image chips for processing (height, width)\n            mask_threshold: Threshold for mask binarization (0.0-1.0)\n\n    Returns:\n        Path to the saved GeoTIFF file\n    \"\"\"\n\n    # Get parameters from kwargs or use instance defaults\n    confidence_threshold = kwargs.get(\n        \"confidence_threshold\", self.confidence_threshold\n    )\n    chip_size = kwargs.get(\"chip_size\", self.chip_size)\n    mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n    overlap = kwargs.get(\"overlap\", self.overlap)\n\n    # Set default output path if not provided\n    if output_path is None:\n        output_path = os.path.splitext(raster_path)[0] + \"_masks.tif\"\n\n    # Print parameters being used\n    print(f\"Processing masks with parameters:\")\n    print(f\"- Confidence threshold: {confidence_threshold}\")\n    print(f\"- Chip size: {chip_size}\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n\n    # Create dataset\n    dataset = CustomDataset(\n        raster_path=raster_path,\n        chip_size=chip_size,\n        overlap=overlap,\n        verbose=verbose,\n    )\n\n    # Store a flag to avoid repetitive messages\n    self.raster_stats = dataset.raster_stats\n    seen_warnings = {\n        \"bands\": False,\n        \"resize\": {},  # Dictionary to track resize warnings by shape\n    }\n\n    # Open original raster to get metadata\n    with rasterio.open(raster_path) as src:\n        # Create output binary mask raster with same dimensions as input\n        output_profile = src.profile.copy()\n        output_profile.update(\n            dtype=rasterio.uint8,\n            count=1,  # Single band for object mask\n            compress=\"lzw\",\n            nodata=0,\n        )\n\n        # Create output mask raster\n        with rasterio.open(output_path, \"w\", **output_profile) as dst:\n            # Initialize mask with zeros\n            mask_array = np.zeros((src.height, src.width), dtype=np.uint8)\n\n            # Custom collate function to handle Shapely objects\n            def custom_collate(batch):\n                \"\"\"Custom collate function for DataLoader\"\"\"\n                elem = batch[0]\n                if isinstance(elem, dict):\n                    result = {}\n                    for key in elem:\n                        if key == \"bbox\":\n                            # Don't collate shapely objects, keep as list\n                            result[key] = [d[key] for d in batch]\n                        else:\n                            # For tensors and other collatable types\n                            try:\n                                result[key] = (\n                                    torch.utils.data._utils.collate.default_collate(\n                                        [d[key] for d in batch]\n                                    )\n                                )\n                            except TypeError:\n                                # Fall back to list for non-collatable types\n                                result[key] = [d[key] for d in batch]\n                    return result\n                else:\n                    # Default collate for non-dict types\n                    return torch.utils.data._utils.collate.default_collate(batch)\n\n            # Create dataloader\n            dataloader = torch.utils.data.DataLoader(\n                dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=0,\n                collate_fn=custom_collate,\n            )\n\n            # Process batches\n            print(f\"Processing raster with {len(dataloader)} batches\")\n            for batch in tqdm(dataloader):\n                # Move images to device\n                images = batch[\"image\"].to(self.device)\n                coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n\n                # Run inference\n                with torch.no_grad():\n                    predictions = self.model(images)\n\n                # Process predictions\n                for idx, prediction in enumerate(predictions):\n                    masks = prediction[\"masks\"].cpu().numpy()\n                    scores = prediction[\"scores\"].cpu().numpy()\n\n                    # Skip if no predictions\n                    if len(scores) == 0:\n                        continue\n\n                    # Filter by confidence threshold\n                    valid_indices = scores &gt;= confidence_threshold\n                    masks = masks[valid_indices]\n                    scores = scores[valid_indices]\n\n                    # Skip if no valid predictions\n                    if len(scores) == 0:\n                        continue\n\n                    # Get window coordinates\n                    if isinstance(coords, list):\n                        coord_item = coords[idx]\n                        if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                            i, j = coord_item\n                        elif isinstance(coord_item, torch.Tensor):\n                            i, j = coord_item.cpu().numpy().tolist()\n                        else:\n                            print(f\"Unexpected coords format: {type(coord_item)}\")\n                            continue\n                    elif isinstance(coords, torch.Tensor):\n                        i, j = coords[idx].cpu().numpy().tolist()\n                    else:\n                        print(f\"Unexpected coords type: {type(coords)}\")\n                        continue\n\n                    # Get window size\n                    if isinstance(batch[\"window_size\"], list):\n                        window_item = batch[\"window_size\"][idx]\n                        if isinstance(window_item, tuple) and len(window_item) == 2:\n                            window_width, window_height = window_item\n                        elif isinstance(window_item, torch.Tensor):\n                            window_width, window_height = (\n                                window_item.cpu().numpy().tolist()\n                            )\n                        else:\n                            print(\n                                f\"Unexpected window_size format: {type(window_item)}\"\n                            )\n                            continue\n                    elif isinstance(batch[\"window_size\"], torch.Tensor):\n                        window_width, window_height = (\n                            batch[\"window_size\"][idx].cpu().numpy().tolist()\n                        )\n                    else:\n                        print(\n                            f\"Unexpected window_size type: {type(batch['window_size'])}\"\n                        )\n                        continue\n\n                    # Combine all masks for this window\n                    combined_mask = np.zeros(\n                        (window_height, window_width), dtype=np.uint8\n                    )\n\n                    for mask in masks:\n                        # Get the binary mask\n                        binary_mask = (mask[0] &gt; mask_threshold).astype(\n                            np.uint8\n                        ) * 255\n\n                        # Handle size mismatch - resize binary_mask if needed\n                        mask_h, mask_w = binary_mask.shape\n                        if mask_h != window_height or mask_w != window_width:\n                            resize_key = f\"{(mask_h, mask_w)}-&gt;{(window_height, window_width)}\"\n                            if resize_key not in seen_warnings[\"resize\"]:\n                                if verbose:\n                                    print(\n                                        f\"Resizing mask from {binary_mask.shape} to {(window_height, window_width)}\"\n                                    )\n                                else:\n                                    if not seen_warnings[\n                                        \"resize\"\n                                    ]:  # If this is the first resize warning\n                                        print(\n                                            f\"Resizing masks at image edges (set verbose=True for details)\"\n                                        )\n                                seen_warnings[\"resize\"][resize_key] = True\n\n                            # Crop or pad the binary mask to match window size\n                            resized_mask = np.zeros(\n                                (window_height, window_width), dtype=np.uint8\n                            )\n                            copy_h = min(mask_h, window_height)\n                            copy_w = min(mask_w, window_width)\n                            resized_mask[:copy_h, :copy_w] = binary_mask[\n                                :copy_h, :copy_w\n                            ]\n                            binary_mask = resized_mask\n\n                        # Update combined mask (taking maximum where masks overlap)\n                        combined_mask = np.maximum(combined_mask, binary_mask)\n\n                    # Write combined mask to output array\n                    # Handle edge cases where window might be smaller than chip size\n                    h, w = combined_mask.shape\n                    valid_h = min(h, src.height - j)\n                    valid_w = min(w, src.width - i)\n\n                    if valid_h &gt; 0 and valid_w &gt; 0:\n                        mask_array[j : j + valid_h, i : i + valid_w] = np.maximum(\n                            mask_array[j : j + valid_h, i : i + valid_w],\n                            combined_mask[:valid_h, :valid_w],\n                        )\n\n            # Write the final mask to the output file\n            dst.write(mask_array, 1)\n\n    print(f\"Object masks saved to {output_path}\")\n    return output_path\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.vectorize_masks","title":"<code>vectorize_masks(self, masks_path, output_path=None, confidence_threshold=0.5, min_object_area=100, max_object_area=None, n_workers=None, **kwargs)</code>","text":"<p>Convert masks with confidence to vector polygons.</p> <p>Parameters:</p> Name Type Description Default <code>masks_path</code> <p>Path to masks GeoTIFF with confidence band.</p> required <code>output_path</code> <p>Path for output GeoJSON.</p> <code>None</code> <code>confidence_threshold</code> <p>Minimum confidence score (0.0-1.0). Default: 0.5</p> <code>0.5</code> <code>min_object_area</code> <p>Minimum area in pixels to keep an object. Default: 100</p> <code>100</code> <code>max_object_area</code> <p>Maximum area in pixels to keep an object. Default: None</p> <code>None</code> <code>n_workers</code> <p>int, default=None The number of worker threads to use. \"None\" means single-threaded processing. \"-1\"   means using all available CPU processors. Positive integer means using that specific number of threads.</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <p>GeoDataFrame with car detections and confidence values</p> Source code in <code>geoai/extract.py</code> <pre><code>def vectorize_masks(\n    self,\n    masks_path,\n    output_path=None,\n    confidence_threshold=0.5,\n    min_object_area=100,\n    max_object_area=None,\n    n_workers=None,\n    **kwargs,\n):\n    \"\"\"\n    Convert masks with confidence to vector polygons.\n\n    Args:\n        masks_path: Path to masks GeoTIFF with confidence band.\n        output_path: Path for output GeoJSON.\n        confidence_threshold: Minimum confidence score (0.0-1.0). Default: 0.5\n        min_object_area: Minimum area in pixels to keep an object. Default: 100\n        max_object_area: Maximum area in pixels to keep an object. Default: None\n        n_workers: int, default=None\n            The number of worker threads to use.\n            \"None\" means single-threaded processing.\n            \"-1\"   means using all available CPU processors.\n            Positive integer means using that specific number of threads.\n        **kwargs: Additional parameters\n\n    Returns:\n        GeoDataFrame with car detections and confidence values\n    \"\"\"\n\n    def _process_single_component(\n        component_mask,\n        conf_data,\n        transform,\n        confidence_threshold,\n        min_object_area,\n        max_object_area,\n    ):\n        # Get confidence value\n        conf_region = conf_data[component_mask &gt; 0]\n        if len(conf_region) &gt; 0:\n            confidence = np.mean(conf_region) / 255.0\n        else:\n            confidence = 0.0\n\n        # Skip if confidence is below threshold\n        if confidence &lt; confidence_threshold:\n            return None\n\n        # Find contours\n        contours, _ = cv2.findContours(\n            component_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n        )\n\n        results = []\n\n        for contour in contours:\n            # Filter by size\n            area = cv2.contourArea(contour)\n            if area &lt; min_object_area:\n                continue\n\n            if max_object_area is not None and area &gt; max_object_area:\n                continue\n\n            # Get minimum area rectangle\n            rect = cv2.minAreaRect(contour)\n            box_points = cv2.boxPoints(rect)\n\n            # Convert to geographic coordinates\n            geo_points = []\n            for x, y in box_points:\n                gx, gy = transform * (x, y)\n                geo_points.append((gx, gy))\n\n            # Create polygon\n            poly = Polygon(geo_points)\n            results.append((poly, confidence, area))\n\n        return results\n\n    import concurrent.futures\n    from functools import partial\n\n    def process_component(args):\n        \"\"\"\n        Helper function to process a single component\n        \"\"\"\n        (\n            label,\n            labeled_mask,\n            conf_data,\n            transform,\n            confidence_threshold,\n            min_object_area,\n            max_object_area,\n        ) = args\n\n        # Create mask for this component\n        component_mask = (labeled_mask == label).astype(np.uint8)\n\n        return _process_single_component(\n            component_mask,\n            conf_data,\n            transform,\n            confidence_threshold,\n            min_object_area,\n            max_object_area,\n        )\n\n    start_time = time.time()\n    print(f\"Processing masks from: {masks_path}\")\n\n    if n_workers == -1:\n        n_workers = os.cpu_count()\n\n    with rasterio.open(masks_path) as src:\n        # Read mask and confidence bands\n        mask_data = src.read(1)\n        conf_data = src.read(2)\n        transform = src.transform\n        crs = src.crs\n\n        # Convert to binary mask\n        binary_mask = mask_data &gt; 0\n\n        # Find connected components\n        labeled_mask, num_features = ndimage.label(binary_mask)\n        print(f\"Found {num_features} connected components\")\n\n        # Process each component\n        polygons = []\n        confidences = []\n        pixels = []\n\n        if n_workers is None or n_workers == 1:\n            print(\n                \"Using single-threaded processing, you can speed up processing by setting n_workers &gt; 1\"\n            )\n            # Add progress bar\n            for label in tqdm(\n                range(1, num_features + 1), desc=\"Processing components\"\n            ):\n                # Create mask for this component\n                component_mask = (labeled_mask == label).astype(np.uint8)\n\n                result = _process_single_component(\n                    component_mask,\n                    conf_data,\n                    transform,\n                    confidence_threshold,\n                    min_object_area,\n                    max_object_area,\n                )\n\n                if result:\n                    for poly, confidence, area in result:\n                        # Add to lists\n                        polygons.append(poly)\n                        confidences.append(confidence)\n                        pixels.append(area)\n\n        else:\n            # Process components in parallel\n            print(f\"Using {n_workers} workers for parallel processing\")\n\n            process_args = [\n                (\n                    label,\n                    labeled_mask,\n                    conf_data,\n                    transform,\n                    confidence_threshold,\n                    min_object_area,\n                    max_object_area,\n                )\n                for label in range(1, num_features + 1)\n            ]\n\n            with concurrent.futures.ThreadPoolExecutor(\n                max_workers=n_workers\n            ) as executor:\n                results = list(\n                    tqdm(\n                        executor.map(process_component, process_args),\n                        total=num_features,\n                        desc=\"Processing components\",\n                    )\n                )\n\n                for result in results:\n                    if result:\n                        for poly, confidence, area in result:\n                            # Add to lists\n                            polygons.append(poly)\n                            confidences.append(confidence)\n                            pixels.append(area)\n\n        # Create GeoDataFrame\n        if polygons:\n            gdf = gpd.GeoDataFrame(\n                {\n                    \"geometry\": polygons,\n                    \"confidence\": confidences,\n                    \"class\": [1] * len(polygons),\n                    \"pixels\": pixels,\n                },\n                crs=crs,\n            )\n\n            # Save to file if requested\n            if output_path:\n                gdf.to_file(output_path, driver=\"GeoJSON\")\n                print(f\"Saved {len(gdf)} objects with confidence to {output_path}\")\n\n            end_time = time.time()\n            print(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n            return gdf\n        else:\n            end_time = time.time()\n            print(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n            print(\"No valid polygons found\")\n            return None\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.visualize_results","title":"<code>visualize_results(self, raster_path, gdf=None, output_path=None, figsize=(12, 12))</code>","text":"<p>Visualize object detection results with proper coordinate transformation.</p> <p>This function displays objects on top of the raster image, ensuring proper alignment between the GeoDataFrame polygons and the image.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <p>Path to input raster</p> required <code>gdf</code> <p>GeoDataFrame with object polygons (optional)</p> <code>None</code> <code>output_path</code> <p>Path to save visualization (optional)</p> <code>None</code> <code>figsize</code> <p>Figure size (width, height) in inches</p> <code>(12, 12)</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if visualization was successful</p> Source code in <code>geoai/extract.py</code> <pre><code>def visualize_results(\n    self, raster_path, gdf=None, output_path=None, figsize=(12, 12)\n):\n    \"\"\"\n    Visualize object detection results with proper coordinate transformation.\n\n    This function displays objects on top of the raster image,\n    ensuring proper alignment between the GeoDataFrame polygons and the image.\n\n    Args:\n        raster_path: Path to input raster\n        gdf: GeoDataFrame with object polygons (optional)\n        output_path: Path to save visualization (optional)\n        figsize: Figure size (width, height) in inches\n\n    Returns:\n        bool: True if visualization was successful\n    \"\"\"\n    # Check if raster file exists\n    if not os.path.exists(raster_path):\n        print(f\"Error: Raster file '{raster_path}' not found.\")\n        return False\n\n    # Process raster if GeoDataFrame not provided\n    if gdf is None:\n        gdf = self.process_raster(raster_path)\n\n    if gdf is None or len(gdf) == 0:\n        print(\"No objects to visualize\")\n        return False\n\n    # Check if confidence column exists in the GeoDataFrame\n    has_confidence = False\n    if hasattr(gdf, \"columns\") and \"confidence\" in gdf.columns:\n        # Try to access a confidence value to confirm it works\n        try:\n            if len(gdf) &gt; 0:\n                # Try getitem access\n                conf_val = gdf[\"confidence\"].iloc[0]\n                has_confidence = True\n                print(\n                    f\"Using confidence values (range: {gdf['confidence'].min():.2f} - {gdf['confidence'].max():.2f})\"\n                )\n        except Exception as e:\n            print(f\"Confidence column exists but couldn't access values: {e}\")\n            has_confidence = False\n    else:\n        print(\"No confidence column found in GeoDataFrame\")\n        has_confidence = False\n\n    # Read raster for visualization\n    with rasterio.open(raster_path) as src:\n        # Read the entire image or a subset if it's very large\n        if src.height &gt; 2000 or src.width &gt; 2000:\n            # Calculate scale factor to reduce size\n            scale = min(2000 / src.height, 2000 / src.width)\n            out_shape = (\n                int(src.count),\n                int(src.height * scale),\n                int(src.width * scale),\n            )\n\n            # Read and resample\n            image = src.read(\n                out_shape=out_shape, resampling=rasterio.enums.Resampling.bilinear\n            )\n\n            # Create a scaled transform for the resampled image\n            # Calculate scaling factors\n            x_scale = src.width / out_shape[2]\n            y_scale = src.height / out_shape[1]\n\n            # Get the original transform\n            orig_transform = src.transform\n\n            # Create a scaled transform\n            scaled_transform = rasterio.transform.Affine(\n                orig_transform.a * x_scale,\n                orig_transform.b,\n                orig_transform.c,\n                orig_transform.d,\n                orig_transform.e * y_scale,\n                orig_transform.f,\n            )\n        else:\n            image = src.read()\n            scaled_transform = src.transform\n\n        # Convert to RGB for display\n        if image.shape[0] &gt; 3:\n            image = image[:3]\n        elif image.shape[0] == 1:\n            image = np.repeat(image, 3, axis=0)\n\n        # Normalize image for display\n        image = image.transpose(1, 2, 0)  # CHW to HWC\n        image = image.astype(np.float32)\n\n        if image.max() &gt; 10:  # Likely 0-255 range\n            image = image / 255.0\n\n        image = np.clip(image, 0, 1)\n\n        # Get image bounds\n        bounds = src.bounds\n        crs = src.crs\n\n    # Create figure with appropriate aspect ratio\n    aspect_ratio = image.shape[1] / image.shape[0]  # width / height\n    plt.figure(figsize=(figsize[0], figsize[0] / aspect_ratio))\n    ax = plt.gca()\n\n    # Display image\n    ax.imshow(image)\n\n    # Make sure the GeoDataFrame has the same CRS as the raster\n    if gdf.crs != crs:\n        print(f\"Reprojecting GeoDataFrame from {gdf.crs} to {crs}\")\n        gdf = gdf.to_crs(crs)\n\n    # Set up colors for confidence visualization\n    if has_confidence:\n        try:\n            import matplotlib.cm as cm\n            from matplotlib.colors import Normalize\n\n            # Get min/max confidence values\n            min_conf = gdf[\"confidence\"].min()\n            max_conf = gdf[\"confidence\"].max()\n\n            # Set up normalization and colormap\n            norm = Normalize(vmin=min_conf, vmax=max_conf)\n            cmap = cm.viridis\n\n            # Create scalar mappable for colorbar\n            sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n            sm.set_array([])\n\n            # Add colorbar\n            cbar = plt.colorbar(\n                sm, ax=ax, orientation=\"vertical\", shrink=0.7, pad=0.01\n            )\n            cbar.set_label(\"Confidence Score\")\n        except Exception as e:\n            print(f\"Error setting up confidence visualization: {e}\")\n            has_confidence = False\n\n    # Function to convert coordinates\n    def geo_to_pixel(geometry, transform):\n        \"\"\"Convert geometry to pixel coordinates using the provided transform.\"\"\"\n        if geometry.is_empty:\n            return None\n\n        if geometry.geom_type == \"Polygon\":\n            # Get exterior coordinates\n            exterior_coords = list(geometry.exterior.coords)\n\n            # Convert to pixel coordinates\n            pixel_coords = [~transform * (x, y) for x, y in exterior_coords]\n\n            # Split into x and y lists\n            pixel_x = [coord[0] for coord in pixel_coords]\n            pixel_y = [coord[1] for coord in pixel_coords]\n\n            return pixel_x, pixel_y\n        else:\n            print(f\"Unsupported geometry type: {geometry.geom_type}\")\n            return None\n\n    # Plot each object\n    for idx, row in gdf.iterrows():\n        try:\n            # Convert polygon to pixel coordinates\n            coords = geo_to_pixel(row.geometry, scaled_transform)\n\n            if coords:\n                pixel_x, pixel_y = coords\n\n                if has_confidence:\n                    try:\n                        # Get confidence value using different methods\n                        # Method 1: Try direct attribute access\n                        confidence = None\n                        try:\n                            confidence = row.confidence\n                        except:\n                            pass\n\n                        # Method 2: Try dictionary-style access\n                        if confidence is None:\n                            try:\n                                confidence = row[\"confidence\"]\n                            except:\n                                pass\n\n                        # Method 3: Try accessing by index from the GeoDataFrame\n                        if confidence is None:\n                            try:\n                                confidence = gdf.iloc[idx][\"confidence\"]\n                            except:\n                                pass\n\n                        if confidence is not None:\n                            color = cmap(norm(confidence))\n                            # Fill polygon with semi-transparent color\n                            ax.fill(pixel_x, pixel_y, color=color, alpha=0.5)\n                            # Draw border\n                            ax.plot(\n                                pixel_x,\n                                pixel_y,\n                                color=color,\n                                linewidth=1,\n                                alpha=0.8,\n                            )\n                        else:\n                            # Fall back to red if confidence value couldn't be accessed\n                            ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n                    except Exception as e:\n                        print(\n                            f\"Error using confidence value for polygon {idx}: {e}\"\n                        )\n                        ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n                else:\n                    # No confidence data, just plot outlines in red\n                    ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n        except Exception as e:\n            print(f\"Error plotting polygon {idx}: {e}\")\n\n    # Remove axes\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(f\"objects (Found: {len(gdf)})\")\n\n    # Save if requested\n    if output_path:\n        plt.tight_layout()\n        plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n        print(f\"Visualization saved to {output_path}\")\n\n    plt.close()\n\n    # Create a simpler visualization focused just on a subset of objects\n    if len(gdf) &gt; 0:\n        plt.figure(figsize=figsize)\n        ax = plt.gca()\n\n        # Choose a subset of the image to show\n        with rasterio.open(raster_path) as src:\n            # Get centroid of first object\n            sample_geom = gdf.iloc[0].geometry\n            centroid = sample_geom.centroid\n\n            # Convert to pixel coordinates\n            center_x, center_y = ~src.transform * (centroid.x, centroid.y)\n\n            # Define a window around this object\n            window_size = 500  # pixels\n            window = rasterio.windows.Window(\n                max(0, int(center_x - window_size / 2)),\n                max(0, int(center_y - window_size / 2)),\n                min(window_size, src.width - int(center_x - window_size / 2)),\n                min(window_size, src.height - int(center_y - window_size / 2)),\n            )\n\n            # Read this window\n            sample_image = src.read(window=window)\n\n            # Convert to RGB for display\n            if sample_image.shape[0] &gt; 3:\n                sample_image = sample_image[:3]\n            elif sample_image.shape[0] == 1:\n                sample_image = np.repeat(sample_image, 3, axis=0)\n\n            # Normalize image for display\n            sample_image = sample_image.transpose(1, 2, 0)  # CHW to HWC\n            sample_image = sample_image.astype(np.float32)\n\n            if sample_image.max() &gt; 10:  # Likely 0-255 range\n                sample_image = sample_image / 255.0\n\n            sample_image = np.clip(sample_image, 0, 1)\n\n            # Display sample image\n            ax.imshow(sample_image, extent=[0, window.width, window.height, 0])\n\n            # Get the correct transform for this window\n            window_transform = src.window_transform(window)\n\n            # Calculate bounds of the window\n            window_bounds = rasterio.windows.bounds(window, src.transform)\n            window_box = box(*window_bounds)\n\n            # Filter objects that intersect with this window\n            visible_gdf = gdf[gdf.intersects(window_box)]\n\n            # Set up colors for sample view if confidence data exists\n            if has_confidence:\n                try:\n                    # Reuse the same normalization and colormap from main view\n                    sample_sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n                    sample_sm.set_array([])\n\n                    # Add colorbar to sample view\n                    sample_cbar = plt.colorbar(\n                        sample_sm,\n                        ax=ax,\n                        orientation=\"vertical\",\n                        shrink=0.7,\n                        pad=0.01,\n                    )\n                    sample_cbar.set_label(\"Confidence Score\")\n                except Exception as e:\n                    print(f\"Error setting up sample confidence visualization: {e}\")\n\n            # Plot objects in sample view\n            for idx, row in visible_gdf.iterrows():\n                try:\n                    # Get window-relative pixel coordinates\n                    geom = row.geometry\n\n                    # Skip empty geometries\n                    if geom.is_empty:\n                        continue\n\n                    # Get exterior coordinates\n                    exterior_coords = list(geom.exterior.coords)\n\n                    # Convert to pixel coordinates relative to window origin\n                    pixel_coords = []\n                    for x, y in exterior_coords:\n                        px, py = ~src.transform * (x, y)  # Convert to image pixels\n                        # Make coordinates relative to window\n                        px = px - window.col_off\n                        py = py - window.row_off\n                        pixel_coords.append((px, py))\n\n                    # Extract x and y coordinates\n                    pixel_x = [coord[0] for coord in pixel_coords]\n                    pixel_y = [coord[1] for coord in pixel_coords]\n\n                    # Use confidence colors if available\n                    if has_confidence:\n                        try:\n                            # Try different methods to access confidence\n                            confidence = None\n                            try:\n                                confidence = row.confidence\n                            except:\n                                pass\n\n                            if confidence is None:\n                                try:\n                                    confidence = row[\"confidence\"]\n                                except:\n                                    pass\n\n                            if confidence is None:\n                                try:\n                                    confidence = visible_gdf.iloc[idx][\"confidence\"]\n                                except:\n                                    pass\n\n                            if confidence is not None:\n                                color = cmap(norm(confidence))\n                                # Fill polygon with semi-transparent color\n                                ax.fill(pixel_x, pixel_y, color=color, alpha=0.5)\n                                # Draw border\n                                ax.plot(\n                                    pixel_x,\n                                    pixel_y,\n                                    color=color,\n                                    linewidth=1.5,\n                                    alpha=0.8,\n                                )\n                            else:\n                                ax.plot(\n                                    pixel_x, pixel_y, color=\"red\", linewidth=1.5\n                                )\n                        except Exception as e:\n                            print(\n                                f\"Error using confidence in sample view for polygon {idx}: {e}\"\n                            )\n                            ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                    else:\n                        ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                except Exception as e:\n                    print(f\"Error plotting polygon in sample view: {e}\")\n\n            # Set title\n            ax.set_title(f\"Sample Area - objects (Showing: {len(visible_gdf)})\")\n\n            # Remove axes\n            ax.set_xticks([])\n            ax.set_yticks([])\n\n            # Save if requested\n            if output_path:\n                sample_output = (\n                    os.path.splitext(output_path)[0]\n                    + \"_sample\"\n                    + os.path.splitext(output_path)[1]\n                )\n                plt.tight_layout()\n                plt.savefig(sample_output, dpi=300, bbox_inches=\"tight\")\n                print(f\"Sample visualization saved to {sample_output}\")\n</code></pre>"},{"location":"extract/#geoai.extract.ParkingSplotDetector","title":"<code> ParkingSplotDetector            (ObjectDetector)         </code>","text":"<p>Car detection using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for car detection.</p> Source code in <code>geoai/extract.py</code> <pre><code>class ParkingSplotDetector(ObjectDetector):\n    \"\"\"\n    Car detection using a pre-trained Mask R-CNN model.\n\n    This class extends the `ObjectDetector` class with additional methods for car detection.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path=\"parking_spot_detection.pth\",\n        repo_id=None,\n        model=None,\n        num_classes=3,\n        device=None,\n    ):\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            num_classes: Number of classes for the model. Default: 3\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path,\n            repo_id=repo_id,\n            model=model,\n            num_classes=num_classes,\n            device=device,\n        )\n</code></pre>"},{"location":"extract/#geoai.extract.ParkingSplotDetector.__init__","title":"<code>__init__(self, model_path='parking_spot_detection.pth', repo_id=None, model=None, num_classes=3, device=None)</code>  <code>special</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <p>Path to the .pth model file.</p> <code>'parking_spot_detection.pth'</code> <code>repo_id</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <p>Custom model to use for inference.</p> <code>None</code> <code>num_classes</code> <p>Number of classes for the model. Default: 3</p> <code>3</code> <code>device</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path=\"parking_spot_detection.pth\",\n    repo_id=None,\n    model=None,\n    num_classes=3,\n    device=None,\n):\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        num_classes: Number of classes for the model. Default: 3\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path,\n        repo_id=repo_id,\n        model=model,\n        num_classes=num_classes,\n        device=device,\n    )\n</code></pre>"},{"location":"extract/#geoai.extract.ShipDetector","title":"<code> ShipDetector            (ObjectDetector)         </code>","text":"<p>Ship detection using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for ship detection.\"</p> Source code in <code>geoai/extract.py</code> <pre><code>class ShipDetector(ObjectDetector):\n    \"\"\"\n    Ship detection using a pre-trained Mask R-CNN model.\n\n    This class extends the\n    `ObjectDetector` class with additional methods for ship detection.\"\n    \"\"\"\n\n    def __init__(\n        self, model_path=\"ship_detection.pth\", repo_id=None, model=None, device=None\n    ):\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n</code></pre>"},{"location":"extract/#geoai.extract.ShipDetector.__init__","title":"<code>__init__(self, model_path='ship_detection.pth', repo_id=None, model=None, device=None)</code>  <code>special</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <p>Path to the .pth model file.</p> <code>'ship_detection.pth'</code> <code>repo_id</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self, model_path=\"ship_detection.pth\", repo_id=None, model=None, device=None\n):\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n</code></pre>"},{"location":"extract/#geoai.extract.SolarPanelDetector","title":"<code> SolarPanelDetector            (ObjectDetector)         </code>","text":"<p>Solar panel detection using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for solar panel detection.\"</p> Source code in <code>geoai/extract.py</code> <pre><code>class SolarPanelDetector(ObjectDetector):\n    \"\"\"\n    Solar panel detection using a pre-trained Mask R-CNN model.\n\n    This class extends the\n    `ObjectDetector` class with additional methods for solar panel detection.\"\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path=\"solar_panel_detection.pth\",\n        repo_id=None,\n        model=None,\n        device=None,\n    ):\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n</code></pre>"},{"location":"extract/#geoai.extract.SolarPanelDetector.__init__","title":"<code>__init__(self, model_path='solar_panel_detection.pth', repo_id=None, model=None, device=None)</code>  <code>special</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <p>Path to the .pth model file.</p> <code>'solar_panel_detection.pth'</code> <code>repo_id</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path=\"solar_panel_detection.pth\",\n    repo_id=None,\n    model=None,\n    device=None,\n):\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n</code></pre>"},{"location":"faq/","title":"FAQ","text":""},{"location":"geoai/","title":"geoai module","text":"<p>Main module.</p>"},{"location":"geoai/#geoai.geoai.Map","title":"<code> Map            (Map)         </code>","text":"<p>A subclass of leafmap.Map for GeoAI applications.</p> Source code in <code>geoai/geoai.py</code> <pre><code>class Map(leafmap.Map):\n    \"\"\"A subclass of leafmap.Map for GeoAI applications.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the Map class.\"\"\"\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"geoai/#geoai.geoai.Map.__init__","title":"<code>__init__(self, *args, **kwargs)</code>  <code>special</code>","text":"<p>Initialize the Map class.</p> Source code in <code>geoai/geoai.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Initialize the Map class.\"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"hf/","title":"hf module","text":"<p>This module contains utility functions for working with Hugging Face models.</p>"},{"location":"hf/#geoai.hf.get_model_config","title":"<code>get_model_config(model_id)</code>","text":"<p>Get the model configuration for a Hugging Face model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The Hugging Face model ID.</p> required <p>Returns:</p> Type Description <code>transformers.configuration_utils.PretrainedConfig</code> <p>The model configuration.</p> Source code in <code>geoai/hf.py</code> <pre><code>def get_model_config(model_id):\n    \"\"\"\n    Get the model configuration for a Hugging Face model.\n\n    Args:\n        model_id (str): The Hugging Face model ID.\n\n    Returns:\n        transformers.configuration_utils.PretrainedConfig: The model configuration.\n    \"\"\"\n    return AutoConfig.from_pretrained(model_id)\n</code></pre>"},{"location":"hf/#geoai.hf.get_model_input_channels","title":"<code>get_model_input_channels(model_id)</code>","text":"<p>Check the number of input channels supported by a Hugging Face model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The Hugging Face model ID.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of input channels the model accepts.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If unable to determine the number of input channels.</p> Source code in <code>geoai/hf.py</code> <pre><code>def get_model_input_channels(model_id):\n    \"\"\"\n    Check the number of input channels supported by a Hugging Face model.\n\n    Args:\n        model_id (str): The Hugging Face model ID.\n\n    Returns:\n        int: The number of input channels the model accepts.\n\n    Raises:\n        ValueError: If unable to determine the number of input channels.\n    \"\"\"\n    # Load the model configuration\n    config = AutoConfig.from_pretrained(model_id)\n\n    # For Mask2Former models\n    if hasattr(config, \"backbone_config\"):\n        if hasattr(config.backbone_config, \"num_channels\"):\n            return config.backbone_config.num_channels\n\n    # Try to load the model and inspect its architecture\n    try:\n        model = AutoModelForMaskedImageModeling.from_pretrained(model_id)\n\n        # For Swin Transformer-based models like Mask2Former\n        if hasattr(model, \"backbone\") and hasattr(model.backbone, \"embeddings\"):\n            if hasattr(model.backbone.embeddings, \"patch_embeddings\"):\n                # Swin models typically have patch embeddings that indicate channel count\n                return model.backbone.embeddings.patch_embeddings.in_channels\n    except Exception as e:\n        print(f\"Couldn't inspect model architecture: {e}\")\n\n    # Default for most vision models\n    return 3\n</code></pre>"},{"location":"hf/#geoai.hf.image_segmentation","title":"<code>image_segmentation(tif_path, output_path, labels_to_extract=None, dtype='uint8', model_name=None, segmenter_args=None, **kwargs)</code>","text":"<p>Segments an image with a Hugging Face segmentation model and saves the results as a single georeferenced image where each class has a unique integer value.</p> <p>Parameters:</p> Name Type Description Default <code>tif_path</code> <code>str</code> <p>Path to the input georeferenced TIF file.</p> required <code>output_path</code> <code>str</code> <p>Path where the output georeferenced segmentation will be saved.</p> required <code>labels_to_extract</code> <code>list</code> <p>List of labels to extract. If None, extracts all labels.</p> <code>None</code> <code>dtype</code> <code>str</code> <p>Data type to use for the output mask. Defaults to \"uint8\".</p> <code>'uint8'</code> <code>model_name</code> <code>str</code> <p>Name of the Hugging Face model to use for segmentation, such as \"facebook/mask2former-swin-large-cityscapes-semantic\". Defaults to None. See https://huggingface.co/models?pipeline_tag=image-segmentation&amp;sort=trending for options.</p> <code>None</code> <code>segmenter_args</code> <code>dict</code> <p>Additional arguments to pass to the segmenter. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the segmentation pipeline</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(Path to saved image, dictionary mapping label names to their assigned values,     dictionary mapping label names to confidence scores)</p> Source code in <code>geoai/hf.py</code> <pre><code>def image_segmentation(\n    tif_path,\n    output_path,\n    labels_to_extract=None,\n    dtype=\"uint8\",\n    model_name=None,\n    segmenter_args=None,\n    **kwargs,\n):\n    \"\"\"\n    Segments an image with a Hugging Face segmentation model and saves the results\n    as a single georeferenced image where each class has a unique integer value.\n\n    Args:\n        tif_path (str): Path to the input georeferenced TIF file.\n        output_path (str): Path where the output georeferenced segmentation will be saved.\n        labels_to_extract (list, optional): List of labels to extract. If None, extracts all labels.\n        dtype (str, optional): Data type to use for the output mask. Defaults to \"uint8\".\n        model_name (str, optional): Name of the Hugging Face model to use for segmentation,\n            such as \"facebook/mask2former-swin-large-cityscapes-semantic\". Defaults to None.\n            See https://huggingface.co/models?pipeline_tag=image-segmentation&amp;sort=trending for options.\n        segmenter_args (dict, optional): Additional arguments to pass to the segmenter.\n            Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the segmentation pipeline\n\n    Returns:\n        tuple: (Path to saved image, dictionary mapping label names to their assigned values,\n            dictionary mapping label names to confidence scores)\n    \"\"\"\n    # Load the original georeferenced image to extract metadata\n    with rasterio.open(tif_path) as src:\n        # Save the metadata for later use\n        meta = src.meta.copy()\n        # Get the dimensions\n        height = src.height\n        width = src.width\n        # Get the transform and CRS for georeferencing\n        # transform = src.transform\n        # crs = src.crs\n\n    # Initialize the segmentation pipeline\n    if model_name is None:\n        model_name = \"facebook/mask2former-swin-large-cityscapes-semantic\"\n\n    kwargs[\"task\"] = \"image-segmentation\"\n\n    segmenter = pipeline(model=model_name, **kwargs)\n\n    # Run the segmentation on the GeoTIFF\n    if segmenter_args is None:\n        segmenter_args = {}\n\n    segments = segmenter(tif_path, **segmenter_args)\n\n    # If no specific labels are requested, extract all available ones\n    if labels_to_extract is None:\n        labels_to_extract = [segment[\"label\"] for segment in segments]\n\n    # Create an empty mask to hold all the labels\n    # Using uint8 for up to 255 classes, switch to uint16 for more\n    combined_mask = np.zeros((height, width), dtype=np.uint8)\n\n    # Create a dictionary to map labels to values and store scores\n    label_to_value = {}\n    label_to_score = {}\n\n    # Process each segment we want to keep\n    for i, segment in enumerate(\n        [s for s in segments if s[\"label\"] in labels_to_extract]\n    ):\n        # Assign a unique value to each label (starting from 1)\n        value = i + 1\n        label = segment[\"label\"]\n        score = segment[\"score\"]\n\n        label_to_value[label] = value\n        label_to_score[label] = score\n\n        # Convert PIL image to numpy array\n        mask = np.array(segment[\"mask\"])\n\n        # Apply a threshold if it's a probability mask (not binary)\n        if mask.dtype == float:\n            mask = (mask &gt; 0.5).astype(np.uint8)\n\n        # Resize if needed to match original dimensions\n        if mask.shape != (height, width):\n            mask_img = Image.fromarray(mask)\n            mask_img = mask_img.resize((width, height))\n            mask = np.array(mask_img)\n\n        # Add this class to the combined mask\n        # Only overwrite if the pixel isn't already assigned to another class\n        # This handles overlapping segments by giving priority to earlier segments\n        combined_mask = np.where(\n            (mask &gt; 0) &amp; (combined_mask == 0), value, combined_mask\n        )\n\n    # Update metadata for the output raster\n    meta.update(\n        {\n            \"count\": 1,  # One band for the mask\n            \"dtype\": dtype,  # Use uint8 for up to 255 classes\n            \"nodata\": 0,  # 0 represents no class\n        }\n    )\n\n    # Save the mask as a new georeferenced GeoTIFF\n    with rasterio.open(output_path, \"w\", **meta) as dst:\n        dst.write(combined_mask[np.newaxis, :, :])  # Add channel dimension\n\n    # Create a CSV colormap file with scores included\n    csv_path = os.path.splitext(output_path)[0] + \"_colormap.csv\"\n    with open(csv_path, \"w\", newline=\"\") as csvfile:\n        fieldnames = [\"ClassValue\", \"ClassName\", \"ConfidenceScore\"]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for label, value in label_to_value.items():\n            writer.writerow(\n                {\n                    \"ClassValue\": value,\n                    \"ClassName\": label,\n                    \"ConfidenceScore\": f\"{label_to_score[label]:.4f}\",\n                }\n            )\n\n    return output_path, label_to_value, label_to_score\n</code></pre>"},{"location":"hf/#geoai.hf.mask_generation","title":"<code>mask_generation(input_path, output_mask_path, output_csv_path, model='facebook/sam-vit-base', confidence_threshold=0.5, points_per_side=32, crop_size=None, batch_size=1, band_indices=None, min_object_size=0, generator_kwargs=None, **kwargs)</code>","text":"<p>Process a GeoTIFF using SAM mask generation and save results as a GeoTIFF and CSV.</p> <p>The function reads a GeoTIFF image, applies the SAM mask generator from the Hugging Face transformers pipeline, rasterizes the resulting masks to create a labeled mask GeoTIFF, and saves mask scores and geometries to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input GeoTIFF image.</p> required <code>output_mask_path</code> <code>str</code> <p>Path where the output mask GeoTIFF will be saved.</p> required <code>output_csv_path</code> <code>str</code> <p>Path where the mask scores CSV will be saved.</p> required <code>model</code> <code>str</code> <p>HuggingFace model checkpoint for the SAM model.</p> <code>'facebook/sam-vit-base'</code> <code>confidence_threshold</code> <code>float</code> <p>Minimum confidence score for masks to be included.</p> <code>0.5</code> <code>points_per_side</code> <code>int</code> <p>Number of points to sample along each side of the image.</p> <code>32</code> <code>crop_size</code> <code>Optional[int]</code> <p>Size of image crops for processing. If None, process the full image.</p> <code>None</code> <code>band_indices</code> <code>Optional[List[int]]</code> <p>List of band indices to use. If None, use all bands.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference.</p> <code>1</code> <code>min_object_size</code> <code>int</code> <p>Minimum size in pixels for objects to be included. Smaller masks will be filtered out.</p> <code>0</code> <code>generator_kwargs</code> <code>Optional[Dict]</code> <p>Additional keyword arguments to pass to the mask generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>Tuple containing the paths to the saved mask GeoTIFF and CSV file.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the input file cannot be opened or processed.</p> <code>RuntimeError</code> <p>If mask generation fails.</p> Source code in <code>geoai/hf.py</code> <pre><code>def mask_generation(\n    input_path: str,\n    output_mask_path: str,\n    output_csv_path: str,\n    model: str = \"facebook/sam-vit-base\",\n    confidence_threshold: float = 0.5,\n    points_per_side: int = 32,\n    crop_size: Optional[int] = None,\n    batch_size: int = 1,\n    band_indices: Optional[List[int]] = None,\n    min_object_size: int = 0,\n    generator_kwargs: Optional[Dict] = None,\n    **kwargs,\n) -&gt; Tuple[str, str]:\n    \"\"\"\n    Process a GeoTIFF using SAM mask generation and save results as a GeoTIFF and CSV.\n\n    The function reads a GeoTIFF image, applies the SAM mask generator from the\n    Hugging Face transformers pipeline, rasterizes the resulting masks to create\n    a labeled mask GeoTIFF, and saves mask scores and geometries to a CSV file.\n\n    Args:\n        input_path: Path to the input GeoTIFF image.\n        output_mask_path: Path where the output mask GeoTIFF will be saved.\n        output_csv_path: Path where the mask scores CSV will be saved.\n        model: HuggingFace model checkpoint for the SAM model.\n        confidence_threshold: Minimum confidence score for masks to be included.\n        points_per_side: Number of points to sample along each side of the image.\n        crop_size: Size of image crops for processing. If None, process the full image.\n        band_indices: List of band indices to use. If None, use all bands.\n        batch_size: Batch size for inference.\n        min_object_size: Minimum size in pixels for objects to be included. Smaller masks will be filtered out.\n        generator_kwargs: Additional keyword arguments to pass to the mask generator.\n\n    Returns:\n        Tuple containing the paths to the saved mask GeoTIFF and CSV file.\n\n    Raises:\n        ValueError: If the input file cannot be opened or processed.\n        RuntimeError: If mask generation fails.\n    \"\"\"\n    # Set up the mask generator\n    print(\"Setting up mask generator...\")\n    mask_generator = pipeline(model=model, task=\"mask-generation\", **kwargs)\n\n    # Open the GeoTIFF file\n    try:\n        print(f\"Reading input GeoTIFF: {input_path}\")\n        with rasterio.open(input_path) as src:\n            # Read metadata\n            profile = src.profile\n            # transform = src.transform\n            # crs = src.crs\n\n            # Read the image data\n            if band_indices is not None:\n                print(f\"Using specified bands: {band_indices}\")\n                image_data = np.stack([src.read(i + 1) for i in band_indices])\n            else:\n                print(\"Using all bands\")\n                image_data = src.read()\n\n            # Handle image with more than 3 bands (convert to RGB for visualization)\n            if image_data.shape[0] &gt; 3:\n                print(\n                    f\"Converting {image_data.shape[0]} bands to RGB (using first 3 bands)\"\n                )\n                # Select first three bands or perform other band combination\n                image_data = image_data[:3]\n            elif image_data.shape[0] == 1:\n                print(\"Duplicating single band to create 3-band image\")\n                # Duplicate single band to create a 3-band image\n                image_data = np.vstack([image_data] * 3)\n\n            # Transpose to HWC format for the model\n            image_data = np.transpose(image_data, (1, 2, 0))\n\n            # Normalize the image if needed\n            if image_data.dtype != np.uint8:\n                print(f\"Normalizing image from {image_data.dtype} to uint8\")\n                image_data = (image_data / image_data.max() * 255).astype(np.uint8)\n    except Exception as e:\n        raise ValueError(f\"Failed to open or process input GeoTIFF: {e}\")\n\n    # Process the image with the mask generator\n    try:\n        # Convert numpy array to PIL Image for the pipeline\n        # Ensure the array is in the right format (HWC and uint8)\n        if image_data.dtype != np.uint8:\n            image_data = (image_data / image_data.max() * 255).astype(np.uint8)\n\n        # Create a PIL Image from the numpy array\n        print(\"Converting to PIL Image for mask generation\")\n        pil_image = Image.fromarray(image_data)\n\n        # Use the SAM pipeline for mask generation\n        if generator_kwargs is None:\n            generator_kwargs = {}\n\n        print(\"Running mask generation...\")\n        mask_results = mask_generator(\n            pil_image,\n            points_per_side=points_per_side,\n            crop_n_points_downscale_factor=1 if crop_size is None else 2,\n            point_grids=None,\n            pred_iou_thresh=confidence_threshold,\n            stability_score_thresh=confidence_threshold,\n            crops_n_layers=0 if crop_size is None else 1,\n            crop_overlap_ratio=0.5,\n            batch_size=batch_size,\n            **generator_kwargs,\n        )\n\n        print(\n            f\"Number of initial masks: {len(mask_results['masks']) if isinstance(mask_results, dict) and 'masks' in mask_results else len(mask_results)}\"\n        )\n\n    except Exception as e:\n        raise RuntimeError(f\"Mask generation failed: {e}\")\n\n    # Create a mask raster with unique IDs for each mask\n    mask_raster = np.zeros((image_data.shape[0], image_data.shape[1]), dtype=np.uint32)\n    mask_records = []\n\n    # Process each mask based on the structure of mask_results\n    if (\n        isinstance(mask_results, dict)\n        and \"masks\" in mask_results\n        and \"scores\" in mask_results\n    ):\n        # Handle dictionary with 'masks' and 'scores' lists\n        print(\"Processing masks...\")\n        total_masks = len(mask_results[\"masks\"])\n\n        # Create progress bar\n        for i, (mask_data, score) in enumerate(\n            tqdm(\n                zip(mask_results[\"masks\"], mask_results[\"scores\"]),\n                total=total_masks,\n                desc=\"Processing masks\",\n            )\n        ):\n            mask_id = i + 1  # Start IDs at 1\n\n            # Convert to numpy if not already\n            if not isinstance(mask_data, np.ndarray):\n                # Try to convert from tensor or other format if needed\n                try:\n                    mask_data = np.array(mask_data)\n                except:\n                    print(f\"Could not convert mask at index {i} to numpy array\")\n                    continue\n\n            mask_binary = mask_data.astype(bool)\n            area_pixels = np.sum(mask_binary)\n\n            # Skip if mask is smaller than the minimum size\n            if area_pixels &lt; min_object_size:\n                continue\n\n            # Add the mask to the raster with a unique ID\n            mask_raster[mask_binary] = mask_id\n\n            # Create a record for the CSV - without geometry calculation\n            mask_records.append(\n                {\"mask_id\": mask_id, \"score\": float(score), \"area_pixels\": area_pixels}\n            )\n    elif isinstance(mask_results, list):\n        # Handle list of dictionaries format (SAM original format)\n        print(\"Processing masks...\")\n        total_masks = len(mask_results)\n\n        # Create progress bar\n        for i, mask_result in enumerate(tqdm(mask_results, desc=\"Processing masks\")):\n            mask_id = i + 1  # Start IDs at 1\n\n            # Try different possible key names for masks and scores\n            mask_data = None\n            score = None\n\n            if isinstance(mask_result, dict):\n                # Try to find mask data\n                if \"segmentation\" in mask_result:\n                    mask_data = mask_result[\"segmentation\"]\n                elif \"mask\" in mask_result:\n                    mask_data = mask_result[\"mask\"]\n\n                # Try to find score\n                if \"score\" in mask_result:\n                    score = mask_result[\"score\"]\n                elif \"predicted_iou\" in mask_result:\n                    score = mask_result[\"predicted_iou\"]\n                elif \"stability_score\" in mask_result:\n                    score = mask_result[\"stability_score\"]\n                else:\n                    score = 1.0  # Default score if none found\n            else:\n                # If mask_result is not a dict, it might be the mask directly\n                try:\n                    mask_data = np.array(mask_result)\n                    score = 1.0  # Default score\n                except:\n                    print(f\"Could not process mask at index {i}\")\n                    continue\n\n            if mask_data is not None:\n                # Convert to numpy if not already\n                if not isinstance(mask_data, np.ndarray):\n                    try:\n                        mask_data = np.array(mask_data)\n                    except:\n                        print(f\"Could not convert mask at index {i} to numpy array\")\n                        continue\n\n                mask_binary = mask_data.astype(bool)\n                area_pixels = np.sum(mask_binary)\n\n                # Skip if mask is smaller than the minimum size\n                if area_pixels &lt; min_object_size:\n                    continue\n\n                # Add the mask to the raster with a unique ID\n                mask_raster[mask_binary] = mask_id\n\n                # Create a record for the CSV - without geometry calculation\n                mask_records.append(\n                    {\n                        \"mask_id\": mask_id,\n                        \"score\": float(score),\n                        \"area_pixels\": area_pixels,\n                    }\n                )\n    else:\n        # If we couldn't figure out the format, raise an error\n        raise ValueError(f\"Unexpected format for mask_results: {type(mask_results)}\")\n\n    print(f\"Number of final masks (after size filtering): {len(mask_records)}\")\n\n    # Save the mask raster as a GeoTIFF\n    print(f\"Saving mask GeoTIFF to {output_mask_path}\")\n    output_profile = profile.copy()\n    output_profile.update(dtype=rasterio.uint32, count=1, compress=\"lzw\", nodata=0)\n\n    with rasterio.open(output_mask_path, \"w\", **output_profile) as dst:\n        dst.write(mask_raster.astype(rasterio.uint32), 1)\n\n    # Save the mask data as a CSV\n    print(f\"Saving mask metadata to {output_csv_path}\")\n    mask_df = pd.DataFrame(mask_records)\n    mask_df.to_csv(output_csv_path, index=False)\n\n    print(\"Processing complete!\")\n    return output_mask_path, output_csv_path\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>This guide covers various methods for installing GeoAI on different platforms with different package managers.</p>"},{"location":"installation/#prerequisites","title":"\u2705 Prerequisites","text":"<p>GeoAI requires:</p> <ul> <li>Python 3.9 or above</li> <li>The required dependencies will be installed automatically</li> </ul>"},{"location":"installation/#recommended-installation-methods","title":"\ud83d\ude80 Recommended Installation Methods","text":""},{"location":"installation/#using-pip","title":"\ud83d\udc0d Using pip","text":"<p>The simplest way to install the latest stable release of GeoAI is via pip:</p> <pre><code>pip install geoai-py\n</code></pre> <p>To install GeoAI with all optional dependencies for additional features:</p> <pre><code>pip install \"geoai-py[all]\"\n</code></pre>"},{"location":"installation/#using-uv","title":"\ud83d\udc0d Using uv","text":"<p>To install the latest stable release of GeoAI with uv, a faster alternative to pip:</p> <pre><code>uv pip install geoai-py\n</code></pre>"},{"location":"installation/#using-conda","title":"\ud83d\udc3c Using conda","text":"<p>For Anaconda/Miniconda users, we recommend installing GeoAI via conda-forge, which handles dependencies like GDAL more elegantly:</p> <pre><code>conda install -c conda-forge geoai\n</code></pre>"},{"location":"installation/#using-mamba","title":"\ud83e\udda1 Using mamba","text":"<p>Mamba provides faster dependency resolution compared to conda. This is especially useful for large packages like GeoAI:</p> <pre><code>conda create -n geo python=3.12\nconda activate geo\nconda install -c conda-forge mamba\nmamba install -c conda-forge geoai\n</code></pre>"},{"location":"installation/#advanced-installation-options","title":"\ud83d\udd27 Advanced Installation Options","text":""},{"location":"installation/#gpu-support","title":"\ud83d\udda5\ufe0f GPU Support","text":"<p>To enable GPU acceleration for deep learning models (requires NVIDIA GPU):</p> <pre><code>mamba install -c conda-forge geoai \"pytorch=*=cuda*\"\n</code></pre> <p>This will install the appropriate PyTorch version with CUDA support.</p> <p>If you run into issues with the ipympl package, you can install it using the following command:</p> <pre><code>mamba install -c conda-forge geoai \"pytorch=*=cuda*\" jupyterlab ipympl\n</code></pre> <p>If you encounter issues with the sqlite package, you can update it using the following command:</p> <pre><code>mamba update -c conda-forge sqlite\n</code></pre>"},{"location":"installation/#notes-for-windows-users","title":"Notes for Windows Users","text":"<p>If you use mamba to install geoai, you may not have the latest version of torchgeo, which may cause issues when importing geoai. To fix this, you can install the latest version of torchgeo using the following command:</p> <pre><code>pip install -U torchgeo\n</code></pre>"},{"location":"installation/#development-installation","title":"\ud83d\udc69\u200d\ud83d\udcbb Development Installation","text":"<p>For contributing to GeoAI development, install directly from the source repository:</p> <pre><code>git clone https://github.com/opengeos/geoai.git\ncd geoai\npip install -e .\n</code></pre> <p>The <code>-e</code> flag installs the package in development mode, allowing you to modify the code and immediately see the effects.</p>"},{"location":"installation/#installing-from-github","title":"\ud83d\udce6 Installing from GitHub","text":"<p>To install the latest development version directly from GitHub:</p> <pre><code>pip install git+https://github.com/opengeos/geoai.git\n</code></pre> <p>For a specific branch:</p> <pre><code>pip install git+https://github.com/opengeos/geoai.git@branch-name\n</code></pre>"},{"location":"installation/#verifying-installation","title":"\u2713 Verifying Installation","text":"<p>To verify your installation, run:</p> <pre><code>import geoai\nprint(geoai.__version__)\n</code></pre>"},{"location":"installation/#troubleshooting","title":"\u26a0\ufe0f Troubleshooting","text":"<p>If you encounter installation problems:</p> <ol> <li>Check the FAQ section of our documentation</li> <li>Search for similar issues in our GitHub Issues</li> <li>Ask for help in our GitHub Discussions</li> </ol>"},{"location":"installation/#upgrading","title":"\ud83d\udd04 Upgrading","text":"<p>To upgrade GeoAI to the latest version:</p> <pre><code>pip install -U geoai-py\n</code></pre> <p>Or with conda:</p> <pre><code>conda update -c conda-forge geoai\n</code></pre>"},{"location":"segment/","title":"segment module","text":"<p>This module provides functionality for segmenting high-resolution satellite imagery using vision-language models.</p>"},{"location":"segment/#geoai.segment.CLIPSegmentation","title":"<code> CLIPSegmentation        </code>","text":"<p>A class for segmenting high-resolution satellite imagery using text prompts with CLIP-based models.</p> <p>This segmenter utilizes the CLIP-Seg model to perform semantic segmentation based on text prompts. It can process large GeoTIFF files by tiling them and handles proper georeferencing in the output.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CLIP-Seg model to use. Defaults to \"CIDAS/clipseg-rd64-refined\".</p> <code>'CIDAS/clipseg-rd64-refined'</code> <code>device</code> <code>str</code> <p>Device to run the model on ('cuda', 'cpu'). If None, will use CUDA if available.</p> <code>None</code> <code>tile_size</code> <code>int</code> <p>Size of tiles to process the image in chunks. Defaults to 352.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between tiles to avoid edge artifacts. Defaults to 16.</p> <code>32</code> <p>Attributes:</p> Name Type Description <code>processor</code> <code>CLIPSegProcessor</code> <p>The processor for the CLIP-Seg model.</p> <code>model</code> <code>CLIPSegForImageSegmentation</code> <p>The CLIP-Seg model for segmentation.</p> <code>device</code> <code>str</code> <p>The device being used ('cuda' or 'cpu').</p> <code>tile_size</code> <code>int</code> <p>Size of tiles for processing.</p> <code>overlap</code> <code>int</code> <p>Overlap between tiles.</p> Source code in <code>geoai/segment.py</code> <pre><code>class CLIPSegmentation:\n    \"\"\"\n    A class for segmenting high-resolution satellite imagery using text prompts with CLIP-based models.\n\n    This segmenter utilizes the CLIP-Seg model to perform semantic segmentation based on text prompts.\n    It can process large GeoTIFF files by tiling them and handles proper georeferencing in the output.\n\n    Args:\n        model_name (str): Name of the CLIP-Seg model to use. Defaults to \"CIDAS/clipseg-rd64-refined\".\n        device (str): Device to run the model on ('cuda', 'cpu'). If None, will use CUDA if available.\n        tile_size (int): Size of tiles to process the image in chunks. Defaults to 352.\n        overlap (int): Overlap between tiles to avoid edge artifacts. Defaults to 16.\n\n    Attributes:\n        processor (CLIPSegProcessor): The processor for the CLIP-Seg model.\n        model (CLIPSegForImageSegmentation): The CLIP-Seg model for segmentation.\n        device (str): The device being used ('cuda' or 'cpu').\n        tile_size (int): Size of tiles for processing.\n        overlap (int): Overlap between tiles.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name=\"CIDAS/clipseg-rd64-refined\",\n        device=None,\n        tile_size=512,\n        overlap=32,\n    ):\n        \"\"\"\n        Initialize the ImageSegmenter with the specified model and settings.\n\n        Args:\n            model_name (str): Name of the CLIP-Seg model to use. Defaults to \"CIDAS/clipseg-rd64-refined\".\n            device (str): Device to run the model on ('cuda', 'cpu'). If None, will use CUDA if available.\n            tile_size (int): Size of tiles to process the image in chunks. Defaults to 512.\n            overlap (int): Overlap between tiles to avoid edge artifacts. Defaults to 32.\n        \"\"\"\n        self.tile_size = tile_size\n        self.overlap = overlap\n\n        # Set device\n        if device is None:\n            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        else:\n            self.device = device\n\n        # Load model and processor\n        self.processor = CLIPSegProcessor.from_pretrained(model_name)\n        self.model = CLIPSegForImageSegmentation.from_pretrained(model_name).to(\n            self.device\n        )\n\n        print(f\"Model loaded on {self.device}\")\n\n    def segment_image(\n        self, input_path, output_path, text_prompt, threshold=0.5, smoothing_sigma=1.0\n    ):\n        \"\"\"\n        Segment a GeoTIFF image using the provided text prompt.\n\n        The function processes the image in tiles and saves the result as a GeoTIFF with two bands:\n        - Band 1: Binary segmentation mask (0 or 1)\n        - Band 2: Probability scores (0.0 to 1.0)\n\n        Args:\n            input_path (str): Path to the input GeoTIFF file.\n            output_path (str): Path where the output GeoTIFF will be saved.\n            text_prompt (str): Text description of what to segment (e.g., \"water\", \"buildings\").\n            threshold (float): Threshold for binary segmentation (0.0 to 1.0). Defaults to 0.5.\n            smoothing_sigma (float): Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.\n\n        Returns:\n            str: Path to the saved output file.\n        \"\"\"\n        # Open the input GeoTIFF\n        with rasterio.open(input_path) as src:\n            # Get metadata\n            meta = src.meta\n            height = src.height\n            width = src.width\n\n            # Create output metadata\n            out_meta = meta.copy()\n            out_meta.update({\"count\": 2, \"dtype\": \"float32\", \"nodata\": None})\n\n            # Create arrays for results\n            segmentation = np.zeros((height, width), dtype=np.float32)\n            probabilities = np.zeros((height, width), dtype=np.float32)\n\n            # Calculate effective tile size (accounting for overlap)\n            effective_tile_size = self.tile_size - 2 * self.overlap\n\n            # Calculate number of tiles\n            n_tiles_x = max(1, int(np.ceil(width / effective_tile_size)))\n            n_tiles_y = max(1, int(np.ceil(height / effective_tile_size)))\n            total_tiles = n_tiles_x * n_tiles_y\n\n            # Process tiles with tqdm progress bar\n            with tqdm(total=total_tiles, desc=\"Processing tiles\") as pbar:\n                # Iterate through tiles\n                for y in range(n_tiles_y):\n                    for x in range(n_tiles_x):\n                        # Calculate tile coordinates with overlap\n                        x_start = max(0, x * effective_tile_size - self.overlap)\n                        y_start = max(0, y * effective_tile_size - self.overlap)\n                        x_end = min(width, (x + 1) * effective_tile_size + self.overlap)\n                        y_end = min(\n                            height, (y + 1) * effective_tile_size + self.overlap\n                        )\n\n                        tile_width = x_end - x_start\n                        tile_height = y_end - y_start\n\n                        # Read the tile\n                        window = Window(x_start, y_start, tile_width, tile_height)\n                        tile_data = src.read(window=window)\n\n                        # Process the tile\n                        try:\n                            # Convert to RGB if necessary (handling different satellite bands)\n                            if tile_data.shape[0] &gt; 3:\n                                # Use first three bands for RGB representation\n                                rgb_tile = tile_data[:3].transpose(1, 2, 0)\n                                # Normalize data to 0-255 range if needed\n                                if rgb_tile.max() &gt; 0:\n                                    rgb_tile = (\n                                        (rgb_tile - rgb_tile.min())\n                                        / (rgb_tile.max() - rgb_tile.min())\n                                        * 255\n                                    ).astype(np.uint8)\n                            elif tile_data.shape[0] == 1:\n                                # Create RGB from grayscale\n                                rgb_tile = np.repeat(\n                                    tile_data[0][:, :, np.newaxis], 3, axis=2\n                                )\n                                # Normalize if needed\n                                if rgb_tile.max() &gt; 0:\n                                    rgb_tile = (\n                                        (rgb_tile - rgb_tile.min())\n                                        / (rgb_tile.max() - rgb_tile.min())\n                                        * 255\n                                    ).astype(np.uint8)\n                            else:\n                                # Already 3-channel, assume RGB\n                                rgb_tile = tile_data.transpose(1, 2, 0)\n                                # Normalize if needed\n                                if rgb_tile.max() &gt; 0:\n                                    rgb_tile = (\n                                        (rgb_tile - rgb_tile.min())\n                                        / (rgb_tile.max() - rgb_tile.min())\n                                        * 255\n                                    ).astype(np.uint8)\n\n                            # Convert to PIL Image\n                            pil_image = Image.fromarray(rgb_tile)\n\n                            # Resize if needed to match model's requirements\n                            if (\n                                pil_image.width &gt; self.tile_size\n                                or pil_image.height &gt; self.tile_size\n                            ):\n                                # Keep aspect ratio\n                                pil_image.thumbnail(\n                                    (self.tile_size, self.tile_size), Image.LANCZOS\n                                )\n\n                            # Process with CLIP-Seg\n                            inputs = self.processor(\n                                text=text_prompt, images=pil_image, return_tensors=\"pt\"\n                            ).to(self.device)\n\n                            # Forward pass\n                            with torch.no_grad():\n                                outputs = self.model(**inputs)\n\n                            # Get logits and resize to original tile size\n                            logits = outputs.logits[0]\n\n                            # Convert logits to probabilities with sigmoid\n                            probs = torch.sigmoid(logits).cpu().numpy()\n\n                            # Resize back to original tile size if needed\n                            if probs.shape != (tile_height, tile_width):\n                                # Use bicubic interpolation for smoother results\n                                probs_resized = np.array(\n                                    Image.fromarray(probs).resize(\n                                        (tile_width, tile_height), Image.BICUBIC\n                                    )\n                                )\n                            else:\n                                probs_resized = probs\n\n                            # Apply gaussian blur to reduce blockiness\n                            try:\n                                from scipy.ndimage import gaussian_filter\n\n                                probs_resized = gaussian_filter(\n                                    probs_resized, sigma=smoothing_sigma\n                                )\n                            except ImportError:\n                                pass  # Continue without smoothing if scipy is not available\n\n                            # Store results in the full arrays\n                            # Only store the non-overlapping part (except at edges)\n                            valid_x_start = self.overlap if x &gt; 0 else 0\n                            valid_y_start = self.overlap if y &gt; 0 else 0\n                            valid_x_end = (\n                                tile_width - self.overlap\n                                if x &lt; n_tiles_x - 1\n                                else tile_width\n                            )\n                            valid_y_end = (\n                                tile_height - self.overlap\n                                if y &lt; n_tiles_y - 1\n                                else tile_height\n                            )\n\n                            dest_x_start = x_start + valid_x_start\n                            dest_y_start = y_start + valid_y_start\n                            dest_x_end = x_start + valid_x_end\n                            dest_y_end = y_start + valid_y_end\n\n                            # Store probabilities\n                            probabilities[\n                                dest_y_start:dest_y_end, dest_x_start:dest_x_end\n                            ] = probs_resized[\n                                valid_y_start:valid_y_end, valid_x_start:valid_x_end\n                            ]\n\n                        except Exception as e:\n                            print(f\"Error processing tile at ({x}, {y}): {str(e)}\")\n                            # Continue with next tile\n\n                        # Update progress bar\n                        pbar.update(1)\n\n            # Create binary segmentation from probabilities\n            segmentation = (probabilities &gt;= threshold).astype(np.float32)\n\n            # Write the output GeoTIFF\n            with rasterio.open(output_path, \"w\", **out_meta) as dst:\n                dst.write(segmentation, 1)\n                dst.write(probabilities, 2)\n\n                # Add descriptions to bands\n                dst.set_band_description(1, \"Binary Segmentation\")\n                dst.set_band_description(2, \"Probability Scores\")\n\n            print(f\"Segmentation saved to {output_path}\")\n            return output_path\n\n    def segment_image_batch(\n        self,\n        input_paths,\n        output_dir,\n        text_prompt,\n        threshold=0.5,\n        smoothing_sigma=1.0,\n        suffix=\"_segmented\",\n    ):\n        \"\"\"\n        Segment multiple GeoTIFF images using the provided text prompt.\n\n        Args:\n            input_paths (list): List of paths to input GeoTIFF files.\n            output_dir (str): Directory where output GeoTIFFs will be saved.\n            text_prompt (str): Text description of what to segment.\n            threshold (float): Threshold for binary segmentation. Defaults to 0.5.\n            smoothing_sigma (float): Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.\n            suffix (str): Suffix to add to output filenames. Defaults to \"_segmented\".\n\n        Returns:\n            list: Paths to all saved output files.\n        \"\"\"\n        # Create output directory if it doesn't exist\n        os.makedirs(output_dir, exist_ok=True)\n\n        output_paths = []\n\n        # Process each input file\n        for input_path in tqdm(input_paths, desc=\"Processing files\"):\n            # Generate output path\n            filename = os.path.basename(input_path)\n            base_name, ext = os.path.splitext(filename)\n            output_path = os.path.join(output_dir, f\"{base_name}{suffix}{ext}\")\n\n            # Segment the image\n            result_path = self.segment_image(\n                input_path, output_path, text_prompt, threshold, smoothing_sigma\n            )\n            output_paths.append(result_path)\n\n        return output_paths\n</code></pre>"},{"location":"segment/#geoai.segment.CLIPSegmentation.__init__","title":"<code>__init__(self, model_name='CIDAS/clipseg-rd64-refined', device=None, tile_size=512, overlap=32)</code>  <code>special</code>","text":"<p>Initialize the ImageSegmenter with the specified model and settings.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CLIP-Seg model to use. Defaults to \"CIDAS/clipseg-rd64-refined\".</p> <code>'CIDAS/clipseg-rd64-refined'</code> <code>device</code> <code>str</code> <p>Device to run the model on ('cuda', 'cpu'). If None, will use CUDA if available.</p> <code>None</code> <code>tile_size</code> <code>int</code> <p>Size of tiles to process the image in chunks. Defaults to 512.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between tiles to avoid edge artifacts. Defaults to 32.</p> <code>32</code> Source code in <code>geoai/segment.py</code> <pre><code>def __init__(\n    self,\n    model_name=\"CIDAS/clipseg-rd64-refined\",\n    device=None,\n    tile_size=512,\n    overlap=32,\n):\n    \"\"\"\n    Initialize the ImageSegmenter with the specified model and settings.\n\n    Args:\n        model_name (str): Name of the CLIP-Seg model to use. Defaults to \"CIDAS/clipseg-rd64-refined\".\n        device (str): Device to run the model on ('cuda', 'cpu'). If None, will use CUDA if available.\n        tile_size (int): Size of tiles to process the image in chunks. Defaults to 512.\n        overlap (int): Overlap between tiles to avoid edge artifacts. Defaults to 32.\n    \"\"\"\n    self.tile_size = tile_size\n    self.overlap = overlap\n\n    # Set device\n    if device is None:\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    else:\n        self.device = device\n\n    # Load model and processor\n    self.processor = CLIPSegProcessor.from_pretrained(model_name)\n    self.model = CLIPSegForImageSegmentation.from_pretrained(model_name).to(\n        self.device\n    )\n\n    print(f\"Model loaded on {self.device}\")\n</code></pre>"},{"location":"segment/#geoai.segment.CLIPSegmentation.segment_image","title":"<code>segment_image(self, input_path, output_path, text_prompt, threshold=0.5, smoothing_sigma=1.0)</code>","text":"<p>Segment a GeoTIFF image using the provided text prompt.</p> <p>The function processes the image in tiles and saves the result as a GeoTIFF with two bands: - Band 1: Binary segmentation mask (0 or 1) - Band 2: Probability scores (0.0 to 1.0)</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>Path where the output GeoTIFF will be saved.</p> required <code>text_prompt</code> <code>str</code> <p>Text description of what to segment (e.g., \"water\", \"buildings\").</p> required <code>threshold</code> <code>float</code> <p>Threshold for binary segmentation (0.0 to 1.0). Defaults to 0.5.</p> <code>0.5</code> <code>smoothing_sigma</code> <code>float</code> <p>Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the saved output file.</p> Source code in <code>geoai/segment.py</code> <pre><code>def segment_image(\n    self, input_path, output_path, text_prompt, threshold=0.5, smoothing_sigma=1.0\n):\n    \"\"\"\n    Segment a GeoTIFF image using the provided text prompt.\n\n    The function processes the image in tiles and saves the result as a GeoTIFF with two bands:\n    - Band 1: Binary segmentation mask (0 or 1)\n    - Band 2: Probability scores (0.0 to 1.0)\n\n    Args:\n        input_path (str): Path to the input GeoTIFF file.\n        output_path (str): Path where the output GeoTIFF will be saved.\n        text_prompt (str): Text description of what to segment (e.g., \"water\", \"buildings\").\n        threshold (float): Threshold for binary segmentation (0.0 to 1.0). Defaults to 0.5.\n        smoothing_sigma (float): Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.\n\n    Returns:\n        str: Path to the saved output file.\n    \"\"\"\n    # Open the input GeoTIFF\n    with rasterio.open(input_path) as src:\n        # Get metadata\n        meta = src.meta\n        height = src.height\n        width = src.width\n\n        # Create output metadata\n        out_meta = meta.copy()\n        out_meta.update({\"count\": 2, \"dtype\": \"float32\", \"nodata\": None})\n\n        # Create arrays for results\n        segmentation = np.zeros((height, width), dtype=np.float32)\n        probabilities = np.zeros((height, width), dtype=np.float32)\n\n        # Calculate effective tile size (accounting for overlap)\n        effective_tile_size = self.tile_size - 2 * self.overlap\n\n        # Calculate number of tiles\n        n_tiles_x = max(1, int(np.ceil(width / effective_tile_size)))\n        n_tiles_y = max(1, int(np.ceil(height / effective_tile_size)))\n        total_tiles = n_tiles_x * n_tiles_y\n\n        # Process tiles with tqdm progress bar\n        with tqdm(total=total_tiles, desc=\"Processing tiles\") as pbar:\n            # Iterate through tiles\n            for y in range(n_tiles_y):\n                for x in range(n_tiles_x):\n                    # Calculate tile coordinates with overlap\n                    x_start = max(0, x * effective_tile_size - self.overlap)\n                    y_start = max(0, y * effective_tile_size - self.overlap)\n                    x_end = min(width, (x + 1) * effective_tile_size + self.overlap)\n                    y_end = min(\n                        height, (y + 1) * effective_tile_size + self.overlap\n                    )\n\n                    tile_width = x_end - x_start\n                    tile_height = y_end - y_start\n\n                    # Read the tile\n                    window = Window(x_start, y_start, tile_width, tile_height)\n                    tile_data = src.read(window=window)\n\n                    # Process the tile\n                    try:\n                        # Convert to RGB if necessary (handling different satellite bands)\n                        if tile_data.shape[0] &gt; 3:\n                            # Use first three bands for RGB representation\n                            rgb_tile = tile_data[:3].transpose(1, 2, 0)\n                            # Normalize data to 0-255 range if needed\n                            if rgb_tile.max() &gt; 0:\n                                rgb_tile = (\n                                    (rgb_tile - rgb_tile.min())\n                                    / (rgb_tile.max() - rgb_tile.min())\n                                    * 255\n                                ).astype(np.uint8)\n                        elif tile_data.shape[0] == 1:\n                            # Create RGB from grayscale\n                            rgb_tile = np.repeat(\n                                tile_data[0][:, :, np.newaxis], 3, axis=2\n                            )\n                            # Normalize if needed\n                            if rgb_tile.max() &gt; 0:\n                                rgb_tile = (\n                                    (rgb_tile - rgb_tile.min())\n                                    / (rgb_tile.max() - rgb_tile.min())\n                                    * 255\n                                ).astype(np.uint8)\n                        else:\n                            # Already 3-channel, assume RGB\n                            rgb_tile = tile_data.transpose(1, 2, 0)\n                            # Normalize if needed\n                            if rgb_tile.max() &gt; 0:\n                                rgb_tile = (\n                                    (rgb_tile - rgb_tile.min())\n                                    / (rgb_tile.max() - rgb_tile.min())\n                                    * 255\n                                ).astype(np.uint8)\n\n                        # Convert to PIL Image\n                        pil_image = Image.fromarray(rgb_tile)\n\n                        # Resize if needed to match model's requirements\n                        if (\n                            pil_image.width &gt; self.tile_size\n                            or pil_image.height &gt; self.tile_size\n                        ):\n                            # Keep aspect ratio\n                            pil_image.thumbnail(\n                                (self.tile_size, self.tile_size), Image.LANCZOS\n                            )\n\n                        # Process with CLIP-Seg\n                        inputs = self.processor(\n                            text=text_prompt, images=pil_image, return_tensors=\"pt\"\n                        ).to(self.device)\n\n                        # Forward pass\n                        with torch.no_grad():\n                            outputs = self.model(**inputs)\n\n                        # Get logits and resize to original tile size\n                        logits = outputs.logits[0]\n\n                        # Convert logits to probabilities with sigmoid\n                        probs = torch.sigmoid(logits).cpu().numpy()\n\n                        # Resize back to original tile size if needed\n                        if probs.shape != (tile_height, tile_width):\n                            # Use bicubic interpolation for smoother results\n                            probs_resized = np.array(\n                                Image.fromarray(probs).resize(\n                                    (tile_width, tile_height), Image.BICUBIC\n                                )\n                            )\n                        else:\n                            probs_resized = probs\n\n                        # Apply gaussian blur to reduce blockiness\n                        try:\n                            from scipy.ndimage import gaussian_filter\n\n                            probs_resized = gaussian_filter(\n                                probs_resized, sigma=smoothing_sigma\n                            )\n                        except ImportError:\n                            pass  # Continue without smoothing if scipy is not available\n\n                        # Store results in the full arrays\n                        # Only store the non-overlapping part (except at edges)\n                        valid_x_start = self.overlap if x &gt; 0 else 0\n                        valid_y_start = self.overlap if y &gt; 0 else 0\n                        valid_x_end = (\n                            tile_width - self.overlap\n                            if x &lt; n_tiles_x - 1\n                            else tile_width\n                        )\n                        valid_y_end = (\n                            tile_height - self.overlap\n                            if y &lt; n_tiles_y - 1\n                            else tile_height\n                        )\n\n                        dest_x_start = x_start + valid_x_start\n                        dest_y_start = y_start + valid_y_start\n                        dest_x_end = x_start + valid_x_end\n                        dest_y_end = y_start + valid_y_end\n\n                        # Store probabilities\n                        probabilities[\n                            dest_y_start:dest_y_end, dest_x_start:dest_x_end\n                        ] = probs_resized[\n                            valid_y_start:valid_y_end, valid_x_start:valid_x_end\n                        ]\n\n                    except Exception as e:\n                        print(f\"Error processing tile at ({x}, {y}): {str(e)}\")\n                        # Continue with next tile\n\n                    # Update progress bar\n                    pbar.update(1)\n\n        # Create binary segmentation from probabilities\n        segmentation = (probabilities &gt;= threshold).astype(np.float32)\n\n        # Write the output GeoTIFF\n        with rasterio.open(output_path, \"w\", **out_meta) as dst:\n            dst.write(segmentation, 1)\n            dst.write(probabilities, 2)\n\n            # Add descriptions to bands\n            dst.set_band_description(1, \"Binary Segmentation\")\n            dst.set_band_description(2, \"Probability Scores\")\n\n        print(f\"Segmentation saved to {output_path}\")\n        return output_path\n</code></pre>"},{"location":"segment/#geoai.segment.CLIPSegmentation.segment_image_batch","title":"<code>segment_image_batch(self, input_paths, output_dir, text_prompt, threshold=0.5, smoothing_sigma=1.0, suffix='_segmented')</code>","text":"<p>Segment multiple GeoTIFF images using the provided text prompt.</p> <p>Parameters:</p> Name Type Description Default <code>input_paths</code> <code>list</code> <p>List of paths to input GeoTIFF files.</p> required <code>output_dir</code> <code>str</code> <p>Directory where output GeoTIFFs will be saved.</p> required <code>text_prompt</code> <code>str</code> <p>Text description of what to segment.</p> required <code>threshold</code> <code>float</code> <p>Threshold for binary segmentation. Defaults to 0.5.</p> <code>0.5</code> <code>smoothing_sigma</code> <code>float</code> <p>Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.</p> <code>1.0</code> <code>suffix</code> <code>str</code> <p>Suffix to add to output filenames. Defaults to \"_segmented\".</p> <code>'_segmented'</code> <p>Returns:</p> Type Description <code>list</code> <p>Paths to all saved output files.</p> Source code in <code>geoai/segment.py</code> <pre><code>def segment_image_batch(\n    self,\n    input_paths,\n    output_dir,\n    text_prompt,\n    threshold=0.5,\n    smoothing_sigma=1.0,\n    suffix=\"_segmented\",\n):\n    \"\"\"\n    Segment multiple GeoTIFF images using the provided text prompt.\n\n    Args:\n        input_paths (list): List of paths to input GeoTIFF files.\n        output_dir (str): Directory where output GeoTIFFs will be saved.\n        text_prompt (str): Text description of what to segment.\n        threshold (float): Threshold for binary segmentation. Defaults to 0.5.\n        smoothing_sigma (float): Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.\n        suffix (str): Suffix to add to output filenames. Defaults to \"_segmented\".\n\n    Returns:\n        list: Paths to all saved output files.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    output_paths = []\n\n    # Process each input file\n    for input_path in tqdm(input_paths, desc=\"Processing files\"):\n        # Generate output path\n        filename = os.path.basename(input_path)\n        base_name, ext = os.path.splitext(filename)\n        output_path = os.path.join(output_dir, f\"{base_name}{suffix}{ext}\")\n\n        # Segment the image\n        result_path = self.segment_image(\n            input_path, output_path, text_prompt, threshold, smoothing_sigma\n        )\n        output_paths.append(result_path)\n\n    return output_paths\n</code></pre>"},{"location":"segmentation/","title":"segmentation module","text":""},{"location":"segmentation/#geoai.segmentation.CustomDataset","title":"<code> CustomDataset            (Dataset)         </code>","text":"<p>Custom Dataset for loading images and masks.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>class CustomDataset(Dataset):\n    \"\"\"Custom Dataset for loading images and masks.\"\"\"\n\n    def __init__(\n        self,\n        images_dir: str,\n        masks_dir: str,\n        transform: A.Compose = None,\n        target_size: tuple = (256, 256),\n        num_classes: int = 2,\n    ):\n        \"\"\"\n        Args:\n            images_dir (str): Directory containing images.\n            masks_dir (str): Directory containing masks.\n            transform (A.Compose, optional): Transformations to be applied on the images and masks.\n            target_size (tuple, optional): Target size for resizing images and masks.\n            num_classes (int, optional): Number of classes in the masks.\n        \"\"\"\n        self.images_dir = images_dir\n        self.masks_dir = masks_dir\n        self.transform = transform\n        self.target_size = target_size\n        self.num_classes = num_classes\n        self.images = sorted(os.listdir(images_dir))\n        self.masks = sorted(os.listdir(masks_dir))\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the total number of samples.\"\"\"\n        return len(self.images)\n\n    def __getitem__(self, idx: int) -&gt; dict:\n        \"\"\"\n        Args:\n            idx (int): Index of the sample to fetch.\n\n        Returns:\n            dict: A dictionary with 'pixel_values' and 'labels'.\n        \"\"\"\n        img_path = os.path.join(self.images_dir, self.images[idx])\n        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n        image = Image.open(img_path).convert(\"RGB\")\n        mask = Image.open(mask_path).convert(\"L\")\n\n        image = image.resize(self.target_size)\n        mask = mask.resize(self.target_size)\n\n        image = np.array(image)\n        mask = np.array(mask)\n\n        mask = (mask &gt; 127).astype(np.uint8)\n\n        if self.transform:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed[\"image\"]\n            mask = transformed[\"mask\"]\n\n        assert (\n            mask.max() &lt; self.num_classes\n        ), f\"Mask values should be less than {self.num_classes}, but found {mask.max()}\"\n        assert (\n            mask.min() &gt;= 0\n        ), f\"Mask values should be greater than or equal to 0, but found {mask.min()}\"\n\n        mask = mask.clone().detach().long()\n\n        return {\"pixel_values\": image, \"labels\": mask}\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.CustomDataset.__getitem__","title":"<code>__getitem__(self, idx)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the sample to fetch.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with 'pixel_values' and 'labels'.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; dict:\n    \"\"\"\n    Args:\n        idx (int): Index of the sample to fetch.\n\n    Returns:\n        dict: A dictionary with 'pixel_values' and 'labels'.\n    \"\"\"\n    img_path = os.path.join(self.images_dir, self.images[idx])\n    mask_path = os.path.join(self.masks_dir, self.masks[idx])\n    image = Image.open(img_path).convert(\"RGB\")\n    mask = Image.open(mask_path).convert(\"L\")\n\n    image = image.resize(self.target_size)\n    mask = mask.resize(self.target_size)\n\n    image = np.array(image)\n    mask = np.array(mask)\n\n    mask = (mask &gt; 127).astype(np.uint8)\n\n    if self.transform:\n        transformed = self.transform(image=image, mask=mask)\n        image = transformed[\"image\"]\n        mask = transformed[\"mask\"]\n\n    assert (\n        mask.max() &lt; self.num_classes\n    ), f\"Mask values should be less than {self.num_classes}, but found {mask.max()}\"\n    assert (\n        mask.min() &gt;= 0\n    ), f\"Mask values should be greater than or equal to 0, but found {mask.min()}\"\n\n    mask = mask.clone().detach().long()\n\n    return {\"pixel_values\": image, \"labels\": mask}\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.CustomDataset.__init__","title":"<code>__init__(self, images_dir, masks_dir, transform=None, target_size=(256, 256), num_classes=2)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Directory containing images.</p> required <code>masks_dir</code> <code>str</code> <p>Directory containing masks.</p> required <code>transform</code> <code>A.Compose</code> <p>Transformations to be applied on the images and masks.</p> <code>None</code> <code>target_size</code> <code>tuple</code> <p>Target size for resizing images and masks.</p> <code>(256, 256)</code> <code>num_classes</code> <code>int</code> <p>Number of classes in the masks.</p> <code>2</code> Source code in <code>geoai/segmentation.py</code> <pre><code>def __init__(\n    self,\n    images_dir: str,\n    masks_dir: str,\n    transform: A.Compose = None,\n    target_size: tuple = (256, 256),\n    num_classes: int = 2,\n):\n    \"\"\"\n    Args:\n        images_dir (str): Directory containing images.\n        masks_dir (str): Directory containing masks.\n        transform (A.Compose, optional): Transformations to be applied on the images and masks.\n        target_size (tuple, optional): Target size for resizing images and masks.\n        num_classes (int, optional): Number of classes in the masks.\n    \"\"\"\n    self.images_dir = images_dir\n    self.masks_dir = masks_dir\n    self.transform = transform\n    self.target_size = target_size\n    self.num_classes = num_classes\n    self.images = sorted(os.listdir(images_dir))\n    self.masks = sorted(os.listdir(masks_dir))\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.CustomDataset.__len__","title":"<code>__len__(self)</code>  <code>special</code>","text":"<p>Returns the total number of samples.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the total number of samples.\"\"\"\n    return len(self.images)\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.get_transform","title":"<code>get_transform()</code>","text":"<p>Returns:</p> Type Description <code>A.Compose</code> <p>A composition of image transformations.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def get_transform() -&gt; A.Compose:\n    \"\"\"\n    Returns:\n        A.Compose: A composition of image transformations.\n    \"\"\"\n    return A.Compose(\n        [\n            A.Resize(256, 256),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.RandomRotate90(p=0.5),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            ToTensorV2(),\n        ]\n    )\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.load_model","title":"<code>load_model(model_path, device)</code>","text":"<p>Loads the fine-tuned model from the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model.</p> required <code>device</code> <code>torch.device</code> <p>Device to load the model on.</p> required <p>Returns:</p> Type Description <code>SegformerForSemanticSegmentation</code> <p>Loaded model.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def load_model(\n    model_path: str, device: torch.device\n) -&gt; SegformerForSemanticSegmentation:\n    \"\"\"\n    Loads the fine-tuned model from the specified path.\n\n    Args:\n        model_path (str): Path to the model.\n        device (torch.device): Device to load the model on.\n\n    Returns:\n        SegformerForSemanticSegmentation: Loaded model.\n    \"\"\"\n    model = SegformerForSemanticSegmentation.from_pretrained(model_path)\n    model.to(device)\n    model.eval()\n    return model\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.predict_image","title":"<code>predict_image(model, image_tensor, original_size, device)</code>","text":"<p>Predicts the segmentation mask for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>SegformerForSemanticSegmentation</code> <p>Fine-tuned model.</p> required <code>image_tensor</code> <code>torch.Tensor</code> <p>Preprocessed image tensor.</p> required <code>original_size</code> <code>tuple</code> <p>Original size of the image (width, height).</p> required <code>device</code> <code>torch.device</code> <p>Device to perform inference on.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Predicted segmentation mask.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def predict_image(\n    model: SegformerForSemanticSegmentation,\n    image_tensor: torch.Tensor,\n    original_size: tuple,\n    device: torch.device,\n) -&gt; np.ndarray:\n    \"\"\"\n    Predicts the segmentation mask for the input image.\n\n    Args:\n        model (SegformerForSemanticSegmentation): Fine-tuned model.\n        image_tensor (torch.Tensor): Preprocessed image tensor.\n        original_size (tuple): Original size of the image (width, height).\n        device (torch.device): Device to perform inference on.\n\n    Returns:\n        np.ndarray: Predicted segmentation mask.\n    \"\"\"\n    with torch.no_grad():\n        image_tensor = image_tensor.to(device)\n        outputs = model(pixel_values=image_tensor)\n        logits = outputs.logits\n        upsampled_logits = F.interpolate(\n            logits, size=original_size[::-1], mode=\"bilinear\", align_corners=False\n        )\n        predictions = torch.argmax(upsampled_logits, dim=1).cpu().numpy()\n    return predictions[0]\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.prepare_datasets","title":"<code>prepare_datasets(images_dir, masks_dir, transform, test_size=0.2, random_state=42)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Directory containing images.</p> required <code>masks_dir</code> <code>str</code> <p>Directory containing masks.</p> required <code>transform</code> <code>A.Compose</code> <p>Transformations to be applied.</p> required <code>test_size</code> <code>float</code> <p>Proportion of the dataset to include in the validation split.</p> <code>0.2</code> <code>random_state</code> <code>int</code> <p>Random seed for shuffling the dataset.</p> <code>42</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Training and validation datasets.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def prepare_datasets(\n    images_dir: str,\n    masks_dir: str,\n    transform: A.Compose,\n    test_size: float = 0.2,\n    random_state: int = 42,\n) -&gt; tuple:\n    \"\"\"\n    Args:\n        images_dir (str): Directory containing images.\n        masks_dir (str): Directory containing masks.\n        transform (A.Compose): Transformations to be applied.\n        test_size (float, optional): Proportion of the dataset to include in the validation split.\n        random_state (int, optional): Random seed for shuffling the dataset.\n\n    Returns:\n        tuple: Training and validation datasets.\n    \"\"\"\n    dataset = CustomDataset(images_dir, masks_dir, transform)\n    train_indices, val_indices = train_test_split(\n        list(range(len(dataset))), test_size=test_size, random_state=random_state\n    )\n    train_dataset = Subset(dataset, train_indices)\n    val_dataset = Subset(dataset, val_indices)\n    return train_dataset, val_dataset\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.preprocess_image","title":"<code>preprocess_image(image_path, target_size=(256, 256))</code>","text":"<p>Preprocesses the input image for prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the input image.</p> required <code>target_size</code> <code>tuple</code> <p>Target size for resizing the image.</p> <code>(256, 256)</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Preprocessed image tensor.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def preprocess_image(image_path: str, target_size: tuple = (256, 256)) -&gt; torch.Tensor:\n    \"\"\"\n    Preprocesses the input image for prediction.\n\n    Args:\n        image_path (str): Path to the input image.\n        target_size (tuple, optional): Target size for resizing the image.\n\n    Returns:\n        torch.Tensor: Preprocessed image tensor.\n    \"\"\"\n    image = Image.open(image_path).convert(\"RGB\")\n    transform = A.Compose(\n        [\n            A.Resize(target_size[0], target_size[1]),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            ToTensorV2(),\n        ]\n    )\n    image = np.array(image)\n    transformed = transform(image=image)\n    return transformed[\"image\"].unsqueeze(0)\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.segment_image","title":"<code>segment_image(image_path, model_path, target_size=(256, 256), device=device(type='cpu'))</code>","text":"<p>Segments the input image using the fine-tuned model.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the input image.</p> required <code>model_path</code> <code>str</code> <p>Path to the fine-tuned model.</p> required <code>target_size</code> <code>tuple</code> <p>Target size for resizing the image.</p> <code>(256, 256)</code> <code>device</code> <code>torch.device</code> <p>Device to perform inference on.</p> <code>device(type='cpu')</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Predicted segmentation mask.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def segment_image(\n    image_path: str,\n    model_path: str,\n    target_size: tuple = (256, 256),\n    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n) -&gt; np.ndarray:\n    \"\"\"\n    Segments the input image using the fine-tuned model.\n\n    Args:\n        image_path (str): Path to the input image.\n        model_path (str): Path to the fine-tuned model.\n        target_size (tuple, optional): Target size for resizing the image.\n        device (torch.device, optional): Device to perform inference on.\n\n    Returns:\n        np.ndarray: Predicted segmentation mask.\n    \"\"\"\n    model = load_model(model_path, device)\n    image = Image.open(image_path).convert(\"RGB\")\n    original_size = image.size\n    image_tensor = preprocess_image(image_path, target_size)\n    predictions = predict_image(model, image_tensor, original_size, device)\n    return predictions\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.train_model","title":"<code>train_model(train_dataset, val_dataset, pretrained_model='nvidia/segformer-b0-finetuned-ade-512-512', model_save_path='./model', output_dir='./results', num_epochs=10, batch_size=8, learning_rate=5e-05)</code>","text":"<p>Trains the model and saves the fine-tuned model to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataset</code> <code>Dataset</code> <p>Training dataset.</p> required <code>val_dataset</code> <code>Dataset</code> <p>Validation dataset.</p> required <code>pretrained_model</code> <code>str</code> <p>Pretrained model to fine-tune.</p> <code>'nvidia/segformer-b0-finetuned-ade-512-512'</code> <code>model_save_path</code> <code>str</code> <p>Path to save the fine-tuned model. Defaults to './model'.</p> <code>'./model'</code> <code>output_dir</code> <code>str</code> <p>Directory to save training outputs.</p> <code>'./results'</code> <code>num_epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>Batch size for training and evaluation.</p> <code>8</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for training.</p> <code>5e-05</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the saved fine-tuned model.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def train_model(\n    train_dataset: Dataset,\n    val_dataset: Dataset,\n    pretrained_model: str = \"nvidia/segformer-b0-finetuned-ade-512-512\",\n    model_save_path: str = \"./model\",\n    output_dir: str = \"./results\",\n    num_epochs: int = 10,\n    batch_size: int = 8,\n    learning_rate: float = 5e-5,\n) -&gt; str:\n    \"\"\"\n    Trains the model and saves the fine-tuned model to the specified path.\n\n    Args:\n        train_dataset (Dataset): Training dataset.\n        val_dataset (Dataset): Validation dataset.\n        pretrained_model (str, optional): Pretrained model to fine-tune.\n        model_save_path (str): Path to save the fine-tuned model. Defaults to './model'.\n        output_dir (str, optional): Directory to save training outputs.\n        num_epochs (int, optional): Number of training epochs.\n        batch_size (int, optional): Batch size for training and evaluation.\n        learning_rate (float, optional): Learning rate for training.\n\n    Returns:\n        str: Path to the saved fine-tuned model.\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = SegformerForSemanticSegmentation.from_pretrained(pretrained_model).to(\n        device\n    )\n    data_collator = DefaultDataCollator(return_tensors=\"pt\")\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=num_epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_dir=\"./logs\",\n        learning_rate=learning_rate,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n    )\n\n    trainer.train()\n    model.save_pretrained(model_save_path)\n    print(f\"Model saved to {model_save_path}\")\n    return model_save_path\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.visualize_predictions","title":"<code>visualize_predictions(image_path, segmented_mask, target_size=(256, 256), reference_image_path=None)</code>","text":"<p>Visualizes the original image, segmented mask, and optionally the reference image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the original image.</p> required <code>segmented_mask</code> <code>np.ndarray</code> <p>Predicted segmentation mask.</p> required <code>target_size</code> <code>tuple</code> <p>Target size for resizing images.</p> <code>(256, 256)</code> <code>reference_image_path</code> <code>str</code> <p>Path to the reference image.</p> <code>None</code> Source code in <code>geoai/segmentation.py</code> <pre><code>def visualize_predictions(\n    image_path: str,\n    segmented_mask: np.ndarray,\n    target_size: tuple = (256, 256),\n    reference_image_path: str = None,\n) -&gt; None:\n    \"\"\"\n    Visualizes the original image, segmented mask, and optionally the reference image.\n\n    Args:\n        image_path (str): Path to the original image.\n        segmented_mask (np.ndarray): Predicted segmentation mask.\n        target_size (tuple, optional): Target size for resizing images.\n        reference_image_path (str, optional): Path to the reference image.\n    \"\"\"\n    original_image = Image.open(image_path).convert(\"RGB\")\n    original_image = original_image.resize(target_size)\n    segmented_image = Image.fromarray((segmented_mask * 255).astype(np.uint8))\n\n    if reference_image_path:\n        reference_image = Image.open(reference_image_path).convert(\"RGB\")\n        reference_image = reference_image.resize(target_size)\n        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n        axes[1].imshow(reference_image)\n        axes[1].set_title(\"Reference Image\")\n        axes[1].axis(\"off\")\n    else:\n        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n    axes[0].imshow(original_image)\n    axes[0].set_title(\"Original Image\")\n    axes[0].axis(\"off\")\n\n    if reference_image_path:\n        axes[2].imshow(segmented_image, cmap=\"gray\")\n        axes[2].set_title(\"Segmented Image\")\n        axes[2].axis(\"off\")\n    else:\n        axes[1].imshow(segmented_image, cmap=\"gray\")\n        axes[1].set_title(\"Segmented Image\")\n        axes[1].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"train/","title":"train module","text":""},{"location":"train/#geoai.train.Compose","title":"<code> Compose        </code>","text":"<p>Custom compose transform that works with image and target.</p> Source code in <code>geoai/train.py</code> <pre><code>class Compose:\n    \"\"\"Custom compose transform that works with image and target.\"\"\"\n\n    def __init__(self, transforms):\n        \"\"\"\n        Initialize compose transform.\n\n        Args:\n            transforms (list): List of transforms to apply.\n        \"\"\"\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n</code></pre>"},{"location":"train/#geoai.train.Compose.__init__","title":"<code>__init__(self, transforms)</code>  <code>special</code>","text":"<p>Initialize compose transform.</p> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>list</code> <p>List of transforms to apply.</p> required Source code in <code>geoai/train.py</code> <pre><code>def __init__(self, transforms):\n    \"\"\"\n    Initialize compose transform.\n\n    Args:\n        transforms (list): List of transforms to apply.\n    \"\"\"\n    self.transforms = transforms\n</code></pre>"},{"location":"train/#geoai.train.ObjectDetectionDataset","title":"<code> ObjectDetectionDataset            (Dataset)         </code>","text":"<p>Dataset for object detection from GeoTIFF images and labels.</p> Source code in <code>geoai/train.py</code> <pre><code>class ObjectDetectionDataset(Dataset):\n    \"\"\"Dataset for object detection from GeoTIFF images and labels.\"\"\"\n\n    def __init__(self, image_paths, label_paths, transforms=None, num_channels=None):\n        \"\"\"\n        Initialize dataset.\n\n        Args:\n            image_paths (list): List of paths to image GeoTIFF files.\n            label_paths (list): List of paths to label GeoTIFF files.\n            transforms (callable, optional): Transformations to apply to images and masks.\n            num_channels (int, optional): Number of channels to use from images. If None,\n                auto-detected from the first image.\n        \"\"\"\n        self.image_paths = image_paths\n        self.label_paths = label_paths\n        self.transforms = transforms\n\n        # Auto-detect the number of channels if not specified\n        if num_channels is None:\n            with rasterio.open(self.image_paths[0]) as src:\n                self.num_channels = src.count\n        else:\n            self.num_channels = num_channels\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        # Load image\n        with rasterio.open(self.image_paths[idx]) as src:\n            # Read as [C, H, W] format\n            image = src.read().astype(np.float32)\n\n            # Normalize image to [0, 1] range\n            image = image / 255.0\n\n            # Handle different number of channels\n            if image.shape[0] &gt; self.num_channels:\n                image = image[\n                    : self.num_channels\n                ]  # Keep only first 4 bands if more exist\n            elif image.shape[0] &lt; self.num_channels:\n                # Pad with zeros if less than 4 bands\n                padded = np.zeros(\n                    (self.num_channels, image.shape[1], image.shape[2]),\n                    dtype=np.float32,\n                )\n                padded[: image.shape[0]] = image\n                image = padded\n\n            # Convert to CHW tensor\n            image = torch.as_tensor(image, dtype=torch.float32)\n\n        # Load label mask\n        with rasterio.open(self.label_paths[idx]) as src:\n            label_mask = src.read(1)\n            binary_mask = (label_mask &gt; 0).astype(np.uint8)\n\n        # Find all building instances using connected components\n        labeled_mask, num_instances = measure.label(\n            binary_mask, return_num=True, connectivity=2\n        )\n\n        # Create list to hold masks for each building instance\n        masks = []\n        boxes = []\n        labels = []\n\n        for i in range(1, num_instances + 1):\n            # Create mask for this instance\n            instance_mask = (labeled_mask == i).astype(np.uint8)\n\n            # Calculate area and filter out tiny instances (noise)\n            area = instance_mask.sum()\n            if area &lt; 10:  # Minimum area threshold\n                continue\n\n            # Find bounding box coordinates\n            pos = np.where(instance_mask)\n            if len(pos[0]) == 0:  # Skip if mask is empty\n                continue\n\n            xmin = np.min(pos[1])\n            xmax = np.max(pos[1])\n            ymin = np.min(pos[0])\n            ymax = np.max(pos[0])\n\n            # Skip invalid boxes\n            if xmax &lt;= xmin or ymax &lt;= ymin:\n                continue\n\n            # Add small padding to ensure the mask is within the box\n            xmin = max(0, xmin - 1)\n            ymin = max(0, ymin - 1)\n            xmax = min(binary_mask.shape[1] - 1, xmax + 1)\n            ymax = min(binary_mask.shape[0] - 1, ymax + 1)\n\n            boxes.append([xmin, ymin, xmax, ymax])\n            masks.append(instance_mask)\n            labels.append(1)  # 1 for building class\n\n        # Handle case with no valid instances\n        if len(boxes) == 0:\n            # Create a dummy target with minimal required fields\n            target = {\n                \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n                \"labels\": torch.zeros((0), dtype=torch.int64),\n                \"masks\": torch.zeros(\n                    (0, binary_mask.shape[0], binary_mask.shape[1]), dtype=torch.uint8\n                ),\n                \"image_id\": torch.tensor([idx]),\n                \"area\": torch.zeros((0), dtype=torch.float32),\n                \"iscrowd\": torch.zeros((0), dtype=torch.int64),\n            }\n        else:\n            # Convert to tensors\n            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n            labels = torch.as_tensor(labels, dtype=torch.int64)\n            masks = torch.as_tensor(np.array(masks), dtype=torch.uint8)\n\n            # Calculate area of boxes\n            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n            # Prepare target dictionary\n            target = {\n                \"boxes\": boxes,\n                \"labels\": labels,\n                \"masks\": masks,\n                \"image_id\": torch.tensor([idx]),\n                \"area\": area,\n                \"iscrowd\": torch.zeros_like(labels),  # Assume no crowd instances\n            }\n\n        # Apply transforms if specified\n        if self.transforms is not None:\n            image, target = self.transforms(image, target)\n\n        return image, target\n</code></pre>"},{"location":"train/#geoai.train.ObjectDetectionDataset.__init__","title":"<code>__init__(self, image_paths, label_paths, transforms=None, num_channels=None)</code>  <code>special</code>","text":"<p>Initialize dataset.</p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>list</code> <p>List of paths to image GeoTIFF files.</p> required <code>label_paths</code> <code>list</code> <p>List of paths to label GeoTIFF files.</p> required <code>transforms</code> <code>callable</code> <p>Transformations to apply to images and masks.</p> <code>None</code> <code>num_channels</code> <code>int</code> <p>Number of channels to use from images. If None, auto-detected from the first image.</p> <code>None</code> Source code in <code>geoai/train.py</code> <pre><code>def __init__(self, image_paths, label_paths, transforms=None, num_channels=None):\n    \"\"\"\n    Initialize dataset.\n\n    Args:\n        image_paths (list): List of paths to image GeoTIFF files.\n        label_paths (list): List of paths to label GeoTIFF files.\n        transforms (callable, optional): Transformations to apply to images and masks.\n        num_channels (int, optional): Number of channels to use from images. If None,\n            auto-detected from the first image.\n    \"\"\"\n    self.image_paths = image_paths\n    self.label_paths = label_paths\n    self.transforms = transforms\n\n    # Auto-detect the number of channels if not specified\n    if num_channels is None:\n        with rasterio.open(self.image_paths[0]) as src:\n            self.num_channels = src.count\n    else:\n        self.num_channels = num_channels\n</code></pre>"},{"location":"train/#geoai.train.RandomHorizontalFlip","title":"<code> RandomHorizontalFlip        </code>","text":"<p>Random horizontal flip transform.</p> Source code in <code>geoai/train.py</code> <pre><code>class RandomHorizontalFlip:\n    \"\"\"Random horizontal flip transform.\"\"\"\n\n    def __init__(self, prob=0.5):\n        \"\"\"\n        Initialize random horizontal flip.\n\n        Args:\n            prob (float): Probability of applying the flip.\n        \"\"\"\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() &lt; self.prob:\n            # Flip image\n            image = torch.flip(image, dims=[2])  # Flip along width dimension\n\n            # Flip masks\n            if \"masks\" in target and len(target[\"masks\"]) &gt; 0:\n                target[\"masks\"] = torch.flip(target[\"masks\"], dims=[2])\n\n            # Update boxes\n            if \"boxes\" in target and len(target[\"boxes\"]) &gt; 0:\n                boxes = target[\"boxes\"]\n                width = image.shape[2]\n                boxes[:, 0], boxes[:, 2] = width - boxes[:, 2], width - boxes[:, 0]\n                target[\"boxes\"] = boxes\n\n        return image, target\n</code></pre>"},{"location":"train/#geoai.train.RandomHorizontalFlip.__init__","title":"<code>__init__(self, prob=0.5)</code>  <code>special</code>","text":"<p>Initialize random horizontal flip.</p> <p>Parameters:</p> Name Type Description Default <code>prob</code> <code>float</code> <p>Probability of applying the flip.</p> <code>0.5</code> Source code in <code>geoai/train.py</code> <pre><code>def __init__(self, prob=0.5):\n    \"\"\"\n    Initialize random horizontal flip.\n\n    Args:\n        prob (float): Probability of applying the flip.\n    \"\"\"\n    self.prob = prob\n</code></pre>"},{"location":"train/#geoai.train.ToTensor","title":"<code> ToTensor        </code>","text":"<p>Convert numpy.ndarray to tensor.</p> Source code in <code>geoai/train.py</code> <pre><code>class ToTensor:\n    \"\"\"Convert numpy.ndarray to tensor.\"\"\"\n\n    def __call__(self, image, target):\n        \"\"\"\n        Apply transform to image and target.\n\n        Args:\n            image (torch.Tensor): Input image.\n            target (dict): Target annotations.\n\n        Returns:\n            tuple: Transformed image and target.\n        \"\"\"\n        return image, target\n</code></pre>"},{"location":"train/#geoai.train.ToTensor.__call__","title":"<code>__call__(self, image, target)</code>  <code>special</code>","text":"<p>Apply transform to image and target.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>torch.Tensor</code> <p>Input image.</p> required <code>target</code> <code>dict</code> <p>Target annotations.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>Transformed image and target.</p> Source code in <code>geoai/train.py</code> <pre><code>def __call__(self, image, target):\n    \"\"\"\n    Apply transform to image and target.\n\n    Args:\n        image (torch.Tensor): Input image.\n        target (dict): Target annotations.\n\n    Returns:\n        tuple: Transformed image and target.\n    \"\"\"\n    return image, target\n</code></pre>"},{"location":"train/#geoai.train.collate_fn","title":"<code>collate_fn(batch)</code>","text":"<p>Custom collate function for batching samples.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list</code> <p>List of (image, target) tuples.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple of images and targets.</p> Source code in <code>geoai/train.py</code> <pre><code>def collate_fn(batch):\n    \"\"\"\n    Custom collate function for batching samples.\n\n    Args:\n        batch (list): List of (image, target) tuples.\n\n    Returns:\n        tuple: Tuple of images and targets.\n    \"\"\"\n    return tuple(zip(*batch))\n</code></pre>"},{"location":"train/#geoai.train.evaluate","title":"<code>evaluate(model, data_loader, device)</code>","text":"<p>Evaluate the model on the validation set.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>The model to evaluate.</p> required <code>data_loader</code> <code>torch.utils.data.DataLoader</code> <p>DataLoader for validation data.</p> required <code>device</code> <code>torch.device</code> <p>Device to evaluate on.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Evaluation metrics including loss and IoU.</p> Source code in <code>geoai/train.py</code> <pre><code>def evaluate(model, data_loader, device):\n    \"\"\"\n    Evaluate the model on the validation set.\n\n    Args:\n        model (torch.nn.Module): The model to evaluate.\n        data_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n        device (torch.device): Device to evaluate on.\n\n    Returns:\n        dict: Evaluation metrics including loss and IoU.\n    \"\"\"\n    model.eval()\n\n    # Initialize metrics\n    total_loss = 0\n    iou_scores = []\n\n    with torch.no_grad():\n        for images, targets in data_loader:\n            # Move to device\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            # During evaluation, Mask R-CNN directly returns predictions, not losses\n            # So we'll only get loss when we provide targets explicitly\n            if len(targets) &gt; 0:\n                try:\n                    # Try to get loss dict (this works in some implementations)\n                    loss_dict = model(images, targets)\n                    if isinstance(loss_dict, dict):\n                        losses = sum(loss for loss in loss_dict.values())\n                        total_loss += losses.item()\n                except Exception as e:\n                    print(f\"Warning: Could not compute loss during evaluation: {e}\")\n                    # If we can't compute loss, we'll just focus on IoU\n                    pass\n\n            # Get predictions\n            outputs = model(images)\n\n            # Calculate IoU for each image\n            for i, output in enumerate(outputs):\n                if len(output[\"masks\"]) == 0 or len(targets[i][\"masks\"]) == 0:\n                    continue\n\n                # Convert predicted masks to binary (threshold at 0.5)\n                pred_masks = (output[\"masks\"].squeeze(1) &gt; 0.5).float()\n\n                # Combine all instance masks into a single binary mask\n                pred_combined = (\n                    torch.max(pred_masks, dim=0)[0]\n                    if pred_masks.shape[0] &gt; 0\n                    else torch.zeros_like(targets[i][\"masks\"][0])\n                )\n                target_combined = (\n                    torch.max(targets[i][\"masks\"], dim=0)[0]\n                    if targets[i][\"masks\"].shape[0] &gt; 0\n                    else torch.zeros_like(pred_combined)\n                )\n\n                # Calculate IoU\n                intersection = (pred_combined * target_combined).sum().item()\n                union = ((pred_combined + target_combined) &gt; 0).sum().item()\n\n                if union &gt; 0:\n                    iou = intersection / union\n                    iou_scores.append(iou)\n\n    # Calculate metrics\n    avg_loss = total_loss / len(data_loader) if total_loss &gt; 0 else float(\"inf\")\n    avg_iou = sum(iou_scores) / len(iou_scores) if iou_scores else 0\n\n    return {\"loss\": avg_loss, \"IoU\": avg_iou}\n</code></pre>"},{"location":"train/#geoai.train.get_instance_segmentation_model","title":"<code>get_instance_segmentation_model(num_classes=2, num_channels=3, pretrained=True)</code>","text":"<p>Get Mask R-CNN model with custom input channels and output classes.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Number of output classes (including background).</p> <code>2</code> <code>num_channels</code> <code>int</code> <p>Number of input channels (3 for RGB, 4 for RGBN).</p> <code>3</code> <code>pretrained</code> <code>bool</code> <p>Whether to use pretrained backbone.</p> <code>True</code> <p>Returns:</p> Type Description <code>torch.nn.Module</code> <p>Mask R-CNN model with specified input channels and output classes.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If num_channels is less than 3.</p> Source code in <code>geoai/train.py</code> <pre><code>def get_instance_segmentation_model(num_classes=2, num_channels=3, pretrained=True):\n    \"\"\"\n    Get Mask R-CNN model with custom input channels and output classes.\n\n    Args:\n        num_classes (int): Number of output classes (including background).\n        num_channels (int): Number of input channels (3 for RGB, 4 for RGBN).\n        pretrained (bool): Whether to use pretrained backbone.\n\n    Returns:\n        torch.nn.Module: Mask R-CNN model with specified input channels and output classes.\n\n    Raises:\n        ValueError: If num_channels is less than 3.\n    \"\"\"\n    # Validate num_channels\n    if num_channels &lt; 3:\n        raise ValueError(\"num_channels must be at least 3\")\n\n    # Load pre-trained model\n    model = maskrcnn_resnet50_fpn(\n        pretrained=pretrained,\n        progress=True,\n        weights=(\n            torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n            if pretrained\n            else None\n        ),\n    )\n\n    # Modify transform if num_channels is different from 3\n    if num_channels != 3:\n        # Get the transform\n        transform = model.transform\n\n        # Default values are [0.485, 0.456, 0.406] and [0.229, 0.224, 0.225]\n        # Calculate means and stds for additional channels\n        rgb_mean = [0.485, 0.456, 0.406]\n        rgb_std = [0.229, 0.224, 0.225]\n\n        # Extend them to num_channels (use the mean value for additional channels)\n        mean_of_means = sum(rgb_mean) / len(rgb_mean)\n        mean_of_stds = sum(rgb_std) / len(rgb_std)\n\n        # Create new lists with appropriate length\n        transform.image_mean = rgb_mean + [mean_of_means] * (num_channels - 3)\n        transform.image_std = rgb_std + [mean_of_stds] * (num_channels - 3)\n\n    # Get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n\n    # Replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # Get number of input features for mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n\n    # Replace mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n        in_features_mask, hidden_layer, num_classes\n    )\n\n    # Modify the first layer if num_channels is different from 3\n    if num_channels != 3:\n        original_layer = model.backbone.body.conv1\n        model.backbone.body.conv1 = torch.nn.Conv2d(\n            num_channels,\n            original_layer.out_channels,\n            kernel_size=original_layer.kernel_size,\n            stride=original_layer.stride,\n            padding=original_layer.padding,\n            bias=original_layer.bias is not None,\n        )\n\n        # Copy weights from the original 3 channels to the new layer\n        with torch.no_grad():\n            # Copy the weights for the first 3 channels\n            model.backbone.body.conv1.weight[:, :3, :, :] = original_layer.weight\n\n            # Initialize additional channels with the mean of the first 3 channels\n            mean_weight = original_layer.weight.mean(dim=1, keepdim=True)\n            for i in range(3, num_channels):\n                model.backbone.body.conv1.weight[:, i : i + 1, :, :] = mean_weight\n\n            # Copy bias if it exists\n            if original_layer.bias is not None:\n                model.backbone.body.conv1.bias = original_layer.bias\n\n    return model\n</code></pre>"},{"location":"train/#geoai.train.get_transform","title":"<code>get_transform(train)</code>","text":"<p>Get transforms for data augmentation.</p> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>bool</code> <p>Whether to include training-specific transforms.</p> required <p>Returns:</p> Type Description <code>Compose</code> <p>Composed transforms.</p> Source code in <code>geoai/train.py</code> <pre><code>def get_transform(train):\n    \"\"\"\n    Get transforms for data augmentation.\n\n    Args:\n        train (bool): Whether to include training-specific transforms.\n\n    Returns:\n        Compose: Composed transforms.\n    \"\"\"\n    transforms = []\n    transforms.append(ToTensor())\n\n    if train:\n        transforms.append(RandomHorizontalFlip(0.5))\n\n    return Compose(transforms)\n</code></pre>"},{"location":"train/#geoai.train.inference_on_geotiff","title":"<code>inference_on_geotiff(model, geotiff_path, output_path, window_size=512, overlap=256, confidence_threshold=0.5, batch_size=4, num_channels=3, device=None, **kwargs)</code>","text":"<p>Perform inference on a large GeoTIFF using a sliding window approach with improved blending.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>Trained model for inference.</p> required <code>geotiff_path</code> <code>str</code> <p>Path to input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>Path to save output mask GeoTIFF.</p> required <code>window_size</code> <code>int</code> <p>Size of sliding window for inference.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between adjacent windows.</p> <code>256</code> <code>confidence_threshold</code> <code>float</code> <p>Confidence threshold for predictions (0-1).</p> <code>0.5</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference.</p> <code>4</code> <code>num_channels</code> <code>int</code> <p>Number of channels to use from the input image.</p> <code>3</code> <code>device</code> <code>torch.device</code> <p>Device to run inference on. If None, uses CUDA if available.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple containing output path and inference time in seconds.</p> Source code in <code>geoai/train.py</code> <pre><code>def inference_on_geotiff(\n    model,\n    geotiff_path,\n    output_path,\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.5,\n    batch_size=4,\n    num_channels=3,\n    device=None,\n    **kwargs,\n):\n    \"\"\"\n    Perform inference on a large GeoTIFF using a sliding window approach with improved blending.\n\n    Args:\n        model (torch.nn.Module): Trained model for inference.\n        geotiff_path (str): Path to input GeoTIFF file.\n        output_path (str): Path to save output mask GeoTIFF.\n        window_size (int): Size of sliding window for inference.\n        overlap (int): Overlap between adjacent windows.\n        confidence_threshold (float): Confidence threshold for predictions (0-1).\n        batch_size (int): Batch size for inference.\n        num_channels (int): Number of channels to use from the input image.\n        device (torch.device, optional): Device to run inference on. If None, uses CUDA if available.\n        **kwargs: Additional arguments.\n\n    Returns:\n        tuple: Tuple containing output path and inference time in seconds.\n    \"\"\"\n    if device is None:\n        device = (\n            torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        )\n\n    # Put model in evaluation mode\n    model.to(device)\n    model.eval()\n\n    # Open the GeoTIFF\n    with rasterio.open(geotiff_path) as src:\n        # Read metadata\n        meta = src.meta\n        height = src.height\n        width = src.width\n\n        # Update metadata for output raster\n        out_meta = meta.copy()\n        out_meta.update(\n            {\"count\": 1, \"dtype\": \"uint8\"}  # Single band for mask  # Binary mask\n        )\n\n        # We'll use two arrays:\n        # 1. For accumulating predictions\n        pred_accumulator = np.zeros((height, width), dtype=np.float32)\n        # 2. For tracking how many predictions contribute to each pixel\n        count_accumulator = np.zeros((height, width), dtype=np.float32)\n\n        # Calculate the number of windows needed to cover the entire image\n        steps_y = math.ceil((height - overlap) / (window_size - overlap))\n        steps_x = math.ceil((width - overlap) / (window_size - overlap))\n\n        # Ensure we cover the entire image\n        last_y = height - window_size\n        last_x = width - window_size\n\n        total_windows = steps_y * steps_x\n        print(\n            f\"Processing {total_windows} windows with size {window_size}x{window_size} and overlap {overlap}...\"\n        )\n\n        # Create progress bar\n        pbar = tqdm(total=total_windows)\n\n        # Process in batches\n        batch_inputs = []\n        batch_positions = []\n        batch_count = 0\n\n        start_time = time.time()\n\n        # Slide window over the image - make sure we cover the entire image\n        for i in range(steps_y + 1):  # +1 to ensure we reach the edge\n            y = min(i * (window_size - overlap), last_y)\n            y = max(0, y)  # Prevent negative indices\n\n            if y &gt; last_y and i &gt; 0:  # Skip if we've already covered the entire height\n                continue\n\n            for j in range(steps_x + 1):  # +1 to ensure we reach the edge\n                x = min(j * (window_size - overlap), last_x)\n                x = max(0, x)  # Prevent negative indices\n\n                if (\n                    x &gt; last_x and j &gt; 0\n                ):  # Skip if we've already covered the entire width\n                    continue\n\n                # Read window\n                window = src.read(window=Window(x, y, window_size, window_size))\n\n                # Check if window is valid\n                if window.shape[1] != window_size or window.shape[2] != window_size:\n                    # This can happen at image edges - adjust window size\n                    current_height = window.shape[1]\n                    current_width = window.shape[2]\n                    if current_height == 0 or current_width == 0:\n                        continue  # Skip empty windows\n                else:\n                    current_height = window_size\n                    current_width = window_size\n\n                # Normalize and prepare input\n                image = window.astype(np.float32) / 255.0\n\n                # Handle different number of bands\n                if image.shape[0] &gt; num_channels:\n                    image = image[:num_channels]\n                elif image.shape[0] &lt; num_channels:\n                    padded = np.zeros(\n                        (num_channels, current_height, current_width), dtype=np.float32\n                    )\n                    padded[: image.shape[0]] = image\n                    image = padded\n\n                # Convert to tensor\n                image_tensor = torch.tensor(image, device=device)\n\n                # Add to batch\n                batch_inputs.append(image_tensor)\n                batch_positions.append((y, x, current_height, current_width))\n                batch_count += 1\n\n                # Process batch when it reaches the batch size or at the end\n                if batch_count == batch_size or (i == steps_y and j == steps_x):\n                    # Forward pass\n                    with torch.no_grad():\n                        outputs = model(batch_inputs)\n\n                    # Process each output in the batch\n                    for idx, output in enumerate(outputs):\n                        y_pos, x_pos, h, w = batch_positions[idx]\n\n                        # Create weight matrix that gives higher weight to center pixels\n                        # This helps with smooth blending at boundaries\n                        y_grid, x_grid = np.mgrid[0:h, 0:w]\n\n                        # Calculate distance from each edge\n                        dist_from_left = x_grid\n                        dist_from_right = w - x_grid - 1\n                        dist_from_top = y_grid\n                        dist_from_bottom = h - y_grid - 1\n\n                        # Combine distances (minimum distance to any edge)\n                        edge_distance = np.minimum.reduce(\n                            [\n                                dist_from_left,\n                                dist_from_right,\n                                dist_from_top,\n                                dist_from_bottom,\n                            ]\n                        )\n\n                        # Convert to weight (higher weight for center pixels)\n                        # Normalize to [0, 1]\n                        edge_distance = np.minimum(edge_distance, overlap / 2)\n                        weight = edge_distance / (overlap / 2)\n\n                        # Get masks for predictions above threshold\n                        if len(output[\"scores\"]) &gt; 0:\n                            # Get all instances that meet confidence threshold\n                            keep = output[\"scores\"] &gt; confidence_threshold\n                            masks = output[\"masks\"][keep].squeeze(1)\n\n                            # Combine all instances into one mask\n                            if len(masks) &gt; 0:\n                                combined_mask = torch.max(masks, dim=0)[0] &gt; 0.5\n                                combined_mask = (\n                                    combined_mask.cpu().numpy().astype(np.float32)\n                                )\n\n                                # Apply weight to prediction\n                                weighted_pred = combined_mask * weight\n\n                                # Add to accumulators\n                                pred_accumulator[\n                                    y_pos : y_pos + h, x_pos : x_pos + w\n                                ] += weighted_pred\n                                count_accumulator[\n                                    y_pos : y_pos + h, x_pos : x_pos + w\n                                ] += weight\n\n                    # Reset batch\n                    batch_inputs = []\n                    batch_positions = []\n                    batch_count = 0\n\n                    # Update progress bar\n                    pbar.update(len(outputs))\n\n        # Close progress bar\n        pbar.close()\n\n        # Calculate final mask by dividing accumulated predictions by counts\n        # Handle division by zero\n        mask = np.zeros((height, width), dtype=np.uint8)\n        valid_pixels = count_accumulator &gt; 0\n        if np.any(valid_pixels):\n            # Average predictions where we have data\n            mask[valid_pixels] = (\n                pred_accumulator[valid_pixels] / count_accumulator[valid_pixels] &gt; 0.5\n            ).astype(np.uint8)\n\n        # Record time\n        inference_time = time.time() - start_time\n        print(f\"Inference completed in {inference_time:.2f} seconds\")\n\n        # Save output\n        with rasterio.open(output_path, \"w\", **out_meta) as dst:\n            dst.write(mask, 1)\n\n        print(f\"Saved prediction to {output_path}\")\n\n        return output_path, inference_time\n</code></pre>"},{"location":"train/#geoai.train.object_detection","title":"<code>object_detection(input_path, output_path, model_path, window_size=512, overlap=256, confidence_threshold=0.5, batch_size=4, num_channels=3, pretrained=True, device=None, **kwargs)</code>","text":"<p>Perform object detection on a GeoTIFF using a pre-trained Mask R-CNN model.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>Path to save output mask GeoTIFF.</p> required <code>model_path</code> <code>str</code> <p>Path to trained model weights.</p> required <code>window_size</code> <code>int</code> <p>Size of sliding window for inference.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between adjacent windows.</p> <code>256</code> <code>confidence_threshold</code> <code>float</code> <p>Confidence threshold for predictions (0-1).</p> <code>0.5</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference.</p> <code>4</code> <code>num_channels</code> <code>int</code> <p>Number of channels in the input image and model.</p> <code>3</code> <code>pretrained</code> <code>bool</code> <p>Whether to use pretrained backbone for model loading.</p> <code>True</code> <code>device</code> <code>torch.device</code> <p>Device to run inference on. If None, uses CUDA if available.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to inference_on_geotiff.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>Output mask is saved to output_path.</p> Source code in <code>geoai/train.py</code> <pre><code>def object_detection(\n    input_path,\n    output_path,\n    model_path,\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.5,\n    batch_size=4,\n    num_channels=3,\n    pretrained=True,\n    device=None,\n    **kwargs,\n):\n    \"\"\"\n    Perform object detection on a GeoTIFF using a pre-trained Mask R-CNN model.\n\n    Args:\n        input_path (str): Path to input GeoTIFF file.\n        output_path (str): Path to save output mask GeoTIFF.\n        model_path (str): Path to trained model weights.\n        window_size (int): Size of sliding window for inference.\n        overlap (int): Overlap between adjacent windows.\n        confidence_threshold (float): Confidence threshold for predictions (0-1).\n        batch_size (int): Batch size for inference.\n        num_channels (int): Number of channels in the input image and model.\n        pretrained (bool): Whether to use pretrained backbone for model loading.\n        device (torch.device, optional): Device to run inference on. If None, uses CUDA if available.\n        **kwargs: Additional arguments passed to inference_on_geotiff.\n\n    Returns:\n        None: Output mask is saved to output_path.\n    \"\"\"\n    # Load your trained model\n    if device is None:\n        device = (\n            torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        )\n    model = get_instance_segmentation_model(\n        num_classes=2, num_channels=num_channels, pretrained=pretrained\n    )\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.to(device)\n    model.eval()\n\n    inference_on_geotiff(\n        model=model,\n        geotiff_path=input_path,\n        output_path=output_path,\n        window_size=window_size,  # Adjust based on your model and memory\n        overlap=overlap,  # Overlap to avoid edge artifacts\n        confidence_threshold=confidence_threshold,\n        batch_size=batch_size,  # Adjust based on your GPU memory\n        num_channels=num_channels,\n        device=device,\n        **kwargs,\n    )\n</code></pre>"},{"location":"train/#geoai.train.train_MaskRCNN_model","title":"<code>train_MaskRCNN_model(images_dir, labels_dir, output_dir, num_channels=3, pretrained=True, pretrained_model_path=None, batch_size=4, num_epochs=10, learning_rate=0.005, seed=42, val_split=0.2, visualize=False, resume_training=False)</code>","text":"<p>Train and evaluate Mask R-CNN model for instance segmentation.</p> <p>This function trains a Mask R-CNN model for instance segmentation using the provided dataset. It supports loading a pretrained model to either initialize the backbone or to continue training from a specific checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Directory containing image GeoTIFF files.</p> required <code>labels_dir</code> <code>str</code> <p>Directory containing label GeoTIFF files.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save model checkpoints and results.</p> required <code>num_channels</code> <code>int</code> <p>Number of input channels. If None, auto-detected. Defaults to 3.</p> <code>3</code> <code>pretrained</code> <code>bool</code> <p>Whether to use pretrained backbone. This is ignored if pretrained_model_path is provided. Defaults to True.</p> <code>True</code> <code>pretrained_model_path</code> <code>str</code> <p>Path to a .pth file to load as a pretrained model for continued training. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for training. Defaults to 4.</p> <code>4</code> <code>num_epochs</code> <code>int</code> <p>Number of training epochs. Defaults to 10.</p> <code>10</code> <code>learning_rate</code> <code>float</code> <p>Initial learning rate. Defaults to 0.005.</p> <code>0.005</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <code>val_split</code> <code>float</code> <p>Fraction of data to use for validation (0-1). Defaults to 0.2.</p> <code>0.2</code> <code>visualize</code> <code>bool</code> <p>Whether to generate visualizations of model predictions. Defaults to False.</p> <code>False</code> <code>resume_training</code> <code>bool</code> <p>If True and pretrained_model_path is provided, will try to load optimizer and scheduler states as well. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>Model weights are saved to output_dir.</p> <p>Exceptions:</p> Type Description <code>FileNotFoundError</code> <p>If pretrained_model_path is provided but file doesn't exist.</p> <code>RuntimeError</code> <p>If there's an issue loading the pretrained model.</p> Source code in <code>geoai/train.py</code> <pre><code>def train_MaskRCNN_model(\n    images_dir,\n    labels_dir,\n    output_dir,\n    num_channels=3,\n    pretrained=True,\n    pretrained_model_path=None,\n    batch_size=4,\n    num_epochs=10,\n    learning_rate=0.005,\n    seed=42,\n    val_split=0.2,\n    visualize=False,\n    resume_training=False,\n):\n    \"\"\"Train and evaluate Mask R-CNN model for instance segmentation.\n\n    This function trains a Mask R-CNN model for instance segmentation using the\n    provided dataset. It supports loading a pretrained model to either initialize\n    the backbone or to continue training from a specific checkpoint.\n\n    Args:\n        images_dir (str): Directory containing image GeoTIFF files.\n        labels_dir (str): Directory containing label GeoTIFF files.\n        output_dir (str): Directory to save model checkpoints and results.\n        num_channels (int, optional): Number of input channels. If None, auto-detected.\n            Defaults to 3.\n        pretrained (bool): Whether to use pretrained backbone. This is ignored if\n            pretrained_model_path is provided. Defaults to True.\n        pretrained_model_path (str, optional): Path to a .pth file to load as a\n            pretrained model for continued training. Defaults to None.\n        batch_size (int): Batch size for training. Defaults to 4.\n        num_epochs (int): Number of training epochs. Defaults to 10.\n        learning_rate (float): Initial learning rate. Defaults to 0.005.\n        seed (int): Random seed for reproducibility. Defaults to 42.\n        val_split (float): Fraction of data to use for validation (0-1). Defaults to 0.2.\n        visualize (bool): Whether to generate visualizations of model predictions.\n            Defaults to False.\n        resume_training (bool): If True and pretrained_model_path is provided,\n            will try to load optimizer and scheduler states as well. Defaults to False.\n\n    Returns:\n        None: Model weights are saved to output_dir.\n\n    Raises:\n        FileNotFoundError: If pretrained_model_path is provided but file doesn't exist.\n        RuntimeError: If there's an issue loading the pretrained model.\n    \"\"\"\n\n    import datetime\n\n    # Set random seeds for reproducibility\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Get device\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Get all image and label files\n    image_files = sorted(\n        [\n            os.path.join(images_dir, f)\n            for f in os.listdir(images_dir)\n            if f.endswith(\".tif\")\n        ]\n    )\n    label_files = sorted(\n        [\n            os.path.join(labels_dir, f)\n            for f in os.listdir(labels_dir)\n            if f.endswith(\".tif\")\n        ]\n    )\n\n    print(f\"Found {len(image_files)} image files and {len(label_files)} label files\")\n\n    # Ensure matching files\n    if len(image_files) != len(label_files):\n        print(\"Warning: Number of image files and label files don't match!\")\n        # Find matching files by basename\n        basenames = [os.path.basename(f) for f in image_files]\n        label_files = [\n            os.path.join(labels_dir, os.path.basename(f))\n            for f in image_files\n            if os.path.exists(os.path.join(labels_dir, os.path.basename(f)))\n        ]\n        image_files = [\n            f\n            for f, b in zip(image_files, basenames)\n            if os.path.exists(os.path.join(labels_dir, b))\n        ]\n        print(f\"Using {len(image_files)} matching files\")\n\n    # Split data into train and validation sets\n    train_imgs, val_imgs, train_labels, val_labels = train_test_split(\n        image_files, label_files, test_size=val_split, random_state=seed\n    )\n\n    print(f\"Training on {len(train_imgs)} images, validating on {len(val_imgs)} images\")\n\n    # Create datasets\n    train_dataset = ObjectDetectionDataset(\n        train_imgs, train_labels, transforms=get_transform(train=True)\n    )\n    val_dataset = ObjectDetectionDataset(\n        val_imgs, val_labels, transforms=get_transform(train=False)\n    )\n\n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collate_fn,\n        num_workers=4,\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collate_fn,\n        num_workers=4,\n    )\n\n    # Initialize model (2 classes: background and building)\n    model = get_instance_segmentation_model(\n        num_classes=2, num_channels=num_channels, pretrained=pretrained\n    )\n    model.to(device)\n\n    # Set up optimizer\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(\n        params, lr=learning_rate, momentum=0.9, weight_decay=0.0005\n    )\n\n    # Set up learning rate scheduler\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n\n    # Initialize training variables\n    start_epoch = 0\n    best_iou = 0\n\n    # Load pretrained model if provided\n    if pretrained_model_path:\n        if not os.path.exists(pretrained_model_path):\n            raise FileNotFoundError(\n                f\"Pretrained model file not found: {pretrained_model_path}\"\n            )\n\n        print(f\"Loading pretrained model from: {pretrained_model_path}\")\n        try:\n            # Check if it's a full checkpoint or just model weights\n            checkpoint = torch.load(pretrained_model_path, map_location=device)\n\n            if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n                # It's a checkpoint with extra information\n                model.load_state_dict(checkpoint[\"model_state_dict\"])\n\n                if resume_training:\n                    # Resume from checkpoint\n                    start_epoch = checkpoint.get(\"epoch\", 0) + 1\n                    best_iou = checkpoint.get(\"best_iou\", 0)\n\n                    if \"optimizer_state_dict\" in checkpoint:\n                        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n                    if \"scheduler_state_dict\" in checkpoint:\n                        lr_scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n\n                    print(f\"Resuming training from epoch {start_epoch}\")\n                    print(f\"Previous best IoU: {best_iou:.4f}\")\n            else:\n                # Assume it's just the model weights\n                model.load_state_dict(checkpoint)\n\n            print(\"Pretrained model loaded successfully\")\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load pretrained model: {str(e)}\")\n\n    # Training loop\n    for epoch in range(start_epoch, num_epochs):\n        # Train one epoch\n        train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n\n        # Update learning rate\n        lr_scheduler.step()\n\n        # Evaluate\n        eval_metrics = evaluate(model, val_loader, device)\n\n        # Print metrics\n        print(\n            f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Val Loss: {eval_metrics['loss']:.4f}, Val IoU: {eval_metrics['IoU']:.4f}\"\n        )\n\n        # Save best model\n        if eval_metrics[\"IoU\"] &gt; best_iou:\n            best_iou = eval_metrics[\"IoU\"]\n            print(f\"Saving best model with IoU: {best_iou:.4f}\")\n            torch.save(model.state_dict(), os.path.join(output_dir, \"best_model.pth\"))\n\n        # Save checkpoint every 10 epochs\n        if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n            torch.save(\n                {\n                    \"epoch\": epoch,\n                    \"model_state_dict\": model.state_dict(),\n                    \"optimizer_state_dict\": optimizer.state_dict(),\n                    \"scheduler_state_dict\": lr_scheduler.state_dict(),\n                    \"best_iou\": best_iou,\n                },\n                os.path.join(output_dir, f\"checkpoint_epoch_{epoch+1}.pth\"),\n            )\n\n    # Save final model\n    torch.save(model.state_dict(), os.path.join(output_dir, \"final_model.pth\"))\n\n    # Save full checkpoint of final state\n    torch.save(\n        {\n            \"epoch\": num_epochs - 1,\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n            \"scheduler_state_dict\": lr_scheduler.state_dict(),\n            \"best_iou\": best_iou,\n        },\n        os.path.join(output_dir, \"final_checkpoint.pth\"),\n    )\n\n    # Load best model for evaluation and visualization\n    model.load_state_dict(torch.load(os.path.join(output_dir, \"best_model.pth\")))\n\n    # Final evaluation\n    final_metrics = evaluate(model, val_loader, device)\n    print(\n        f\"Final Evaluation - Loss: {final_metrics['loss']:.4f}, IoU: {final_metrics['IoU']:.4f}\"\n    )\n\n    # Visualize results\n    if visualize:\n        print(\"Generating visualizations...\")\n        visualize_predictions(\n            model,\n            val_dataset,\n            device,\n            num_samples=5,\n            output_dir=os.path.join(output_dir, \"visualizations\"),\n        )\n\n    # Save training summary\n    with open(os.path.join(output_dir, \"training_summary.txt\"), \"w\") as f:\n        f.write(\n            f\"Training completed on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n        )\n        f.write(f\"Total epochs: {num_epochs}\\n\")\n        f.write(f\"Best validation IoU: {best_iou:.4f}\\n\")\n        f.write(f\"Final validation IoU: {final_metrics['IoU']:.4f}\\n\")\n        f.write(f\"Final validation loss: {final_metrics['loss']:.4f}\\n\")\n\n        if pretrained_model_path:\n            f.write(f\"Started from pretrained model: {pretrained_model_path}\\n\")\n            if resume_training:\n                f.write(f\"Resumed training from epoch {start_epoch}\\n\")\n\n    print(f\"Training complete! Trained model saved to {output_dir}\")\n</code></pre>"},{"location":"train/#geoai.train.train_one_epoch","title":"<code>train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)</code>","text":"<p>Train the model for one epoch.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>The model to train.</p> required <code>optimizer</code> <code>torch.optim.Optimizer</code> <p>The optimizer to use.</p> required <code>data_loader</code> <code>torch.utils.data.DataLoader</code> <p>DataLoader for training data.</p> required <code>device</code> <code>torch.device</code> <p>Device to train on.</p> required <code>epoch</code> <code>int</code> <p>Current epoch number.</p> required <code>print_freq</code> <code>int</code> <p>How often to print progress.</p> <code>10</code> <p>Returns:</p> Type Description <code>float</code> <p>Average loss for the epoch.</p> Source code in <code>geoai/train.py</code> <pre><code>def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10):\n    \"\"\"\n    Train the model for one epoch.\n\n    Args:\n        model (torch.nn.Module): The model to train.\n        optimizer (torch.optim.Optimizer): The optimizer to use.\n        data_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        device (torch.device): Device to train on.\n        epoch (int): Current epoch number.\n        print_freq (int): How often to print progress.\n\n    Returns:\n        float: Average loss for the epoch.\n    \"\"\"\n    model.train()\n    total_loss = 0\n\n    start_time = time.time()\n\n    for i, (images, targets) in enumerate(data_loader):\n        # Move images and targets to device\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        # Forward pass\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        # Backward pass\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        # Track loss\n        total_loss += losses.item()\n\n        # Print progress\n        if i % print_freq == 0:\n            elapsed_time = time.time() - start_time\n            print(\n                f\"Epoch: {epoch}, Batch: {i}/{len(data_loader)}, Loss: {losses.item():.4f}, Time: {elapsed_time:.2f}s\"\n            )\n            start_time = time.time()\n\n    # Calculate average loss\n    avg_loss = total_loss / len(data_loader)\n    return avg_loss\n</code></pre>"},{"location":"train/#geoai.train.visualize_predictions","title":"<code>visualize_predictions(model, dataset, device, num_samples=5, output_dir=None)</code>","text":"<p>Visualize model predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>Trained model.</p> required <code>dataset</code> <code>torch.utils.data.Dataset</code> <p>Dataset to visualize.</p> required <code>device</code> <code>torch.device</code> <p>Device to run inference on.</p> required <code>num_samples</code> <code>int</code> <p>Number of samples to visualize.</p> <code>5</code> <code>output_dir</code> <code>str</code> <p>Directory to save visualizations. If None, visualizations are displayed but not saved.</p> <code>None</code> Source code in <code>geoai/train.py</code> <pre><code>def visualize_predictions(model, dataset, device, num_samples=5, output_dir=None):\n    \"\"\"\n    Visualize model predictions.\n\n    Args:\n        model (torch.nn.Module): Trained model.\n        dataset (torch.utils.data.Dataset): Dataset to visualize.\n        device (torch.device): Device to run inference on.\n        num_samples (int): Number of samples to visualize.\n        output_dir (str, optional): Directory to save visualizations. If None,\n            visualizations are displayed but not saved.\n    \"\"\"\n    model.eval()\n\n    # Create output directory if needed\n    if output_dir:\n        os.makedirs(output_dir, exist_ok=True)\n\n    # Select random samples\n    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n\n    for idx in indices:\n        # Get image and target\n        image, target = dataset[idx]\n\n        # Convert to device and add batch dimension\n        image = image.to(device)\n        image_batch = [image]\n\n        # Get prediction\n        with torch.no_grad():\n            output = model(image_batch)[0]\n\n        # Convert image from CHW to HWC for display (first 3 bands as RGB)\n        rgb_image = image[:3].cpu().numpy()\n        rgb_image = np.transpose(rgb_image, (1, 2, 0))\n        rgb_image = np.clip(rgb_image, 0, 1)  # Ensure values are in [0,1]\n\n        # Create binary ground truth mask (combine all instances)\n        gt_masks = target[\"masks\"].cpu().numpy()\n        gt_combined = (\n            np.max(gt_masks, axis=0)\n            if len(gt_masks) &gt; 0\n            else np.zeros((image.shape[1], image.shape[2]), dtype=np.uint8)\n        )\n\n        # Create binary prediction mask (combine all instances with score &gt; 0.5)\n        pred_masks = output[\"masks\"].cpu().numpy()\n        pred_scores = output[\"scores\"].cpu().numpy()\n        high_conf_indices = pred_scores &gt; 0.5\n\n        pred_combined = np.zeros((image.shape[1], image.shape[2]), dtype=np.float32)\n        if np.any(high_conf_indices):\n            for mask in pred_masks[high_conf_indices]:\n                # Apply threshold to each predicted mask\n                binary_mask = (mask[0] &gt; 0.5).astype(np.float32)\n                # Combine with existing masks\n                pred_combined = np.maximum(pred_combined, binary_mask)\n\n        # Create figure\n        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n        # Show RGB image\n        axs[0].imshow(rgb_image)\n        axs[0].set_title(\"RGB Image\")\n        axs[0].axis(\"off\")\n\n        # Show prediction\n        axs[1].imshow(pred_combined, cmap=\"viridis\")\n        axs[1].set_title(f\"Predicted Buildings: {np.sum(high_conf_indices)} instances\")\n        axs[1].axis(\"off\")\n\n        # Show ground truth\n        axs[2].imshow(gt_combined, cmap=\"viridis\")\n        axs[2].set_title(f\"Ground Truth: {len(gt_masks)} instances\")\n        axs[2].axis(\"off\")\n\n        plt.tight_layout()\n\n        # Save or show\n        if output_dir:\n            plt.savefig(os.path.join(output_dir, f\"prediction_{idx}.png\"))\n            plt.close()\n        else:\n            plt.show()\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use geoai in a project:</p> <pre><code>import geoai\n</code></pre>"},{"location":"utils/","title":"utils module","text":"<p>The utils module contains common functions and classes used by the other modules.</p>"},{"location":"utils/#geoai.utils.adaptive_regularization","title":"<code>adaptive_regularization(building_polygons, simplify_tolerance=0.5, area_threshold=0.9, preserve_shape=True)</code>","text":"<p>Adaptively regularizes building footprints based on their characteristics.</p> <p>This approach determines the best regularization method for each building.</p> <p>Parameters:</p> Name Type Description Default <code>building_polygons</code> <p>GeoDataFrame or list of shapely Polygons</p> required <code>simplify_tolerance</code> <p>Distance tolerance for simplification</p> <code>0.5</code> <code>area_threshold</code> <p>Minimum acceptable area ratio</p> <code>0.9</code> <code>preserve_shape</code> <p>Whether to preserve overall shape for complex buildings</p> <code>True</code> <p>Returns:</p> Type Description <p>GeoDataFrame or list of shapely Polygons with regularized building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def adaptive_regularization(\n    building_polygons, simplify_tolerance=0.5, area_threshold=0.9, preserve_shape=True\n):\n    \"\"\"\n    Adaptively regularizes building footprints based on their characteristics.\n\n    This approach determines the best regularization method for each building.\n\n    Args:\n        building_polygons: GeoDataFrame or list of shapely Polygons\n        simplify_tolerance: Distance tolerance for simplification\n        area_threshold: Minimum acceptable area ratio\n        preserve_shape: Whether to preserve overall shape for complex buildings\n\n    Returns:\n        GeoDataFrame or list of shapely Polygons with regularized building footprints\n    \"\"\"\n    from shapely.affinity import rotate\n    from shapely.geometry import Polygon\n\n    # Analyze the overall dataset to set appropriate parameters\n    if is_gdf := isinstance(building_polygons, gpd.GeoDataFrame):\n        geom_objects = building_polygons.geometry\n    else:\n        geom_objects = building_polygons\n\n    results = []\n\n    for building in geom_objects:\n        # Skip invalid geometries\n        if not hasattr(building, \"exterior\") or building.is_empty:\n            results.append(building)\n            continue\n\n        # Measure building complexity\n        complexity = building.length / (4 * np.sqrt(building.area))\n\n        # Determine if the building has a clear principal direction\n        coords = np.array(building.exterior.coords)[:-1]\n        segments = np.diff(np.vstack([coords, coords[0]]), axis=0)\n        segment_lengths = np.sqrt(segments[:, 0] ** 2 + segments[:, 1] ** 2)\n        angles = np.arctan2(segments[:, 1], segments[:, 0]) * 180 / np.pi\n\n        # Normalize angles to 0-180 range and get histogram\n        norm_angles = angles % 180\n        hist, bins = np.histogram(\n            norm_angles, bins=18, range=(0, 180), weights=segment_lengths\n        )\n\n        # Calculate direction clarity (ratio of longest direction to total)\n        direction_clarity = np.max(hist) / np.sum(hist) if np.sum(hist) &gt; 0 else 0\n\n        # Choose regularization method based on building characteristics\n        if complexity &lt; 1.2 and direction_clarity &gt; 0.5:\n            # Simple building with clear direction: use rotated rectangle\n            bin_max = np.argmax(hist)\n            bin_centers = (bins[:-1] + bins[1:]) / 2\n            dominant_angle = bin_centers[bin_max]\n\n            # Rotate to align with coordinate system\n            rotated = rotate(building, -dominant_angle, origin=\"centroid\")\n\n            # Create bounding box in rotated space\n            bounds = rotated.bounds\n            rect = Polygon(\n                [\n                    (bounds[0], bounds[1]),\n                    (bounds[2], bounds[1]),\n                    (bounds[2], bounds[3]),\n                    (bounds[0], bounds[3]),\n                ]\n            )\n\n            # Rotate back\n            result = rotate(rect, dominant_angle, origin=\"centroid\")\n\n            # Quality check\n            if (\n                result.area / building.area &lt; area_threshold\n                or result.area / building.area &gt; (1.0 / area_threshold)\n            ):\n                # Too much area change, use simplified original\n                result = building.simplify(simplify_tolerance, preserve_topology=True)\n\n        else:\n            # Complex building or no clear direction: preserve shape\n            if preserve_shape:\n                # Simplify with topology preservation\n                result = building.simplify(simplify_tolerance, preserve_topology=True)\n            else:\n                # Fall back to convex hull for very complex shapes\n                result = building.convex_hull\n\n        results.append(result)\n\n    # Return in same format as input\n    if is_gdf:\n        return gpd.GeoDataFrame(geometry=results, crs=building_polygons.crs)\n    else:\n        return results\n</code></pre>"},{"location":"utils/#geoai.utils.add_geometric_properties","title":"<code>add_geometric_properties(data, properties=None, area_unit='m2', length_unit='m')</code>","text":"<p>Calculates geometric properties and adds them to the GeoDataFrame.</p> <p>This function calculates various geometric properties of features in a GeoDataFrame and adds them as new columns without modifying existing attributes.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>GeoDataFrame containing vector features.</p> required <code>properties</code> <p>List of geometric properties to calculate. Options include: 'area', 'length', 'perimeter', 'centroid_x', 'centroid_y', 'bounds', 'convex_hull_area', 'orientation', 'complexity', 'area_bbox', 'area_convex', 'area_filled', 'major_length', 'minor_length', 'eccentricity', 'diameter_areagth', 'extent', 'solidity', 'elongation'. Defaults to ['area', 'length'] if None.</p> <code>None</code> <code>area_unit</code> <p>String specifying the unit for area calculation ('m2', 'km2', 'ha'). Defaults to 'm2'.</p> <code>'m2'</code> <code>length_unit</code> <p>String specifying the unit for length calculation ('m', 'km'). Defaults to 'm'.</p> <code>'m'</code> <p>Returns:</p> Type Description <code>geopandas.GeoDataFrame</code> <p>A copy of the input GeoDataFrame with added geometric property columns.</p> Source code in <code>geoai/utils.py</code> <pre><code>def add_geometric_properties(data, properties=None, area_unit=\"m2\", length_unit=\"m\"):\n    \"\"\"Calculates geometric properties and adds them to the GeoDataFrame.\n\n    This function calculates various geometric properties of features in a\n    GeoDataFrame and adds them as new columns without modifying existing attributes.\n\n    Args:\n        data: GeoDataFrame containing vector features.\n        properties: List of geometric properties to calculate. Options include:\n            'area', 'length', 'perimeter', 'centroid_x', 'centroid_y', 'bounds',\n            'convex_hull_area', 'orientation', 'complexity', 'area_bbox',\n            'area_convex', 'area_filled', 'major_length', 'minor_length',\n            'eccentricity', 'diameter_areagth', 'extent', 'solidity',\n            'elongation'.\n            Defaults to ['area', 'length'] if None.\n        area_unit: String specifying the unit for area calculation ('m2', 'km2',\n            'ha'). Defaults to 'm2'.\n        length_unit: String specifying the unit for length calculation ('m', 'km').\n            Defaults to 'm'.\n\n    Returns:\n        geopandas.GeoDataFrame: A copy of the input GeoDataFrame with added\n        geometric property columns.\n    \"\"\"\n    from shapely.ops import unary_union\n\n    if isinstance(data, str):\n        data = read_vector(data)\n\n    # Make a copy to avoid modifying the original\n    result = data.copy()\n\n    # Default properties to calculate\n    if properties is None:\n        properties = [\n            \"area\",\n            \"length\",\n            \"perimeter\",\n            \"convex_hull_area\",\n            \"orientation\",\n            \"complexity\",\n            \"area_bbox\",\n            \"area_convex\",\n            \"area_filled\",\n            \"major_length\",\n            \"minor_length\",\n            \"eccentricity\",\n            \"diameter_area\",\n            \"extent\",\n            \"solidity\",\n            \"elongation\",\n        ]\n\n    # Make sure we're working with a GeoDataFrame with a valid CRS\n\n    if not isinstance(result, gpd.GeoDataFrame):\n        raise ValueError(\"Input must be a GeoDataFrame\")\n\n    if result.crs is None:\n        raise ValueError(\n            \"GeoDataFrame must have a defined coordinate reference system (CRS)\"\n        )\n\n    # Ensure we're working with a projected CRS for accurate measurements\n    if result.crs.is_geographic:\n        # Reproject to a suitable projected CRS for accurate measurements\n        result = result.to_crs(result.estimate_utm_crs())\n\n    # Basic area calculation with unit conversion\n    if \"area\" in properties:\n        # Calculate area (only for polygons)\n        result[\"area\"] = result.geometry.apply(\n            lambda geom: geom.area if isinstance(geom, (Polygon, MultiPolygon)) else 0\n        )\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area\"] = result[\"area\"] / 1_000_000  # m\u00b2 to km\u00b2\n            result.rename(columns={\"area\": \"area_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area\"] = result[\"area\"] / 10_000  # m\u00b2 to hectares\n            result.rename(columns={\"area\": \"area_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area\": \"area_m2\"}, inplace=True)\n\n    # Length calculation with unit conversion\n    if \"length\" in properties:\n        # Calculate length (works for lines and polygon boundaries)\n        result[\"length\"] = result.geometry.length\n\n        # Convert to requested units\n        if length_unit == \"km\":\n            result[\"length\"] = result[\"length\"] / 1_000  # m to km\n            result.rename(columns={\"length\": \"length_km\"}, inplace=True)\n        else:  # Default is m\n            result.rename(columns={\"length\": \"length_m\"}, inplace=True)\n\n    # Perimeter calculation (for polygons)\n    if \"perimeter\" in properties:\n        result[\"perimeter\"] = result.geometry.apply(\n            lambda geom: (\n                geom.boundary.length if isinstance(geom, (Polygon, MultiPolygon)) else 0\n            )\n        )\n\n        # Convert to requested units\n        if length_unit == \"km\":\n            result[\"perimeter\"] = result[\"perimeter\"] / 1_000  # m to km\n            result.rename(columns={\"perimeter\": \"perimeter_km\"}, inplace=True)\n        else:  # Default is m\n            result.rename(columns={\"perimeter\": \"perimeter_m\"}, inplace=True)\n\n    # Centroid coordinates\n    if \"centroid_x\" in properties or \"centroid_y\" in properties:\n        centroids = result.geometry.centroid\n\n        if \"centroid_x\" in properties:\n            result[\"centroid_x\"] = centroids.x\n\n        if \"centroid_y\" in properties:\n            result[\"centroid_y\"] = centroids.y\n\n    # Bounding box properties\n    if \"bounds\" in properties:\n        bounds = result.geometry.bounds\n        result[\"minx\"] = bounds.minx\n        result[\"miny\"] = bounds.miny\n        result[\"maxx\"] = bounds.maxx\n        result[\"maxy\"] = bounds.maxy\n\n    # Area of bounding box\n    if \"area_bbox\" in properties:\n        bounds = result.geometry.bounds\n        result[\"area_bbox\"] = (bounds.maxx - bounds.minx) * (bounds.maxy - bounds.miny)\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area_bbox\"] = result[\"area_bbox\"] / 1_000_000\n            result.rename(columns={\"area_bbox\": \"area_bbox_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area_bbox\"] = result[\"area_bbox\"] / 10_000\n            result.rename(columns={\"area_bbox\": \"area_bbox_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area_bbox\": \"area_bbox_m2\"}, inplace=True)\n\n    # Area of convex hull\n    if \"area_convex\" in properties or \"convex_hull_area\" in properties:\n        result[\"area_convex\"] = result.geometry.convex_hull.area\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area_convex\"] = result[\"area_convex\"] / 1_000_000\n            result.rename(columns={\"area_convex\": \"area_convex_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area_convex\"] = result[\"area_convex\"] / 10_000\n            result.rename(columns={\"area_convex\": \"area_convex_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area_convex\": \"area_convex_m2\"}, inplace=True)\n\n        # For backward compatibility\n        if \"convex_hull_area\" in properties and \"area_convex\" not in properties:\n            result[\"convex_hull_area\"] = result[\"area_convex\"]\n            if area_unit == \"km2\":\n                result.rename(\n                    columns={\"convex_hull_area\": \"convex_hull_area_km2\"}, inplace=True\n                )\n            elif area_unit == \"ha\":\n                result.rename(\n                    columns={\"convex_hull_area\": \"convex_hull_area_ha\"}, inplace=True\n                )\n            else:\n                result.rename(\n                    columns={\"convex_hull_area\": \"convex_hull_area_m2\"}, inplace=True\n                )\n\n    # Area of filled geometry (no holes)\n    if \"area_filled\" in properties:\n\n        def get_filled_area(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)):\n                return 0\n\n            if isinstance(geom, MultiPolygon):\n                # For MultiPolygon, fill all constituent polygons\n                filled_polys = [Polygon(p.exterior) for p in geom.geoms]\n                return unary_union(filled_polys).area\n            else:\n                # For single Polygon, create a new one with just the exterior ring\n                return Polygon(geom.exterior).area\n\n        result[\"area_filled\"] = result.geometry.apply(get_filled_area)\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area_filled\"] = result[\"area_filled\"] / 1_000_000\n            result.rename(columns={\"area_filled\": \"area_filled_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area_filled\"] = result[\"area_filled\"] / 10_000\n            result.rename(columns={\"area_filled\": \"area_filled_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area_filled\": \"area_filled_m2\"}, inplace=True)\n\n    # Axes lengths, eccentricity, orientation, and elongation\n    if any(\n        p in properties\n        for p in [\n            \"major_length\",\n            \"minor_length\",\n            \"eccentricity\",\n            \"orientation\",\n            \"elongation\",\n        ]\n    ):\n\n        def get_axes_properties(geom):\n            # Skip non-polygons\n            if not isinstance(geom, (Polygon, MultiPolygon)):\n                return None, None, None, None, None\n\n            # Handle multipolygons by using the largest polygon\n            if isinstance(geom, MultiPolygon):\n                # Get the polygon with the largest area\n                geom = sorted(list(geom.geoms), key=lambda p: p.area, reverse=True)[0]\n\n            try:\n                # Get the minimum rotated rectangle\n                rect = geom.minimum_rotated_rectangle\n\n                # Extract coordinates\n                coords = list(rect.exterior.coords)[\n                    :-1\n                ]  # Remove the duplicated last point\n\n                if len(coords) &lt; 4:\n                    return None, None, None, None, None\n\n                # Calculate lengths of all four sides\n                sides = []\n                for i in range(len(coords)):\n                    p1 = coords[i]\n                    p2 = coords[(i + 1) % len(coords)]\n                    dx = p2[0] - p1[0]\n                    dy = p2[1] - p1[1]\n                    length = np.sqrt(dx**2 + dy**2)\n                    angle = np.degrees(np.arctan2(dy, dx)) % 180\n                    sides.append((length, angle, p1, p2))\n\n                # Group sides by length (allowing for small differences due to floating point precision)\n                # This ensures we correctly identify the rectangle's dimensions\n                sides_grouped = {}\n                tolerance = 1e-6  # Tolerance for length comparison\n\n                for s in sides:\n                    length, angle = s[0], s[1]\n                    matched = False\n\n                    for key in sides_grouped:\n                        if abs(length - key) &lt; tolerance:\n                            sides_grouped[key].append(s)\n                            matched = True\n                            break\n\n                    if not matched:\n                        sides_grouped[length] = [s]\n\n                # Get unique lengths (should be 2 for a rectangle, parallel sides have equal length)\n                unique_lengths = sorted(sides_grouped.keys(), reverse=True)\n\n                if len(unique_lengths) != 2:\n                    # If we don't get exactly 2 unique lengths, something is wrong with the rectangle\n                    # Fall back to simpler method using bounds\n                    bounds = rect.bounds\n                    width = bounds[2] - bounds[0]\n                    height = bounds[3] - bounds[1]\n                    major_length = max(width, height)\n                    minor_length = min(width, height)\n                    orientation = 0 if width &gt; height else 90\n                else:\n                    major_length = unique_lengths[0]\n                    minor_length = unique_lengths[1]\n                    # Get orientation from the major axis\n                    orientation = sides_grouped[major_length][0][1]\n\n                # Calculate eccentricity\n                if major_length &gt; 0:\n                    # Eccentricity for an ellipse: e = sqrt(1 - (b\u00b2/a\u00b2))\n                    # where a is the semi-major axis and b is the semi-minor axis\n                    eccentricity = np.sqrt(\n                        1 - ((minor_length / 2) ** 2 / (major_length / 2) ** 2)\n                    )\n                else:\n                    eccentricity = 0\n\n                # Calculate elongation (ratio of minor to major axis)\n                elongation = major_length / minor_length if major_length &gt; 0 else 1\n\n                return major_length, minor_length, eccentricity, orientation, elongation\n\n            except Exception as e:\n                # For debugging\n                # print(f\"Error calculating axes: {e}\")\n                return None, None, None, None, None\n\n        # Apply the function and split the results\n        axes_data = result.geometry.apply(get_axes_properties)\n\n        if \"major_length\" in properties:\n            result[\"major_length\"] = axes_data.apply(lambda x: x[0] if x else None)\n            # Convert to requested units\n            if length_unit == \"km\":\n                result[\"major_length\"] = result[\"major_length\"] / 1_000\n                result.rename(columns={\"major_length\": \"major_length_km\"}, inplace=True)\n            else:\n                result.rename(columns={\"major_length\": \"major_length_m\"}, inplace=True)\n\n        if \"minor_length\" in properties:\n            result[\"minor_length\"] = axes_data.apply(lambda x: x[1] if x else None)\n            # Convert to requested units\n            if length_unit == \"km\":\n                result[\"minor_length\"] = result[\"minor_length\"] / 1_000\n                result.rename(columns={\"minor_length\": \"minor_length_km\"}, inplace=True)\n            else:\n                result.rename(columns={\"minor_length\": \"minor_length_m\"}, inplace=True)\n\n        if \"eccentricity\" in properties:\n            result[\"eccentricity\"] = axes_data.apply(lambda x: x[2] if x else None)\n\n        if \"orientation\" in properties:\n            result[\"orientation\"] = axes_data.apply(lambda x: x[3] if x else None)\n\n        if \"elongation\" in properties:\n            result[\"elongation\"] = axes_data.apply(lambda x: x[4] if x else None)\n\n    # Equivalent diameter based on area\n    if \"diameter_areagth\" in properties:\n\n        def get_equivalent_diameter(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)) or geom.area &lt;= 0:\n                return None\n            # Diameter of a circle with the same area: d = 2 * sqrt(A / \u03c0)\n            return 2 * np.sqrt(geom.area / np.pi)\n\n        result[\"diameter_areagth\"] = result.geometry.apply(get_equivalent_diameter)\n\n        # Convert to requested units\n        if length_unit == \"km\":\n            result[\"diameter_areagth\"] = result[\"diameter_areagth\"] / 1_000\n            result.rename(\n                columns={\"diameter_areagth\": \"equivalent_diameter_area_km\"},\n                inplace=True,\n            )\n        else:\n            result.rename(\n                columns={\"diameter_areagth\": \"equivalent_diameter_area_m\"},\n                inplace=True,\n            )\n\n    # Extent (ratio of shape area to bounding box area)\n    if \"extent\" in properties:\n\n        def get_extent(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)) or geom.area &lt;= 0:\n                return None\n\n            bounds = geom.bounds\n            bbox_area = (bounds[2] - bounds[0]) * (bounds[3] - bounds[1])\n\n            if bbox_area &gt; 0:\n                return geom.area / bbox_area\n            return None\n\n        result[\"extent\"] = result.geometry.apply(get_extent)\n\n    # Solidity (ratio of shape area to convex hull area)\n    if \"solidity\" in properties:\n\n        def get_solidity(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)) or geom.area &lt;= 0:\n                return None\n\n            convex_hull_area = geom.convex_hull.area\n\n            if convex_hull_area &gt; 0:\n                return geom.area / convex_hull_area\n            return None\n\n        result[\"solidity\"] = result.geometry.apply(get_solidity)\n\n    # Complexity (ratio of perimeter to area)\n    if \"complexity\" in properties:\n\n        def calc_complexity(geom):\n            if isinstance(geom, (Polygon, MultiPolygon)) and geom.area &gt; 0:\n                # Shape index: P / (2 * sqrt(\u03c0 * A))\n                # Normalized to 1 for a circle, higher for more complex shapes\n                return geom.boundary.length / (2 * np.sqrt(np.pi * geom.area))\n            return None\n\n        result[\"complexity\"] = result.geometry.apply(calc_complexity)\n\n    return result\n</code></pre>"},{"location":"utils/#geoai.utils.analyze_vector_attributes","title":"<code>analyze_vector_attributes(vector_path, attribute_name)</code>","text":"<p>Analyze a specific attribute in a vector dataset and create a histogram.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <code>attribute_name</code> <code>str</code> <p>Name of the attribute to analyze</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing analysis results for the attribute</p> Source code in <code>geoai/utils.py</code> <pre><code>def analyze_vector_attributes(vector_path, attribute_name):\n    \"\"\"Analyze a specific attribute in a vector dataset and create a histogram.\n\n    Args:\n        vector_path (str): Path to the vector file\n        attribute_name (str): Name of the attribute to analyze\n\n    Returns:\n        dict: Dictionary containing analysis results for the attribute\n    \"\"\"\n    try:\n        gdf = gpd.read_file(vector_path)\n\n        # Check if attribute exists\n        if attribute_name not in gdf.columns:\n            print(f\"Attribute '{attribute_name}' not found in the dataset\")\n            return None\n\n        # Get the attribute series\n        attr = gdf[attribute_name]\n\n        # Perform different analyses based on data type\n        if pd.api.types.is_numeric_dtype(attr):\n            # Numeric attribute\n            analysis = {\n                \"attribute\": attribute_name,\n                \"type\": \"numeric\",\n                \"count\": attr.count(),\n                \"null_count\": attr.isna().sum(),\n                \"min\": attr.min(),\n                \"max\": attr.max(),\n                \"mean\": attr.mean(),\n                \"median\": attr.median(),\n                \"std\": attr.std(),\n                \"unique_values\": attr.nunique(),\n            }\n\n            # Create histogram\n            plt.figure(figsize=(10, 6))\n            plt.hist(attr.dropna(), bins=20, alpha=0.7, color=\"blue\")\n            plt.title(f\"Histogram of {attribute_name}\")\n            plt.xlabel(attribute_name)\n            plt.ylabel(\"Frequency\")\n            plt.grid(True, alpha=0.3)\n            plt.show()\n\n        else:\n            # Categorical attribute\n            analysis = {\n                \"attribute\": attribute_name,\n                \"type\": \"categorical\",\n                \"count\": attr.count(),\n                \"null_count\": attr.isna().sum(),\n                \"unique_values\": attr.nunique(),\n                \"value_counts\": attr.value_counts().to_dict(),\n            }\n\n            # Create bar plot for top categories\n            top_n = min(10, attr.nunique())\n            plt.figure(figsize=(10, 6))\n            attr.value_counts().head(top_n).plot(kind=\"bar\", color=\"skyblue\")\n            plt.title(f\"Top {top_n} values for {attribute_name}\")\n            plt.xlabel(attribute_name)\n            plt.ylabel(\"Count\")\n            plt.xticks(rotation=45)\n            plt.grid(True, alpha=0.3)\n            plt.tight_layout()\n            plt.show()\n\n        return analysis\n\n    except Exception as e:\n        print(f\"Error analyzing attribute: {str(e)}\")\n        return None\n</code></pre>"},{"location":"utils/#geoai.utils.batch_raster_to_vector","title":"<code>batch_raster_to_vector(input_dir, output_dir, pattern='*.tif', threshold=0, min_area=10, simplify_tolerance=None, class_values=None, attribute_name='class', output_format='geojson', merge_output=False, merge_filename='merged_vectors')</code>","text":"<p>Batch convert multiple raster files to vector polygons.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>Directory containing input raster files.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output vector files.</p> required <code>pattern</code> <code>str</code> <p>Pattern to match raster files (e.g., '*.tif').</p> <code>'*.tif'</code> <code>threshold</code> <code>int/float</code> <p>Pixel values greater than this threshold will be vectorized.</p> <code>0</code> <code>min_area</code> <code>float</code> <p>Minimum polygon area in square map units to keep.</p> <code>10</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for geometry simplification. None for no simplification.</p> <code>None</code> <code>class_values</code> <code>list</code> <p>Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.</p> <code>None</code> <code>attribute_name</code> <code>str</code> <p>Name of the attribute field for the class values.</p> <code>'class'</code> <code>output_format</code> <code>str</code> <p>Format for output files - 'geojson', 'shapefile', 'gpkg'.</p> <code>'geojson'</code> <code>merge_output</code> <code>bool</code> <p>Whether to merge all output vectors into a single file.</p> <code>False</code> <code>merge_filename</code> <code>str</code> <p>Filename for the merged output (without extension).</p> <code>'merged_vectors'</code> <p>Returns:</p> Type Description <code>geopandas.GeoDataFrame or None</code> <p>If merge_output is True, returns the merged GeoDataFrame.</p> Source code in <code>geoai/utils.py</code> <pre><code>def batch_raster_to_vector(\n    input_dir,\n    output_dir,\n    pattern=\"*.tif\",\n    threshold=0,\n    min_area=10,\n    simplify_tolerance=None,\n    class_values=None,\n    attribute_name=\"class\",\n    output_format=\"geojson\",\n    merge_output=False,\n    merge_filename=\"merged_vectors\",\n):\n    \"\"\"\n    Batch convert multiple raster files to vector polygons.\n\n    Args:\n        input_dir (str): Directory containing input raster files.\n        output_dir (str): Directory to save output vector files.\n        pattern (str): Pattern to match raster files (e.g., '*.tif').\n        threshold (int/float): Pixel values greater than this threshold will be vectorized.\n        min_area (float): Minimum polygon area in square map units to keep.\n        simplify_tolerance (float): Tolerance for geometry simplification. None for no simplification.\n        class_values (list): Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.\n        attribute_name (str): Name of the attribute field for the class values.\n        output_format (str): Format for output files - 'geojson', 'shapefile', 'gpkg'.\n        merge_output (bool): Whether to merge all output vectors into a single file.\n        merge_filename (str): Filename for the merged output (without extension).\n\n    Returns:\n        geopandas.GeoDataFrame or None: If merge_output is True, returns the merged GeoDataFrame.\n    \"\"\"\n    import glob\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Get list of raster files\n    raster_files = glob.glob(os.path.join(input_dir, pattern))\n\n    if not raster_files:\n        print(f\"No files matching pattern '{pattern}' found in {input_dir}\")\n        return None\n\n    print(f\"Found {len(raster_files)} raster files to process\")\n\n    # Process each raster file\n    gdfs = []\n    for raster_file in tqdm(raster_files, desc=\"Processing rasters\"):\n        # Get output filename\n        base_name = os.path.splitext(os.path.basename(raster_file))[0]\n        if output_format.lower() == \"geojson\":\n            out_file = os.path.join(output_dir, f\"{base_name}.geojson\")\n        elif output_format.lower() == \"shapefile\":\n            out_file = os.path.join(output_dir, f\"{base_name}.shp\")\n        elif output_format.lower() == \"gpkg\":\n            out_file = os.path.join(output_dir, f\"{base_name}.gpkg\")\n        else:\n            raise ValueError(f\"Unsupported output format: {output_format}\")\n\n        # Convert raster to vector\n        if merge_output:\n            # Don't save individual files if merging\n            gdf = raster_to_vector(\n                raster_file,\n                output_path=None,\n                threshold=threshold,\n                min_area=min_area,\n                simplify_tolerance=simplify_tolerance,\n                class_values=class_values,\n                attribute_name=attribute_name,\n            )\n\n            # Add filename as attribute\n            if not gdf.empty:\n                gdf[\"source_file\"] = base_name\n                gdfs.append(gdf)\n        else:\n            # Save individual files\n            raster_to_vector(\n                raster_file,\n                output_path=out_file,\n                threshold=threshold,\n                min_area=min_area,\n                simplify_tolerance=simplify_tolerance,\n                class_values=class_values,\n                attribute_name=attribute_name,\n                output_format=output_format,\n            )\n\n    # Merge output if requested\n    if merge_output and gdfs:\n        merged_gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True))\n\n        # Set CRS to the CRS of the first GeoDataFrame\n        if merged_gdf.crs is None and gdfs:\n            merged_gdf.crs = gdfs[0].crs\n\n        # Save merged output\n        if output_format.lower() == \"geojson\":\n            merged_file = os.path.join(output_dir, f\"{merge_filename}.geojson\")\n            merged_gdf.to_file(merged_file, driver=\"GeoJSON\")\n        elif output_format.lower() == \"shapefile\":\n            merged_file = os.path.join(output_dir, f\"{merge_filename}.shp\")\n            merged_gdf.to_file(merged_file)\n        elif output_format.lower() == \"gpkg\":\n            merged_file = os.path.join(output_dir, f\"{merge_filename}.gpkg\")\n            merged_gdf.to_file(merged_file, driver=\"GPKG\")\n\n        print(f\"Merged vector data saved to {merged_file}\")\n        return merged_gdf\n\n    return None\n</code></pre>"},{"location":"utils/#geoai.utils.batch_vector_to_raster","title":"<code>batch_vector_to_raster(vector_path, output_dir, attribute_field=None, reference_rasters=None, bounds_list=None, output_filename_pattern='{vector_name}_{index}', pixel_size=1.0, all_touched=False, fill_value=0, dtype=&lt;class 'numpy.uint8'&gt;, nodata=None)</code>","text":"<p>Batch convert vector data to multiple rasters based on different extents or reference rasters.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str or GeoDataFrame</code> <p>Path to the input vector file or a GeoDataFrame.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output raster files.</p> required <code>attribute_field</code> <code>str</code> <p>Field name in the vector data to use for pixel values.</p> <code>None</code> <code>reference_rasters</code> <code>list</code> <p>List of paths to reference rasters for dimensions, transform and CRS.</p> <code>None</code> <code>bounds_list</code> <code>list</code> <p>List of bounds tuples (left, bottom, right, top) to use if reference_rasters not provided.</p> <code>None</code> <code>output_filename_pattern</code> <code>str</code> <p>Pattern for output filenames. Can include {vector_name} and {index} placeholders.</p> <code>'{vector_name}_{index}'</code> <code>pixel_size</code> <code>float or tuple</code> <p>Pixel size to use if reference_rasters not provided.</p> <code>1.0</code> <code>all_touched</code> <code>bool</code> <p>If True, all pixels touched by geometries will be burned in.</p> <code>False</code> <code>fill_value</code> <code>int</code> <p>Value to fill the raster with before burning in features.</p> <code>0</code> <code>dtype</code> <code>numpy.dtype</code> <p>Data type of the output raster.</p> <code>&lt;class 'numpy.uint8'&gt;</code> <code>nodata</code> <code>int</code> <p>No data value for the output raster.</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>List of paths to the created raster files.</p> Source code in <code>geoai/utils.py</code> <pre><code>def batch_vector_to_raster(\n    vector_path,\n    output_dir,\n    attribute_field=None,\n    reference_rasters=None,\n    bounds_list=None,\n    output_filename_pattern=\"{vector_name}_{index}\",\n    pixel_size=1.0,\n    all_touched=False,\n    fill_value=0,\n    dtype=np.uint8,\n    nodata=None,\n):\n    \"\"\"\n    Batch convert vector data to multiple rasters based on different extents or reference rasters.\n\n    Args:\n        vector_path (str or GeoDataFrame): Path to the input vector file or a GeoDataFrame.\n        output_dir (str): Directory to save output raster files.\n        attribute_field (str): Field name in the vector data to use for pixel values.\n        reference_rasters (list): List of paths to reference rasters for dimensions, transform and CRS.\n        bounds_list (list): List of bounds tuples (left, bottom, right, top) to use if reference_rasters not provided.\n        output_filename_pattern (str): Pattern for output filenames.\n            Can include {vector_name} and {index} placeholders.\n        pixel_size (float or tuple): Pixel size to use if reference_rasters not provided.\n        all_touched (bool): If True, all pixels touched by geometries will be burned in.\n        fill_value (int): Value to fill the raster with before burning in features.\n        dtype (numpy.dtype): Data type of the output raster.\n        nodata (int): No data value for the output raster.\n\n    Returns:\n        list: List of paths to the created raster files.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Load vector data if it's a path\n    if isinstance(vector_path, str):\n        gdf = gpd.read_file(vector_path)\n        vector_name = os.path.splitext(os.path.basename(vector_path))[0]\n    else:\n        gdf = vector_path\n        vector_name = \"vector\"\n\n    # Check input parameters\n    if reference_rasters is None and bounds_list is None:\n        raise ValueError(\"Either reference_rasters or bounds_list must be provided.\")\n\n    # Use reference_rasters if provided, otherwise use bounds_list\n    if reference_rasters is not None:\n        sources = reference_rasters\n        is_raster_reference = True\n    else:\n        sources = bounds_list\n        is_raster_reference = False\n\n    # Create output filenames\n    output_files = []\n\n    # Process each source (reference raster or bounds)\n    for i, source in enumerate(tqdm(sources, desc=\"Processing\")):\n        # Generate output filename\n        output_filename = output_filename_pattern.format(\n            vector_name=vector_name, index=i\n        )\n        if not output_filename.endswith(\".tif\"):\n            output_filename += \".tif\"\n        output_path = os.path.join(output_dir, output_filename)\n\n        if is_raster_reference:\n            # Use reference raster\n            vector_to_raster(\n                vector_path=gdf,\n                output_path=output_path,\n                reference_raster=source,\n                attribute_field=attribute_field,\n                all_touched=all_touched,\n                fill_value=fill_value,\n                dtype=dtype,\n                nodata=nodata,\n            )\n        else:\n            # Use bounds\n            vector_to_raster(\n                vector_path=gdf,\n                output_path=output_path,\n                bounds=source,\n                pixel_size=pixel_size,\n                attribute_field=attribute_field,\n                all_touched=all_touched,\n                fill_value=fill_value,\n                dtype=dtype,\n                nodata=nodata,\n            )\n\n        output_files.append(output_path)\n\n    return output_files\n</code></pre>"},{"location":"utils/#geoai.utils.calc_stats","title":"<code>calc_stats(dataset, divide_by=1.0)</code>","text":"<p>Calculate the statistics (mean and std) for the entire dataset.</p> <p>This function is adapted from the plot_batch() function in the torchgeo library at https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html. Credit to the torchgeo developers for the original implementation.</p> <p>Warning: This is an approximation. The correct value should take into account the mean for the whole dataset for computing individual stds.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>RasterDataset</code> <p>The dataset to calculate statistics for.</p> required <code>divide_by</code> <code>float</code> <p>The value to divide the image data by. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tuple[np.ndarray, np.ndarray]</code> <p>The mean and standard deviation for each band.</p> Source code in <code>geoai/utils.py</code> <pre><code>def calc_stats(dataset, divide_by: float = 1.0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate the statistics (mean and std) for the entire dataset.\n\n    This function is adapted from the plot_batch() function in the torchgeo library at\n    https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html.\n    Credit to the torchgeo developers for the original implementation.\n\n    Warning: This is an approximation. The correct value should take into account the\n    mean for the whole dataset for computing individual stds.\n\n    Args:\n        dataset (RasterDataset): The dataset to calculate statistics for.\n        divide_by (float, optional): The value to divide the image data by. Defaults to 1.0.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: The mean and standard deviation for each band.\n    \"\"\"\n\n    # To avoid loading the entire dataset in memory, we will loop through each img\n    # The filenames will be retrieved from the dataset's rtree index\n    files = [\n        item.object\n        for item in dataset.index.intersection(dataset.index.bounds, objects=True)\n    ]\n\n    # Resetting statistics\n    accum_mean = 0\n    accum_std = 0\n\n    for file in files:\n        img = rasterio.open(file).read() / divide_by  # type: ignore\n        accum_mean += img.reshape((img.shape[0], -1)).mean(axis=1)\n        accum_std += img.reshape((img.shape[0], -1)).std(axis=1)\n\n    # at the end, we shall have 2 vectors with length n=chnls\n    # we will average them considering the number of images\n    return accum_mean / len(files), accum_std / len(files)\n</code></pre>"},{"location":"utils/#geoai.utils.clip_raster_by_bbox","title":"<code>clip_raster_by_bbox(input_raster, output_raster, bbox, bands=None, bbox_type='geo', bbox_crs=None)</code>","text":"<p>Clip a raster dataset using a bounding box and optionally select specific bands.</p> <p>Parameters:</p> Name Type Description Default <code>input_raster</code> <code>str</code> <p>Path to the input raster file.</p> required <code>output_raster</code> <code>str</code> <p>Path where the clipped raster will be saved.</p> required <code>bbox</code> <code>tuple</code> <p>Bounding box coordinates either as:          - Geographic coordinates (minx, miny, maxx, maxy) if bbox_type=\"geo\"          - Pixel indices (min_row, min_col, max_row, max_col) if bbox_type=\"pixel\"</p> required <code>bands</code> <code>list</code> <p>List of band indices to keep (1-based indexing).                    If None, all bands will be kept.</p> <code>None</code> <code>bbox_type</code> <code>str</code> <p>Type of bounding box coordinates. Either \"geo\" for                       geographic coordinates or \"pixel\" for row/column indices.                       Default is \"geo\".</p> <code>'geo'</code> <code>bbox_crs</code> <code>str or dict</code> <p>CRS of the bbox if different from the raster CRS.                              Can be provided as EPSG code (e.g., \"EPSG:4326\") or                              as a proj4 string. Only applies when bbox_type=\"geo\".                              If None, assumes bbox is in the same CRS as the raster.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the clipped output raster.</p> <p>Exceptions:</p> Type Description <code>ImportError</code> <p>If required dependencies are not installed.</p> <code>ValueError</code> <p>If the bbox is invalid, bands are out of range, or bbox_type is invalid.</p> <code>RuntimeError</code> <p>If the clipping operation fails.</p> <p>Examples:</p> <p>Clip using geographic coordinates in the same CRS as the raster</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_geo.tif', (100, 200, 300, 400))\n'clipped_geo.tif'\n</code></pre> <p>Clip using WGS84 coordinates when the raster is in a different CRS</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_wgs84.tif', (-122.5, 37.7, -122.4, 37.8),\n...                     bbox_crs=\"EPSG:4326\")\n'clipped_wgs84.tif'\n</code></pre> <p>Clip using row/column indices</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_pixel.tif', (50, 100, 150, 200),\n...                     bbox_type=\"pixel\")\n'clipped_pixel.tif'\n</code></pre> <p>Clip with band selection</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_bands.tif', (100, 200, 300, 400),\n...                     bands=[1, 3])\n'clipped_bands.tif'\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def clip_raster_by_bbox(\n    input_raster, output_raster, bbox, bands=None, bbox_type=\"geo\", bbox_crs=None\n):\n    \"\"\"\n    Clip a raster dataset using a bounding box and optionally select specific bands.\n\n    Args:\n        input_raster (str): Path to the input raster file.\n        output_raster (str): Path where the clipped raster will be saved.\n        bbox (tuple): Bounding box coordinates either as:\n                     - Geographic coordinates (minx, miny, maxx, maxy) if bbox_type=\"geo\"\n                     - Pixel indices (min_row, min_col, max_row, max_col) if bbox_type=\"pixel\"\n        bands (list, optional): List of band indices to keep (1-based indexing).\n                               If None, all bands will be kept.\n        bbox_type (str, optional): Type of bounding box coordinates. Either \"geo\" for\n                                  geographic coordinates or \"pixel\" for row/column indices.\n                                  Default is \"geo\".\n        bbox_crs (str or dict, optional): CRS of the bbox if different from the raster CRS.\n                                         Can be provided as EPSG code (e.g., \"EPSG:4326\") or\n                                         as a proj4 string. Only applies when bbox_type=\"geo\".\n                                         If None, assumes bbox is in the same CRS as the raster.\n\n    Returns:\n        str: Path to the clipped output raster.\n\n    Raises:\n        ImportError: If required dependencies are not installed.\n        ValueError: If the bbox is invalid, bands are out of range, or bbox_type is invalid.\n        RuntimeError: If the clipping operation fails.\n\n    Examples:\n        Clip using geographic coordinates in the same CRS as the raster\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_geo.tif', (100, 200, 300, 400))\n        'clipped_geo.tif'\n\n        Clip using WGS84 coordinates when the raster is in a different CRS\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_wgs84.tif', (-122.5, 37.7, -122.4, 37.8),\n        ...                     bbox_crs=\"EPSG:4326\")\n        'clipped_wgs84.tif'\n\n        Clip using row/column indices\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_pixel.tif', (50, 100, 150, 200),\n        ...                     bbox_type=\"pixel\")\n        'clipped_pixel.tif'\n\n        Clip with band selection\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_bands.tif', (100, 200, 300, 400),\n        ...                     bands=[1, 3])\n        'clipped_bands.tif'\n    \"\"\"\n    from rasterio.transform import from_bounds\n    from rasterio.warp import transform_bounds\n\n    # Validate bbox_type\n    if bbox_type not in [\"geo\", \"pixel\"]:\n        raise ValueError(\"bbox_type must be either 'geo' or 'pixel'\")\n\n    # Validate bbox\n    if len(bbox) != 4:\n        raise ValueError(\"bbox must contain exactly 4 values\")\n\n    # Open the source raster\n    with rasterio.open(input_raster) as src:\n        # Get the source CRS\n        src_crs = src.crs\n\n        # Handle different bbox types\n        if bbox_type == \"geo\":\n            minx, miny, maxx, maxy = bbox\n\n            # Validate geographic bbox\n            if minx &gt;= maxx or miny &gt;= maxy:\n                raise ValueError(\n                    \"Invalid geographic bbox. Expected (minx, miny, maxx, maxy) where minx &lt; maxx and miny &lt; maxy\"\n                )\n\n            # If bbox_crs is provided and different from the source CRS, transform the bbox\n            if bbox_crs is not None and bbox_crs != src_crs:\n                try:\n                    # Transform bbox coordinates from bbox_crs to src_crs\n                    minx, miny, maxx, maxy = transform_bounds(\n                        bbox_crs, src_crs, minx, miny, maxx, maxy\n                    )\n                except Exception as e:\n                    raise ValueError(\n                        f\"Failed to transform bbox from {bbox_crs} to {src_crs}: {str(e)}\"\n                    )\n\n            # Calculate the pixel window from geographic coordinates\n            window = src.window(minx, miny, maxx, maxy)\n\n            # Use the same bounds for the output transform\n            output_bounds = (minx, miny, maxx, maxy)\n\n        else:  # bbox_type == \"pixel\"\n            min_row, min_col, max_row, max_col = bbox\n\n            # Validate pixel bbox\n            if min_row &gt;= max_row or min_col &gt;= max_col:\n                raise ValueError(\n                    \"Invalid pixel bbox. Expected (min_row, min_col, max_row, max_col) where min_row &lt; max_row and min_col &lt; max_col\"\n                )\n\n            if (\n                min_row &lt; 0\n                or min_col &lt; 0\n                or max_row &gt; src.height\n                or max_col &gt; src.width\n            ):\n                raise ValueError(\n                    f\"Pixel indices out of bounds. Raster dimensions are {src.height} rows x {src.width} columns\"\n                )\n\n            # Create a window from pixel coordinates\n            window = Window(min_col, min_row, max_col - min_col, max_row - min_row)\n\n            # Calculate the geographic bounds for this window\n            window_transform = src.window_transform(window)\n            output_bounds = rasterio.transform.array_bounds(\n                window.height, window.width, window_transform\n            )\n            # Reorder to (minx, miny, maxx, maxy)\n            output_bounds = (\n                output_bounds[0],\n                output_bounds[1],\n                output_bounds[2],\n                output_bounds[3],\n            )\n\n        # Get window dimensions\n        window_width = int(window.width)\n        window_height = int(window.height)\n\n        # Check if the window is valid\n        if window_width &lt;= 0 or window_height &lt;= 0:\n            raise ValueError(\"Bounding box results in an empty window\")\n\n        # Handle band selection\n        if bands is None:\n            # Use all bands\n            bands_to_read = list(range(1, src.count + 1))\n        else:\n            # Validate band indices\n            if not all(1 &lt;= b &lt;= src.count for b in bands):\n                raise ValueError(f\"Band indices must be between 1 and {src.count}\")\n            bands_to_read = bands\n\n        # Calculate new transform for the clipped raster\n        new_transform = from_bounds(\n            output_bounds[0],\n            output_bounds[1],\n            output_bounds[2],\n            output_bounds[3],\n            window_width,\n            window_height,\n        )\n\n        # Create a metadata dictionary for the output\n        out_meta = src.meta.copy()\n        out_meta.update(\n            {\n                \"height\": window_height,\n                \"width\": window_width,\n                \"transform\": new_transform,\n                \"count\": len(bands_to_read),\n            }\n        )\n\n        # Read the data for the selected bands\n        data = []\n        for band_idx in bands_to_read:\n            band_data = src.read(band_idx, window=window)\n            data.append(band_data)\n\n        # Stack the bands into a single array\n        if len(data) &gt; 1:\n            clipped_data = np.stack(data)\n        else:\n            clipped_data = data[0][np.newaxis, :, :]\n\n        # Write the output raster\n        with rasterio.open(output_raster, \"w\", **out_meta) as dst:\n            dst.write(clipped_data)\n\n    return output_raster\n</code></pre>"},{"location":"utils/#geoai.utils.create_overview_image","title":"<code>create_overview_image(src, tile_coordinates, output_path, tile_size, stride, geojson_path=None)</code>","text":"<p>Create an overview image showing all tiles and their status, with optional GeoJSON export.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>rasterio.io.DatasetReader</code> <p>The source raster dataset.</p> required <code>tile_coordinates</code> <code>list</code> <p>A list of dictionaries containing tile information.</p> required <code>output_path</code> <code>str</code> <p>The path where the overview image will be saved.</p> required <code>tile_size</code> <code>int</code> <p>The size of each tile in pixels.</p> required <code>stride</code> <code>int</code> <p>The stride between tiles in pixels. Controls overlap between adjacent tiles.</p> required <code>geojson_path</code> <code>str</code> <p>If provided, exports the tile rectangles as GeoJSON to this path.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the saved overview image.</p> Source code in <code>geoai/utils.py</code> <pre><code>def create_overview_image(\n    src, tile_coordinates, output_path, tile_size, stride, geojson_path=None\n):\n    \"\"\"Create an overview image showing all tiles and their status, with optional GeoJSON export.\n\n    Args:\n        src (rasterio.io.DatasetReader): The source raster dataset.\n        tile_coordinates (list): A list of dictionaries containing tile information.\n        output_path (str): The path where the overview image will be saved.\n        tile_size (int): The size of each tile in pixels.\n        stride (int): The stride between tiles in pixels. Controls overlap between adjacent tiles.\n        geojson_path (str, optional): If provided, exports the tile rectangles as GeoJSON to this path.\n\n    Returns:\n        str: Path to the saved overview image.\n    \"\"\"\n    # Read a reduced version of the source image\n    overview_scale = max(\n        1, int(max(src.width, src.height) / 2000)\n    )  # Scale to max ~2000px\n    overview_width = src.width // overview_scale\n    overview_height = src.height // overview_scale\n\n    # Read downsampled image\n    overview_data = src.read(\n        out_shape=(src.count, overview_height, overview_width),\n        resampling=rasterio.enums.Resampling.average,\n    )\n\n    # Create RGB image for display\n    if overview_data.shape[0] &gt;= 3:\n        rgb = np.moveaxis(overview_data[:3], 0, -1)\n    else:\n        # For single band, create grayscale RGB\n        rgb = np.stack([overview_data[0], overview_data[0], overview_data[0]], axis=-1)\n\n    # Normalize for display\n    for i in range(rgb.shape[-1]):\n        band = rgb[..., i]\n        non_zero = band[band &gt; 0]\n        if len(non_zero) &gt; 0:\n            p2, p98 = np.percentile(non_zero, (2, 98))\n            rgb[..., i] = np.clip((band - p2) / (p98 - p2), 0, 1)\n\n    # Create figure\n    plt.figure(figsize=(12, 12))\n    plt.imshow(rgb)\n\n    # If GeoJSON export is requested, prepare GeoJSON structures\n    if geojson_path:\n        features = []\n\n    # Draw tile boundaries\n    for tile in tile_coordinates:\n        # Convert bounds to pixel coordinates in overview\n        bounds = tile[\"bounds\"]\n        # Calculate scaled pixel coordinates\n        x_min = int((tile[\"x\"]) / overview_scale)\n        y_min = int((tile[\"y\"]) / overview_scale)\n        width = int(tile_size / overview_scale)\n        height = int(tile_size / overview_scale)\n\n        # Draw rectangle\n        color = \"lime\" if tile[\"has_features\"] else \"red\"\n        rect = plt.Rectangle(\n            (x_min, y_min), width, height, fill=False, edgecolor=color, linewidth=0.5\n        )\n        plt.gca().add_patch(rect)\n\n        # Add tile number if not too crowded\n        if width &gt; 20 and height &gt; 20:\n            plt.text(\n                x_min + width / 2,\n                y_min + height / 2,\n                str(tile[\"index\"]),\n                color=\"white\",\n                ha=\"center\",\n                va=\"center\",\n                fontsize=8,\n            )\n\n        # Add to GeoJSON features if exporting\n        if geojson_path:\n            # Create a polygon from the bounds (already in geo-coordinates)\n            minx, miny, maxx, maxy = bounds\n            polygon = box(minx, miny, maxx, maxy)\n\n            # Calculate overlap with neighboring tiles\n            overlap = 0\n            if stride &lt; tile_size:\n                overlap = tile_size - stride\n\n            # Create a GeoJSON feature\n            feature = {\n                \"type\": \"Feature\",\n                \"geometry\": mapping(polygon),\n                \"properties\": {\n                    \"index\": tile[\"index\"],\n                    \"has_features\": tile[\"has_features\"],\n                    \"bounds_pixel\": [\n                        tile[\"x\"],\n                        tile[\"y\"],\n                        tile[\"x\"] + tile_size,\n                        tile[\"y\"] + tile_size,\n                    ],\n                    \"tile_size_px\": tile_size,\n                    \"stride_px\": stride,\n                    \"overlap_px\": overlap,\n                },\n            }\n\n            # Add any additional properties from the tile\n            for key, value in tile.items():\n                if key not in [\"x\", \"y\", \"index\", \"has_features\", \"bounds\"]:\n                    feature[\"properties\"][key] = value\n\n            features.append(feature)\n\n    plt.title(\"Tile Overview (Green = Contains Features, Red = Empty)\")\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\n    print(f\"Overview image saved to {output_path}\")\n\n    # Export GeoJSON if requested\n    if geojson_path:\n        geojson_collection = {\n            \"type\": \"FeatureCollection\",\n            \"features\": features,\n            \"properties\": {\n                \"crs\": (\n                    src.crs.to_string()\n                    if hasattr(src.crs, \"to_string\")\n                    else str(src.crs)\n                ),\n                \"total_tiles\": len(features),\n                \"source_raster_dimensions\": [src.width, src.height],\n            },\n        }\n\n        # Save to file\n        with open(geojson_path, \"w\") as f:\n            json.dump(geojson_collection, f)\n\n        print(f\"GeoJSON saved to {geojson_path}\")\n\n    return output_path\n</code></pre>"},{"location":"utils/#geoai.utils.create_split_map","title":"<code>create_split_map(left_layer='TERRAIN', right_layer='OpenTopoMap', left_args=None, right_args=None, left_array_args=None, right_array_args=None, zoom_control=True, fullscreen_control=True, layer_control=True, add_close_button=False, left_label=None, right_label=None, left_position='bottomleft', right_position='bottomright', widget_layout=None, draggable=True, center=[20, 0], zoom=2, height='600px', basemap=None, basemap_args=None, m=None, **kwargs)</code>","text":"<p>Adds split map.</p> <p>Parameters:</p> Name Type Description Default <code>left_layer</code> <code>str</code> <p>The left tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'TERRAIN'.</p> <code>'TERRAIN'</code> <code>right_layer</code> <code>str</code> <p>The right tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'OpenTopoMap'.</p> <code>'OpenTopoMap'</code> <code>left_args</code> <code>dict</code> <p>The arguments for the left tile layer. Defaults to {}.</p> <code>None</code> <code>right_args</code> <code>dict</code> <p>The arguments for the right tile layer. Defaults to {}.</p> <code>None</code> <code>left_array_args</code> <code>dict</code> <p>The arguments for array_to_image for the left layer. Defaults to {}.</p> <code>None</code> <code>right_array_args</code> <code>dict</code> <p>The arguments for array_to_image for the right layer. Defaults to {}.</p> <code>None</code> <code>zoom_control</code> <code>bool</code> <p>Whether to add zoom control. Defaults to True.</p> <code>True</code> <code>fullscreen_control</code> <code>bool</code> <p>Whether to add fullscreen control. Defaults to True.</p> <code>True</code> <code>layer_control</code> <code>bool</code> <p>Whether to add layer control. Defaults to True.</p> <code>True</code> <code>add_close_button</code> <code>bool</code> <p>Whether to add a close button. Defaults to False.</p> <code>False</code> <code>left_label</code> <code>str</code> <p>The label for the left layer. Defaults to None.</p> <code>None</code> <code>right_label</code> <code>str</code> <p>The label for the right layer. Defaults to None.</p> <code>None</code> <code>left_position</code> <code>str</code> <p>The position for the left label. Defaults to \"bottomleft\".</p> <code>'bottomleft'</code> <code>right_position</code> <code>str</code> <p>The position for the right label. Defaults to \"bottomright\".</p> <code>'bottomright'</code> <code>widget_layout</code> <code>dict</code> <p>The layout for the widget. Defaults to None.</p> <code>None</code> <code>draggable</code> <code>bool</code> <p>Whether the split map is draggable. Defaults to True.</p> <code>True</code> Source code in <code>geoai/utils.py</code> <pre><code>def create_split_map(\n    left_layer: Optional[str] = \"TERRAIN\",\n    right_layer: Optional[str] = \"OpenTopoMap\",\n    left_args: Optional[dict] = None,\n    right_args: Optional[dict] = None,\n    left_array_args: Optional[dict] = None,\n    right_array_args: Optional[dict] = None,\n    zoom_control: Optional[bool] = True,\n    fullscreen_control: Optional[bool] = True,\n    layer_control: Optional[bool] = True,\n    add_close_button: Optional[bool] = False,\n    left_label: Optional[str] = None,\n    right_label: Optional[str] = None,\n    left_position: Optional[str] = \"bottomleft\",\n    right_position: Optional[str] = \"bottomright\",\n    widget_layout: Optional[dict] = None,\n    draggable: Optional[bool] = True,\n    center: Optional[List[float]] = [20, 0],\n    zoom: Optional[int] = 2,\n    height: Optional[int] = \"600px\",\n    basemap: Optional[str] = None,\n    basemap_args: Optional[dict] = None,\n    m=None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Adds split map.\n\n    Args:\n        left_layer (str, optional): The left tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'TERRAIN'.\n        right_layer (str, optional): The right tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'OpenTopoMap'.\n        left_args (dict, optional): The arguments for the left tile layer. Defaults to {}.\n        right_args (dict, optional): The arguments for the right tile layer. Defaults to {}.\n        left_array_args (dict, optional): The arguments for array_to_image for the left layer. Defaults to {}.\n        right_array_args (dict, optional): The arguments for array_to_image for the right layer. Defaults to {}.\n        zoom_control (bool, optional): Whether to add zoom control. Defaults to True.\n        fullscreen_control (bool, optional): Whether to add fullscreen control. Defaults to True.\n        layer_control (bool, optional): Whether to add layer control. Defaults to True.\n        add_close_button (bool, optional): Whether to add a close button. Defaults to False.\n        left_label (str, optional): The label for the left layer. Defaults to None.\n        right_label (str, optional): The label for the right layer. Defaults to None.\n        left_position (str, optional): The position for the left label. Defaults to \"bottomleft\".\n        right_position (str, optional): The position for the right label. Defaults to \"bottomright\".\n        widget_layout (dict, optional): The layout for the widget. Defaults to None.\n        draggable (bool, optional): Whether the split map is draggable. Defaults to True.\n    \"\"\"\n\n    if left_args is None:\n        left_args = {}\n\n    if right_args is None:\n        right_args = {}\n\n    if left_array_args is None:\n        left_array_args = {}\n\n    if right_array_args is None:\n        right_array_args = {}\n\n    if basemap_args is None:\n        basemap_args = {}\n\n    if m is None:\n        m = leafmap.Map(center=center, zoom=zoom, height=height, **kwargs)\n        m.clear_layers()\n    if isinstance(basemap, str):\n        if basemap.endswith(\".tif\"):\n            if basemap.startswith(\"http\"):\n                m.add_cog_layer(basemap, name=\"Basemap\", **basemap_args)\n            else:\n                m.add_raster(basemap, name=\"Basemap\", **basemap_args)\n        else:\n            m.add_basemap(basemap)\n    m.split_map(\n        left_layer=left_layer,\n        right_layer=right_layer,\n        left_args=left_args,\n        right_args=right_args,\n        left_array_args=left_array_args,\n        right_array_args=right_array_args,\n        zoom_control=zoom_control,\n        fullscreen_control=fullscreen_control,\n        layer_control=layer_control,\n        add_close_button=add_close_button,\n        left_label=left_label,\n        right_label=right_label,\n        left_position=left_position,\n        right_position=right_position,\n        widget_layout=widget_layout,\n        draggable=draggable,\n    )\n\n    return m\n</code></pre>"},{"location":"utils/#geoai.utils.dict_to_image","title":"<code>dict_to_image(data_dict, output=None, **kwargs)</code>","text":"<p>Convert a dictionary containing spatial data to a rasterio dataset or save it to a file. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo dataset sampler.</p> <p>This function transforms a dictionary with CRS, bounding box, and image data into a rasterio DatasetReader using leafmap's array_to_image utility after first converting to a rioxarray DataArray.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>Dict[str, Any]</code> <p>A dictionary containing: - 'crs': A pyproj CRS object - 'bounds': A BoundingBox object with minx, maxx, miny, maxy attributes   and optionally mint, maxt for temporal bounds - 'image': A tensor or array-like object with image data</p> required <code>output</code> <code>Optional[str]</code> <p>Optional path to save the image to a file. If not provided, the image will be returned as a rasterio DatasetReader object.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to leafmap.array_to_image. Common options include: - colormap: str, name of the colormap (e.g., 'viridis', 'terrain') - vmin: float, minimum value for colormap scaling - vmax: float, maximum value for colormap scaling</p> <code>{}</code> <p>Returns:</p> Type Description <code>DatasetReader</code> <p>A rasterio DatasetReader object that can be used for visualization or further processing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image = dict_to_image(\n...     {'crs': CRS.from_epsg(26911), 'bounds': bbox, 'image': tensor},\n...     colormap='terrain'\n... )\n&gt;&gt;&gt; fig, ax = plt.subplots(figsize=(10, 10))\n&gt;&gt;&gt; show(image, ax=ax)\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def dict_to_image(\n    data_dict: Dict[str, Any], output: Optional[str] = None, **kwargs\n) -&gt; rasterio.DatasetReader:\n    \"\"\"Convert a dictionary containing spatial data to a rasterio dataset or save it to\n    a file. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\".\n    It can be generated from a TorchGeo dataset sampler.\n\n    This function transforms a dictionary with CRS, bounding box, and image data\n    into a rasterio DatasetReader using leafmap's array_to_image utility after\n    first converting to a rioxarray DataArray.\n\n    Args:\n        data_dict: A dictionary containing:\n            - 'crs': A pyproj CRS object\n            - 'bounds': A BoundingBox object with minx, maxx, miny, maxy attributes\n              and optionally mint, maxt for temporal bounds\n            - 'image': A tensor or array-like object with image data\n        output: Optional path to save the image to a file. If not provided, the image\n            will be returned as a rasterio DatasetReader object.\n        **kwargs: Additional keyword arguments to pass to leafmap.array_to_image.\n            Common options include:\n            - colormap: str, name of the colormap (e.g., 'viridis', 'terrain')\n            - vmin: float, minimum value for colormap scaling\n            - vmax: float, maximum value for colormap scaling\n\n    Returns:\n        A rasterio DatasetReader object that can be used for visualization or\n        further processing.\n\n    Examples:\n        &gt;&gt;&gt; image = dict_to_image(\n        ...     {'crs': CRS.from_epsg(26911), 'bounds': bbox, 'image': tensor},\n        ...     colormap='terrain'\n        ... )\n        &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(10, 10))\n        &gt;&gt;&gt; show(image, ax=ax)\n    \"\"\"\n    da = dict_to_rioxarray(data_dict)\n\n    if output is not None:\n        out_dir = os.path.abspath(os.path.dirname(output))\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir, exist_ok=True)\n        da.rio.to_raster(output)\n        return output\n    else:\n        image = leafmap.array_to_image(da, **kwargs)\n        return image\n</code></pre>"},{"location":"utils/#geoai.utils.dict_to_rioxarray","title":"<code>dict_to_rioxarray(data_dict)</code>","text":"<p>Convert a dictionary to a xarray DataArray. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo dataset sampler.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>Dict</code> <p>The dictionary containing the data.</p> required <p>Returns:</p> Type Description <code>xr.DataArray</code> <p>The xarray DataArray.</p> Source code in <code>geoai/utils.py</code> <pre><code>def dict_to_rioxarray(data_dict: Dict) -&gt; xr.DataArray:\n    \"\"\"Convert a dictionary to a xarray DataArray. The dictionary should contain the\n    following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo\n    dataset sampler.\n\n    Args:\n        data_dict (Dict): The dictionary containing the data.\n\n    Returns:\n        xr.DataArray: The xarray DataArray.\n    \"\"\"\n\n    from affine import Affine\n\n    # Extract components from the dictionary\n    crs = data_dict[\"crs\"]\n    bounds = data_dict[\"bounds\"]\n    image_tensor = data_dict[\"image\"]\n\n    # Convert tensor to numpy array if needed\n    if hasattr(image_tensor, \"numpy\"):\n        # For PyTorch tensors\n        image_array = image_tensor.numpy()\n    else:\n        # If it's already a numpy array or similar\n        image_array = np.array(image_tensor)\n\n    # Calculate pixel resolution\n    width = image_array.shape[2]  # Width is the size of the last dimension\n    height = image_array.shape[1]  # Height is the size of the middle dimension\n\n    res_x = (bounds.maxx - bounds.minx) / width\n    res_y = (bounds.maxy - bounds.miny) / height\n\n    # Create the transform matrix\n    transform = Affine(res_x, 0.0, bounds.minx, 0.0, -res_y, bounds.maxy)\n\n    # Create dimensions\n    x_coords = np.linspace(bounds.minx + res_x / 2, bounds.maxx - res_x / 2, width)\n    y_coords = np.linspace(bounds.maxy - res_y / 2, bounds.miny + res_y / 2, height)\n\n    # If time dimension exists in the bounds\n    if hasattr(bounds, \"mint\") and hasattr(bounds, \"maxt\"):\n        # Create a single time value or range if needed\n        t_coords = [\n            bounds.mint\n        ]  # Or np.linspace(bounds.mint, bounds.maxt, num_time_steps)\n\n        # Create DataArray with time dimension\n        dims = (\n            (\"band\", \"y\", \"x\")\n            if image_array.shape[0] &lt;= 10\n            else (\"time\", \"band\", \"y\", \"x\")\n        )\n\n        if dims[0] == \"band\":\n            # For multi-band single time\n            da = xr.DataArray(\n                image_array,\n                dims=dims,\n                coords={\n                    \"band\": np.arange(1, image_array.shape[0] + 1),\n                    \"y\": y_coords,\n                    \"x\": x_coords,\n                },\n            )\n        else:\n            # For multi-time multi-band\n            da = xr.DataArray(\n                image_array,\n                dims=dims,\n                coords={\n                    \"time\": t_coords,\n                    \"band\": np.arange(1, image_array.shape[1] + 1),\n                    \"y\": y_coords,\n                    \"x\": x_coords,\n                },\n            )\n    else:\n        # Create DataArray without time dimension\n        da = xr.DataArray(\n            image_array,\n            dims=(\"band\", \"y\", \"x\"),\n            coords={\n                \"band\": np.arange(1, image_array.shape[0] + 1),\n                \"y\": y_coords,\n                \"x\": x_coords,\n            },\n        )\n\n    # Set spatial attributes\n    da.rio.write_crs(crs, inplace=True)\n    da.rio.write_transform(transform, inplace=True)\n\n    return da\n</code></pre>"},{"location":"utils/#geoai.utils.download_file","title":"<code>download_file(url, output_path=None, overwrite=False)</code>","text":"<p>Download a file from a given URL with a progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the file to download.</p> required <code>output_path</code> <code>str</code> <p>The path where the downloaded file will be saved. If not provided, the filename from the URL will be used.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it already exists.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the downloaded file.</p> Source code in <code>geoai/utils.py</code> <pre><code>def download_file(url, output_path=None, overwrite=False):\n    \"\"\"\n    Download a file from a given URL with a progress bar.\n\n    Args:\n        url (str): The URL of the file to download.\n        output_path (str, optional): The path where the downloaded file will be saved.\n            If not provided, the filename from the URL will be used.\n        overwrite (bool, optional): Whether to overwrite the file if it already exists.\n\n    Returns:\n        str: The path to the downloaded file.\n    \"\"\"\n    # Get the filename from the URL if output_path is not provided\n    if output_path is None:\n        output_path = os.path.basename(url)\n\n    # Check if the file already exists\n    if os.path.exists(output_path) and not overwrite:\n        print(f\"File already exists: {output_path}\")\n        return output_path\n\n    # Send a streaming GET request\n    response = requests.get(url, stream=True, timeout=50)\n    response.raise_for_status()  # Raise an exception for HTTP errors\n\n    # Get the total file size if available\n    total_size = int(response.headers.get(\"content-length\", 0))\n\n    # Open the output file\n    with (\n        open(output_path, \"wb\") as file,\n        tqdm(\n            desc=os.path.basename(output_path),\n            total=total_size,\n            unit=\"B\",\n            unit_scale=True,\n            unit_divisor=1024,\n        ) as progress_bar,\n    ):\n\n        # Download the file in chunks and update the progress bar\n        for chunk in response.iter_content(chunk_size=1024):\n            if chunk:  # filter out keep-alive new chunks\n                file.write(chunk)\n                progress_bar.update(len(chunk))\n\n    return output_path\n</code></pre>"},{"location":"utils/#geoai.utils.export_geotiff_tiles","title":"<code>export_geotiff_tiles(in_raster, out_folder, in_class_data, tile_size=256, stride=128, class_value_field='class', buffer_radius=0, max_tiles=None, quiet=False, all_touched=True, create_overview=False, skip_empty_tiles=False)</code>","text":"<p>Export georeferenced GeoTIFF tiles and labels from raster and classification data.</p> <p>Parameters:</p> Name Type Description Default <code>in_raster</code> <code>str</code> <p>Path to input raster image</p> required <code>out_folder</code> <code>str</code> <p>Path to output folder</p> required <code>in_class_data</code> <code>str</code> <p>Path to classification data - can be vector file or raster</p> required <code>tile_size</code> <code>int</code> <p>Size of tiles in pixels (square)</p> <code>256</code> <code>stride</code> <code>int</code> <p>Step size between tiles</p> <code>128</code> <code>class_value_field</code> <code>str</code> <p>Field containing class values (for vector data)</p> <code>'class'</code> <code>buffer_radius</code> <code>float</code> <p>Buffer to add around features (in units of the CRS)</p> <code>0</code> <code>max_tiles</code> <code>int</code> <p>Maximum number of tiles to process (None for all)</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>If True, suppress non-essential output</p> <code>False</code> <code>all_touched</code> <code>bool</code> <p>Whether to use all_touched=True in rasterization (for vector data)</p> <code>True</code> <code>create_overview</code> <code>bool</code> <p>Whether to create an overview image of all tiles</p> <code>False</code> <code>skip_empty_tiles</code> <code>bool</code> <p>If True, skip tiles with no features</p> <code>False</code> Source code in <code>geoai/utils.py</code> <pre><code>def export_geotiff_tiles(\n    in_raster,\n    out_folder,\n    in_class_data,\n    tile_size=256,\n    stride=128,\n    class_value_field=\"class\",\n    buffer_radius=0,\n    max_tiles=None,\n    quiet=False,\n    all_touched=True,\n    create_overview=False,\n    skip_empty_tiles=False,\n):\n    \"\"\"\n    Export georeferenced GeoTIFF tiles and labels from raster and classification data.\n\n    Args:\n        in_raster (str): Path to input raster image\n        out_folder (str): Path to output folder\n        in_class_data (str): Path to classification data - can be vector file or raster\n        tile_size (int): Size of tiles in pixels (square)\n        stride (int): Step size between tiles\n        class_value_field (str): Field containing class values (for vector data)\n        buffer_radius (float): Buffer to add around features (in units of the CRS)\n        max_tiles (int): Maximum number of tiles to process (None for all)\n        quiet (bool): If True, suppress non-essential output\n        all_touched (bool): Whether to use all_touched=True in rasterization (for vector data)\n        create_overview (bool): Whether to create an overview image of all tiles\n        skip_empty_tiles (bool): If True, skip tiles with no features\n    \"\"\"\n    # Create output directories\n    os.makedirs(out_folder, exist_ok=True)\n    image_dir = os.path.join(out_folder, \"images\")\n    os.makedirs(image_dir, exist_ok=True)\n    label_dir = os.path.join(out_folder, \"labels\")\n    os.makedirs(label_dir, exist_ok=True)\n    ann_dir = os.path.join(out_folder, \"annotations\")\n    os.makedirs(ann_dir, exist_ok=True)\n\n    # Determine if class data is raster or vector\n    is_class_data_raster = False\n    if isinstance(in_class_data, str):\n        file_ext = Path(in_class_data).suffix.lower()\n        # Common raster extensions\n        if file_ext in [\".tif\", \".tiff\", \".img\", \".jp2\", \".png\", \".bmp\", \".gif\"]:\n            try:\n                with rasterio.open(in_class_data) as src:\n                    is_class_data_raster = True\n                    if not quiet:\n                        print(f\"Detected in_class_data as raster: {in_class_data}\")\n                        print(f\"Raster CRS: {src.crs}\")\n                        print(f\"Raster dimensions: {src.width} x {src.height}\")\n            except Exception:\n                is_class_data_raster = False\n                if not quiet:\n                    print(f\"Unable to open {in_class_data} as raster, trying as vector\")\n\n    # Open the input raster\n    with rasterio.open(in_raster) as src:\n        if not quiet:\n            print(f\"\\nRaster info for {in_raster}:\")\n            print(f\"  CRS: {src.crs}\")\n            print(f\"  Dimensions: {src.width} x {src.height}\")\n            print(f\"  Resolution: {src.res}\")\n            print(f\"  Bands: {src.count}\")\n            print(f\"  Bounds: {src.bounds}\")\n\n        # Calculate number of tiles\n        num_tiles_x = math.ceil((src.width - tile_size) / stride) + 1\n        num_tiles_y = math.ceil((src.height - tile_size) / stride) + 1\n        total_tiles = num_tiles_x * num_tiles_y\n\n        if max_tiles is None:\n            max_tiles = total_tiles\n\n        # Process classification data\n        class_to_id = {}\n\n        if is_class_data_raster:\n            # Load raster class data\n            with rasterio.open(in_class_data) as class_src:\n                # Check if raster CRS matches\n                if class_src.crs != src.crs:\n                    warnings.warn(\n                        f\"CRS mismatch: Class raster ({class_src.crs}) doesn't match input raster ({src.crs}). \"\n                        f\"Results may be misaligned.\"\n                    )\n\n                # Get unique values from raster\n                # Sample to avoid loading huge rasters\n                sample_data = class_src.read(\n                    1,\n                    out_shape=(\n                        1,\n                        min(class_src.height, 1000),\n                        min(class_src.width, 1000),\n                    ),\n                )\n\n                unique_classes = np.unique(sample_data)\n                unique_classes = unique_classes[\n                    unique_classes &gt; 0\n                ]  # Remove 0 as it's typically background\n\n                if not quiet:\n                    print(\n                        f\"Found {len(unique_classes)} unique classes in raster: {unique_classes}\"\n                    )\n\n                # Create class mapping\n                class_to_id = {int(cls): i + 1 for i, cls in enumerate(unique_classes)}\n        else:\n            # Load vector class data\n            try:\n                gdf = gpd.read_file(in_class_data)\n                if not quiet:\n                    print(f\"Loaded {len(gdf)} features from {in_class_data}\")\n                    print(f\"Vector CRS: {gdf.crs}\")\n\n                # Always reproject to match raster CRS\n                if gdf.crs != src.crs:\n                    if not quiet:\n                        print(f\"Reprojecting features from {gdf.crs} to {src.crs}\")\n                    gdf = gdf.to_crs(src.crs)\n\n                # Apply buffer if specified\n                if buffer_radius &gt; 0:\n                    gdf[\"geometry\"] = gdf.buffer(buffer_radius)\n                    if not quiet:\n                        print(f\"Applied buffer of {buffer_radius} units\")\n\n                # Check if class_value_field exists\n                if class_value_field in gdf.columns:\n                    unique_classes = gdf[class_value_field].unique()\n                    if not quiet:\n                        print(\n                            f\"Found {len(unique_classes)} unique classes: {unique_classes}\"\n                        )\n                    # Create class mapping\n                    class_to_id = {cls: i + 1 for i, cls in enumerate(unique_classes)}\n                else:\n                    if not quiet:\n                        print(\n                            f\"WARNING: '{class_value_field}' not found in vector data. Using default class ID 1.\"\n                        )\n                    class_to_id = {1: 1}  # Default mapping\n            except Exception as e:\n                raise ValueError(f\"Error processing vector data: {e}\")\n\n        # Create progress bar\n        pbar = tqdm(\n            total=min(total_tiles, max_tiles),\n            desc=\"Generating tiles\",\n            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}&lt;{remaining}, {rate_fmt}]\",\n        )\n\n        # Track statistics for summary\n        stats = {\n            \"total_tiles\": 0,\n            \"tiles_with_features\": 0,\n            \"feature_pixels\": 0,\n            \"errors\": 0,\n            \"tile_coordinates\": [],  # For overview image\n        }\n\n        # Process tiles\n        tile_index = 0\n        for y in range(num_tiles_y):\n            for x in range(num_tiles_x):\n                if tile_index &gt;= max_tiles:\n                    break\n\n                # Calculate window coordinates\n                window_x = x * stride\n                window_y = y * stride\n\n                # Adjust for edge cases\n                if window_x + tile_size &gt; src.width:\n                    window_x = src.width - tile_size\n                if window_y + tile_size &gt; src.height:\n                    window_y = src.height - tile_size\n\n                # Define window\n                window = Window(window_x, window_y, tile_size, tile_size)\n\n                # Get window transform and bounds\n                window_transform = src.window_transform(window)\n\n                # Calculate window bounds\n                minx = window_transform[2]  # Upper left x\n                maxy = window_transform[5]  # Upper left y\n                maxx = minx + tile_size * window_transform[0]  # Add width\n                miny = maxy + tile_size * window_transform[4]  # Add height\n\n                window_bounds = box(minx, miny, maxx, maxy)\n\n                # Store tile coordinates for overview\n                if create_overview:\n                    stats[\"tile_coordinates\"].append(\n                        {\n                            \"index\": tile_index,\n                            \"x\": window_x,\n                            \"y\": window_y,\n                            \"bounds\": [minx, miny, maxx, maxy],\n                            \"has_features\": False,\n                        }\n                    )\n\n                # Create label mask\n                label_mask = np.zeros((tile_size, tile_size), dtype=np.uint8)\n                has_features = False\n\n                # Process classification data to create labels\n                if is_class_data_raster:\n                    # For raster class data\n                    with rasterio.open(in_class_data) as class_src:\n                        # Calculate window in class raster\n                        src_bounds = src.bounds\n                        class_bounds = class_src.bounds\n\n                        # Check if windows overlap\n                        if (\n                            src_bounds.left &gt; class_bounds.right\n                            or src_bounds.right &lt; class_bounds.left\n                            or src_bounds.bottom &gt; class_bounds.top\n                            or src_bounds.top &lt; class_bounds.bottom\n                        ):\n                            warnings.warn(\n                                \"Class raster and input raster do not overlap.\"\n                            )\n                        else:\n                            # Get corresponding window in class raster\n                            window_class = rasterio.windows.from_bounds(\n                                minx, miny, maxx, maxy, class_src.transform\n                            )\n\n                            # Read label data\n                            try:\n                                label_data = class_src.read(\n                                    1,\n                                    window=window_class,\n                                    boundless=True,\n                                    out_shape=(tile_size, tile_size),\n                                )\n\n                                # Remap class values if needed\n                                if class_to_id:\n                                    remapped_data = np.zeros_like(label_data)\n                                    for orig_val, new_val in class_to_id.items():\n                                        remapped_data[label_data == orig_val] = new_val\n                                    label_mask = remapped_data\n                                else:\n                                    label_mask = label_data\n\n                                # Check if we have any features\n                                if np.any(label_mask &gt; 0):\n                                    has_features = True\n                                    stats[\"feature_pixels\"] += np.count_nonzero(\n                                        label_mask\n                                    )\n                            except Exception as e:\n                                pbar.write(f\"Error reading class raster window: {e}\")\n                                stats[\"errors\"] += 1\n                else:\n                    # For vector class data\n                    # Find features that intersect with window\n                    window_features = gdf[gdf.intersects(window_bounds)]\n\n                    if len(window_features) &gt; 0:\n                        for idx, feature in window_features.iterrows():\n                            # Get class value\n                            if class_value_field in feature:\n                                class_val = feature[class_value_field]\n                                class_id = class_to_id.get(class_val, 1)\n                            else:\n                                class_id = 1\n\n                            # Get geometry in window coordinates\n                            geom = feature.geometry.intersection(window_bounds)\n                            if not geom.is_empty:\n                                try:\n                                    # Rasterize feature\n                                    feature_mask = features.rasterize(\n                                        [(geom, class_id)],\n                                        out_shape=(tile_size, tile_size),\n                                        transform=window_transform,\n                                        fill=0,\n                                        all_touched=all_touched,\n                                    )\n\n                                    # Add to label mask\n                                    label_mask = np.maximum(label_mask, feature_mask)\n\n                                    # Check if the feature was actually rasterized\n                                    if np.any(feature_mask):\n                                        has_features = True\n                                        if create_overview and tile_index &lt; len(\n                                            stats[\"tile_coordinates\"]\n                                        ):\n                                            stats[\"tile_coordinates\"][tile_index][\n                                                \"has_features\"\n                                            ] = True\n                                except Exception as e:\n                                    pbar.write(f\"Error rasterizing feature {idx}: {e}\")\n                                    stats[\"errors\"] += 1\n\n                # Skip tile if no features and skip_empty_tiles is True\n                if skip_empty_tiles and not has_features:\n                    pbar.update(1)\n                    tile_index += 1\n                    continue\n\n                # Read image data\n                image_data = src.read(window=window)\n\n                # Export image as GeoTIFF\n                image_path = os.path.join(image_dir, f\"tile_{tile_index:06d}.tif\")\n\n                # Create profile for image GeoTIFF\n                image_profile = src.profile.copy()\n                image_profile.update(\n                    {\n                        \"height\": tile_size,\n                        \"width\": tile_size,\n                        \"count\": image_data.shape[0],\n                        \"transform\": window_transform,\n                    }\n                )\n\n                # Save image as GeoTIFF\n                try:\n                    with rasterio.open(image_path, \"w\", **image_profile) as dst:\n                        dst.write(image_data)\n                    stats[\"total_tiles\"] += 1\n                except Exception as e:\n                    pbar.write(f\"ERROR saving image GeoTIFF: {e}\")\n                    stats[\"errors\"] += 1\n\n                # Create profile for label GeoTIFF\n                label_profile = {\n                    \"driver\": \"GTiff\",\n                    \"height\": tile_size,\n                    \"width\": tile_size,\n                    \"count\": 1,\n                    \"dtype\": \"uint8\",\n                    \"crs\": src.crs,\n                    \"transform\": window_transform,\n                }\n\n                # Export label as GeoTIFF\n                label_path = os.path.join(label_dir, f\"tile_{tile_index:06d}.tif\")\n                try:\n                    with rasterio.open(label_path, \"w\", **label_profile) as dst:\n                        dst.write(label_mask.astype(np.uint8), 1)\n\n                    if has_features:\n                        stats[\"tiles_with_features\"] += 1\n                        stats[\"feature_pixels\"] += np.count_nonzero(label_mask)\n                except Exception as e:\n                    pbar.write(f\"ERROR saving label GeoTIFF: {e}\")\n                    stats[\"errors\"] += 1\n\n                # Create XML annotation for object detection if using vector class data\n                if (\n                    not is_class_data_raster\n                    and \"gdf\" in locals()\n                    and len(window_features) &gt; 0\n                ):\n                    # Create XML annotation\n                    root = ET.Element(\"annotation\")\n                    ET.SubElement(root, \"folder\").text = \"images\"\n                    ET.SubElement(root, \"filename\").text = f\"tile_{tile_index:06d}.tif\"\n\n                    size = ET.SubElement(root, \"size\")\n                    ET.SubElement(size, \"width\").text = str(tile_size)\n                    ET.SubElement(size, \"height\").text = str(tile_size)\n                    ET.SubElement(size, \"depth\").text = str(image_data.shape[0])\n\n                    # Add georeference information\n                    geo = ET.SubElement(root, \"georeference\")\n                    ET.SubElement(geo, \"crs\").text = str(src.crs)\n                    ET.SubElement(geo, \"transform\").text = str(\n                        window_transform\n                    ).replace(\"\\n\", \"\")\n                    ET.SubElement(geo, \"bounds\").text = (\n                        f\"{minx}, {miny}, {maxx}, {maxy}\"\n                    )\n\n                    # Add objects\n                    for idx, feature in window_features.iterrows():\n                        # Get feature class\n                        if class_value_field in feature:\n                            class_val = feature[class_value_field]\n                        else:\n                            class_val = \"object\"\n\n                        # Get geometry bounds in pixel coordinates\n                        geom = feature.geometry.intersection(window_bounds)\n                        if not geom.is_empty:\n                            # Get bounds in world coordinates\n                            minx_f, miny_f, maxx_f, maxy_f = geom.bounds\n\n                            # Convert to pixel coordinates\n                            col_min, row_min = ~window_transform * (minx_f, maxy_f)\n                            col_max, row_max = ~window_transform * (maxx_f, miny_f)\n\n                            # Ensure coordinates are within tile bounds\n                            xmin = max(0, min(tile_size, int(col_min)))\n                            ymin = max(0, min(tile_size, int(row_min)))\n                            xmax = max(0, min(tile_size, int(col_max)))\n                            ymax = max(0, min(tile_size, int(row_max)))\n\n                            # Only add if the box has non-zero area\n                            if xmax &gt; xmin and ymax &gt; ymin:\n                                obj = ET.SubElement(root, \"object\")\n                                ET.SubElement(obj, \"name\").text = str(class_val)\n                                ET.SubElement(obj, \"difficult\").text = \"0\"\n\n                                bbox = ET.SubElement(obj, \"bndbox\")\n                                ET.SubElement(bbox, \"xmin\").text = str(xmin)\n                                ET.SubElement(bbox, \"ymin\").text = str(ymin)\n                                ET.SubElement(bbox, \"xmax\").text = str(xmax)\n                                ET.SubElement(bbox, \"ymax\").text = str(ymax)\n\n                    # Save XML\n                    tree = ET.ElementTree(root)\n                    xml_path = os.path.join(ann_dir, f\"tile_{tile_index:06d}.xml\")\n                    tree.write(xml_path)\n\n                # Update progress bar\n                pbar.update(1)\n                pbar.set_description(\n                    f\"Generated: {stats['total_tiles']}, With features: {stats['tiles_with_features']}\"\n                )\n\n                tile_index += 1\n                if tile_index &gt;= max_tiles:\n                    break\n\n            if tile_index &gt;= max_tiles:\n                break\n\n        # Close progress bar\n        pbar.close()\n\n        # Create overview image if requested\n        if create_overview and stats[\"tile_coordinates\"]:\n            try:\n                create_overview_image(\n                    src,\n                    stats[\"tile_coordinates\"],\n                    os.path.join(out_folder, \"overview.png\"),\n                    tile_size,\n                    stride,\n                )\n            except Exception as e:\n                print(f\"Failed to create overview image: {e}\")\n\n        # Report results\n        if not quiet:\n            print(\"\\n------- Export Summary -------\")\n            print(f\"Total tiles exported: {stats['total_tiles']}\")\n            print(\n                f\"Tiles with features: {stats['tiles_with_features']} ({stats['tiles_with_features']/max(1, stats['total_tiles'])*100:.1f}%)\"\n            )\n            if stats[\"tiles_with_features\"] &gt; 0:\n                print(\n                    f\"Average feature pixels per tile: {stats['feature_pixels']/stats['tiles_with_features']:.1f}\"\n                )\n            if stats[\"errors\"] &gt; 0:\n                print(f\"Errors encountered: {stats['errors']}\")\n            print(f\"Output saved to: {out_folder}\")\n\n            # Verify georeference in a sample image and label\n            if stats[\"total_tiles\"] &gt; 0:\n                print(\"\\n------- Georeference Verification -------\")\n                sample_image = os.path.join(image_dir, f\"tile_0.tif\")\n                sample_label = os.path.join(label_dir, f\"tile_0.tif\")\n\n                if os.path.exists(sample_image):\n                    try:\n                        with rasterio.open(sample_image) as img:\n                            print(f\"Image CRS: {img.crs}\")\n                            print(f\"Image transform: {img.transform}\")\n                            print(\n                                f\"Image has georeference: {img.crs is not None and img.transform is not None}\"\n                            )\n                            print(\n                                f\"Image dimensions: {img.width}x{img.height}, {img.count} bands, {img.dtypes[0]} type\"\n                            )\n                    except Exception as e:\n                        print(f\"Error verifying image georeference: {e}\")\n\n                if os.path.exists(sample_label):\n                    try:\n                        with rasterio.open(sample_label) as lbl:\n                            print(f\"Label CRS: {lbl.crs}\")\n                            print(f\"Label transform: {lbl.transform}\")\n                            print(\n                                f\"Label has georeference: {lbl.crs is not None and lbl.transform is not None}\"\n                            )\n                            print(\n                                f\"Label dimensions: {lbl.width}x{lbl.height}, {lbl.count} bands, {lbl.dtypes[0]} type\"\n                            )\n                    except Exception as e:\n                        print(f\"Error verifying label georeference: {e}\")\n\n        # Return statistics dictionary for further processing if needed\n        return stats\n</code></pre>"},{"location":"utils/#geoai.utils.export_tiles_to_geojson","title":"<code>export_tiles_to_geojson(tile_coordinates, src, output_path, tile_size=None, stride=None)</code>","text":"<p>Export tile rectangles directly to GeoJSON without creating an overview image.</p> <p>Parameters:</p> Name Type Description Default <code>tile_coordinates</code> <code>list</code> <p>A list of dictionaries containing tile information.</p> required <code>src</code> <code>rasterio.io.DatasetReader</code> <p>The source raster dataset.</p> required <code>output_path</code> <code>str</code> <p>The path where the GeoJSON will be saved.</p> required <code>tile_size</code> <code>int</code> <p>The size of each tile in pixels. Only needed if not in tile_coordinates.</p> <code>None</code> <code>stride</code> <code>int</code> <p>The stride between tiles in pixels. Used to calculate overlaps between tiles.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the saved GeoJSON file.</p> Source code in <code>geoai/utils.py</code> <pre><code>def export_tiles_to_geojson(\n    tile_coordinates, src, output_path, tile_size=None, stride=None\n):\n    \"\"\"\n    Export tile rectangles directly to GeoJSON without creating an overview image.\n\n    Args:\n        tile_coordinates (list): A list of dictionaries containing tile information.\n        src (rasterio.io.DatasetReader): The source raster dataset.\n        output_path (str): The path where the GeoJSON will be saved.\n        tile_size (int, optional): The size of each tile in pixels. Only needed if not in tile_coordinates.\n        stride (int, optional): The stride between tiles in pixels. Used to calculate overlaps between tiles.\n\n    Returns:\n        str: Path to the saved GeoJSON file.\n    \"\"\"\n    features = []\n\n    for tile in tile_coordinates:\n        # Get the size from the tile or use the provided parameter\n        tile_width = tile.get(\"width\", tile.get(\"size\", tile_size))\n        tile_height = tile.get(\"height\", tile.get(\"size\", tile_size))\n\n        if tile_width is None or tile_height is None:\n            raise ValueError(\n                \"Tile size not found in tile data and no tile_size parameter provided\"\n            )\n\n        # Get bounds from the tile\n        if \"bounds\" in tile:\n            # If bounds are already in geo coordinates\n            minx, miny, maxx, maxy = tile[\"bounds\"]\n        else:\n            # Try to calculate bounds from transform if available\n            if hasattr(src, \"transform\"):\n                # Convert pixel coordinates to geo coordinates\n                window_transform = src.transform\n                x, y = tile[\"x\"], tile[\"y\"]\n                minx = window_transform[2] + x * window_transform[0]\n                maxy = window_transform[5] + y * window_transform[4]\n                maxx = minx + tile_width * window_transform[0]\n                miny = maxy + tile_height * window_transform[4]\n            else:\n                raise ValueError(\n                    \"Cannot determine bounds. Neither 'bounds' in tile nor transform in src.\"\n                )\n\n        # Calculate overlap with neighboring tiles if stride is provided\n        overlap = 0\n        if stride is not None and stride &lt; tile_width:\n            overlap = tile_width - stride\n\n        # Create a polygon from the bounds\n        polygon = box(minx, miny, maxx, maxy)\n\n        # Create a GeoJSON feature\n        feature = {\n            \"type\": \"Feature\",\n            \"geometry\": mapping(polygon),\n            \"properties\": {\n                \"index\": tile[\"index\"],\n                \"has_features\": tile.get(\"has_features\", False),\n                \"tile_width_px\": tile_width,\n                \"tile_height_px\": tile_height,\n            },\n        }\n\n        # Add overlap information if stride is provided\n        if stride is not None:\n            feature[\"properties\"][\"stride_px\"] = stride\n            feature[\"properties\"][\"overlap_px\"] = overlap\n\n        # Add additional properties from the tile\n        for key, value in tile.items():\n            if key not in [\"bounds\", \"geometry\"]:\n                feature[\"properties\"][key] = value\n\n        features.append(feature)\n\n    # Create the GeoJSON collection\n    geojson_collection = {\n        \"type\": \"FeatureCollection\",\n        \"features\": features,\n        \"properties\": {\n            \"crs\": (\n                src.crs.to_string() if hasattr(src.crs, \"to_string\") else str(src.crs)\n            ),\n            \"total_tiles\": len(features),\n            \"source_raster_dimensions\": (\n                [src.width, src.height] if hasattr(src, \"width\") else None\n            ),\n        },\n    }\n\n    # Create directory if it doesn't exist\n    os.makedirs(os.path.dirname(os.path.abspath(output_path)) or \".\", exist_ok=True)\n\n    # Save to file\n    with open(output_path, \"w\") as f:\n        json.dump(geojson_collection, f)\n\n    print(f\"GeoJSON saved to {output_path}\")\n    return output_path\n</code></pre>"},{"location":"utils/#geoai.utils.export_training_data","title":"<code>export_training_data(in_raster, out_folder, in_class_data, image_chip_format='GEOTIFF', tile_size_x=256, tile_size_y=256, stride_x=None, stride_y=None, output_nofeature_tiles=True, metadata_format='PASCAL_VOC', start_index=0, class_value_field='class', buffer_radius=0, in_mask_polygons=None, rotation_angle=0, reference_system=None, blacken_around_feature=False, crop_mode='FIXED_SIZE', in_raster2=None, in_instance_data=None, instance_class_value_field=None, min_polygon_overlap_ratio=0.0, all_touched=True, save_geotiff=True, quiet=False)</code>","text":"<p>Export training data for deep learning using TorchGeo with progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>in_raster</code> <code>str</code> <p>Path to input raster image.</p> required <code>out_folder</code> <code>str</code> <p>Output folder path where chips and labels will be saved.</p> required <code>in_class_data</code> <code>str</code> <p>Path to vector file containing class polygons.</p> required <code>image_chip_format</code> <code>str</code> <p>Output image format (PNG, JPEG, TIFF, GEOTIFF).</p> <code>'GEOTIFF'</code> <code>tile_size_x</code> <code>int</code> <p>Width of image chips in pixels.</p> <code>256</code> <code>tile_size_y</code> <code>int</code> <p>Height of image chips in pixels.</p> <code>256</code> <code>stride_x</code> <code>int</code> <p>Horizontal stride between chips. If None, uses tile_size_x.</p> <code>None</code> <code>stride_y</code> <code>int</code> <p>Vertical stride between chips. If None, uses tile_size_y.</p> <code>None</code> <code>output_nofeature_tiles</code> <code>bool</code> <p>Whether to export chips without features.</p> <code>True</code> <code>metadata_format</code> <code>str</code> <p>Output metadata format (PASCAL_VOC, KITTI, COCO).</p> <code>'PASCAL_VOC'</code> <code>start_index</code> <code>int</code> <p>Starting index for chip filenames.</p> <code>0</code> <code>class_value_field</code> <code>str</code> <p>Field name in in_class_data containing class values.</p> <code>'class'</code> <code>buffer_radius</code> <code>float</code> <p>Buffer radius around features (in CRS units).</p> <code>0</code> <code>in_mask_polygons</code> <code>str</code> <p>Path to vector file containing mask polygons.</p> <code>None</code> <code>rotation_angle</code> <code>float</code> <p>Rotation angle in degrees.</p> <code>0</code> <code>reference_system</code> <code>str</code> <p>Reference system code.</p> <code>None</code> <code>blacken_around_feature</code> <code>bool</code> <p>Whether to mask areas outside of features.</p> <code>False</code> <code>crop_mode</code> <code>str</code> <p>Crop mode (FIXED_SIZE, CENTERED_ON_FEATURE).</p> <code>'FIXED_SIZE'</code> <code>in_raster2</code> <code>str</code> <p>Path to secondary raster image.</p> <code>None</code> <code>in_instance_data</code> <code>str</code> <p>Path to vector file containing instance polygons.</p> <code>None</code> <code>instance_class_value_field</code> <code>str</code> <p>Field name in in_instance_data for instance classes.</p> <code>None</code> <code>min_polygon_overlap_ratio</code> <code>float</code> <p>Minimum overlap ratio for polygons.</p> <code>0.0</code> <code>all_touched</code> <code>bool</code> <p>Whether to use all_touched=True in rasterization.</p> <code>True</code> <code>save_geotiff</code> <code>bool</code> <p>Whether to save as GeoTIFF with georeferencing.</p> <code>True</code> <code>quiet</code> <code>bool</code> <p>If True, suppress most output messages.</p> <code>False</code> Source code in <code>geoai/utils.py</code> <pre><code>def export_training_data(\n    in_raster,\n    out_folder,\n    in_class_data,\n    image_chip_format=\"GEOTIFF\",\n    tile_size_x=256,\n    tile_size_y=256,\n    stride_x=None,\n    stride_y=None,\n    output_nofeature_tiles=True,\n    metadata_format=\"PASCAL_VOC\",\n    start_index=0,\n    class_value_field=\"class\",\n    buffer_radius=0,\n    in_mask_polygons=None,\n    rotation_angle=0,\n    reference_system=None,\n    blacken_around_feature=False,\n    crop_mode=\"FIXED_SIZE\",  # Implemented but not fully used yet\n    in_raster2=None,\n    in_instance_data=None,\n    instance_class_value_field=None,  # Implemented but not fully used yet\n    min_polygon_overlap_ratio=0.0,\n    all_touched=True,\n    save_geotiff=True,\n    quiet=False,\n):\n    \"\"\"\n    Export training data for deep learning using TorchGeo with progress bar.\n\n    Args:\n        in_raster (str): Path to input raster image.\n        out_folder (str): Output folder path where chips and labels will be saved.\n        in_class_data (str): Path to vector file containing class polygons.\n        image_chip_format (str): Output image format (PNG, JPEG, TIFF, GEOTIFF).\n        tile_size_x (int): Width of image chips in pixels.\n        tile_size_y (int): Height of image chips in pixels.\n        stride_x (int): Horizontal stride between chips. If None, uses tile_size_x.\n        stride_y (int): Vertical stride between chips. If None, uses tile_size_y.\n        output_nofeature_tiles (bool): Whether to export chips without features.\n        metadata_format (str): Output metadata format (PASCAL_VOC, KITTI, COCO).\n        start_index (int): Starting index for chip filenames.\n        class_value_field (str): Field name in in_class_data containing class values.\n        buffer_radius (float): Buffer radius around features (in CRS units).\n        in_mask_polygons (str): Path to vector file containing mask polygons.\n        rotation_angle (float): Rotation angle in degrees.\n        reference_system (str): Reference system code.\n        blacken_around_feature (bool): Whether to mask areas outside of features.\n        crop_mode (str): Crop mode (FIXED_SIZE, CENTERED_ON_FEATURE).\n        in_raster2 (str): Path to secondary raster image.\n        in_instance_data (str): Path to vector file containing instance polygons.\n        instance_class_value_field (str): Field name in in_instance_data for instance classes.\n        min_polygon_overlap_ratio (float): Minimum overlap ratio for polygons.\n        all_touched (bool): Whether to use all_touched=True in rasterization.\n        save_geotiff (bool): Whether to save as GeoTIFF with georeferencing.\n        quiet (bool): If True, suppress most output messages.\n    \"\"\"\n    # Create output directories\n    image_dir = os.path.join(out_folder, \"images\")\n    os.makedirs(image_dir, exist_ok=True)\n\n    label_dir = os.path.join(out_folder, \"labels\")\n    os.makedirs(label_dir, exist_ok=True)\n\n    # Define annotation directories based on metadata format\n    if metadata_format == \"PASCAL_VOC\":\n        ann_dir = os.path.join(out_folder, \"annotations\")\n        os.makedirs(ann_dir, exist_ok=True)\n    elif metadata_format == \"COCO\":\n        ann_dir = os.path.join(out_folder, \"annotations\")\n        os.makedirs(ann_dir, exist_ok=True)\n        # Initialize COCO annotations dictionary\n        coco_annotations = {\"images\": [], \"annotations\": [], \"categories\": []}\n\n    # Initialize statistics dictionary\n    stats = {\n        \"total_tiles\": 0,\n        \"tiles_with_features\": 0,\n        \"feature_pixels\": 0,\n        \"errors\": 0,\n    }\n\n    # Open raster\n    with rasterio.open(in_raster) as src:\n        if not quiet:\n            print(f\"\\nRaster info for {in_raster}:\")\n            print(f\"  CRS: {src.crs}\")\n            print(f\"  Dimensions: {src.width} x {src.height}\")\n            print(f\"  Bounds: {src.bounds}\")\n\n        # Set defaults for stride if not provided\n        if stride_x is None:\n            stride_x = tile_size_x\n        if stride_y is None:\n            stride_y = tile_size_y\n\n        # Calculate number of tiles in x and y directions\n        num_tiles_x = math.ceil((src.width - tile_size_x) / stride_x) + 1\n        num_tiles_y = math.ceil((src.height - tile_size_y) / stride_y) + 1\n        total_tiles = num_tiles_x * num_tiles_y\n\n        # Read class data\n        gdf = gpd.read_file(in_class_data)\n        if not quiet:\n            print(f\"Loaded {len(gdf)} features from {in_class_data}\")\n            print(f\"Available columns: {gdf.columns.tolist()}\")\n            print(f\"GeoJSON CRS: {gdf.crs}\")\n\n        # Check if class_value_field exists\n        if class_value_field not in gdf.columns:\n            if not quiet:\n                print(\n                    f\"WARNING: '{class_value_field}' field not found in the input data. Using default class value 1.\"\n                )\n            # Add a default class column\n            gdf[class_value_field] = 1\n            unique_classes = [1]\n        else:\n            # Print unique classes for debugging\n            unique_classes = gdf[class_value_field].unique()\n            if not quiet:\n                print(f\"Found {len(unique_classes)} unique classes: {unique_classes}\")\n\n        # CRITICAL: Always reproject to match raster CRS to ensure proper alignment\n        if gdf.crs != src.crs:\n            if not quiet:\n                print(f\"Reprojecting features from {gdf.crs} to {src.crs}\")\n            gdf = gdf.to_crs(src.crs)\n        elif reference_system and gdf.crs != reference_system:\n            if not quiet:\n                print(\n                    f\"Reprojecting features to specified reference system {reference_system}\"\n                )\n            gdf = gdf.to_crs(reference_system)\n\n        # Check overlap between raster and vector data\n        raster_bounds = box(*src.bounds)\n        vector_bounds = box(*gdf.total_bounds)\n        if not raster_bounds.intersects(vector_bounds):\n            if not quiet:\n                print(\n                    \"WARNING: The vector data doesn't intersect with the raster extent!\"\n                )\n                print(f\"Raster bounds: {src.bounds}\")\n                print(f\"Vector bounds: {gdf.total_bounds}\")\n        else:\n            overlap = (\n                raster_bounds.intersection(vector_bounds).area / vector_bounds.area\n            )\n            if not quiet:\n                print(f\"Overlap between raster and vector: {overlap:.2%}\")\n\n        # Apply buffer if specified\n        if buffer_radius &gt; 0:\n            gdf[\"geometry\"] = gdf.buffer(buffer_radius)\n\n        # Initialize class mapping (ensure all classes are mapped to non-zero values)\n        class_to_id = {cls: i + 1 for i, cls in enumerate(unique_classes)}\n\n        # Store category info for COCO format\n        if metadata_format == \"COCO\":\n            for cls_val in unique_classes:\n                coco_annotations[\"categories\"].append(\n                    {\n                        \"id\": class_to_id[cls_val],\n                        \"name\": str(cls_val),\n                        \"supercategory\": \"object\",\n                    }\n                )\n\n        # Load mask polygons if provided\n        mask_gdf = None\n        if in_mask_polygons:\n            mask_gdf = gpd.read_file(in_mask_polygons)\n            if reference_system:\n                mask_gdf = mask_gdf.to_crs(reference_system)\n            elif mask_gdf.crs != src.crs:\n                mask_gdf = mask_gdf.to_crs(src.crs)\n\n        # Process instance data if provided\n        instance_gdf = None\n        if in_instance_data:\n            instance_gdf = gpd.read_file(in_instance_data)\n            if reference_system:\n                instance_gdf = instance_gdf.to_crs(reference_system)\n            elif instance_gdf.crs != src.crs:\n                instance_gdf = instance_gdf.to_crs(src.crs)\n\n        # Load secondary raster if provided\n        src2 = None\n        if in_raster2:\n            src2 = rasterio.open(in_raster2)\n\n        # Set up augmentation if rotation is specified\n        augmentation = None\n        if rotation_angle != 0:\n            # Fixed: Added data_keys parameter to AugmentationSequential\n            augmentation = torchgeo.transforms.AugmentationSequential(\n                torch.nn.ModuleList([RandomRotation(rotation_angle)]),\n                data_keys=[\"image\"],  # Add data_keys parameter\n            )\n\n        # Initialize annotation ID for COCO format\n        ann_id = 0\n\n        # Create progress bar\n        pbar = tqdm(\n            total=total_tiles,\n            desc=f\"Generating tiles (with features: 0)\",\n            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}&lt;{remaining}, {rate_fmt}]\",\n        )\n\n        # Generate tiles\n        chip_index = start_index\n        for y in range(num_tiles_y):\n            for x in range(num_tiles_x):\n                # Calculate window coordinates\n                window_x = x * stride_x\n                window_y = y * stride_y\n\n                # Adjust for edge cases\n                if window_x + tile_size_x &gt; src.width:\n                    window_x = src.width - tile_size_x\n                if window_y + tile_size_y &gt; src.height:\n                    window_y = src.height - tile_size_y\n\n                # Adjust window based on crop_mode\n                if crop_mode == \"CENTERED_ON_FEATURE\" and len(gdf) &gt; 0:\n                    # Find the nearest feature to the center of this window\n                    window_center_x = window_x + tile_size_x // 2\n                    window_center_y = window_y + tile_size_y // 2\n\n                    # Convert center to world coordinates\n                    center_x, center_y = src.xy(window_center_y, window_center_x)\n                    center_point = gpd.points_from_xy([center_x], [center_y])[0]\n\n                    # Find nearest feature\n                    distances = gdf.geometry.distance(center_point)\n                    nearest_idx = distances.idxmin()\n                    nearest_feature = gdf.iloc[nearest_idx]\n\n                    # Get centroid of nearest feature\n                    feature_centroid = nearest_feature.geometry.centroid\n\n                    # Convert feature centroid to pixel coordinates\n                    feature_row, feature_col = src.index(\n                        feature_centroid.x, feature_centroid.y\n                    )\n\n                    # Adjust window to center on feature\n                    window_x = max(\n                        0, min(src.width - tile_size_x, feature_col - tile_size_x // 2)\n                    )\n                    window_y = max(\n                        0, min(src.height - tile_size_y, feature_row - tile_size_y // 2)\n                    )\n\n                # Define window\n                window = Window(window_x, window_y, tile_size_x, tile_size_y)\n\n                # Get window transform and bounds in source CRS\n                window_transform = src.window_transform(window)\n\n                # Calculate window bounds more explicitly and accurately\n                minx = window_transform[2]  # Upper left x\n                maxy = window_transform[5]  # Upper left y\n                maxx = minx + tile_size_x * window_transform[0]  # Add width\n                miny = (\n                    maxy + tile_size_y * window_transform[4]\n                )  # Add height (note: transform[4] is typically negative)\n\n                window_bounds = box(minx, miny, maxx, maxy)\n\n                # Apply rotation if specified\n                if rotation_angle != 0:\n                    window_bounds = rotate(\n                        window_bounds, rotation_angle, origin=\"center\"\n                    )\n\n                # Find features that intersect with window\n                window_features = gdf[gdf.intersects(window_bounds)]\n\n                # Process instance data if provided\n                window_instances = None\n                if instance_gdf is not None and instance_class_value_field is not None:\n                    window_instances = instance_gdf[\n                        instance_gdf.intersects(window_bounds)\n                    ]\n                    if len(window_instances) &gt; 0:\n                        if not quiet:\n                            pbar.write(\n                                f\"Found {len(window_instances)} instances in tile {chip_index}\"\n                            )\n\n                # Skip if no features and output_nofeature_tiles is False\n                if not output_nofeature_tiles and len(window_features) == 0:\n                    pbar.update(1)  # Still update progress bar\n                    continue\n\n                # Check polygon overlap ratio if specified\n                if min_polygon_overlap_ratio &gt; 0 and len(window_features) &gt; 0:\n                    valid_features = []\n                    for _, feature in window_features.iterrows():\n                        overlap_ratio = (\n                            feature.geometry.intersection(window_bounds).area\n                            / feature.geometry.area\n                        )\n                        if overlap_ratio &gt;= min_polygon_overlap_ratio:\n                            valid_features.append(feature)\n\n                    if len(valid_features) &gt; 0:\n                        window_features = gpd.GeoDataFrame(valid_features)\n                    elif not output_nofeature_tiles:\n                        pbar.update(1)  # Still update progress bar\n                        continue\n\n                # Apply mask if provided\n                if mask_gdf is not None:\n                    mask_features = mask_gdf[mask_gdf.intersects(window_bounds)]\n                    if len(mask_features) == 0:\n                        pbar.update(1)  # Still update progress bar\n                        continue\n\n                # Read image data - keep original for GeoTIFF export\n                orig_image_data = src.read(window=window)\n\n                # Create a copy for processing\n                image_data = orig_image_data.copy().astype(np.float32)\n\n                # Normalize image data for processing\n                for band in range(image_data.shape[0]):\n                    band_min, band_max = np.percentile(image_data[band], (1, 99))\n                    if band_max &gt; band_min:\n                        image_data[band] = np.clip(\n                            (image_data[band] - band_min) / (band_max - band_min), 0, 1\n                        )\n\n                # Read secondary image data if provided\n                if src2:\n                    image_data2 = src2.read(window=window)\n                    # Stack the two images\n                    image_data = np.vstack((image_data, image_data2))\n\n                # Apply blacken_around_feature if needed\n                if blacken_around_feature and len(window_features) &gt; 0:\n                    mask = np.zeros((tile_size_y, tile_size_x), dtype=bool)\n                    for _, feature in window_features.iterrows():\n                        # Project feature to pixel coordinates\n                        feature_pixels = features.rasterize(\n                            [(feature.geometry, 1)],\n                            out_shape=(tile_size_y, tile_size_x),\n                            transform=window_transform,\n                        )\n                        mask = np.logical_or(mask, feature_pixels.astype(bool))\n\n                    # Apply mask to image\n                    for band in range(image_data.shape[0]):\n                        temp = image_data[band, :, :]\n                        temp[~mask] = 0\n                        image_data[band, :, :] = temp\n\n                # Apply rotation if specified\n                if augmentation:\n                    # Convert to torch tensor for augmentation\n                    image_tensor = torch.from_numpy(image_data).unsqueeze(\n                        0\n                    )  # Add batch dimension\n                    # Apply augmentation with proper data format\n                    augmented = augmentation({\"image\": image_tensor})\n                    image_data = (\n                        augmented[\"image\"].squeeze(0).numpy()\n                    )  # Remove batch dimension\n\n                # Create a processed version for regular image formats\n                processed_image = (image_data * 255).astype(np.uint8)\n\n                # Create label mask\n                label_mask = np.zeros((tile_size_y, tile_size_x), dtype=np.uint8)\n                has_features = False\n\n                if len(window_features) &gt; 0:\n                    for idx, feature in window_features.iterrows():\n                        # Get class value\n                        class_val = (\n                            feature[class_value_field]\n                            if class_value_field in feature\n                            else 1\n                        )\n                        if isinstance(class_val, str):\n                            # If class is a string, use its position in the unique classes list\n                            class_id = class_to_id.get(class_val, 1)\n                        else:\n                            # If class is already a number, use it directly\n                            class_id = int(class_val) if class_val &gt; 0 else 1\n\n                        # Get the geometry in pixel coordinates\n                        geom = feature.geometry.intersection(window_bounds)\n                        if not geom.is_empty:\n                            try:\n                                # Rasterize the feature\n                                feature_mask = features.rasterize(\n                                    [(geom, class_id)],\n                                    out_shape=(tile_size_y, tile_size_x),\n                                    transform=window_transform,\n                                    fill=0,\n                                    all_touched=all_touched,\n                                )\n\n                                # Update mask with higher class values taking precedence\n                                label_mask = np.maximum(label_mask, feature_mask)\n\n                                # Check if any pixels were added\n                                if np.any(feature_mask):\n                                    has_features = True\n                            except Exception as e:\n                                if not quiet:\n                                    pbar.write(f\"Error rasterizing feature {idx}: {e}\")\n                                stats[\"errors\"] += 1\n\n                # Save as GeoTIFF if requested\n                if save_geotiff or image_chip_format.upper() in [\n                    \"TIFF\",\n                    \"TIF\",\n                    \"GEOTIFF\",\n                ]:\n                    # Standardize extension to .tif for GeoTIFF files\n                    image_filename = f\"tile_{chip_index:06d}.tif\"\n                    image_path = os.path.join(image_dir, image_filename)\n\n                    # Create profile for the GeoTIFF\n                    profile = src.profile.copy()\n                    profile.update(\n                        {\n                            \"height\": tile_size_y,\n                            \"width\": tile_size_x,\n                            \"count\": orig_image_data.shape[0],\n                            \"transform\": window_transform,\n                        }\n                    )\n\n                    # Save the GeoTIFF with original data\n                    try:\n                        with rasterio.open(image_path, \"w\", **profile) as dst:\n                            dst.write(orig_image_data)\n                        stats[\"total_tiles\"] += 1\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(\n                                f\"ERROR saving image GeoTIFF for tile {chip_index}: {e}\"\n                            )\n                        stats[\"errors\"] += 1\n                else:\n                    # For non-GeoTIFF formats, use PIL to save the image\n                    image_filename = (\n                        f\"tile_{chip_index:06d}.{image_chip_format.lower()}\"\n                    )\n                    image_path = os.path.join(image_dir, image_filename)\n\n                    # Create PIL image for saving\n                    if processed_image.shape[0] == 1:\n                        img = Image.fromarray(processed_image[0])\n                    elif processed_image.shape[0] == 3:\n                        # For RGB, need to transpose and make sure it's the right data type\n                        rgb_data = np.transpose(processed_image, (1, 2, 0))\n                        img = Image.fromarray(rgb_data)\n                    else:\n                        # For multiband images, save only RGB or first three bands\n                        rgb_data = np.transpose(processed_image[:3], (1, 2, 0))\n                        img = Image.fromarray(rgb_data)\n\n                    # Save image\n                    try:\n                        img.save(image_path)\n                        stats[\"total_tiles\"] += 1\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(f\"ERROR saving image for tile {chip_index}: {e}\")\n                        stats[\"errors\"] += 1\n\n                # Save label as GeoTIFF\n                label_filename = f\"tile_{chip_index:06d}.tif\"\n                label_path = os.path.join(label_dir, label_filename)\n\n                # Create profile for label GeoTIFF\n                label_profile = {\n                    \"driver\": \"GTiff\",\n                    \"height\": tile_size_y,\n                    \"width\": tile_size_x,\n                    \"count\": 1,\n                    \"dtype\": \"uint8\",\n                    \"crs\": src.crs,\n                    \"transform\": window_transform,\n                }\n\n                # Save label GeoTIFF\n                try:\n                    with rasterio.open(label_path, \"w\", **label_profile) as dst:\n                        dst.write(label_mask, 1)\n\n                    if has_features:\n                        pixel_count = np.count_nonzero(label_mask)\n                        stats[\"tiles_with_features\"] += 1\n                        stats[\"feature_pixels\"] += pixel_count\n                except Exception as e:\n                    if not quiet:\n                        pbar.write(f\"ERROR saving label for tile {chip_index}: {e}\")\n                    stats[\"errors\"] += 1\n\n                # Also save a PNG version for easy visualization if requested\n                if metadata_format == \"PASCAL_VOC\":\n                    try:\n                        # Ensure correct data type for PIL\n                        png_label = label_mask.astype(np.uint8)\n                        label_img = Image.fromarray(png_label)\n                        label_png_path = os.path.join(\n                            label_dir, f\"tile_{chip_index:06d}.png\"\n                        )\n                        label_img.save(label_png_path)\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(\n                                f\"ERROR saving PNG label for tile {chip_index}: {e}\"\n                            )\n                            pbar.write(\n                                f\"  Label mask shape: {label_mask.shape}, dtype: {label_mask.dtype}\"\n                            )\n                            # Try again with explicit conversion\n                            try:\n                                # Alternative approach for problematic arrays\n                                png_data = np.zeros(\n                                    (tile_size_y, tile_size_x), dtype=np.uint8\n                                )\n                                np.copyto(png_data, label_mask, casting=\"unsafe\")\n                                label_img = Image.fromarray(png_data)\n                                label_img.save(label_png_path)\n                                pbar.write(\n                                    f\"  Succeeded using alternative conversion method\"\n                                )\n                            except Exception as e2:\n                                pbar.write(f\"  Second attempt also failed: {e2}\")\n                                stats[\"errors\"] += 1\n\n                # Generate annotations\n                if metadata_format == \"PASCAL_VOC\" and len(window_features) &gt; 0:\n                    # Create XML annotation\n                    root = ET.Element(\"annotation\")\n                    ET.SubElement(root, \"folder\").text = \"images\"\n                    ET.SubElement(root, \"filename\").text = image_filename\n\n                    size = ET.SubElement(root, \"size\")\n                    ET.SubElement(size, \"width\").text = str(tile_size_x)\n                    ET.SubElement(size, \"height\").text = str(tile_size_y)\n                    ET.SubElement(size, \"depth\").text = str(min(image_data.shape[0], 3))\n\n                    # Add georeference information\n                    geo = ET.SubElement(root, \"georeference\")\n                    ET.SubElement(geo, \"crs\").text = str(src.crs)\n                    ET.SubElement(geo, \"transform\").text = str(\n                        window_transform\n                    ).replace(\"\\n\", \"\")\n                    ET.SubElement(geo, \"bounds\").text = (\n                        f\"{minx}, {miny}, {maxx}, {maxy}\"\n                    )\n\n                    for _, feature in window_features.iterrows():\n                        # Convert feature geometry to pixel coordinates\n                        feature_bounds = feature.geometry.intersection(window_bounds)\n                        if feature_bounds.is_empty:\n                            continue\n\n                        # Get pixel coordinates of bounds\n                        minx_f, miny_f, maxx_f, maxy_f = feature_bounds.bounds\n\n                        # Convert to pixel coordinates\n                        col_min, row_min = ~window_transform * (minx_f, maxy_f)\n                        col_max, row_max = ~window_transform * (maxx_f, miny_f)\n\n                        # Ensure coordinates are within bounds\n                        xmin = max(0, min(tile_size_x, int(col_min)))\n                        ymin = max(0, min(tile_size_y, int(row_min)))\n                        xmax = max(0, min(tile_size_x, int(col_max)))\n                        ymax = max(0, min(tile_size_y, int(row_max)))\n\n                        # Skip if box is too small\n                        if xmax - xmin &lt; 1 or ymax - ymin &lt; 1:\n                            continue\n\n                        obj = ET.SubElement(root, \"object\")\n                        ET.SubElement(obj, \"name\").text = str(\n                            feature[class_value_field]\n                        )\n                        ET.SubElement(obj, \"difficult\").text = \"0\"\n\n                        bbox = ET.SubElement(obj, \"bndbox\")\n                        ET.SubElement(bbox, \"xmin\").text = str(xmin)\n                        ET.SubElement(bbox, \"ymin\").text = str(ymin)\n                        ET.SubElement(bbox, \"xmax\").text = str(xmax)\n                        ET.SubElement(bbox, \"ymax\").text = str(ymax)\n\n                    # Save XML\n                    try:\n                        tree = ET.ElementTree(root)\n                        xml_path = os.path.join(ann_dir, f\"tile_{chip_index:06d}.xml\")\n                        tree.write(xml_path)\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(\n                                f\"ERROR saving XML annotation for tile {chip_index}: {e}\"\n                            )\n                        stats[\"errors\"] += 1\n\n                elif metadata_format == \"COCO\" and len(window_features) &gt; 0:\n                    # Add image info\n                    image_id = chip_index\n                    coco_annotations[\"images\"].append(\n                        {\n                            \"id\": image_id,\n                            \"file_name\": image_filename,\n                            \"width\": tile_size_x,\n                            \"height\": tile_size_y,\n                            \"crs\": str(src.crs),\n                            \"transform\": str(window_transform),\n                        }\n                    )\n\n                    # Add annotations for each feature\n                    for _, feature in window_features.iterrows():\n                        feature_bounds = feature.geometry.intersection(window_bounds)\n                        if feature_bounds.is_empty:\n                            continue\n\n                        # Get pixel coordinates of bounds\n                        minx_f, miny_f, maxx_f, maxy_f = feature_bounds.bounds\n\n                        # Convert to pixel coordinates\n                        col_min, row_min = ~window_transform * (minx_f, maxy_f)\n                        col_max, row_max = ~window_transform * (maxx_f, miny_f)\n\n                        # Ensure coordinates are within bounds\n                        xmin = max(0, min(tile_size_x, int(col_min)))\n                        ymin = max(0, min(tile_size_y, int(row_min)))\n                        xmax = max(0, min(tile_size_x, int(col_max)))\n                        ymax = max(0, min(tile_size_y, int(row_max)))\n\n                        # Skip if box is too small\n                        if xmax - xmin &lt; 1 or ymax - ymin &lt; 1:\n                            continue\n\n                        width = xmax - xmin\n                        height = ymax - ymin\n\n                        # Add annotation\n                        ann_id += 1\n                        category_id = class_to_id[feature[class_value_field]]\n\n                        coco_annotations[\"annotations\"].append(\n                            {\n                                \"id\": ann_id,\n                                \"image_id\": image_id,\n                                \"category_id\": category_id,\n                                \"bbox\": [xmin, ymin, width, height],\n                                \"area\": width * height,\n                                \"iscrowd\": 0,\n                            }\n                        )\n\n                # Update progress bar\n                pbar.update(1)\n                pbar.set_description(\n                    f\"Generated: {stats['total_tiles']}, With features: {stats['tiles_with_features']}\"\n                )\n\n                chip_index += 1\n\n        # Close progress bar\n        pbar.close()\n\n        # Save COCO annotations if applicable\n        if metadata_format == \"COCO\":\n            try:\n                with open(os.path.join(ann_dir, \"instances.json\"), \"w\") as f:\n                    json.dump(coco_annotations, f)\n            except Exception as e:\n                if not quiet:\n                    print(f\"ERROR saving COCO annotations: {e}\")\n                stats[\"errors\"] += 1\n\n        # Close secondary raster if opened\n        if src2:\n            src2.close()\n\n    # Print summary\n    if not quiet:\n        print(\"\\n------- Export Summary -------\")\n        print(f\"Total tiles exported: {stats['total_tiles']}\")\n        print(\n            f\"Tiles with features: {stats['tiles_with_features']} ({stats['tiles_with_features']/max(1, stats['total_tiles'])*100:.1f}%)\"\n        )\n        if stats[\"tiles_with_features\"] &gt; 0:\n            print(\n                f\"Average feature pixels per tile: {stats['feature_pixels']/stats['tiles_with_features']:.1f}\"\n            )\n        if stats[\"errors\"] &gt; 0:\n            print(f\"Errors encountered: {stats['errors']}\")\n        print(f\"Output saved to: {out_folder}\")\n\n        # Verify georeference in a sample image and label\n        if stats[\"total_tiles\"] &gt; 0:\n            print(\"\\n------- Georeference Verification -------\")\n            sample_image = os.path.join(image_dir, f\"tile_{start_index}.tif\")\n            sample_label = os.path.join(label_dir, f\"tile_{start_index}.tif\")\n\n            if os.path.exists(sample_image):\n                try:\n                    with rasterio.open(sample_image) as img:\n                        print(f\"Image CRS: {img.crs}\")\n                        print(f\"Image transform: {img.transform}\")\n                        print(\n                            f\"Image has georeference: {img.crs is not None and img.transform is not None}\"\n                        )\n                        print(\n                            f\"Image dimensions: {img.width}x{img.height}, {img.count} bands, {img.dtypes[0]} type\"\n                        )\n                except Exception as e:\n                    print(f\"Error verifying image georeference: {e}\")\n\n            if os.path.exists(sample_label):\n                try:\n                    with rasterio.open(sample_label) as lbl:\n                        print(f\"Label CRS: {lbl.crs}\")\n                        print(f\"Label transform: {lbl.transform}\")\n                        print(\n                            f\"Label has georeference: {lbl.crs is not None and lbl.transform is not None}\"\n                        )\n                        print(\n                            f\"Label dimensions: {lbl.width}x{lbl.height}, {lbl.count} bands, {lbl.dtypes[0]} type\"\n                        )\n                except Exception as e:\n                    print(f\"Error verifying label georeference: {e}\")\n\n    # Return statistics\n    return stats, out_folder\n</code></pre>"},{"location":"utils/#geoai.utils.get_raster_info","title":"<code>get_raster_info(raster_path)</code>","text":"<p>Display basic information about a raster dataset.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the basic information about the raster</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_info(raster_path):\n    \"\"\"Display basic information about a raster dataset.\n\n    Args:\n        raster_path (str): Path to the raster file\n\n    Returns:\n        dict: Dictionary containing the basic information about the raster\n    \"\"\"\n    # Open the raster dataset\n    with rasterio.open(raster_path) as src:\n        # Get basic metadata\n        info = {\n            \"driver\": src.driver,\n            \"width\": src.width,\n            \"height\": src.height,\n            \"count\": src.count,\n            \"dtype\": src.dtypes[0],\n            \"crs\": src.crs.to_string() if src.crs else \"No CRS defined\",\n            \"transform\": src.transform,\n            \"bounds\": src.bounds,\n            \"resolution\": (src.transform[0], -src.transform[4]),\n            \"nodata\": src.nodata,\n        }\n\n        # Calculate statistics for each band\n        stats = []\n        for i in range(1, src.count + 1):\n            band = src.read(i, masked=True)\n            band_stats = {\n                \"band\": i,\n                \"min\": float(band.min()),\n                \"max\": float(band.max()),\n                \"mean\": float(band.mean()),\n                \"std\": float(band.std()),\n            }\n            stats.append(band_stats)\n\n        info[\"band_stats\"] = stats\n\n    return info\n</code></pre>"},{"location":"utils/#geoai.utils.get_raster_info_gdal","title":"<code>get_raster_info_gdal(raster_path)</code>","text":"<p>Get basic information about a raster dataset using GDAL.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the basic information about the raster,     or None if the file cannot be opened</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_info_gdal(raster_path):\n    \"\"\"Get basic information about a raster dataset using GDAL.\n\n    Args:\n        raster_path (str): Path to the raster file\n\n    Returns:\n        dict: Dictionary containing the basic information about the raster,\n            or None if the file cannot be opened\n    \"\"\"\n\n    from osgeo import gdal\n\n    # Open the dataset\n    ds = gdal.Open(raster_path)\n    if ds is None:\n        print(f\"Error: Could not open {raster_path}\")\n        return None\n\n    # Get basic information\n    info = {\n        \"driver\": ds.GetDriver().ShortName,\n        \"width\": ds.RasterXSize,\n        \"height\": ds.RasterYSize,\n        \"count\": ds.RasterCount,\n        \"projection\": ds.GetProjection(),\n        \"geotransform\": ds.GetGeoTransform(),\n    }\n\n    # Calculate resolution\n    gt = ds.GetGeoTransform()\n    if gt:\n        info[\"resolution\"] = (abs(gt[1]), abs(gt[5]))\n        info[\"origin\"] = (gt[0], gt[3])\n\n    # Get band information\n    bands_info = []\n    for i in range(1, ds.RasterCount + 1):\n        band = ds.GetRasterBand(i)\n        stats = band.GetStatistics(True, True)\n        band_info = {\n            \"band\": i,\n            \"datatype\": gdal.GetDataTypeName(band.DataType),\n            \"min\": stats[0],\n            \"max\": stats[1],\n            \"mean\": stats[2],\n            \"std\": stats[3],\n            \"nodata\": band.GetNoDataValue(),\n        }\n        bands_info.append(band_info)\n\n    info[\"bands\"] = bands_info\n\n    # Close the dataset\n    ds = None\n\n    return info\n</code></pre>"},{"location":"utils/#geoai.utils.get_raster_stats","title":"<code>get_raster_stats(raster_path, divide_by=1.0)</code>","text":"<p>Calculate statistics for each band in a raster dataset.</p> <p>This function computes min, max, mean, and standard deviation values for each band in the provided raster, returning results in a dictionary with lists for each statistic type.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <code>divide_by</code> <code>float</code> <p>Value to divide pixel values by. Defaults to 1.0, which keeps the original pixel</p> <code>1.0</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing lists of statistics with keys:     - 'min': List of minimum values for each band     - 'max': List of maximum values for each band     - 'mean': List of mean values for each band     - 'std': List of standard deviation values for each band</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_stats(raster_path, divide_by=1.0):\n    \"\"\"Calculate statistics for each band in a raster dataset.\n\n    This function computes min, max, mean, and standard deviation values\n    for each band in the provided raster, returning results in a dictionary\n    with lists for each statistic type.\n\n    Args:\n        raster_path (str): Path to the raster file\n        divide_by (float, optional): Value to divide pixel values by.\n            Defaults to 1.0, which keeps the original pixel\n\n    Returns:\n        dict: Dictionary containing lists of statistics with keys:\n            - 'min': List of minimum values for each band\n            - 'max': List of maximum values for each band\n            - 'mean': List of mean values for each band\n            - 'std': List of standard deviation values for each band\n    \"\"\"\n    # Initialize the results dictionary with empty lists\n    stats = {\"min\": [], \"max\": [], \"mean\": [], \"std\": []}\n\n    # Open the raster dataset\n    with rasterio.open(raster_path) as src:\n        # Calculate statistics for each band\n        for i in range(1, src.count + 1):\n            band = src.read(i, masked=True)\n\n            # Append statistics for this band to each list\n            stats[\"min\"].append(float(band.min()) / divide_by)\n            stats[\"max\"].append(float(band.max()) / divide_by)\n            stats[\"mean\"].append(float(band.mean()) / divide_by)\n            stats[\"std\"].append(float(band.std()) / divide_by)\n\n    return stats\n</code></pre>"},{"location":"utils/#geoai.utils.get_vector_info","title":"<code>get_vector_info(vector_path)</code>","text":"<p>Display basic information about a vector dataset using GeoPandas.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the basic information about the vector dataset</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_vector_info(vector_path):\n    \"\"\"Display basic information about a vector dataset using GeoPandas.\n\n    Args:\n        vector_path (str): Path to the vector file\n\n    Returns:\n        dict: Dictionary containing the basic information about the vector dataset\n    \"\"\"\n    # Open the vector dataset\n    gdf = (\n        gpd.read_parquet(vector_path)\n        if vector_path.endswith(\".parquet\")\n        else gpd.read_file(vector_path)\n    )\n\n    # Get basic metadata\n    info = {\n        \"file_path\": vector_path,\n        \"driver\": os.path.splitext(vector_path)[1][1:].upper(),  # Format from extension\n        \"feature_count\": len(gdf),\n        \"crs\": str(gdf.crs),\n        \"geometry_type\": str(gdf.geom_type.value_counts().to_dict()),\n        \"attribute_count\": len(gdf.columns) - 1,  # Subtract the geometry column\n        \"attribute_names\": list(gdf.columns[gdf.columns != \"geometry\"]),\n        \"bounds\": gdf.total_bounds.tolist(),\n    }\n\n    # Add statistics about numeric attributes\n    numeric_columns = gdf.select_dtypes(include=[\"number\"]).columns\n    attribute_stats = {}\n    for col in numeric_columns:\n        if col != \"geometry\":\n            attribute_stats[col] = {\n                \"min\": gdf[col].min(),\n                \"max\": gdf[col].max(),\n                \"mean\": gdf[col].mean(),\n                \"std\": gdf[col].std(),\n                \"null_count\": gdf[col].isna().sum(),\n            }\n\n    info[\"attribute_stats\"] = attribute_stats\n\n    return info\n</code></pre>"},{"location":"utils/#geoai.utils.get_vector_info_ogr","title":"<code>get_vector_info_ogr(vector_path)</code>","text":"<p>Get basic information about a vector dataset using OGR.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the basic information about the vector dataset,     or None if the file cannot be opened</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_vector_info_ogr(vector_path):\n    \"\"\"Get basic information about a vector dataset using OGR.\n\n    Args:\n        vector_path (str): Path to the vector file\n\n    Returns:\n        dict: Dictionary containing the basic information about the vector dataset,\n            or None if the file cannot be opened\n    \"\"\"\n    from osgeo import ogr\n\n    # Register all OGR drivers\n    ogr.RegisterAll()\n\n    # Open the dataset\n    ds = ogr.Open(vector_path)\n    if ds is None:\n        print(f\"Error: Could not open {vector_path}\")\n        return None\n\n    # Basic dataset information\n    info = {\n        \"file_path\": vector_path,\n        \"driver\": ds.GetDriver().GetName(),\n        \"layer_count\": ds.GetLayerCount(),\n        \"layers\": [],\n    }\n\n    # Extract information for each layer\n    for i in range(ds.GetLayerCount()):\n        layer = ds.GetLayer(i)\n        layer_info = {\n            \"name\": layer.GetName(),\n            \"feature_count\": layer.GetFeatureCount(),\n            \"geometry_type\": ogr.GeometryTypeToName(layer.GetGeomType()),\n            \"spatial_ref\": (\n                layer.GetSpatialRef().ExportToWkt() if layer.GetSpatialRef() else \"None\"\n            ),\n            \"extent\": layer.GetExtent(),\n            \"fields\": [],\n        }\n\n        # Get field information\n        defn = layer.GetLayerDefn()\n        for j in range(defn.GetFieldCount()):\n            field_defn = defn.GetFieldDefn(j)\n            field_info = {\n                \"name\": field_defn.GetName(),\n                \"type\": field_defn.GetTypeName(),\n                \"width\": field_defn.GetWidth(),\n                \"precision\": field_defn.GetPrecision(),\n            }\n            layer_info[\"fields\"].append(field_info)\n\n        info[\"layers\"].append(layer_info)\n\n    # Close the dataset\n    ds = None\n\n    return info\n</code></pre>"},{"location":"utils/#geoai.utils.hybrid_regularization","title":"<code>hybrid_regularization(building_polygons)</code>","text":"<p>A comprehensive hybrid approach to building footprint regularization.</p> <p>Applies different strategies based on building characteristics.</p> <p>Parameters:</p> Name Type Description Default <code>building_polygons</code> <p>GeoDataFrame or list of shapely Polygons containing building footprints</p> required <p>Returns:</p> Type Description <p>GeoDataFrame or list of shapely Polygons with regularized building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def hybrid_regularization(building_polygons):\n    \"\"\"\n    A comprehensive hybrid approach to building footprint regularization.\n\n    Applies different strategies based on building characteristics.\n\n    Args:\n        building_polygons: GeoDataFrame or list of shapely Polygons containing building footprints\n\n    Returns:\n        GeoDataFrame or list of shapely Polygons with regularized building footprints\n    \"\"\"\n    from shapely.affinity import rotate\n    from shapely.geometry import Polygon\n\n    # Use minimum_rotated_rectangle instead of oriented_envelope\n    try:\n        from shapely.minimum_rotated_rectangle import minimum_rotated_rectangle\n    except ImportError:\n        # For older Shapely versions\n        def minimum_rotated_rectangle(geom):\n            \"\"\"Calculate the minimum rotated rectangle for a geometry\"\"\"\n            # For older Shapely versions, implement a simple version\n            return geom.minimum_rotated_rectangle\n\n    # Determine input type for correct return\n    is_gdf = isinstance(building_polygons, gpd.GeoDataFrame)\n\n    # Extract geometries if GeoDataFrame\n    if is_gdf:\n        geom_objects = building_polygons.geometry\n    else:\n        geom_objects = building_polygons\n\n    results = []\n\n    for building in geom_objects:\n        # 1. Analyze building characteristics\n        if not hasattr(building, \"exterior\") or building.is_empty:\n            results.append(building)\n            continue\n\n        # Calculate shape complexity metrics\n        complexity = building.length / (4 * np.sqrt(building.area))\n\n        # Calculate dominant angle\n        coords = np.array(building.exterior.coords)[:-1]\n        segments = np.diff(np.vstack([coords, coords[0]]), axis=0)\n        segment_lengths = np.sqrt(segments[:, 0] ** 2 + segments[:, 1] ** 2)\n        segment_angles = np.arctan2(segments[:, 1], segments[:, 0]) * 180 / np.pi\n\n        # Weight angles by segment length\n        hist, bins = np.histogram(\n            segment_angles % 180, bins=36, range=(0, 180), weights=segment_lengths\n        )\n        bin_centers = (bins[:-1] + bins[1:]) / 2\n        dominant_angle = bin_centers[np.argmax(hist)]\n\n        # Check if building is close to orthogonal\n        is_orthogonal = min(dominant_angle % 45, 45 - (dominant_angle % 45)) &lt; 5\n\n        # 2. Apply appropriate regularization strategy\n        if complexity &gt; 1.5:\n            # Complex buildings: use minimum rotated rectangle\n            result = minimum_rotated_rectangle(building)\n        elif is_orthogonal:\n            # Near-orthogonal buildings: orthogonalize in place\n            rotated = rotate(building, -dominant_angle, origin=\"centroid\")\n\n            # Create orthogonal hull in rotated space\n            bounds = rotated.bounds\n            ortho_hull = Polygon(\n                [\n                    (bounds[0], bounds[1]),\n                    (bounds[2], bounds[1]),\n                    (bounds[2], bounds[3]),\n                    (bounds[0], bounds[3]),\n                ]\n            )\n\n            result = rotate(ortho_hull, dominant_angle, origin=\"centroid\")\n        else:\n            # Diagonal buildings: use custom approach for diagonal buildings\n            # Rotate to align with axes\n            rotated = rotate(building, -dominant_angle, origin=\"centroid\")\n\n            # Simplify in rotated space\n            simplified = rotated.simplify(0.3, preserve_topology=True)\n\n            # Get the bounds in rotated space\n            bounds = simplified.bounds\n            min_x, min_y, max_x, max_y = bounds\n\n            # Create a rectangular hull in rotated space\n            rect_poly = Polygon(\n                [(min_x, min_y), (max_x, min_y), (max_x, max_y), (min_x, max_y)]\n            )\n\n            # Rotate back to original orientation\n            result = rotate(rect_poly, dominant_angle, origin=\"centroid\")\n\n        results.append(result)\n\n    # Return in same format as input\n    if is_gdf:\n        return gpd.GeoDataFrame(geometry=results, crs=building_polygons.crs)\n    else:\n        return results\n</code></pre>"},{"location":"utils/#geoai.utils.inspect_pth_file","title":"<code>inspect_pth_file(pth_path)</code>","text":"<p>Inspect a PyTorch .pth model file to determine its architecture.</p> <p>Parameters:</p> Name Type Description Default <code>pth_path</code> <p>Path to the .pth file to inspect</p> required <p>Returns:</p> Type Description <p>Information about the model architecture</p> Source code in <code>geoai/utils.py</code> <pre><code>def inspect_pth_file(pth_path):\n    \"\"\"\n    Inspect a PyTorch .pth model file to determine its architecture.\n\n    Args:\n        pth_path: Path to the .pth file to inspect\n\n    Returns:\n        Information about the model architecture\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(pth_path):\n        print(f\"Error: File {pth_path} not found\")\n        return\n\n    # Load the checkpoint\n    try:\n        checkpoint = torch.load(pth_path, map_location=torch.device(\"cpu\"))\n        print(f\"\\n{'='*50}\")\n        print(f\"Inspecting model file: {pth_path}\")\n        print(f\"{'='*50}\\n\")\n\n        # Check if it's a state_dict or a complete model\n        if isinstance(checkpoint, OrderedDict) or isinstance(checkpoint, dict):\n            if \"state_dict\" in checkpoint:\n                print(\"Found 'state_dict' key in the checkpoint.\")\n                state_dict = checkpoint[\"state_dict\"]\n            elif \"model_state_dict\" in checkpoint:\n                print(\"Found 'model_state_dict' key in the checkpoint.\")\n                state_dict = checkpoint[\"model_state_dict\"]\n            else:\n                print(\"Assuming file contains a direct state_dict.\")\n                state_dict = checkpoint\n\n            # Print the keys in the checkpoint\n            print(\"\\nCheckpoint contains the following keys:\")\n            for key in checkpoint.keys():\n                if isinstance(checkpoint[key], dict):\n                    print(f\"- {key} (dictionary with {len(checkpoint[key])} items)\")\n                elif isinstance(checkpoint[key], (torch.Tensor, list, tuple)):\n                    print(\n                        f\"- {key} (shape/size: {len(checkpoint[key]) if isinstance(checkpoint[key], (list, tuple)) else checkpoint[key].shape})\"\n                    )\n                else:\n                    print(f\"- {key} ({type(checkpoint[key]).__name__})\")\n\n            # Try to infer the model architecture from the state_dict keys\n            print(\"\\nAnalyzing model architecture from state_dict...\")\n\n            # Extract layer keys for analysis\n            layer_keys = list(state_dict.keys())\n\n            # Print the first few layer keys to understand naming pattern\n            print(\"\\nFirst 10 layer names in state_dict:\")\n            for i, key in enumerate(layer_keys[:10]):\n                shape = state_dict[key].shape\n                print(f\"- {key} (shape: {shape})\")\n\n            # Look for architecture indicators in the keys\n            architecture_indicators = {\n                \"conv\": 0,\n                \"bn\": 0,\n                \"layer\": 0,\n                \"fc\": 0,\n                \"backbone\": 0,\n                \"encoder\": 0,\n                \"decoder\": 0,\n                \"unet\": 0,\n                \"resnet\": 0,\n                \"classifier\": 0,\n                \"deeplab\": 0,\n                \"fcn\": 0,\n            }\n\n            for key in layer_keys:\n                for indicator in architecture_indicators:\n                    if indicator in key.lower():\n                        architecture_indicators[indicator] += 1\n\n            print(\"\\nArchitecture indicators found in layer names:\")\n            for indicator, count in architecture_indicators.items():\n                if count &gt; 0:\n                    print(f\"- '{indicator}' appears {count} times\")\n\n            # Count total parameters\n            total_params = sum(p.numel() for p in state_dict.values())\n            print(f\"\\nTotal parameters: {total_params:,}\")\n\n            # Try to load the model with different architectures\n            print(\"\\nAttempting to match with common architectures...\")\n\n            # Try to identify if it's a segmentation model\n            if any(\"out\" in k or \"classifier\" in k for k in layer_keys):\n                print(\"Model appears to be a segmentation model.\")\n\n                # Check if it might be a UNet\n                if (\n                    architecture_indicators[\"encoder\"] &gt; 0\n                    and architecture_indicators[\"decoder\"] &gt; 0\n                ):\n                    print(\n                        \"Architecture seems to be a UNet-based model with encoder-decoder structure.\"\n                    )\n                # Check for FCN or DeepLab indicators\n                elif architecture_indicators[\"fcn\"] &gt; 0:\n                    print(\n                        \"Architecture seems to be FCN-based (Fully Convolutional Network).\"\n                    )\n                elif architecture_indicators[\"deeplab\"] &gt; 0:\n                    print(\"Architecture seems to be DeepLab-based.\")\n                elif architecture_indicators[\"backbone\"] &gt; 0:\n                    print(\n                        \"Model has a backbone architecture, likely a modern segmentation model.\"\n                    )\n\n            # Try to infer output classes from the final layer\n            output_layer_keys = [\n                k for k in layer_keys if \"classifier\" in k or k.endswith(\".out.weight\")\n            ]\n            if output_layer_keys:\n                output_shape = state_dict[output_layer_keys[0]].shape\n                if len(output_shape) &gt;= 2:\n                    num_classes = output_shape[0]\n                    print(f\"\\nModel likely has {num_classes} output classes.\")\n\n            print(\"\\nSUMMARY:\")\n            print(\"The model appears to be\", end=\" \")\n            if architecture_indicators[\"unet\"] &gt; 0:\n                print(\"a UNet architecture.\", end=\" \")\n            elif architecture_indicators[\"fcn\"] &gt; 0:\n                print(\"an FCN architecture.\", end=\" \")\n            elif architecture_indicators[\"deeplab\"] &gt; 0:\n                print(\"a DeepLab architecture.\", end=\" \")\n            elif architecture_indicators[\"resnet\"] &gt; 0:\n                print(\"ResNet-based.\", end=\" \")\n            else:\n                print(\"a custom architecture.\", end=\" \")\n\n            # Try to load with common models\n            try_common_architectures(state_dict)\n\n        else:\n            print(\n                \"The file contains an entire model object rather than just a state dictionary.\"\n            )\n            # If it's a complete model, we can directly examine its architecture\n            print(checkpoint)\n\n    except Exception as e:\n        print(f\"Error loading the model file: {str(e)}\")\n</code></pre>"},{"location":"utils/#geoai.utils.install_package","title":"<code>install_package(package)</code>","text":"<p>Install a Python package.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str | list</code> <p>The package name or a GitHub URL or a list of package names or GitHub URLs.</p> required Source code in <code>geoai/utils.py</code> <pre><code>def install_package(package):\n    \"\"\"Install a Python package.\n\n    Args:\n        package (str | list): The package name or a GitHub URL or a list of package names or GitHub URLs.\n    \"\"\"\n    import subprocess\n\n    if isinstance(package, str):\n        packages = [package]\n    elif isinstance(package, list):\n        packages = package\n    else:\n        raise ValueError(\"The package argument must be a string or a list of strings.\")\n\n    for package in packages:\n        if package.startswith(\"https\"):\n            package = f\"git+{package}\"\n\n        # Execute pip install command and show output in real-time\n        command = f\"pip install {package}\"\n        process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n\n        # Print output in real-time\n        while True:\n            output = process.stdout.readline()\n            if output == b\"\" and process.poll() is not None:\n                break\n            if output:\n                print(output.decode(\"utf-8\").strip())\n\n        # Wait for process to complete\n        process.wait()\n</code></pre>"},{"location":"utils/#geoai.utils.masks_to_vector","title":"<code>masks_to_vector(mask_path, output_path=None, simplify_tolerance=1.0, mask_threshold=0.5, min_object_area=100, max_object_area=None, nms_iou_threshold=0.5)</code>","text":"<p>Convert a building mask GeoTIFF to vector polygons and save as a vector dataset.</p> <p>Parameters:</p> Name Type Description Default <code>mask_path</code> <p>Path to the building masks GeoTIFF</p> required <code>output_path</code> <p>Path to save the output GeoJSON (default: mask_path with .geojson extension)</p> <code>None</code> <code>simplify_tolerance</code> <p>Tolerance for polygon simplification (default: self.simplify_tolerance)</p> <code>1.0</code> <code>mask_threshold</code> <p>Threshold for mask binarization (default: self.mask_threshold)</p> <code>0.5</code> <code>min_object_area</code> <p>Minimum area in pixels to keep a building (default: self.min_object_area)</p> <code>100</code> <code>max_object_area</code> <p>Maximum area in pixels to keep a building (default: self.max_object_area)</p> <code>None</code> <code>nms_iou_threshold</code> <p>IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)</p> <code>0.5</code> <p>Returns:</p> Type Description <p>GeoDataFrame with building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def masks_to_vector(\n    mask_path,\n    output_path=None,\n    simplify_tolerance=1.0,\n    mask_threshold=0.5,\n    min_object_area=100,\n    max_object_area=None,\n    nms_iou_threshold=0.5,\n):\n    \"\"\"\n    Convert a building mask GeoTIFF to vector polygons and save as a vector dataset.\n\n    Args:\n        mask_path: Path to the building masks GeoTIFF\n        output_path: Path to save the output GeoJSON (default: mask_path with .geojson extension)\n        simplify_tolerance: Tolerance for polygon simplification (default: self.simplify_tolerance)\n        mask_threshold: Threshold for mask binarization (default: self.mask_threshold)\n        min_object_area: Minimum area in pixels to keep a building (default: self.min_object_area)\n        max_object_area: Maximum area in pixels to keep a building (default: self.max_object_area)\n        nms_iou_threshold: IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)\n\n    Returns:\n        GeoDataFrame with building footprints\n    \"\"\"\n    # Set default output path if not provided\n    # if output_path is None:\n    #     output_path = os.path.splitext(mask_path)[0] + \".geojson\"\n\n    print(f\"Converting mask to GeoJSON with parameters:\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n    print(f\"- Min building area: {min_object_area}\")\n    print(f\"- Simplify tolerance: {simplify_tolerance}\")\n    print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n\n    # Open the mask raster\n    with rasterio.open(mask_path) as src:\n        # Read the mask data\n        mask_data = src.read(1)\n        transform = src.transform\n        crs = src.crs\n\n        # Print mask statistics\n        print(f\"Mask dimensions: {mask_data.shape}\")\n        print(f\"Mask value range: {mask_data.min()} to {mask_data.max()}\")\n\n        # Prepare for connected component analysis\n        # Binarize the mask based on threshold\n        binary_mask = (mask_data &gt; (mask_threshold * 255)).astype(np.uint8)\n\n        # Apply morphological operations for better results (optional)\n        kernel = np.ones((3, 3), np.uint8)\n        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find connected components\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n            binary_mask, connectivity=8\n        )\n\n        print(f\"Found {num_labels-1} potential buildings\")  # Subtract 1 for background\n\n        # Create list to store polygons and confidence values\n        all_polygons = []\n        all_confidences = []\n\n        # Process each component (skip the first one which is background)\n        for i in tqdm(range(1, num_labels)):\n            # Extract this building\n            area = stats[i, cv2.CC_STAT_AREA]\n\n            # Skip if too small\n            if area &lt; min_object_area:\n                continue\n\n            # Skip if too large\n            if max_object_area is not None and area &gt; max_object_area:\n                continue\n\n            # Create a mask for this building\n            building_mask = (labels == i).astype(np.uint8)\n\n            # Find contours\n            contours, _ = cv2.findContours(\n                building_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n            )\n\n            # Process each contour\n            for contour in contours:\n                # Skip if too few points\n                if contour.shape[0] &lt; 3:\n                    continue\n\n                # Simplify contour if it has many points\n                if contour.shape[0] &gt; 50 and simplify_tolerance &gt; 0:\n                    epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                    contour = cv2.approxPolyDP(contour, epsilon, True)\n\n                # Convert to list of (x, y) coordinates\n                polygon_points = contour.reshape(-1, 2)\n\n                # Convert pixel coordinates to geographic coordinates\n                geo_points = []\n                for x, y in polygon_points:\n                    gx, gy = transform * (x, y)\n                    geo_points.append((gx, gy))\n\n                # Create Shapely polygon\n                if len(geo_points) &gt;= 3:\n                    try:\n                        shapely_poly = Polygon(geo_points)\n                        if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                            all_polygons.append(shapely_poly)\n\n                            # Calculate \"confidence\" as normalized size\n                            # This is a proxy since we don't have model confidence scores\n                            normalized_size = min(1.0, area / 1000)  # Cap at 1.0\n                            all_confidences.append(normalized_size)\n                    except Exception as e:\n                        print(f\"Error creating polygon: {e}\")\n\n        print(f\"Created {len(all_polygons)} valid polygons\")\n\n        # Create GeoDataFrame\n        if not all_polygons:\n            print(\"No valid polygons found\")\n            return None\n\n        gdf = gpd.GeoDataFrame(\n            {\n                \"geometry\": all_polygons,\n                \"confidence\": all_confidences,\n                \"class\": 1,  # Building class\n            },\n            crs=crs,\n        )\n\n        def filter_overlapping_polygons(gdf, **kwargs):\n            \"\"\"\n            Filter overlapping polygons using non-maximum suppression.\n\n            Args:\n                gdf: GeoDataFrame with polygons\n                **kwargs: Optional parameters:\n                    nms_iou_threshold: IoU threshold for filtering\n\n            Returns:\n                Filtered GeoDataFrame\n            \"\"\"\n            if len(gdf) &lt;= 1:\n                return gdf\n\n            # Get parameters from kwargs or use instance defaults\n            iou_threshold = kwargs.get(\"nms_iou_threshold\", nms_iou_threshold)\n\n            # Sort by confidence\n            gdf = gdf.sort_values(\"confidence\", ascending=False)\n\n            # Fix any invalid geometries\n            gdf[\"geometry\"] = gdf[\"geometry\"].apply(\n                lambda geom: geom.buffer(0) if not geom.is_valid else geom\n            )\n\n            keep_indices = []\n            polygons = gdf.geometry.values\n\n            for i in range(len(polygons)):\n                if i in keep_indices:\n                    continue\n\n                keep = True\n                for j in keep_indices:\n                    # Skip invalid geometries\n                    if not polygons[i].is_valid or not polygons[j].is_valid:\n                        continue\n\n                    # Calculate IoU\n                    try:\n                        intersection = polygons[i].intersection(polygons[j]).area\n                        union = polygons[i].area + polygons[j].area - intersection\n                        iou = intersection / union if union &gt; 0 else 0\n\n                        if iou &gt; iou_threshold:\n                            keep = False\n                            break\n                    except Exception:\n                        # Skip on topology exceptions\n                        continue\n\n                if keep:\n                    keep_indices.append(i)\n\n            return gdf.iloc[keep_indices]\n\n        # Apply non-maximum suppression to remove overlapping polygons\n        gdf = filter_overlapping_polygons(gdf, nms_iou_threshold=nms_iou_threshold)\n\n        print(f\"Final building count after filtering: {len(gdf)}\")\n\n        # Save to file\n        if output_path is not None:\n            gdf.to_file(output_path)\n            print(f\"Saved {len(gdf)} building footprints to {output_path}\")\n\n        return gdf\n</code></pre>"},{"location":"utils/#geoai.utils.mosaic_geotiffs","title":"<code>mosaic_geotiffs(input_dir, output_file, mask_file=None)</code>","text":"<p>Create a mosaic from all GeoTIFF files as a Cloud Optimized GeoTIFF (COG).</p> <p>This function identifies all GeoTIFF files in the specified directory, creates a seamless mosaic with proper handling of nodata values, and saves as a Cloud Optimized GeoTIFF format. If a mask file is provided, the output will be clipped to the extent of the mask.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>Path to the directory containing GeoTIFF files.</p> required <code>output_file</code> <code>str</code> <p>Path to the output Cloud Optimized GeoTIFF file.</p> required <code>mask_file</code> <code>str</code> <p>Path to a mask file to clip the output. If provided, the output will be clipped to the extent of this mask. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the mosaic was created successfully, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mosaic_geotiffs('naip', 'merged_naip.tif')\nTrue\n&gt;&gt;&gt; mosaic_geotiffs('naip', 'merged_naip.tif', 'boundary.tif')\nTrue\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def mosaic_geotiffs(input_dir, output_file, mask_file=None):\n    \"\"\"Create a mosaic from all GeoTIFF files as a Cloud Optimized GeoTIFF (COG).\n\n    This function identifies all GeoTIFF files in the specified directory,\n    creates a seamless mosaic with proper handling of nodata values, and saves\n    as a Cloud Optimized GeoTIFF format. If a mask file is provided, the output\n    will be clipped to the extent of the mask.\n\n    Args:\n        input_dir (str): Path to the directory containing GeoTIFF files.\n        output_file (str): Path to the output Cloud Optimized GeoTIFF file.\n        mask_file (str, optional): Path to a mask file to clip the output.\n            If provided, the output will be clipped to the extent of this mask.\n            Defaults to None.\n\n    Returns:\n        bool: True if the mosaic was created successfully, False otherwise.\n\n    Examples:\n        &gt;&gt;&gt; mosaic_geotiffs('naip', 'merged_naip.tif')\n        True\n        &gt;&gt;&gt; mosaic_geotiffs('naip', 'merged_naip.tif', 'boundary.tif')\n        True\n    \"\"\"\n    import glob\n    from osgeo import gdal\n\n    gdal.UseExceptions()\n    # Get all tif files in the directory\n    tif_files = glob.glob(os.path.join(input_dir, \"*.tif\"))\n\n    if not tif_files:\n        print(\"No GeoTIFF files found in the specified directory.\")\n        return False\n\n    # Analyze the first input file to determine compression and nodata settings\n    ds = gdal.Open(tif_files[0])\n    if ds is None:\n        print(f\"Unable to open {tif_files[0]}\")\n        return False\n\n    # Get driver metadata from the first file\n    driver = ds.GetDriver()\n    creation_options = []\n\n    # Check compression type\n    metadata = ds.GetMetadata(\"IMAGE_STRUCTURE\")\n    if \"COMPRESSION\" in metadata:\n        compression = metadata[\"COMPRESSION\"]\n        creation_options.append(f\"COMPRESS={compression}\")\n    else:\n        # Default compression if none detected\n        creation_options.append(\"COMPRESS=LZW\")\n\n    # Add COG-specific creation options\n    creation_options.extend([\"TILED=YES\", \"BLOCKXSIZE=512\", \"BLOCKYSIZE=512\"])\n\n    # Check for nodata value in the first band of the first file\n    band = ds.GetRasterBand(1)\n    has_nodata = band.GetNoDataValue() is not None\n    nodata_value = band.GetNoDataValue() if has_nodata else None\n\n    # Close the dataset\n    ds = None\n\n    # Create a temporary VRT (Virtual Dataset)\n    vrt_path = os.path.join(input_dir, \"temp_mosaic.vrt\")\n\n    # Build VRT from input files with proper nodata handling\n    vrt_options = gdal.BuildVRTOptions(\n        resampleAlg=\"nearest\",\n        srcNodata=nodata_value if has_nodata else None,\n        VRTNodata=nodata_value if has_nodata else None,\n    )\n    vrt_dataset = gdal.BuildVRT(vrt_path, tif_files, options=vrt_options)\n\n    # Close the VRT dataset to flush it to disk\n    vrt_dataset = None\n\n    # Create temp mosaic\n    temp_mosaic = output_file + \".temp.tif\"\n\n    # Convert VRT to GeoTIFF with the same compression as input\n    translate_options = gdal.TranslateOptions(\n        format=\"GTiff\",\n        creationOptions=creation_options,\n        noData=nodata_value if has_nodata else None,\n    )\n    gdal.Translate(temp_mosaic, vrt_path, options=translate_options)\n\n    # Apply mask if provided\n    if mask_file and os.path.exists(mask_file):\n        print(f\"Clipping mosaic to mask: {mask_file}\")\n\n        # Create a temporary clipped file\n        clipped_mosaic = output_file + \".clipped.tif\"\n\n        # Open mask file\n        mask_ds = gdal.Open(mask_file)\n        if mask_ds is None:\n            print(f\"Unable to open mask file: {mask_file}\")\n            # Continue without clipping\n        else:\n            # Get mask extent\n            mask_geotransform = mask_ds.GetGeoTransform()\n            mask_projection = mask_ds.GetProjection()\n            mask_ulx = mask_geotransform[0]\n            mask_uly = mask_geotransform[3]\n            mask_lrx = mask_ulx + (mask_geotransform[1] * mask_ds.RasterXSize)\n            mask_lry = mask_uly + (mask_geotransform[5] * mask_ds.RasterYSize)\n\n            # Close mask dataset\n            mask_ds = None\n\n            # Use warp options to clip\n            warp_options = gdal.WarpOptions(\n                format=\"GTiff\",\n                outputBounds=[mask_ulx, mask_lry, mask_lrx, mask_uly],\n                dstSRS=mask_projection,\n                creationOptions=creation_options,\n                srcNodata=nodata_value if has_nodata else None,\n                dstNodata=nodata_value if has_nodata else None,\n            )\n\n            # Apply clipping\n            gdal.Warp(clipped_mosaic, temp_mosaic, options=warp_options)\n\n            # Remove the unclipped temp mosaic and use the clipped one\n            os.remove(temp_mosaic)\n            temp_mosaic = clipped_mosaic\n\n    # Create internal overviews for the temp mosaic\n    ds = gdal.Open(temp_mosaic, gdal.GA_Update)\n    overview_list = [2, 4, 8, 16, 32]\n    ds.BuildOverviews(\"NEAREST\", overview_list)\n    ds = None  # Close the dataset to ensure overviews are written\n\n    # Convert the temp mosaic to a proper COG\n    cog_options = gdal.TranslateOptions(\n        format=\"GTiff\",\n        creationOptions=[\n            \"TILED=YES\",\n            \"COPY_SRC_OVERVIEWS=YES\",\n            \"COMPRESS=DEFLATE\",\n            \"PREDICTOR=2\",\n            \"BLOCKXSIZE=512\",\n            \"BLOCKYSIZE=512\",\n        ],\n        noData=nodata_value if has_nodata else None,\n    )\n    gdal.Translate(output_file, temp_mosaic, options=cog_options)\n\n    # Clean up temporary files\n    if os.path.exists(vrt_path):\n        os.remove(vrt_path)\n    if os.path.exists(temp_mosaic):\n        os.remove(temp_mosaic)\n\n    print(f\"Cloud Optimized GeoTIFF mosaic created successfully: {output_file}\")\n    return True\n</code></pre>"},{"location":"utils/#geoai.utils.orthogonalize","title":"<code>orthogonalize(input_path, output_path=None, epsilon=0.2, min_area=10, min_segments=4, area_tolerance=0.7, detect_triangles=True)</code>","text":"<p>Orthogonalizes object masks in a GeoTIFF file.</p> <p>This function reads a GeoTIFF containing object masks (binary or labeled regions), converts the raster masks to vector polygons, applies orthogonalization to each polygon, and optionally writes the result to a GeoJSON file. The source code is adapted from the Solar Panel Detection algorithm by Esri. See https://www.arcgis.com/home/item.html?id=c2508d72f2614104bfcfd5ccf1429284. Credits to Esri for the original code.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>Path to save the output GeoJSON file. If None, no file is saved.</p> <code>None</code> <code>epsilon</code> <code>float</code> <p>Simplification tolerance for the Douglas-Peucker algorithm. Higher values result in more simplification. Default is 0.2.</p> <code>0.2</code> <code>min_area</code> <code>float</code> <p>Minimum area of polygons to process (smaller ones are kept as-is).</p> <code>10</code> <code>min_segments</code> <code>int</code> <p>Minimum number of segments to keep after simplification. Default is 4 (for rectangular shapes).</p> <code>4</code> <code>area_tolerance</code> <code>float</code> <p>Allowed ratio of area change. Values less than 1.0 restrict area change. Default is 0.7 (allows reduction to 70% of original area).</p> <code>0.7</code> <code>detect_triangles</code> <code>bool</code> <p>If True, performs additional check to avoid creating triangular shapes.</p> <code>True</code> <p>Returns:</p> Type Description <code>geopandas.GeoDataFrame</code> <p>A GeoDataFrame containing the orthogonalized features.</p> Source code in <code>geoai/utils.py</code> <pre><code>def orthogonalize(\n    input_path,\n    output_path=None,\n    epsilon=0.2,\n    min_area=10,\n    min_segments=4,\n    area_tolerance=0.7,\n    detect_triangles=True,\n):\n    \"\"\"\n    Orthogonalizes object masks in a GeoTIFF file.\n\n    This function reads a GeoTIFF containing object masks (binary or labeled regions),\n    converts the raster masks to vector polygons, applies orthogonalization to each polygon,\n    and optionally writes the result to a GeoJSON file.\n    The source code is adapted from the Solar Panel Detection algorithm by Esri.\n    See https://www.arcgis.com/home/item.html?id=c2508d72f2614104bfcfd5ccf1429284.\n    Credits to Esri for the original code.\n\n    Args:\n        input_path (str): Path to the input GeoTIFF file.\n        output_path (str, optional): Path to save the output GeoJSON file. If None, no file is saved.\n        epsilon (float, optional): Simplification tolerance for the Douglas-Peucker algorithm.\n            Higher values result in more simplification. Default is 0.2.\n        min_area (float, optional): Minimum area of polygons to process (smaller ones are kept as-is).\n        min_segments (int, optional): Minimum number of segments to keep after simplification.\n            Default is 4 (for rectangular shapes).\n        area_tolerance (float, optional): Allowed ratio of area change. Values less than 1.0 restrict\n            area change. Default is 0.7 (allows reduction to 70% of original area).\n        detect_triangles (bool, optional): If True, performs additional check to avoid creating triangular shapes.\n\n    Returns:\n        geopandas.GeoDataFrame: A GeoDataFrame containing the orthogonalized features.\n    \"\"\"\n\n    from functools import partial\n\n    def orthogonalize_ring(ring, epsilon=0.2, min_segments=4):\n        \"\"\"\n        Orthogonalizes a ring (list of coordinates).\n\n        Args:\n            ring (list): List of [x, y] coordinates forming a ring\n            epsilon (float, optional): Simplification tolerance\n            min_segments (int, optional): Minimum number of segments to keep\n\n        Returns:\n            list: Orthogonalized list of coordinates\n        \"\"\"\n        if len(ring) &lt;= 3:\n            return ring\n\n        # Convert to numpy array\n        ring_arr = np.array(ring)\n\n        # Get orientation\n        angle = math.degrees(get_orientation(ring_arr))\n\n        # Simplify using Ramer-Douglas-Peucker algorithm\n        ring_arr = simplify(ring_arr, eps=epsilon)\n\n        # If simplified too much, adjust epsilon to maintain minimum segments\n        if len(ring_arr) &lt; min_segments:\n            # Try with smaller epsilon until we get at least min_segments points\n            for adjust_factor in [0.75, 0.5, 0.25, 0.1]:\n                test_arr = simplify(np.array(ring), eps=epsilon * adjust_factor)\n                if len(test_arr) &gt;= min_segments:\n                    ring_arr = test_arr\n                    break\n\n        # Convert to dataframe for processing\n        df = to_dataframe(ring_arr)\n\n        # Add orientation information\n        add_orientation(df, angle)\n\n        # Align segments to orthogonal directions\n        df = align(df)\n\n        # Merge collinear line segments\n        df = merge_lines(df)\n\n        if len(df) == 0:\n            return ring\n\n        # If we have a triangle-like result (3 segments), return the original shape\n        if len(df) &lt;= 3:\n            return ring\n\n        # Join the orthogonalized segments back into a ring\n        joined_ring = join_ring(df)\n\n        # If the join operation didn't produce a valid ring, return the original\n        if len(joined_ring) == 0 or len(joined_ring[0]) &lt; 3:\n            return ring\n\n        # Basic validation: if result has 3 or fewer points (triangle), use original\n        if len(joined_ring[0]) &lt;= 3:\n            return ring\n\n        # Convert back to a list and ensure it's closed\n        result = joined_ring[0].tolist()\n        if len(result) &gt; 0 and (result[0] != result[-1]):\n            result.append(result[0])\n\n        return result\n\n    def vectorize_mask(mask, transform):\n        \"\"\"\n        Converts a binary mask to vector polygons.\n\n        Args:\n            mask (numpy.ndarray): Binary mask where non-zero values represent objects\n            transform (rasterio.transform.Affine): Affine transformation matrix\n\n        Returns:\n            list: List of GeoJSON features\n        \"\"\"\n        shapes = features.shapes(mask, transform=transform)\n        features_list = []\n\n        for shape, value in shapes:\n            if value &gt; 0:  # Only process non-zero values (actual objects)\n                features_list.append(\n                    {\n                        \"type\": \"Feature\",\n                        \"properties\": {\"value\": int(value)},\n                        \"geometry\": shape,\n                    }\n                )\n\n        return features_list\n\n    def rasterize_features(features, shape, transform, dtype=np.uint8):\n        \"\"\"\n        Converts vector features back to a raster mask.\n\n        Args:\n            features (list): List of GeoJSON features\n            shape (tuple): Shape of the output raster (height, width)\n            transform (rasterio.transform.Affine): Affine transformation matrix\n            dtype (numpy.dtype, optional): Data type of the output raster\n\n        Returns:\n            numpy.ndarray: Rasterized mask\n        \"\"\"\n        mask = features.rasterize(\n            [\n                (feature[\"geometry\"], feature[\"properties\"][\"value\"])\n                for feature in features\n            ],\n            out_shape=shape,\n            transform=transform,\n            fill=0,\n            dtype=dtype,\n        )\n\n        return mask\n\n    # The following helper functions are from the original code\n    def get_orientation(contour):\n        \"\"\"\n        Calculate the orientation angle of a contour.\n\n        Args:\n            contour (numpy.ndarray): Array of shape (n, 2) containing point coordinates\n\n        Returns:\n            float: Orientation angle in radians\n        \"\"\"\n        box = cv2.minAreaRect(contour.astype(int))\n        (cx, cy), (w, h), angle = box\n        return math.radians(angle)\n\n    def simplify(contour, eps=0.2):\n        \"\"\"\n        Simplify a contour using the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            contour (numpy.ndarray): Array of shape (n, 2) containing point coordinates\n            eps (float, optional): Epsilon value for simplification\n\n        Returns:\n            numpy.ndarray: Simplified contour\n        \"\"\"\n        return rdp(contour, epsilon=eps)\n\n    def to_dataframe(ring):\n        \"\"\"\n        Convert a ring to a pandas DataFrame with line segment information.\n\n        Args:\n            ring (numpy.ndarray): Array of shape (n, 2) containing point coordinates\n\n        Returns:\n            pandas.DataFrame: DataFrame with line segment information\n        \"\"\"\n        df = pd.DataFrame(ring, columns=[\"x1\", \"y1\"])\n        df[\"x2\"] = df[\"x1\"].shift(-1)\n        df[\"y2\"] = df[\"y1\"].shift(-1)\n        df.dropna(inplace=True)\n        df[\"angle_atan\"] = np.arctan2((df[\"y2\"] - df[\"y1\"]), (df[\"x2\"] - df[\"x1\"]))\n        df[\"angle_atan_deg\"] = df[\"angle_atan\"] * 57.2958\n        df[\"len\"] = np.sqrt((df[\"y2\"] - df[\"y1\"]) ** 2 + (df[\"x2\"] - df[\"x1\"]) ** 2)\n        df[\"cx\"] = (df[\"x2\"] + df[\"x1\"]) / 2.0\n        df[\"cy\"] = (df[\"y2\"] + df[\"y1\"]) / 2.0\n        return df\n\n    def add_orientation(df, angle):\n        \"\"\"\n        Add orientation information to the DataFrame.\n\n        Args:\n            df (pandas.DataFrame): DataFrame with line segment information\n            angle (float): Orientation angle in degrees\n\n        Returns:\n            None: Modifies the DataFrame in-place\n        \"\"\"\n        rtangle = angle + 90\n        is_parallel = (\n            (df[\"angle_atan_deg\"] &gt; (angle - 45))\n            &amp; (df[\"angle_atan_deg\"] &lt; (angle + 45))\n        ) | (\n            (df[\"angle_atan_deg\"] + 180 &gt; (angle - 45))\n            &amp; (df[\"angle_atan_deg\"] + 180 &lt; (angle + 45))\n        )\n        df[\"angle\"] = math.radians(angle)\n        df[\"angle\"] = df[\"angle\"].where(is_parallel, math.radians(rtangle))\n\n    def align(df):\n        \"\"\"\n        Align line segments to their nearest orthogonal direction.\n\n        Args:\n            df (pandas.DataFrame): DataFrame with line segment information\n\n        Returns:\n            pandas.DataFrame: DataFrame with aligned line segments\n        \"\"\"\n        # Handle edge case with empty dataframe\n        if len(df) == 0:\n            return df.copy()\n\n        df_clone = df.copy()\n\n        # Ensure angle column exists and has valid values\n        if \"angle\" not in df_clone.columns or df_clone[\"angle\"].isna().any():\n            # If angle data is missing, add default angles based on atan2\n            df_clone[\"angle\"] = df_clone[\"angle_atan\"]\n\n        # Ensure length and center point data is valid\n        if \"len\" not in df_clone.columns or df_clone[\"len\"].isna().any():\n            # Recalculate lengths if missing\n            df_clone[\"len\"] = np.sqrt(\n                (df_clone[\"x2\"] - df_clone[\"x1\"]) ** 2\n                + (df_clone[\"y2\"] - df_clone[\"y1\"]) ** 2\n            )\n\n        if \"cx\" not in df_clone.columns or df_clone[\"cx\"].isna().any():\n            df_clone[\"cx\"] = (df_clone[\"x1\"] + df_clone[\"x2\"]) / 2.0\n\n        if \"cy\" not in df_clone.columns or df_clone[\"cy\"].isna().any():\n            df_clone[\"cy\"] = (df_clone[\"y1\"] + df_clone[\"y2\"]) / 2.0\n\n        # Apply orthogonal alignment\n        df_clone[\"x1\"] = df_clone[\"cx\"] - ((df_clone[\"len\"] / 2) * np.cos(df[\"angle\"]))\n        df_clone[\"x2\"] = df_clone[\"cx\"] + ((df_clone[\"len\"] / 2) * np.cos(df[\"angle\"]))\n        df_clone[\"y1\"] = df_clone[\"cy\"] - ((df_clone[\"len\"] / 2) * np.sin(df[\"angle\"]))\n        df_clone[\"y2\"] = df_clone[\"cy\"] + ((df_clone[\"len\"] / 2) * np.sin(df[\"angle\"]))\n\n        return df_clone\n\n    def merge_lines(df_aligned):\n        \"\"\"\n        Merge collinear line segments.\n\n        Args:\n            df_aligned (pandas.DataFrame): DataFrame with aligned line segments\n\n        Returns:\n            pandas.DataFrame: DataFrame with merged line segments\n        \"\"\"\n        ortho_lines = []\n        groups = df_aligned.groupby(\n            (df_aligned[\"angle\"].shift() != df_aligned[\"angle\"]).cumsum()\n        )\n        for x, y in groups:\n            group_cx = (y[\"cx\"] * y[\"len\"]).sum() / y[\"len\"].sum()\n            group_cy = (y[\"cy\"] * y[\"len\"]).sum() / y[\"len\"].sum()\n            cumlen = y[\"len\"].sum()\n\n            ortho_lines.append((group_cx, group_cy, cumlen, y[\"angle\"].iloc[0]))\n\n        ortho_list = []\n        for cx, cy, length, rot_angle in ortho_lines:\n            X1 = cx - (length / 2) * math.cos(rot_angle)\n            X2 = cx + (length / 2) * math.cos(rot_angle)\n            Y1 = cy - (length / 2) * math.sin(rot_angle)\n            Y2 = cy + (length / 2) * math.sin(rot_angle)\n\n            ortho_list.append(\n                {\n                    \"x1\": X1,\n                    \"y1\": Y1,\n                    \"x2\": X2,\n                    \"y2\": Y2,\n                    \"len\": length,\n                    \"cx\": cx,\n                    \"cy\": cy,\n                    \"angle\": rot_angle,\n                }\n            )\n\n        if (\n            len(ortho_list) &gt; 0 and ortho_list[0][\"angle\"] == ortho_list[-1][\"angle\"]\n        ):  # join first and last segment if they're in same direction\n            totlen = ortho_list[0][\"len\"] + ortho_list[-1][\"len\"]\n            merge_cx = (\n                (ortho_list[0][\"cx\"] * ortho_list[0][\"len\"])\n                + (ortho_list[-1][\"cx\"] * ortho_list[-1][\"len\"])\n            ) / totlen\n\n            merge_cy = (\n                (ortho_list[0][\"cy\"] * ortho_list[0][\"len\"])\n                + (ortho_list[-1][\"cy\"] * ortho_list[-1][\"len\"])\n            ) / totlen\n\n            rot_angle = ortho_list[0][\"angle\"]\n            X1 = merge_cx - (totlen / 2) * math.cos(rot_angle)\n            X2 = merge_cx + (totlen / 2) * math.cos(rot_angle)\n            Y1 = merge_cy - (totlen / 2) * math.sin(rot_angle)\n            Y2 = merge_cy + (totlen / 2) * math.sin(rot_angle)\n\n            ortho_list[-1] = {\n                \"x1\": X1,\n                \"y1\": Y1,\n                \"x2\": X2,\n                \"y2\": Y2,\n                \"len\": totlen,\n                \"cx\": merge_cx,\n                \"cy\": merge_cy,\n                \"angle\": rot_angle,\n            }\n            ortho_list = ortho_list[1:]\n        ortho_df = pd.DataFrame(ortho_list)\n        return ortho_df\n\n    def find_intersection(x1, y1, x2, y2, x3, y3, x4, y4):\n        \"\"\"\n        Find the intersection point of two line segments.\n\n        Args:\n            x1, y1, x2, y2: Coordinates of the first line segment\n            x3, y3, x4, y4: Coordinates of the second line segment\n\n        Returns:\n            list: [x, y] coordinates of the intersection point\n\n        Raises:\n            ZeroDivisionError: If the lines are parallel or collinear\n        \"\"\"\n        # Calculate the denominator of the intersection formula\n        denominator = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)\n\n        # Check if lines are parallel or collinear (denominator close to zero)\n        if abs(denominator) &lt; 1e-10:\n            raise ZeroDivisionError(\"Lines are parallel or collinear\")\n\n        px = (\n            (x1 * y2 - y1 * x2) * (x3 - x4) - (x1 - x2) * (x3 * y4 - y3 * x4)\n        ) / denominator\n        py = (\n            (x1 * y2 - y1 * x2) * (y3 - y4) - (y1 - y2) * (x3 * y4 - y3 * x4)\n        ) / denominator\n\n        # Check if the intersection point is within a reasonable distance\n        # from both line segments to avoid extreme extrapolation\n        def point_on_segment(x, y, x1, y1, x2, y2, tolerance=2.0):\n            # Check if point (x,y) is near the line segment from (x1,y1) to (x2,y2)\n            # First check if it's near the infinite line\n            line_len = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n            if line_len &lt; 1e-10:\n                return np.sqrt((x - x1) ** 2 + (y - y1) ** 2) &lt;= tolerance\n\n            t = ((x - x1) * (x2 - x1) + (y - y1) * (y2 - y1)) / (line_len**2)\n\n            # Check distance to the infinite line\n            proj_x = x1 + t * (x2 - x1)\n            proj_y = y1 + t * (y2 - y1)\n            dist_to_line = np.sqrt((x - proj_x) ** 2 + (y - proj_y) ** 2)\n\n            # Check if the projection is near the segment, not just the infinite line\n            if t &lt; -tolerance or t &gt; 1 + tolerance:\n                # If far from the segment, compute distance to the nearest endpoint\n                dist_to_start = np.sqrt((x - x1) ** 2 + (y - y1) ** 2)\n                dist_to_end = np.sqrt((x - x2) ** 2 + (y - y2) ** 2)\n                return min(dist_to_start, dist_to_end) &lt;= tolerance * 2\n\n            return dist_to_line &lt;= tolerance\n\n        # Check if intersection is reasonably close to both line segments\n        if not (\n            point_on_segment(px, py, x1, y1, x2, y2)\n            and point_on_segment(px, py, x3, y3, x4, y4)\n        ):\n            # If intersection is far from segments, it's probably extrapolating too much\n            raise ValueError(\"Intersection point too far from line segments\")\n\n        return [px, py]\n\n    def join_ring(merged_df):\n        \"\"\"\n        Join line segments to form a closed ring.\n\n        Args:\n            merged_df (pandas.DataFrame): DataFrame with merged line segments\n\n        Returns:\n            numpy.ndarray: Array of shape (1, n, 2) containing the ring coordinates\n        \"\"\"\n        # Handle edge cases\n        if len(merged_df) &lt; 3:\n            # Not enough segments to form a valid polygon\n            return np.array([[]])\n\n        ring = []\n\n        # Find intersections between adjacent line segments\n        for i in range(len(merged_df) - 1):\n            x1, y1, x2, y2, *_ = merged_df.iloc[i]\n            x3, y3, x4, y4, *_ = merged_df.iloc[i + 1]\n\n            try:\n                intersection = find_intersection(x1, y1, x2, y2, x3, y3, x4, y4)\n\n                # Check if the intersection point is too far from either line segment\n                # This helps prevent extending edges beyond reasonable bounds\n                dist_to_seg1 = min(\n                    np.sqrt((intersection[0] - x1) ** 2 + (intersection[1] - y1) ** 2),\n                    np.sqrt((intersection[0] - x2) ** 2 + (intersection[1] - y2) ** 2),\n                )\n                dist_to_seg2 = min(\n                    np.sqrt((intersection[0] - x3) ** 2 + (intersection[1] - y3) ** 2),\n                    np.sqrt((intersection[0] - x4) ** 2 + (intersection[1] - y4) ** 2),\n                )\n\n                # Use the maximum of line segment lengths as a reference\n                max_len = max(\n                    np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2),\n                    np.sqrt((x4 - x3) ** 2 + (y4 - y3) ** 2),\n                )\n\n                # If intersection is too far away, use the endpoint of the first segment instead\n                if dist_to_seg1 &gt; max_len * 0.5 or dist_to_seg2 &gt; max_len * 0.5:\n                    ring.append([x2, y2])\n                else:\n                    ring.append(intersection)\n            except Exception as e:\n                # If intersection calculation fails, use the endpoint of the first segment\n                ring.append([x2, y2])\n\n        # Connect last segment with first segment\n        x1, y1, x2, y2, *_ = merged_df.iloc[-1]\n        x3, y3, x4, y4, *_ = merged_df.iloc[0]\n\n        try:\n            intersection = find_intersection(x1, y1, x2, y2, x3, y3, x4, y4)\n\n            # Check if the intersection point is too far from either line segment\n            dist_to_seg1 = min(\n                np.sqrt((intersection[0] - x1) ** 2 + (intersection[1] - y1) ** 2),\n                np.sqrt((intersection[0] - x2) ** 2 + (intersection[1] - y2) ** 2),\n            )\n            dist_to_seg2 = min(\n                np.sqrt((intersection[0] - x3) ** 2 + (intersection[1] - y3) ** 2),\n                np.sqrt((intersection[0] - x4) ** 2 + (intersection[1] - y4) ** 2),\n            )\n\n            max_len = max(\n                np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2),\n                np.sqrt((x4 - x3) ** 2 + (y4 - y3) ** 2),\n            )\n\n            if dist_to_seg1 &gt; max_len * 0.5 or dist_to_seg2 &gt; max_len * 0.5:\n                ring.append([x2, y2])\n            else:\n                ring.append(intersection)\n        except Exception as e:\n            # If intersection calculation fails, use the endpoint of the last segment\n            ring.append([x2, y2])\n\n        # Ensure the ring is closed\n        if len(ring) &gt; 0 and (ring[0][0] != ring[-1][0] or ring[0][1] != ring[-1][1]):\n            ring.append(ring[0])\n\n        return np.array([ring])\n\n    def rdp(M, epsilon=0, dist=None, algo=\"iter\", return_mask=False):\n        \"\"\"\n        Simplifies a given array of points using the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            M (numpy.ndarray): Array of shape (n, d) containing point coordinates\n            epsilon (float, optional): Epsilon value for simplification\n            dist (callable, optional): Distance function\n            algo (str, optional): Algorithm to use ('iter' or 'rec')\n            return_mask (bool, optional): Whether to return a mask instead of the simplified array\n\n        Returns:\n            numpy.ndarray or list: Simplified points or mask\n        \"\"\"\n        if dist is None:\n            dist = pldist\n\n        if algo == \"iter\":\n            algo = partial(rdp_iter, return_mask=return_mask)\n        elif algo == \"rec\":\n            if return_mask:\n                raise NotImplementedError(\n                    'return_mask=True not supported with algo=\"rec\"'\n                )\n            algo = rdp_rec\n\n        if \"numpy\" in str(type(M)):\n            return algo(M, epsilon, dist)\n\n        return algo(np.array(M), epsilon, dist).tolist()\n\n    def pldist(point, start, end):\n        \"\"\"\n        Calculates the distance from 'point' to the line given by 'start' and 'end'.\n\n        Args:\n            point (numpy.ndarray): Point coordinates\n            start (numpy.ndarray): Start point of the line\n            end (numpy.ndarray): End point of the line\n\n        Returns:\n            float: Distance from point to line\n        \"\"\"\n        if np.all(np.equal(start, end)):\n            return np.linalg.norm(point - start)\n\n        # Fix for NumPy 2.0 deprecation warning - handle 2D vectors properly\n        # Instead of using cross product directly, calculate the area of the\n        # parallelogram formed by the vectors and divide by the length of the line\n        line_vec = end - start\n        point_vec = point - start\n\n        # Area of parallelogram = |a|*|b|*sin(\u03b8)\n        # For 2D vectors: |a\u00d7b| = |a|*|b|*sin(\u03b8) = determinant([ax, ay], [bx, by])\n        area = abs(line_vec[0] * point_vec[1] - line_vec[1] * point_vec[0])\n\n        # Distance = Area / |line_vec|\n        return area / np.linalg.norm(line_vec)\n\n    def rdp_rec(M, epsilon, dist=pldist):\n        \"\"\"\n        Recursive implementation of the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            M (numpy.ndarray): Array of shape (n, d) containing point coordinates\n            epsilon (float): Epsilon value for simplification\n            dist (callable, optional): Distance function\n\n        Returns:\n            numpy.ndarray: Simplified points\n        \"\"\"\n        dmax = 0.0\n        index = -1\n\n        for i in range(1, M.shape[0]):\n            d = dist(M[i], M[0], M[-1])\n\n            if d &gt; dmax:\n                index = i\n                dmax = d\n\n        if dmax &gt; epsilon:\n            r1 = rdp_rec(M[: index + 1], epsilon, dist)\n            r2 = rdp_rec(M[index:], epsilon, dist)\n\n            return np.vstack((r1[:-1], r2))\n        else:\n            return np.vstack((M[0], M[-1]))\n\n    def _rdp_iter(M, start_index, last_index, epsilon, dist=pldist):\n        \"\"\"\n        Internal iterative implementation of the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            M (numpy.ndarray): Array of shape (n, d) containing point coordinates\n            start_index (int): Start index\n            last_index (int): Last index\n            epsilon (float): Epsilon value for simplification\n            dist (callable, optional): Distance function\n\n        Returns:\n            numpy.ndarray: Boolean mask of points to keep\n        \"\"\"\n        stk = []\n        stk.append([start_index, last_index])\n        global_start_index = start_index\n        indices = np.ones(last_index - start_index + 1, dtype=bool)\n\n        while stk:\n            start_index, last_index = stk.pop()\n\n            dmax = 0.0\n            index = start_index\n\n            for i in range(index + 1, last_index):\n                if indices[i - global_start_index]:\n                    d = dist(M[i], M[start_index], M[last_index])\n                    if d &gt; dmax:\n                        index = i\n                        dmax = d\n\n            if dmax &gt; epsilon:\n                stk.append([start_index, index])\n                stk.append([index, last_index])\n            else:\n                for i in range(start_index + 1, last_index):\n                    indices[i - global_start_index] = False\n\n        return indices\n\n    def rdp_iter(M, epsilon, dist=pldist, return_mask=False):\n        \"\"\"\n        Iterative implementation of the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            M (numpy.ndarray): Array of shape (n, d) containing point coordinates\n            epsilon (float): Epsilon value for simplification\n            dist (callable, optional): Distance function\n            return_mask (bool, optional): Whether to return a mask instead of the simplified array\n\n        Returns:\n            numpy.ndarray: Simplified points or boolean mask\n        \"\"\"\n        mask = _rdp_iter(M, 0, len(M) - 1, epsilon, dist)\n\n        if return_mask:\n            return mask\n\n        return M[mask]\n\n    # Read the raster data\n    with rasterio.open(input_path) as src:\n        # Read the first band (assuming it contains the mask)\n        mask = src.read(1)\n        transform = src.transform\n        crs = src.crs\n\n        # Extract shapes from the raster mask\n        shapes = list(features.shapes(mask, transform=transform))\n\n        # Initialize progress bar\n        print(f\"Processing {len(shapes)} features...\")\n\n        # Convert shapes to GeoJSON features\n        features_list = []\n        for shape, value in tqdm(shapes, desc=\"Converting features\", unit=\"shape\"):\n            if value &gt; 0:  # Only process non-zero values (actual objects)\n                # Convert GeoJSON geometry to Shapely polygon\n                polygon = Polygon(shape[\"coordinates\"][0])\n\n                # Skip tiny polygons\n                if polygon.area &lt; min_area:\n                    features_list.append(\n                        {\n                            \"type\": \"Feature\",\n                            \"properties\": {\"value\": int(value)},\n                            \"geometry\": shape,\n                        }\n                    )\n                    continue\n\n                # Check if shape is triangular and if we want to avoid triangular shapes\n                if detect_triangles:\n                    # Create a simplified version to check number of vertices\n                    simple_polygon = polygon.simplify(epsilon)\n                    if (\n                        len(simple_polygon.exterior.coords) &lt;= 4\n                    ):  # 3 points + closing point\n                        # Likely a triangular shape - skip orthogonalization\n                        features_list.append(\n                            {\n                                \"type\": \"Feature\",\n                                \"properties\": {\"value\": int(value)},\n                                \"geometry\": shape,\n                            }\n                        )\n                        continue\n\n                # Process larger, non-triangular polygons\n                try:\n                    # Convert shapely polygon to a ring format for orthogonalization\n                    exterior_ring = list(polygon.exterior.coords)\n                    interior_rings = [\n                        list(interior.coords) for interior in polygon.interiors\n                    ]\n\n                    # Calculate bounding box aspect ratio to help with parameter tuning\n                    minx, miny, maxx, maxy = polygon.bounds\n                    width = maxx - minx\n                    height = maxy - miny\n                    aspect_ratio = max(width, height) / max(1.0, min(width, height))\n\n                    # Determine if this shape is likely to be a building/rectangular object\n                    # Long thin objects might require different treatment\n                    is_rectangular = aspect_ratio &lt; 3.0\n\n                    # Rectangular objects usually need more careful orthogonalization\n                    epsilon_adjusted = epsilon\n                    min_segments_adjusted = min_segments\n\n                    if is_rectangular:\n                        # For rectangular objects, use more conservative epsilon\n                        epsilon_adjusted = epsilon * 0.75\n                        # Ensure we get at least 4 points for a proper rectangle\n                        min_segments_adjusted = max(4, min_segments)\n\n                    # Orthogonalize the exterior and interior rings\n                    orthogonalized_exterior = orthogonalize_ring(\n                        exterior_ring,\n                        epsilon=epsilon_adjusted,\n                        min_segments=min_segments_adjusted,\n                    )\n\n                    orthogonalized_interiors = [\n                        orthogonalize_ring(\n                            ring,\n                            epsilon=epsilon_adjusted,\n                            min_segments=min_segments_adjusted,\n                        )\n                        for ring in interior_rings\n                    ]\n\n                    # Validate the result - calculate area change\n                    original_area = polygon.area\n                    orthogonalized_poly = Polygon(orthogonalized_exterior)\n\n                    if orthogonalized_poly.is_valid:\n                        area_ratio = (\n                            orthogonalized_poly.area / original_area\n                            if original_area &gt; 0\n                            else 0\n                        )\n\n                        # If area changed too much, revert to original\n                        if area_ratio &lt; area_tolerance or area_ratio &gt; (\n                            1.0 / area_tolerance\n                        ):\n                            # Use original polygon instead\n                            geometry = shape\n                        else:\n                            # Create a new geometry with orthogonalized rings\n                            geometry = {\n                                \"type\": \"Polygon\",\n                                \"coordinates\": [orthogonalized_exterior],\n                            }\n\n                            # Add interior rings if they exist\n                            if orthogonalized_interiors:\n                                geometry[\"coordinates\"].extend(\n                                    [ring for ring in orthogonalized_interiors]\n                                )\n                    else:\n                        # If resulting polygon is invalid, use original\n                        geometry = shape\n\n                    # Add the feature to the list\n                    features_list.append(\n                        {\n                            \"type\": \"Feature\",\n                            \"properties\": {\"value\": int(value)},\n                            \"geometry\": geometry,\n                        }\n                    )\n                except Exception as e:\n                    # Keep the original shape if orthogonalization fails\n                    features_list.append(\n                        {\n                            \"type\": \"Feature\",\n                            \"properties\": {\"value\": int(value)},\n                            \"geometry\": shape,\n                        }\n                    )\n\n        # Create the final GeoJSON structure\n        geojson = {\n            \"type\": \"FeatureCollection\",\n            \"crs\": {\"type\": \"name\", \"properties\": {\"name\": str(crs)}},\n            \"features\": features_list,\n        }\n\n        # Convert to GeoDataFrame and set the CRS\n        gdf = gpd.GeoDataFrame.from_features(geojson[\"features\"], crs=crs)\n\n        # Save to file if output_path is provided\n        if output_path:\n            print(f\"Saving to {output_path}...\")\n            gdf.to_file(output_path)\n            print(\"Done!\")\n\n        return gdf\n</code></pre>"},{"location":"utils/#geoai.utils.plot_batch","title":"<code>plot_batch(batch, bright=1.0, cols=4, width=5, chnls=[2, 1, 0], cmap='Blues')</code>","text":"<p>Plot a batch of images and masks. This function is adapted from the plot_batch() function in the torchgeo library at https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html Credit to the torchgeo developers for the original implementation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch containing images and masks.</p> required <code>bright</code> <code>float</code> <p>The brightness factor. Defaults to 1.0.</p> <code>1.0</code> <code>cols</code> <code>int</code> <p>The number of columns in the plot grid. Defaults to 4.</p> <code>4</code> <code>width</code> <code>int</code> <p>The width of each plot. Defaults to 5.</p> <code>5</code> <code>chnls</code> <code>List[int]</code> <p>The channels to use for RGB. Defaults to [2, 1, 0].</p> <code>[2, 1, 0]</code> <code>cmap</code> <code>str</code> <p>The colormap to use for masks. Defaults to \"Blues\".</p> <code>'Blues'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_batch(\n    batch: Dict[str, Any],\n    bright: float = 1.0,\n    cols: int = 4,\n    width: int = 5,\n    chnls: List[int] = [2, 1, 0],\n    cmap: str = \"Blues\",\n) -&gt; None:\n    \"\"\"\n    Plot a batch of images and masks. This function is adapted from the plot_batch()\n    function in the torchgeo library at\n    https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html\n    Credit to the torchgeo developers for the original implementation.\n\n    Args:\n        batch (Dict[str, Any]): The batch containing images and masks.\n        bright (float, optional): The brightness factor. Defaults to 1.0.\n        cols (int, optional): The number of columns in the plot grid. Defaults to 4.\n        width (int, optional): The width of each plot. Defaults to 5.\n        chnls (List[int], optional): The channels to use for RGB. Defaults to [2, 1, 0].\n        cmap (str, optional): The colormap to use for masks. Defaults to \"Blues\".\n\n    Returns:\n        None\n    \"\"\"\n\n    try:\n        from torchgeo.datasets import unbind_samples\n    except ImportError as e:\n        raise ImportError(\n            \"Your torchgeo version is too old. Please upgrade to the latest version using 'pip install -U torchgeo'.\"\n        )\n\n    # Get the samples and the number of items in the batch\n    samples = unbind_samples(batch.copy())\n\n    # if batch contains images and masks, the number of images will be doubled\n    n = 2 * len(samples) if (\"image\" in batch) and (\"mask\" in batch) else len(samples)\n\n    # calculate the number of rows in the grid\n    rows = n // cols + (1 if n % cols != 0 else 0)\n\n    # create a grid\n    _, axs = plt.subplots(rows, cols, figsize=(cols * width, rows * width))\n\n    if (\"image\" in batch) and (\"mask\" in batch):\n        # plot the images on the even axis\n        plot_images(\n            images=map(lambda x: x[\"image\"], samples),\n            axs=axs.reshape(-1)[::2],\n            chnls=chnls,\n            bright=bright,\n        )\n\n        # plot the masks on the odd axis\n        plot_masks(masks=map(lambda x: x[\"mask\"], samples), axs=axs.reshape(-1)[1::2])\n\n    else:\n        if \"image\" in batch:\n            plot_images(\n                images=map(lambda x: x[\"image\"], samples),\n                axs=axs.reshape(-1),\n                chnls=chnls,\n                bright=bright,\n            )\n\n        elif \"mask\" in batch:\n            plot_masks(\n                masks=map(lambda x: x[\"mask\"], samples), axs=axs.reshape(-1), cmap=cmap\n            )\n</code></pre>"},{"location":"utils/#geoai.utils.plot_images","title":"<code>plot_images(images, axs, chnls=[2, 1, 0], bright=1.0)</code>","text":"<p>Plot a list of images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Iterable[torch.Tensor]</code> <p>The images to plot.</p> required <code>axs</code> <code>Iterable[plt.Axes]</code> <p>The axes to plot the images on.</p> required <code>chnls</code> <code>List[int]</code> <p>The channels to use for RGB. Defaults to [2, 1, 0].</p> <code>[2, 1, 0]</code> <code>bright</code> <code>float</code> <p>The brightness factor. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_images(\n    images: Iterable[torch.Tensor],\n    axs: Iterable[plt.Axes],\n    chnls: List[int] = [2, 1, 0],\n    bright: float = 1.0,\n) -&gt; None:\n    \"\"\"\n    Plot a list of images.\n\n    Args:\n        images (Iterable[torch.Tensor]): The images to plot.\n        axs (Iterable[plt.Axes]): The axes to plot the images on.\n        chnls (List[int], optional): The channels to use for RGB. Defaults to [2, 1, 0].\n        bright (float, optional): The brightness factor. Defaults to 1.0.\n\n    Returns:\n        None\n    \"\"\"\n    for img, ax in zip(images, axs):\n        arr = torch.clamp(bright * img, min=0, max=1).numpy()\n        rgb = arr.transpose(1, 2, 0)[:, :, chnls]\n        ax.imshow(rgb)\n        ax.axis(\"off\")\n</code></pre>"},{"location":"utils/#geoai.utils.plot_masks","title":"<code>plot_masks(masks, axs, cmap='Blues')</code>","text":"<p>Plot a list of masks.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>Iterable[torch.Tensor]</code> <p>The masks to plot.</p> required <code>axs</code> <code>Iterable[plt.Axes]</code> <p>The axes to plot the masks on.</p> required <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \"Blues\".</p> <code>'Blues'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_masks(\n    masks: Iterable[torch.Tensor], axs: Iterable[plt.Axes], cmap: str = \"Blues\"\n) -&gt; None:\n    \"\"\"\n    Plot a list of masks.\n\n    Args:\n        masks (Iterable[torch.Tensor]): The masks to plot.\n        axs (Iterable[plt.Axes]): The axes to plot the masks on.\n        cmap (str, optional): The colormap to use. Defaults to \"Blues\".\n\n    Returns:\n        None\n    \"\"\"\n    for mask, ax in zip(masks, axs):\n        ax.imshow(mask.squeeze().numpy(), cmap=cmap)\n        ax.axis(\"off\")\n</code></pre>"},{"location":"utils/#geoai.utils.print_raster_info","title":"<code>print_raster_info(raster_path, show_preview=True, figsize=(10, 8))</code>","text":"<p>Print formatted information about a raster dataset and optionally show a preview.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <code>show_preview</code> <code>bool</code> <p>Whether to display a visual preview of the raster. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (10, 8).</p> <code>(10, 8)</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing raster information if successful, None otherwise</p> Source code in <code>geoai/utils.py</code> <pre><code>def print_raster_info(raster_path, show_preview=True, figsize=(10, 8)):\n    \"\"\"Print formatted information about a raster dataset and optionally show a preview.\n\n    Args:\n        raster_path (str): Path to the raster file\n        show_preview (bool, optional): Whether to display a visual preview of the raster.\n            Defaults to True.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n\n    Returns:\n        dict: Dictionary containing raster information if successful, None otherwise\n    \"\"\"\n    try:\n        info = get_raster_info(raster_path)\n\n        # Print basic information\n        print(f\"===== RASTER INFORMATION: {raster_path} =====\")\n        print(f\"Driver: {info['driver']}\")\n        print(f\"Dimensions: {info['width']} x {info['height']} pixels\")\n        print(f\"Number of bands: {info['count']}\")\n        print(f\"Data type: {info['dtype']}\")\n        print(f\"Coordinate Reference System: {info['crs']}\")\n        print(f\"Georeferenced Bounds: {info['bounds']}\")\n        print(f\"Pixel Resolution: {info['resolution'][0]}, {info['resolution'][1]}\")\n        print(f\"NoData Value: {info['nodata']}\")\n\n        # Print band statistics\n        print(\"\\n----- Band Statistics -----\")\n        for band_stat in info[\"band_stats\"]:\n            print(f\"Band {band_stat['band']}:\")\n            print(f\"  Min: {band_stat['min']:.2f}\")\n            print(f\"  Max: {band_stat['max']:.2f}\")\n            print(f\"  Mean: {band_stat['mean']:.2f}\")\n            print(f\"  Std Dev: {band_stat['std']:.2f}\")\n\n        # Show a preview if requested\n        if show_preview:\n            with rasterio.open(raster_path) as src:\n                # For multi-band images, show RGB composite or first band\n                if src.count &gt;= 3:\n                    # Try to show RGB composite\n                    rgb = np.dstack([src.read(i) for i in range(1, 4)])\n                    plt.figure(figsize=figsize)\n                    plt.imshow(rgb)\n                    plt.title(f\"RGB Preview: {raster_path}\")\n                else:\n                    # Show first band for single-band images\n                    plt.figure(figsize=figsize)\n                    show(\n                        src.read(1),\n                        cmap=\"viridis\",\n                        title=f\"Band 1 Preview: {raster_path}\",\n                    )\n                    plt.colorbar(label=\"Pixel Value\")\n                plt.show()\n\n    except Exception as e:\n        print(f\"Error reading raster: {str(e)}\")\n</code></pre>"},{"location":"utils/#geoai.utils.print_vector_info","title":"<code>print_vector_info(vector_path, show_preview=True, figsize=(10, 8))</code>","text":"<p>Print formatted information about a vector dataset and optionally show a preview.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <code>show_preview</code> <code>bool</code> <p>Whether to display a visual preview of the vector data. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (10, 8).</p> <code>(10, 8)</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing vector information if successful, None otherwise</p> Source code in <code>geoai/utils.py</code> <pre><code>def print_vector_info(vector_path, show_preview=True, figsize=(10, 8)):\n    \"\"\"Print formatted information about a vector dataset and optionally show a preview.\n\n    Args:\n        vector_path (str): Path to the vector file\n        show_preview (bool, optional): Whether to display a visual preview of the vector data.\n            Defaults to True.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n\n    Returns:\n        dict: Dictionary containing vector information if successful, None otherwise\n    \"\"\"\n    try:\n        info = get_vector_info(vector_path)\n\n        # Print basic information\n        print(f\"===== VECTOR INFORMATION: {vector_path} =====\")\n        print(f\"Driver: {info['driver']}\")\n        print(f\"Feature count: {info['feature_count']}\")\n        print(f\"Geometry types: {info['geometry_type']}\")\n        print(f\"Coordinate Reference System: {info['crs']}\")\n        print(f\"Bounds: {info['bounds']}\")\n        print(f\"Number of attributes: {info['attribute_count']}\")\n        print(f\"Attribute names: {', '.join(info['attribute_names'])}\")\n\n        # Print attribute statistics\n        if info[\"attribute_stats\"]:\n            print(\"\\n----- Attribute Statistics -----\")\n            for attr, stats in info[\"attribute_stats\"].items():\n                print(f\"Attribute: {attr}\")\n                for stat_name, stat_value in stats.items():\n                    print(\n                        f\"  {stat_name}: {stat_value:.4f}\"\n                        if isinstance(stat_value, float)\n                        else f\"  {stat_name}: {stat_value}\"\n                    )\n\n        # Show a preview if requested\n        if show_preview:\n            gdf = (\n                gpd.read_parquet(vector_path)\n                if vector_path.endswith(\".parquet\")\n                else gpd.read_file(vector_path)\n            )\n            fig, ax = plt.subplots(figsize=figsize)\n            gdf.plot(ax=ax, cmap=\"viridis\")\n            ax.set_title(f\"Preview: {vector_path}\")\n            plt.tight_layout()\n            plt.show()\n\n            # # Show a sample of the attribute table\n            # if not gdf.empty:\n            #     print(\"\\n----- Sample of attribute table (first 5 rows) -----\")\n            #     print(gdf.head().to_string())\n\n    except Exception as e:\n        print(f\"Error reading vector data: {str(e)}\")\n</code></pre>"},{"location":"utils/#geoai.utils.raster_to_vector","title":"<code>raster_to_vector(raster_path, output_path=None, threshold=0, min_area=10, simplify_tolerance=None, class_values=None, attribute_name='class', unique_attribute_value=False, output_format='geojson', plot_result=False)</code>","text":"<p>Convert a raster label mask to vector polygons.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the input raster file (e.g., GeoTIFF).</p> required <code>output_path</code> <code>str</code> <p>Path to save the output vector file. If None, returns GeoDataFrame without saving.</p> <code>None</code> <code>threshold</code> <code>int/float</code> <p>Pixel values greater than this threshold will be vectorized.</p> <code>0</code> <code>min_area</code> <code>float</code> <p>Minimum polygon area in square map units to keep.</p> <code>10</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for geometry simplification. None for no simplification.</p> <code>None</code> <code>class_values</code> <code>list</code> <p>Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.</p> <code>None</code> <code>attribute_name</code> <code>str</code> <p>Name of the attribute field for the class values.</p> <code>'class'</code> <code>unique_attribute_value</code> <code>bool</code> <p>Whether to generate unique values for each shape within a class.</p> <code>False</code> <code>output_format</code> <code>str</code> <p>Format for output file - 'geojson', 'shapefile', 'gpkg'.</p> <code>'geojson'</code> <code>plot_result</code> <code>bool</code> <p>Whether to plot the resulting polygons overlaid on the raster.</p> <code>False</code> <p>Returns:</p> Type Description <code>geopandas.GeoDataFrame</code> <p>A GeoDataFrame containing the vectorized polygons.</p> Source code in <code>geoai/utils.py</code> <pre><code>def raster_to_vector(\n    raster_path,\n    output_path=None,\n    threshold=0,\n    min_area=10,\n    simplify_tolerance=None,\n    class_values=None,\n    attribute_name=\"class\",\n    unique_attribute_value=False,\n    output_format=\"geojson\",\n    plot_result=False,\n):\n    \"\"\"\n    Convert a raster label mask to vector polygons.\n\n    Args:\n        raster_path (str): Path to the input raster file (e.g., GeoTIFF).\n        output_path (str): Path to save the output vector file. If None, returns GeoDataFrame without saving.\n        threshold (int/float): Pixel values greater than this threshold will be vectorized.\n        min_area (float): Minimum polygon area in square map units to keep.\n        simplify_tolerance (float): Tolerance for geometry simplification. None for no simplification.\n        class_values (list): Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.\n        attribute_name (str): Name of the attribute field for the class values.\n        unique_attribute_value (bool): Whether to generate unique values for each shape within a class.\n        output_format (str): Format for output file - 'geojson', 'shapefile', 'gpkg'.\n        plot_result (bool): Whether to plot the resulting polygons overlaid on the raster.\n\n    Returns:\n        geopandas.GeoDataFrame: A GeoDataFrame containing the vectorized polygons.\n    \"\"\"\n    # Open the raster file\n    with rasterio.open(raster_path) as src:\n        # Read the data\n        data = src.read(1)\n\n        # Get metadata\n        transform = src.transform\n        crs = src.crs\n\n        # Create mask based on threshold and class values\n        if class_values is not None:\n            # Create a mask for each specified class value\n            masks = {val: (data == val) for val in class_values}\n        else:\n            # Create a mask for values above threshold\n            masks = {1: (data &gt; threshold)}\n            class_values = [1]  # Default class\n\n        # Initialize list to store features\n        all_features = []\n\n        # Process each class value\n        for class_val in class_values:\n            mask = masks[class_val]\n            shape_count = 1\n            # Vectorize the mask\n            for geom, value in features.shapes(\n                mask.astype(np.uint8), mask=mask, transform=transform\n            ):\n                # Convert to shapely geometry\n                geom = shape(geom)\n\n                # Skip small polygons\n                if geom.area &lt; min_area:\n                    continue\n\n                # Simplify geometry if requested\n                if simplify_tolerance is not None:\n                    geom = geom.simplify(simplify_tolerance)\n\n                # Add to features list with class value\n                if unique_attribute_value:\n                    all_features.append(\n                        {\"geometry\": geom, attribute_name: class_val * shape_count}\n                    )\n                else:\n                    all_features.append({\"geometry\": geom, attribute_name: class_val})\n\n                shape_count += 1\n\n        # Create GeoDataFrame\n        if all_features:\n            gdf = gpd.GeoDataFrame(all_features, crs=crs)\n        else:\n            print(\"Warning: No features were extracted from the raster.\")\n            # Return empty GeoDataFrame with correct CRS\n            gdf = gpd.GeoDataFrame([], geometry=[], crs=crs)\n\n        # Save to file if requested\n        if output_path is not None:\n            # Create directory if it doesn't exist\n            os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n\n            # Save to file based on format\n            if output_format.lower() == \"geojson\":\n                gdf.to_file(output_path, driver=\"GeoJSON\")\n            elif output_format.lower() == \"shapefile\":\n                gdf.to_file(output_path)\n            elif output_format.lower() == \"gpkg\":\n                gdf.to_file(output_path, driver=\"GPKG\")\n            else:\n                raise ValueError(f\"Unsupported output format: {output_format}\")\n\n            print(f\"Vectorized data saved to {output_path}\")\n\n        # Plot result if requested\n        if plot_result:\n            fig, ax = plt.subplots(figsize=(12, 12))\n\n            # Plot raster\n            raster_img = src.read()\n            if raster_img.shape[0] == 1:\n                plt.imshow(raster_img[0], cmap=\"viridis\", alpha=0.7)\n            else:\n                # Use first 3 bands for RGB display\n                rgb = raster_img[:3].transpose(1, 2, 0)\n                # Normalize for display\n                rgb = np.clip(rgb / rgb.max(), 0, 1)\n                plt.imshow(rgb)\n\n            # Plot vector boundaries\n            if not gdf.empty:\n                gdf.plot(ax=ax, facecolor=\"none\", edgecolor=\"red\", linewidth=2)\n\n            plt.title(\"Raster with Vectorized Boundaries\")\n            plt.axis(\"off\")\n            plt.tight_layout()\n            plt.show()\n\n        return gdf\n</code></pre>"},{"location":"utils/#geoai.utils.read_raster","title":"<code>read_raster(source, band=None, masked=True, **kwargs)</code>","text":"<p>Reads raster data from various formats using rioxarray.</p> <p>This function reads raster data from local files or URLs into a rioxarray data structure with preserved geospatial metadata.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <p>String path to the raster file or URL.</p> required <code>band</code> <p>Integer or list of integers specifying which band(s) to read. Defaults to None (all bands).</p> <code>None</code> <code>masked</code> <p>Boolean indicating whether to mask nodata values. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to rioxarray.open_rasterio.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xarray.DataArray</code> <p>A DataArray containing the raster data with geospatial     metadata preserved.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the file format is not supported or source cannot be accessed.</p> <p>Examples:</p> <p>Read a local GeoTIFF</p> <pre><code>&gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\")\n&gt;&gt;&gt;\nRead only band 1 from a remote GeoTIFF\n&gt;&gt;&gt; raster = read_raster(\"https://example.com/data.tif\", band=1)\n&gt;&gt;&gt;\nRead a raster without masking nodata values\n&gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\", masked=False)\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def read_raster(source, band=None, masked=True, **kwargs):\n    \"\"\"Reads raster data from various formats using rioxarray.\n\n    This function reads raster data from local files or URLs into a rioxarray\n    data structure with preserved geospatial metadata.\n\n    Args:\n        source: String path to the raster file or URL.\n        band: Integer or list of integers specifying which band(s) to read.\n            Defaults to None (all bands).\n        masked: Boolean indicating whether to mask nodata values.\n            Defaults to True.\n        **kwargs: Additional keyword arguments to pass to rioxarray.open_rasterio.\n\n    Returns:\n        xarray.DataArray: A DataArray containing the raster data with geospatial\n            metadata preserved.\n\n    Raises:\n        ValueError: If the file format is not supported or source cannot be accessed.\n\n    Examples:\n        Read a local GeoTIFF\n        &gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\")\n        &gt;&gt;&gt;\n        Read only band 1 from a remote GeoTIFF\n        &gt;&gt;&gt; raster = read_raster(\"https://example.com/data.tif\", band=1)\n        &gt;&gt;&gt;\n        Read a raster without masking nodata values\n        &gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\", masked=False)\n    \"\"\"\n    import urllib.parse\n\n    from rasterio.errors import RasterioIOError\n\n    # Determine if source is a URL or local file\n    parsed_url = urllib.parse.urlparse(source)\n    is_url = parsed_url.scheme in [\"http\", \"https\"]\n\n    # If it's a local file, check if it exists\n    if not is_url and not os.path.exists(source):\n        raise ValueError(f\"Raster file does not exist: {source}\")\n\n    try:\n        # Open the raster with rioxarray\n        raster = rxr.open_rasterio(source, masked=masked, **kwargs)\n\n        # Handle band selection if specified\n        if band is not None:\n            if isinstance(band, (list, tuple)):\n                # Convert from 1-based indexing to 0-based indexing\n                band_indices = [b - 1 for b in band]\n                raster = raster.isel(band=band_indices)\n            else:\n                # Single band selection (convert from 1-based to 0-based indexing)\n                raster = raster.isel(band=band - 1)\n\n        return raster\n\n    except RasterioIOError as e:\n        raise ValueError(f\"Could not read raster from source '{source}': {str(e)}\")\n    except Exception as e:\n        raise ValueError(f\"Error reading raster data: {str(e)}\")\n</code></pre>"},{"location":"utils/#geoai.utils.read_vector","title":"<code>read_vector(source, layer=None, **kwargs)</code>","text":"<p>Reads vector data from various formats including GeoParquet.</p> <p>This function dynamically determines the file type based on extension and reads it into a GeoDataFrame. It supports both local files and HTTP/HTTPS URLs.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <p>String path to the vector file or URL.</p> required <code>layer</code> <p>String or integer specifying which layer to read from multi-layer files (only applicable for formats like GPKG, GeoJSON, etc.). Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the underlying reader.</p> <code>{}</code> <p>Returns:</p> Type Description <code>geopandas.GeoDataFrame</code> <p>A GeoDataFrame containing the vector data.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the file format is not supported or source cannot be accessed.</p> <p>Examples:</p> <p>Read a local shapefile</p> <pre><code>&gt;&gt;&gt; gdf = read_vector(\"path/to/data.shp\")\n&gt;&gt;&gt;\nRead a GeoParquet file from URL\n&gt;&gt;&gt; gdf = read_vector(\"https://example.com/data.parquet\")\n&gt;&gt;&gt;\nRead a specific layer from a GeoPackage\n&gt;&gt;&gt; gdf = read_vector(\"path/to/data.gpkg\", layer=\"layer_name\")\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def read_vector(source, layer=None, **kwargs):\n    \"\"\"Reads vector data from various formats including GeoParquet.\n\n    This function dynamically determines the file type based on extension\n    and reads it into a GeoDataFrame. It supports both local files and HTTP/HTTPS URLs.\n\n    Args:\n        source: String path to the vector file or URL.\n        layer: String or integer specifying which layer to read from multi-layer\n            files (only applicable for formats like GPKG, GeoJSON, etc.).\n            Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the underlying reader.\n\n    Returns:\n        geopandas.GeoDataFrame: A GeoDataFrame containing the vector data.\n\n    Raises:\n        ValueError: If the file format is not supported or source cannot be accessed.\n\n    Examples:\n        Read a local shapefile\n        &gt;&gt;&gt; gdf = read_vector(\"path/to/data.shp\")\n        &gt;&gt;&gt;\n        Read a GeoParquet file from URL\n        &gt;&gt;&gt; gdf = read_vector(\"https://example.com/data.parquet\")\n        &gt;&gt;&gt;\n        Read a specific layer from a GeoPackage\n        &gt;&gt;&gt; gdf = read_vector(\"path/to/data.gpkg\", layer=\"layer_name\")\n    \"\"\"\n\n    import urllib.parse\n\n    import fiona\n\n    # Determine if source is a URL or local file\n    parsed_url = urllib.parse.urlparse(source)\n    is_url = parsed_url.scheme in [\"http\", \"https\"]\n\n    # If it's a local file, check if it exists\n    if not is_url and not os.path.exists(source):\n        raise ValueError(f\"File does not exist: {source}\")\n\n    # Get file extension\n    _, ext = os.path.splitext(source)\n    ext = ext.lower()\n\n    # Handle GeoParquet files\n    if ext in [\".parquet\", \".pq\", \".geoparquet\"]:\n        return gpd.read_parquet(source, **kwargs)\n\n    # Handle common vector formats\n    if ext in [\".shp\", \".geojson\", \".json\", \".gpkg\", \".gml\", \".kml\", \".gpx\"]:\n        # For formats that might have multiple layers\n        if ext in [\".gpkg\", \".gml\"] and layer is not None:\n            return gpd.read_file(source, layer=layer, **kwargs)\n        return gpd.read_file(source, **kwargs)\n\n    # Try to use fiona to identify valid layers for formats that might have them\n    # Only attempt this for local files as fiona.listlayers might not work with URLs\n    if layer is None and ext in [\".gpkg\", \".gml\"] and not is_url:\n        try:\n            layers = fiona.listlayers(source)\n            if layers:\n                return gpd.read_file(source, layer=layers[0], **kwargs)\n        except Exception:\n            # If listing layers fails, we'll fall through to the generic read attempt\n            pass\n\n    # For other formats or when layer listing fails, attempt to read using GeoPandas\n    try:\n        return gpd.read_file(source, **kwargs)\n    except Exception as e:\n        raise ValueError(f\"Could not read from source '{source}': {str(e)}\")\n</code></pre>"},{"location":"utils/#geoai.utils.region_groups","title":"<code>region_groups(image, connectivity=1, min_size=10, max_size=None, threshold=None, properties=None, intensity_image=None, out_csv=None, out_vector=None, out_image=None, **kwargs)</code>","text":"<p>Segment regions in an image and filter them based on size.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, xr.DataArray, np.ndarray]</code> <p>Input image, can be a file path, xarray DataArray, or numpy array.</p> required <code>connectivity</code> <code>int</code> <p>Connectivity for labeling. Defaults to 1 for 4-connectivity. Use 2 for 8-connectivity.</p> <code>1</code> <code>min_size</code> <code>int</code> <p>Minimum size of regions to keep. Defaults to 10.</p> <code>10</code> <code>max_size</code> <code>Optional[int]</code> <p>Maximum size of regions to keep. Defaults to None.</p> <code>None</code> <code>threshold</code> <code>Optional[int]</code> <p>Threshold for filling holes. Defaults to None, which is equal to min_size.</p> <code>None</code> <code>properties</code> <code>Optional[List[str]]</code> <p>List of properties to measure. See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops Defaults to None.</p> <code>None</code> <code>intensity_image</code> <code>Optional[Union[str, xr.DataArray, np.ndarray]]</code> <p>Intensity image to measure properties. Defaults to None.</p> <code>None</code> <code>out_csv</code> <code>Optional[str]</code> <p>Path to save the properties as a CSV file. Defaults to None.</p> <code>None</code> <code>out_vector</code> <code>Optional[str]</code> <p>Path to save the vector file. Defaults to None.</p> <code>None</code> <code>out_image</code> <code>Optional[str]</code> <p>Path to save the output image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]</code> <p>Labeled image and properties DataFrame.</p> Source code in <code>geoai/utils.py</code> <pre><code>def region_groups(\n    image: Union[str, \"xr.DataArray\", np.ndarray],\n    connectivity: int = 1,\n    min_size: int = 10,\n    max_size: Optional[int] = None,\n    threshold: Optional[int] = None,\n    properties: Optional[List[str]] = None,\n    intensity_image: Optional[Union[str, \"xr.DataArray\", np.ndarray]] = None,\n    out_csv: Optional[str] = None,\n    out_vector: Optional[str] = None,\n    out_image: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Union[Tuple[np.ndarray, \"pd.DataFrame\"], Tuple[\"xr.DataArray\", \"pd.DataFrame\"]]:\n    \"\"\"\n    Segment regions in an image and filter them based on size.\n\n    Args:\n        image (Union[str, xr.DataArray, np.ndarray]): Input image, can be a file\n            path, xarray DataArray, or numpy array.\n        connectivity (int, optional): Connectivity for labeling. Defaults to 1\n            for 4-connectivity. Use 2 for 8-connectivity.\n        min_size (int, optional): Minimum size of regions to keep. Defaults to 10.\n        max_size (Optional[int], optional): Maximum size of regions to keep.\n            Defaults to None.\n        threshold (Optional[int], optional): Threshold for filling holes.\n            Defaults to None, which is equal to min_size.\n        properties (Optional[List[str]], optional): List of properties to measure.\n            See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops\n            Defaults to None.\n        intensity_image (Optional[Union[str, xr.DataArray, np.ndarray]], optional):\n            Intensity image to measure properties. Defaults to None.\n        out_csv (Optional[str], optional): Path to save the properties as a CSV file.\n            Defaults to None.\n        out_vector (Optional[str], optional): Path to save the vector file.\n            Defaults to None.\n        out_image (Optional[str], optional): Path to save the output image.\n            Defaults to None.\n\n    Returns:\n        Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.\n    \"\"\"\n    import scipy.ndimage as ndi\n    from skimage import measure\n\n    if isinstance(image, str):\n        ds = rxr.open_rasterio(image)\n        da = ds.sel(band=1)\n        array = da.values.squeeze()\n    elif isinstance(image, xr.DataArray):\n        da = image\n        array = image.values.squeeze()\n    elif isinstance(image, np.ndarray):\n        array = image\n    else:\n        raise ValueError(\n            \"The input image must be a file path, xarray DataArray, or numpy array.\"\n        )\n\n    if threshold is None:\n        threshold = min_size\n\n    # Define a custom function to calculate median intensity\n    def intensity_median(region, intensity_image):\n        # Extract the intensity values for the region\n        return np.median(intensity_image[region])\n\n    # Add your custom function to the list of extra properties\n    if intensity_image is not None:\n        extra_props = (intensity_median,)\n    else:\n        extra_props = None\n\n    if properties is None:\n        properties = [\n            \"label\",\n            \"area\",\n            \"area_bbox\",\n            \"area_convex\",\n            \"area_filled\",\n            \"axis_major_length\",\n            \"axis_minor_length\",\n            \"eccentricity\",\n            \"diameter_areagth\",\n            \"extent\",\n            \"orientation\",\n            \"perimeter\",\n            \"solidity\",\n        ]\n\n        if intensity_image is not None:\n\n            properties += [\n                \"intensity_max\",\n                \"intensity_mean\",\n                \"intensity_min\",\n                \"intensity_std\",\n            ]\n\n    if intensity_image is not None:\n        if isinstance(intensity_image, str):\n            ds = rxr.open_rasterio(intensity_image)\n            intensity_da = ds.sel(band=1)\n            intensity_image = intensity_da.values.squeeze()\n        elif isinstance(intensity_image, xr.DataArray):\n            intensity_image = intensity_image.values.squeeze()\n        elif isinstance(intensity_image, np.ndarray):\n            pass\n        else:\n            raise ValueError(\n                \"The intensity_image must be a file path, xarray DataArray, or numpy array.\"\n            )\n\n    label_image = measure.label(array, connectivity=connectivity)\n    props = measure.regionprops_table(\n        label_image, properties=properties, intensity_image=intensity_image, **kwargs\n    )\n\n    df = pd.DataFrame(props)\n\n    # Get the labels of regions with area smaller than the threshold\n    small_regions = df[df[\"area\"] &lt; min_size][\"label\"].values\n    # Set the corresponding labels in the label_image to zero\n    for region_label in small_regions:\n        label_image[label_image == region_label] = 0\n\n    if max_size is not None:\n        large_regions = df[df[\"area\"] &gt; max_size][\"label\"].values\n        for region_label in large_regions:\n            label_image[label_image == region_label] = 0\n\n    # Find the background (holes) which are zeros\n    holes = label_image == 0\n\n    # Label the holes (connected components in the background)\n    labeled_holes, _ = ndi.label(holes)\n\n    # Measure properties of the labeled holes, including area and bounding box\n    hole_props = measure.regionprops(labeled_holes)\n\n    # Loop through each hole and fill it if it is smaller than the threshold\n    for prop in hole_props:\n        if prop.area &lt; threshold:\n            # Get the coordinates of the small hole\n            coords = prop.coords\n\n            # Find the surrounding region's ID (non-zero value near the hole)\n            surrounding_region_values = []\n            for coord in coords:\n                x, y = coord\n                # Get a 3x3 neighborhood around the hole pixel\n                neighbors = label_image[max(0, x - 1) : x + 2, max(0, y - 1) : y + 2]\n                # Exclude the hole pixels (zeros) and get region values\n                region_values = neighbors[neighbors != 0]\n                if region_values.size &gt; 0:\n                    surrounding_region_values.append(\n                        region_values[0]\n                    )  # Take the first non-zero value\n\n            if surrounding_region_values:\n                # Fill the hole with the mode (most frequent) of the surrounding region values\n                fill_value = max(\n                    set(surrounding_region_values), key=surrounding_region_values.count\n                )\n                label_image[coords[:, 0], coords[:, 1]] = fill_value\n\n    label_image, num_labels = measure.label(\n        label_image, connectivity=connectivity, return_num=True\n    )\n    props = measure.regionprops_table(\n        label_image,\n        properties=properties,\n        intensity_image=intensity_image,\n        extra_properties=extra_props,\n        **kwargs,\n    )\n\n    df = pd.DataFrame(props)\n    df[\"elongation\"] = df[\"axis_major_length\"] / df[\"axis_minor_length\"]\n\n    dtype = \"uint8\"\n    if num_labels &gt; 255 and num_labels &lt;= 65535:\n        dtype = \"uint16\"\n    elif num_labels &gt; 65535:\n        dtype = \"uint32\"\n\n    if out_csv is not None:\n        df.to_csv(out_csv, index=False)\n\n    if isinstance(image, np.ndarray):\n        return label_image, df\n    else:\n        da.values = label_image\n        if out_image is not None:\n            da.rio.to_raster(out_image, dtype=dtype)\n\n        if out_vector is not None:\n            tmp_raster = None\n            tmp_vector = None\n            try:\n                if out_image is None:\n                    tmp_raster = temp_file_path(\".tif\")\n                    da.rio.to_raster(tmp_raster, dtype=dtype)\n                    tmp_vector = temp_file_path(\".gpkg\")\n                    raster_to_vector(\n                        tmp_raster,\n                        tmp_vector,\n                        attribute_name=\"value\",\n                        unique_attribute_value=True,\n                    )\n                else:\n                    tmp_vector = temp_file_path(\".gpkg\")\n                    raster_to_vector(\n                        out_image,\n                        tmp_vector,\n                        attribute_name=\"value\",\n                        unique_attribute_value=True,\n                    )\n                gdf = gpd.read_file(tmp_vector)\n                gdf[\"label\"] = gdf[\"value\"].astype(int)\n                gdf.drop(columns=[\"value\"], inplace=True)\n                gdf2 = pd.merge(gdf, df, on=\"label\", how=\"left\")\n                gdf2.to_file(out_vector)\n                gdf2.sort_values(\"label\", inplace=True)\n                df = gdf2\n            finally:\n                try:\n                    if tmp_raster is not None and os.path.exists(tmp_raster):\n                        os.remove(tmp_raster)\n                    if tmp_vector is not None and os.path.exists(tmp_vector):\n                        os.remove(tmp_vector)\n                except Exception as e:\n                    print(f\"Warning: Failed to delete temporary files: {str(e)}\")\n\n        return da, df\n</code></pre>"},{"location":"utils/#geoai.utils.regularization","title":"<code>regularization(building_polygons, angle_tolerance=10, simplify_tolerance=0.5, orthogonalize=True, preserve_topology=True)</code>","text":"<p>Regularizes building footprint polygons with multiple techniques beyond minimum rotated rectangles.</p> <p>Parameters:</p> Name Type Description Default <code>building_polygons</code> <p>GeoDataFrame or list of shapely Polygons containing building footprints</p> required <code>angle_tolerance</code> <p>Degrees within which angles will be regularized to 90/180 degrees</p> <code>10</code> <code>simplify_tolerance</code> <p>Distance tolerance for Douglas-Peucker simplification</p> <code>0.5</code> <code>orthogonalize</code> <p>Whether to enforce orthogonal angles in the final polygons</p> <code>True</code> <code>preserve_topology</code> <p>Whether to preserve topology during simplification</p> <code>True</code> <p>Returns:</p> Type Description <p>GeoDataFrame or list of shapely Polygons with regularized building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def regularization(\n    building_polygons,\n    angle_tolerance=10,\n    simplify_tolerance=0.5,\n    orthogonalize=True,\n    preserve_topology=True,\n):\n    \"\"\"\n    Regularizes building footprint polygons with multiple techniques beyond minimum\n    rotated rectangles.\n\n    Args:\n        building_polygons: GeoDataFrame or list of shapely Polygons containing building footprints\n        angle_tolerance: Degrees within which angles will be regularized to 90/180 degrees\n        simplify_tolerance: Distance tolerance for Douglas-Peucker simplification\n        orthogonalize: Whether to enforce orthogonal angles in the final polygons\n        preserve_topology: Whether to preserve topology during simplification\n\n    Returns:\n        GeoDataFrame or list of shapely Polygons with regularized building footprints\n    \"\"\"\n    from shapely import wkt\n    from shapely.affinity import rotate, translate\n    from shapely.geometry import Polygon, shape\n\n    regularized_buildings = []\n\n    # Check if we're dealing with a GeoDataFrame\n    if isinstance(building_polygons, gpd.GeoDataFrame):\n        geom_objects = building_polygons.geometry\n    else:\n        geom_objects = building_polygons\n\n    for building in geom_objects:\n        # Handle potential string representations of geometries\n        if isinstance(building, str):\n            try:\n                # Try to parse as WKT\n                building = wkt.loads(building)\n            except Exception:\n                print(f\"Failed to parse geometry string: {building[:30]}...\")\n                continue\n\n        # Ensure we have a valid geometry\n        if not hasattr(building, \"simplify\"):\n            print(f\"Invalid geometry type: {type(building)}\")\n            continue\n\n        # Step 1: Simplify to remove noise and small vertices\n        simplified = building.simplify(\n            simplify_tolerance, preserve_topology=preserve_topology\n        )\n\n        if orthogonalize:\n            # Make sure we have a valid polygon with an exterior\n            if not hasattr(simplified, \"exterior\") or simplified.exterior is None:\n                print(f\"Simplified geometry has no exterior: {simplified}\")\n                regularized_buildings.append(building)  # Use original instead\n                continue\n\n            # Step 2: Get the dominant angle to rotate building\n            coords = np.array(simplified.exterior.coords)\n\n            # Make sure we have enough coordinates for angle calculation\n            if len(coords) &lt; 3:\n                print(f\"Not enough coordinates for angle calculation: {len(coords)}\")\n                regularized_buildings.append(building)  # Use original instead\n                continue\n\n            segments = np.diff(coords, axis=0)\n            angles = np.arctan2(segments[:, 1], segments[:, 0]) * 180 / np.pi\n\n            # Find most common angle classes (0, 90, 180, 270 degrees)\n            binned_angles = np.round(angles / 90) * 90\n            dominant_angle = np.bincount(binned_angles.astype(int) % 180).argmax()\n\n            # Step 3: Rotate to align with axes, regularize, then rotate back\n            rotated = rotate(simplified, -dominant_angle, origin=\"centroid\")\n\n            # Step 4: Rectify coordinates to enforce right angles\n            ext_coords = np.array(rotated.exterior.coords)\n            rect_coords = []\n\n            # Regularize each vertex to create orthogonal corners\n            for i in range(len(ext_coords) - 1):\n                rect_coords.append(ext_coords[i])\n\n                # Check if we need to add a right-angle vertex\n                angle = (\n                    np.arctan2(\n                        ext_coords[(i + 1) % (len(ext_coords) - 1), 1]\n                        - ext_coords[i, 1],\n                        ext_coords[(i + 1) % (len(ext_coords) - 1), 0]\n                        - ext_coords[i, 0],\n                    )\n                    * 180\n                    / np.pi\n                )\n\n                if abs(angle % 90) &gt; angle_tolerance and abs(angle % 90) &lt; (\n                    90 - angle_tolerance\n                ):\n                    # Add intermediate point to create right angle\n                    rect_coords.append(\n                        [\n                            ext_coords[(i + 1) % (len(ext_coords) - 1), 0],\n                            ext_coords[i, 1],\n                        ]\n                    )\n\n            # Close the polygon by adding the first point again\n            rect_coords.append(rect_coords[0])\n\n            # Create regularized polygon and rotate back\n            regularized = Polygon(rect_coords)\n            final_building = rotate(regularized, dominant_angle, origin=\"centroid\")\n        else:\n            final_building = simplified\n\n        regularized_buildings.append(final_building)\n\n    # If input was a GeoDataFrame, return a GeoDataFrame\n    if isinstance(building_polygons, gpd.GeoDataFrame):\n        return gpd.GeoDataFrame(\n            geometry=regularized_buildings, crs=building_polygons.crs\n        )\n    else:\n        return regularized_buildings\n</code></pre>"},{"location":"utils/#geoai.utils.temp_file_path","title":"<code>temp_file_path(ext)</code>","text":"<p>Returns a temporary file path.</p> <p>Parameters:</p> Name Type Description Default <code>ext</code> <code>str</code> <p>The file extension.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The temporary file path.</p> Source code in <code>geoai/utils.py</code> <pre><code>def temp_file_path(ext):\n    \"\"\"Returns a temporary file path.\n\n    Args:\n        ext (str): The file extension.\n\n    Returns:\n        str: The temporary file path.\n    \"\"\"\n\n    import tempfile\n    import uuid\n\n    if not ext.startswith(\".\"):\n        ext = \".\" + ext\n    file_id = str(uuid.uuid4())\n    file_path = os.path.join(tempfile.gettempdir(), f\"{file_id}{ext}\")\n\n    return file_path\n</code></pre>"},{"location":"utils/#geoai.utils.try_common_architectures","title":"<code>try_common_architectures(state_dict)</code>","text":"<p>Try to load the state_dict into common architectures to see which one fits.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <p>The model's state dictionary</p> required Source code in <code>geoai/utils.py</code> <pre><code>def try_common_architectures(state_dict):\n    \"\"\"\n    Try to load the state_dict into common architectures to see which one fits.\n\n    Args:\n        state_dict: The model's state dictionary\n    \"\"\"\n    import torchinfo\n\n    # Test models and their initializations\n    models_to_try = {\n        \"FCN-ResNet50\": lambda: fcn_resnet50(num_classes=9),\n        \"DeepLabV3-ResNet50\": lambda: deeplabv3_resnet50(num_classes=9),\n    }\n\n    print(\"\\nTrying to load state_dict into common architectures:\")\n\n    for name, model_fn in models_to_try.items():\n        try:\n            model = model_fn()\n            # Sometimes state_dict keys have 'model.' prefix\n            if all(k.startswith(\"model.\") for k in state_dict.keys()):\n                cleaned_state_dict = {k[6:]: v for k, v in state_dict.items()}\n                model.load_state_dict(cleaned_state_dict, strict=False)\n            else:\n                model.load_state_dict(state_dict, strict=False)\n\n            print(\n                f\"- {name}: Successfully loaded (may have missing or unexpected keys)\"\n            )\n\n            # Generate model summary\n            print(f\"\\nSummary of {name} architecture:\")\n            summary = torchinfo.summary(model, input_size=(1, 3, 224, 224), verbose=0)\n            print(summary)\n\n        except Exception as e:\n            print(f\"- {name}: Failed to load - {str(e)}\")\n</code></pre>"},{"location":"utils/#geoai.utils.vector_to_raster","title":"<code>vector_to_raster(vector_path, output_path=None, reference_raster=None, attribute_field=None, output_shape=None, transform=None, pixel_size=None, bounds=None, crs=None, all_touched=False, fill_value=0, dtype=&lt;class 'numpy.uint8'&gt;, nodata=None, plot_result=False)</code>","text":"<p>Convert vector data to a raster.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str or GeoDataFrame</code> <p>Path to the input vector file or a GeoDataFrame.</p> required <code>output_path</code> <code>str</code> <p>Path to save the output raster file. If None, returns the array without saving.</p> <code>None</code> <code>reference_raster</code> <code>str</code> <p>Path to a reference raster for dimensions, transform and CRS.</p> <code>None</code> <code>attribute_field</code> <code>str</code> <p>Field name in the vector data to use for pixel values. If None, all vector features will be burned with value 1.</p> <code>None</code> <code>output_shape</code> <code>tuple</code> <p>Shape of the output raster as (height, width). Required if reference_raster is not provided.</p> <code>None</code> <code>transform</code> <code>affine.Affine</code> <p>Affine transformation matrix. Required if reference_raster is not provided.</p> <code>None</code> <code>pixel_size</code> <code>float or tuple</code> <p>Pixel size (resolution) as single value or (x_res, y_res). Used to calculate transform if transform is not provided.</p> <code>None</code> <code>bounds</code> <code>tuple</code> <p>Bounds of the output raster as (left, bottom, right, top). Used to calculate transform if transform is not provided.</p> <code>None</code> <code>crs</code> <code>str or CRS</code> <p>Coordinate reference system of the output raster. Required if reference_raster is not provided.</p> <code>None</code> <code>all_touched</code> <code>bool</code> <p>If True, all pixels touched by geometries will be burned in. If False, only pixels whose center is within the geometry will be burned in.</p> <code>False</code> <code>fill_value</code> <code>int</code> <p>Value to fill the raster with before burning in features.</p> <code>0</code> <code>dtype</code> <code>numpy.dtype</code> <p>Data type of the output raster.</p> <code>&lt;class 'numpy.uint8'&gt;</code> <code>nodata</code> <code>int</code> <p>No data value for the output raster.</p> <code>None</code> <code>plot_result</code> <code>bool</code> <p>Whether to plot the resulting raster.</p> <code>False</code> <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>The rasterized data array if output_path is None, else None.</p> Source code in <code>geoai/utils.py</code> <pre><code>def vector_to_raster(\n    vector_path,\n    output_path=None,\n    reference_raster=None,\n    attribute_field=None,\n    output_shape=None,\n    transform=None,\n    pixel_size=None,\n    bounds=None,\n    crs=None,\n    all_touched=False,\n    fill_value=0,\n    dtype=np.uint8,\n    nodata=None,\n    plot_result=False,\n):\n    \"\"\"\n    Convert vector data to a raster.\n\n    Args:\n        vector_path (str or GeoDataFrame): Path to the input vector file or a GeoDataFrame.\n        output_path (str): Path to save the output raster file. If None, returns the array without saving.\n        reference_raster (str): Path to a reference raster for dimensions, transform and CRS.\n        attribute_field (str): Field name in the vector data to use for pixel values.\n            If None, all vector features will be burned with value 1.\n        output_shape (tuple): Shape of the output raster as (height, width).\n            Required if reference_raster is not provided.\n        transform (affine.Affine): Affine transformation matrix.\n            Required if reference_raster is not provided.\n        pixel_size (float or tuple): Pixel size (resolution) as single value or (x_res, y_res).\n            Used to calculate transform if transform is not provided.\n        bounds (tuple): Bounds of the output raster as (left, bottom, right, top).\n            Used to calculate transform if transform is not provided.\n        crs (str or CRS): Coordinate reference system of the output raster.\n            Required if reference_raster is not provided.\n        all_touched (bool): If True, all pixels touched by geometries will be burned in.\n            If False, only pixels whose center is within the geometry will be burned in.\n        fill_value (int): Value to fill the raster with before burning in features.\n        dtype (numpy.dtype): Data type of the output raster.\n        nodata (int): No data value for the output raster.\n        plot_result (bool): Whether to plot the resulting raster.\n\n    Returns:\n        numpy.ndarray: The rasterized data array if output_path is None, else None.\n    \"\"\"\n    # Load vector data\n    if isinstance(vector_path, gpd.GeoDataFrame):\n        gdf = vector_path\n    else:\n        gdf = gpd.read_file(vector_path)\n\n    # Check if vector data is empty\n    if gdf.empty:\n        warnings.warn(\"The input vector data is empty. Creating an empty raster.\")\n\n    # Get CRS from vector data if not provided\n    if crs is None and reference_raster is None:\n        crs = gdf.crs\n\n    # Get transform and output shape from reference raster if provided\n    if reference_raster is not None:\n        with rasterio.open(reference_raster) as src:\n            transform = src.transform\n            output_shape = src.shape\n            crs = src.crs\n            if nodata is None:\n                nodata = src.nodata\n    else:\n        # Check if we have all required parameters\n        if transform is None:\n            if pixel_size is None or bounds is None:\n                raise ValueError(\n                    \"Either reference_raster, transform, or both pixel_size and bounds must be provided.\"\n                )\n\n            # Calculate transform from pixel size and bounds\n            if isinstance(pixel_size, (int, float)):\n                x_res = y_res = float(pixel_size)\n            else:\n                x_res, y_res = pixel_size\n                y_res = abs(y_res) * -1  # Convert to negative for north-up raster\n\n            left, bottom, right, top = bounds\n            transform = rasterio.transform.from_bounds(\n                left,\n                bottom,\n                right,\n                top,\n                int((right - left) / x_res),\n                int((top - bottom) / abs(y_res)),\n            )\n\n        if output_shape is None:\n            # Calculate output shape from bounds and pixel size\n            if bounds is None or pixel_size is None:\n                raise ValueError(\n                    \"output_shape must be provided if reference_raster is not provided and \"\n                    \"cannot be calculated from bounds and pixel_size.\"\n                )\n\n            if isinstance(pixel_size, (int, float)):\n                x_res = y_res = float(pixel_size)\n            else:\n                x_res, y_res = pixel_size\n\n            left, bottom, right, top = bounds\n            width = int((right - left) / x_res)\n            height = int((top - bottom) / abs(y_res))\n            output_shape = (height, width)\n\n    # Ensure CRS is set\n    if crs is None:\n        raise ValueError(\n            \"CRS must be provided either directly, from reference_raster, or from input vector data.\"\n        )\n\n    # Reproject vector data if its CRS doesn't match the output CRS\n    if gdf.crs != crs:\n        print(f\"Reprojecting vector data from {gdf.crs} to {crs}\")\n        gdf = gdf.to_crs(crs)\n\n    # Create empty raster filled with fill_value\n    raster_data = np.full(output_shape, fill_value, dtype=dtype)\n\n    # Burn vector features into raster\n    if not gdf.empty:\n        # Prepare shapes for burning\n        if attribute_field is not None and attribute_field in gdf.columns:\n            # Use attribute field for values\n            shapes = [\n                (geom, value) for geom, value in zip(gdf.geometry, gdf[attribute_field])\n            ]\n        else:\n            # Burn with value 1\n            shapes = [(geom, 1) for geom in gdf.geometry]\n\n        # Burn shapes into raster\n        burned = features.rasterize(\n            shapes=shapes,\n            out_shape=output_shape,\n            transform=transform,\n            fill=fill_value,\n            all_touched=all_touched,\n            dtype=dtype,\n        )\n\n        # Update raster data\n        raster_data = burned\n\n    # Save raster if output path is provided\n    if output_path is not None:\n        # Create directory if it doesn't exist\n        os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n\n        # Define metadata\n        metadata = {\n            \"driver\": \"GTiff\",\n            \"height\": output_shape[0],\n            \"width\": output_shape[1],\n            \"count\": 1,\n            \"dtype\": raster_data.dtype,\n            \"crs\": crs,\n            \"transform\": transform,\n        }\n\n        # Add nodata value if provided\n        if nodata is not None:\n            metadata[\"nodata\"] = nodata\n\n        # Write raster\n        with rasterio.open(output_path, \"w\", **metadata) as dst:\n            dst.write(raster_data, 1)\n\n        print(f\"Rasterized data saved to {output_path}\")\n\n    # Plot result if requested\n    if plot_result:\n        fig, ax = plt.subplots(figsize=(10, 10))\n\n        # Plot raster\n        im = ax.imshow(raster_data, cmap=\"viridis\")\n        plt.colorbar(im, ax=ax, label=attribute_field if attribute_field else \"Value\")\n\n        # Plot vector boundaries for reference\n        if output_path is not None:\n            # Get the extent of the raster\n            with rasterio.open(output_path) as src:\n                bounds = src.bounds\n                raster_bbox = box(*bounds)\n        else:\n            # Calculate extent from transform and shape\n            height, width = output_shape\n            left, top = transform * (0, 0)\n            right, bottom = transform * (width, height)\n            raster_bbox = box(left, bottom, right, top)\n\n        # Clip vector to raster extent for clarity in plot\n        if not gdf.empty:\n            gdf_clipped = gpd.clip(gdf, raster_bbox)\n            if not gdf_clipped.empty:\n                gdf_clipped.boundary.plot(ax=ax, color=\"red\", linewidth=1)\n\n        plt.title(\"Rasterized Vector Data\")\n        plt.tight_layout()\n        plt.show()\n\n    return raster_data\n</code></pre>"},{"location":"utils/#geoai.utils.view_image","title":"<code>view_image(image, transpose=False, bdx=None, scale_factor=1.0, figsize=(10, 5), axis_off=True, title=None, **kwargs)</code>","text":"<p>Visualize an image using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[np.ndarray, torch.Tensor]</code> <p>The image to visualize.</p> required <code>transpose</code> <code>bool</code> <p>Whether to transpose the image. Defaults to False.</p> <code>False</code> <code>bdx</code> <code>Optional[int]</code> <p>The band index to visualize. Defaults to None.</p> <code>None</code> <code>scale_factor</code> <code>float</code> <p>The scale factor to apply to the image. Defaults to 1.0.</p> <code>1.0</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>The size of the figure. Defaults to (10, 5).</p> <code>(10, 5)</code> <code>axis_off</code> <code>bool</code> <p>Whether to turn off the axis. Defaults to True.</p> <code>True</code> <code>title</code> <code>Optional[str]</code> <p>The title of the plot. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for plt.imshow().</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def view_image(\n    image: Union[np.ndarray, torch.Tensor],\n    transpose: bool = False,\n    bdx: Optional[int] = None,\n    scale_factor: float = 1.0,\n    figsize: Tuple[int, int] = (10, 5),\n    axis_off: bool = True,\n    title: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Visualize an image using matplotlib.\n\n    Args:\n        image (Union[np.ndarray, torch.Tensor]): The image to visualize.\n        transpose (bool, optional): Whether to transpose the image. Defaults to False.\n        bdx (Optional[int], optional): The band index to visualize. Defaults to None.\n        scale_factor (float, optional): The scale factor to apply to the image. Defaults to 1.0.\n        figsize (Tuple[int, int], optional): The size of the figure. Defaults to (10, 5).\n        axis_off (bool, optional): Whether to turn off the axis. Defaults to True.\n        title (Optional[str], optional): The title of the plot. Defaults to None.\n        **kwargs (Any): Additional keyword arguments for plt.imshow().\n\n    Returns:\n        None\n    \"\"\"\n\n    if isinstance(image, torch.Tensor):\n        image = image.cpu().numpy()\n    elif isinstance(image, str):\n        image = rasterio.open(image).read().transpose(1, 2, 0)\n\n    plt.figure(figsize=figsize)\n\n    if transpose:\n        image = image.transpose(1, 2, 0)\n\n    if bdx is not None:\n        image = image[:, :, bdx]\n\n    if len(image.shape) &gt; 2 and image.shape[2] &gt; 3:\n        image = image[:, :, 0:3]\n\n    if scale_factor != 1.0:\n        image = np.clip(image * scale_factor, 0, 1)\n\n    plt.imshow(image, **kwargs)\n    if axis_off:\n        plt.axis(\"off\")\n    if title is not None:\n        plt.title(title)\n    plt.show()\n    plt.close()\n</code></pre>"},{"location":"utils/#geoai.utils.view_raster","title":"<code>view_raster(source, indexes=None, colormap=None, vmin=None, vmax=None, nodata=None, attribution=None, layer_name='Raster', layer_index=None, zoom_to_layer=True, visible=True, opacity=1.0, array_args=None, client_args={'cors_all': False}, basemap='OpenStreetMap', basemap_args=None, backend='folium', **kwargs)</code>","text":"<p>Visualize a raster using leafmap.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the raster.</p> required <code>indexes</code> <code>Optional[int]</code> <p>The band indexes to visualize. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>Optional[str]</code> <p>The colormap to apply. Defaults to None.</p> <code>None</code> <code>vmin</code> <code>Optional[float]</code> <p>The minimum value for colormap scaling. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>Optional[float]</code> <p>The maximum value for colormap scaling. Defaults to None.</p> <code>None</code> <code>nodata</code> <code>Optional[float]</code> <p>The nodata value. Defaults to None.</p> <code>None</code> <code>attribution</code> <code>Optional[str]</code> <p>The attribution for the raster. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>Optional[str]</code> <p>The name of the layer. Defaults to \"Raster\".</p> <code>'Raster'</code> <code>layer_index</code> <code>Optional[int]</code> <p>The index of the layer. Defaults to None.</p> <code>None</code> <code>zoom_to_layer</code> <code>Optional[bool]</code> <p>Whether to zoom to the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>Optional[bool]</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>opacity</code> <code>Optional[float]</code> <p>The opacity of the layer. Defaults to 1.0.</p> <code>1.0</code> <code>array_args</code> <code>Optional[Dict]</code> <p>Additional arguments for array processing. Defaults to {}.</p> <code>None</code> <code>client_args</code> <code>Optional[Dict]</code> <p>Additional arguments for the client. Defaults to {\"cors_all\": False}.</p> <code>{'cors_all': False}</code> <code>basemap</code> <code>Optional[str]</code> <p>The basemap to use. Defaults to \"OpenStreetMap\".</p> <code>'OpenStreetMap'</code> <code>basemap_args</code> <code>Optional[Dict]</code> <p>Additional arguments for the basemap. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>leafmap.Map</code> <p>The map object with the raster layer added.</p> Source code in <code>geoai/utils.py</code> <pre><code>def view_raster(\n    source: str,\n    indexes: Optional[int] = None,\n    colormap: Optional[str] = None,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n    nodata: Optional[float] = None,\n    attribution: Optional[str] = None,\n    layer_name: Optional[str] = \"Raster\",\n    layer_index: Optional[int] = None,\n    zoom_to_layer: Optional[bool] = True,\n    visible: Optional[bool] = True,\n    opacity: Optional[float] = 1.0,\n    array_args: Optional[Dict] = None,\n    client_args: Optional[Dict] = {\"cors_all\": False},\n    basemap: Optional[str] = \"OpenStreetMap\",\n    basemap_args: Optional[Dict] = None,\n    backend: Optional[str] = \"folium\",\n    **kwargs,\n):\n    \"\"\"\n    Visualize a raster using leafmap.\n\n    Args:\n        source (str): The source of the raster.\n        indexes (Optional[int], optional): The band indexes to visualize. Defaults to None.\n        colormap (Optional[str], optional): The colormap to apply. Defaults to None.\n        vmin (Optional[float], optional): The minimum value for colormap scaling. Defaults to None.\n        vmax (Optional[float], optional): The maximum value for colormap scaling. Defaults to None.\n        nodata (Optional[float], optional): The nodata value. Defaults to None.\n        attribution (Optional[str], optional): The attribution for the raster. Defaults to None.\n        layer_name (Optional[str], optional): The name of the layer. Defaults to \"Raster\".\n        layer_index (Optional[int], optional): The index of the layer. Defaults to None.\n        zoom_to_layer (Optional[bool], optional): Whether to zoom to the layer. Defaults to True.\n        visible (Optional[bool], optional): Whether the layer is visible. Defaults to True.\n        opacity (Optional[float], optional): The opacity of the layer. Defaults to 1.0.\n        array_args (Optional[Dict], optional): Additional arguments for array processing. Defaults to {}.\n        client_args (Optional[Dict], optional): Additional arguments for the client. Defaults to {\"cors_all\": False}.\n        basemap (Optional[str], optional): The basemap to use. Defaults to \"OpenStreetMap\".\n        basemap_args (Optional[Dict], optional): Additional arguments for the basemap. Defaults to None.\n        **kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        leafmap.Map: The map object with the raster layer added.\n    \"\"\"\n\n    if backend == \"folium\":\n        import leafmap.foliumap as leafmap\n    else:\n        import leafmap.leafmap as leafmap\n\n    if basemap_args is None:\n        basemap_args = {}\n\n    if array_args is None:\n        array_args = {}\n\n    m = leafmap.Map()\n\n    if isinstance(basemap, str):\n        if basemap.lower().endswith(\".tif\"):\n            if basemap.lower().startswith(\"http\"):\n                if \"name\" not in basemap_args:\n                    basemap_args[\"name\"] = \"Basemap\"\n                m.add_cog_layer(basemap, **basemap_args)\n            else:\n                if \"layer_name\" not in basemap_args:\n                    basemap_args[\"layer_name\"] = \"Basemap\"\n                m.add_raster(basemap, **basemap_args)\n    else:\n        m.add_basemap(basemap, **basemap_args)\n\n    if isinstance(source, dict):\n        source = dict_to_image(source)\n\n    if (\n        isinstance(source, str)\n        and source.lower().endswith(\".tif\")\n        and source.startswith(\"http\")\n    ):\n        if indexes is not None:\n            kwargs[\"bidx\"] = indexes\n        if colormap is not None:\n            kwargs[\"colormap_name\"] = colormap\n        if attribution is None:\n            attribution = \"TiTiler\"\n\n        m.add_cog_layer(\n            source,\n            name=layer_name,\n            opacity=opacity,\n            attribution=attribution,\n            zoom_to_layer=zoom_to_layer,\n            **kwargs,\n        )\n    else:\n        m.add_raster(\n            source=source,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            layer_index=layer_index,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            opacity=opacity,\n            array_args=array_args,\n            client_args=client_args,\n            **kwargs,\n        )\n    return m\n</code></pre>"},{"location":"utils/#geoai.utils.view_vector","title":"<code>view_vector(vector_data, column=None, cmap='viridis', figsize=(10, 10), title=None, legend=True, basemap=False, basemap_type='streets', alpha=0.7, edge_color='black', classification='quantiles', n_classes=5, highlight_index=None, highlight_color='red', scheme=None, save_path=None, dpi=300)</code>","text":"<p>Visualize vector datasets with options for styling, classification, basemaps and more.</p> <p>This function visualizes GeoDataFrame objects with customizable symbology. It supports different vector types (points, lines, polygons), attribute-based classification, and background basemaps.</p> <p>Parameters:</p> Name Type Description Default <code>vector_data</code> <code>geopandas.GeoDataFrame</code> <p>The vector dataset to visualize.</p> required <code>column</code> <code>str</code> <p>Column to use for choropleth mapping. If None, a single color will be used. Defaults to None.</p> <code>None</code> <code>cmap</code> <code>str or matplotlib.colors.Colormap</code> <p>Colormap to use for choropleth mapping. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height) in inches. Defaults to (10, 10).</p> <code>(10, 10)</code> <code>title</code> <code>str</code> <p>Title for the plot. Defaults to None.</p> <code>None</code> <code>legend</code> <code>bool</code> <p>Whether to display a legend. Defaults to True.</p> <code>True</code> <code>basemap</code> <code>bool</code> <p>Whether to add a web basemap. Requires contextily. Defaults to False.</p> <code>False</code> <code>basemap_type</code> <code>str</code> <p>Type of basemap to use. Options: 'streets', 'satellite'. Defaults to 'streets'.</p> <code>'streets'</code> <code>alpha</code> <code>float</code> <p>Transparency of the vector features, between 0-1. Defaults to 0.7.</p> <code>0.7</code> <code>edge_color</code> <code>str</code> <p>Color for feature edges. Defaults to \"black\".</p> <code>'black'</code> <code>classification</code> <code>str</code> <p>Classification method for choropleth maps. Options: \"quantiles\", \"equal_interval\", \"natural_breaks\". Defaults to \"quantiles\".</p> <code>'quantiles'</code> <code>n_classes</code> <code>int</code> <p>Number of classes for choropleth maps. Defaults to 5.</p> <code>5</code> <code>highlight_index</code> <code>list</code> <p>List of indices to highlight. Defaults to None.</p> <code>None</code> <code>highlight_color</code> <code>str</code> <p>Color to use for highlighted features. Defaults to \"red\".</p> <code>'red'</code> <code>scheme</code> <code>str</code> <p>MapClassify classification scheme. Overrides classification parameter if provided. Defaults to None.</p> <code>None</code> <code>save_path</code> <code>str</code> <p>Path to save the figure. If None, the figure is not saved. Defaults to None.</p> <code>None</code> <code>dpi</code> <code>int</code> <p>DPI for saved figure. Defaults to 300.</p> <code>300</code> <p>Returns:</p> Type Description <code>matplotlib.axes.Axes</code> <p>The Axes object containing the plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n&gt;&gt;&gt; view_vector(cities, \"population\", cmap=\"Reds\", basemap=True)\n</code></pre> <pre><code>&gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n&gt;&gt;&gt; view_vector(roads, \"type\", basemap=True, figsize=(12, 8))\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def view_vector(\n    vector_data,\n    column=None,\n    cmap=\"viridis\",\n    figsize=(10, 10),\n    title=None,\n    legend=True,\n    basemap=False,\n    basemap_type=\"streets\",\n    alpha=0.7,\n    edge_color=\"black\",\n    classification=\"quantiles\",\n    n_classes=5,\n    highlight_index=None,\n    highlight_color=\"red\",\n    scheme=None,\n    save_path=None,\n    dpi=300,\n):\n    \"\"\"\n    Visualize vector datasets with options for styling, classification, basemaps and more.\n\n    This function visualizes GeoDataFrame objects with customizable symbology.\n    It supports different vector types (points, lines, polygons), attribute-based\n    classification, and background basemaps.\n\n    Args:\n        vector_data (geopandas.GeoDataFrame): The vector dataset to visualize.\n        column (str, optional): Column to use for choropleth mapping. If None,\n            a single color will be used. Defaults to None.\n        cmap (str or matplotlib.colors.Colormap, optional): Colormap to use for\n            choropleth mapping. Defaults to \"viridis\".\n        figsize (tuple, optional): Figure size as (width, height) in inches.\n            Defaults to (10, 10).\n        title (str, optional): Title for the plot. Defaults to None.\n        legend (bool, optional): Whether to display a legend. Defaults to True.\n        basemap (bool, optional): Whether to add a web basemap. Requires contextily.\n            Defaults to False.\n        basemap_type (str, optional): Type of basemap to use. Options: 'streets', 'satellite'.\n            Defaults to 'streets'.\n        alpha (float, optional): Transparency of the vector features, between 0-1.\n            Defaults to 0.7.\n        edge_color (str, optional): Color for feature edges. Defaults to \"black\".\n        classification (str, optional): Classification method for choropleth maps.\n            Options: \"quantiles\", \"equal_interval\", \"natural_breaks\".\n            Defaults to \"quantiles\".\n        n_classes (int, optional): Number of classes for choropleth maps.\n            Defaults to 5.\n        highlight_index (list, optional): List of indices to highlight.\n            Defaults to None.\n        highlight_color (str, optional): Color to use for highlighted features.\n            Defaults to \"red\".\n        scheme (str, optional): MapClassify classification scheme. Overrides\n            classification parameter if provided. Defaults to None.\n        save_path (str, optional): Path to save the figure. If None, the figure\n            is not saved. Defaults to None.\n        dpi (int, optional): DPI for saved figure. Defaults to 300.\n\n    Returns:\n        matplotlib.axes.Axes: The Axes object containing the plot.\n\n    Examples:\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n        &gt;&gt;&gt; view_vector(cities, \"population\", cmap=\"Reds\", basemap=True)\n\n        &gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n        &gt;&gt;&gt; view_vector(roads, \"type\", basemap=True, figsize=(12, 8))\n    \"\"\"\n    import contextily as ctx\n\n    if isinstance(vector_data, str):\n        vector_data = gpd.read_file(vector_data)\n\n    # Check if input is a GeoDataFrame\n    if not isinstance(vector_data, gpd.GeoDataFrame):\n        raise TypeError(\"Input data must be a GeoDataFrame\")\n\n    # Make a copy to avoid changing the original data\n    gdf = vector_data.copy()\n\n    # Set up figure and axis\n    fig, ax = plt.subplots(figsize=figsize)\n\n    # Determine geometry type\n    geom_type = gdf.geometry.iloc[0].geom_type\n\n    # Plotting parameters\n    plot_kwargs = {\"alpha\": alpha, \"ax\": ax}\n\n    # Set up keyword arguments based on geometry type\n    if \"Point\" in geom_type:\n        plot_kwargs[\"markersize\"] = 50\n        plot_kwargs[\"edgecolor\"] = edge_color\n    elif \"Line\" in geom_type:\n        plot_kwargs[\"linewidth\"] = 1\n    elif \"Polygon\" in geom_type:\n        plot_kwargs[\"edgecolor\"] = edge_color\n\n    # Classification options\n    if column is not None:\n        if scheme is not None:\n            # Use mapclassify scheme if provided\n            plot_kwargs[\"scheme\"] = scheme\n        else:\n            # Use classification parameter\n            if classification == \"quantiles\":\n                plot_kwargs[\"scheme\"] = \"quantiles\"\n            elif classification == \"equal_interval\":\n                plot_kwargs[\"scheme\"] = \"equal_interval\"\n            elif classification == \"natural_breaks\":\n                plot_kwargs[\"scheme\"] = \"fisher_jenks\"\n\n        plot_kwargs[\"k\"] = n_classes\n        plot_kwargs[\"cmap\"] = cmap\n        plot_kwargs[\"column\"] = column\n        plot_kwargs[\"legend\"] = legend\n\n    # Plot the main data\n    gdf.plot(**plot_kwargs)\n\n    # Highlight specific features if requested\n    if highlight_index is not None:\n        gdf.iloc[highlight_index].plot(\n            ax=ax, color=highlight_color, edgecolor=\"black\", linewidth=2, zorder=5\n        )\n\n    if basemap:\n        try:\n            basemap_options = {\n                \"streets\": ctx.providers.OpenStreetMap.Mapnik,\n                \"satellite\": ctx.providers.Esri.WorldImagery,\n            }\n            ctx.add_basemap(ax, crs=gdf.crs, source=basemap_options[basemap_type])\n        except Exception as e:\n            print(f\"Could not add basemap: {e}\")\n\n    # Set title if provided\n    if title:\n        ax.set_title(title, fontsize=14)\n\n    # Remove axes if not needed\n    ax.set_axis_off()\n\n    # Adjust layout\n    plt.tight_layout()\n\n    # Save figure if a path is provided\n    if save_path:\n        plt.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n\n    return ax\n</code></pre>"},{"location":"utils/#geoai.utils.view_vector_interactive","title":"<code>view_vector_interactive(vector_data, layer_name='Vector Layer', tiles_args=None, **kwargs)</code>","text":"<p>Visualize vector datasets with options for styling, classification, basemaps and more.</p> <p>This function visualizes GeoDataFrame objects with customizable symbology. It supports different vector types (points, lines, polygons), attribute-based classification, and background basemaps.</p> <p>Parameters:</p> Name Type Description Default <code>vector_data</code> <code>geopandas.GeoDataFrame</code> <p>The vector dataset to visualize.</p> required <code>layer_name</code> <code>str</code> <p>The name of the layer. Defaults to \"Vector Layer\".</p> <code>'Vector Layer'</code> <code>tiles_args</code> <code>dict</code> <p>Additional arguments for the localtileserver client. get_folium_tile_layer function. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to GeoDataFrame.explore() function.</p> <code>{}</code> <code>See</code> <code>https</code> <p>//geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.explore.html</p> required <p>Returns:</p> Type Description <code>folium.Map</code> <p>The map object with the vector data added.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n&gt;&gt;&gt; view_vector_interactive(cities)\n</code></pre> <pre><code>&gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n&gt;&gt;&gt; view_vector_interactive(roads, figsize=(12, 8))\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def view_vector_interactive(\n    vector_data,\n    layer_name=\"Vector Layer\",\n    tiles_args=None,\n    **kwargs,\n):\n    \"\"\"\n    Visualize vector datasets with options for styling, classification, basemaps and more.\n\n    This function visualizes GeoDataFrame objects with customizable symbology.\n    It supports different vector types (points, lines, polygons), attribute-based\n    classification, and background basemaps.\n\n    Args:\n        vector_data (geopandas.GeoDataFrame): The vector dataset to visualize.\n        layer_name (str, optional): The name of the layer. Defaults to \"Vector Layer\".\n        tiles_args (dict, optional): Additional arguments for the localtileserver client.\n            get_folium_tile_layer function. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to GeoDataFrame.explore() function.\n        See https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.explore.html\n\n    Returns:\n        folium.Map: The map object with the vector data added.\n\n    Examples:\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n        &gt;&gt;&gt; view_vector_interactive(cities)\n\n        &gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n        &gt;&gt;&gt; view_vector_interactive(roads, figsize=(12, 8))\n    \"\"\"\n    import folium\n    import folium.plugins as plugins\n    from leafmap import cog_tile\n    from localtileserver import TileClient, get_folium_tile_layer\n\n    google_tiles = {\n        \"Roadmap\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=m&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Maps\",\n        },\n        \"Satellite\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=s&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Satellite\",\n        },\n        \"Terrain\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=p&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Terrain\",\n        },\n        \"Hybrid\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=y&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Hybrid\",\n        },\n    }\n\n    basemap_layer_name = None\n    raster_layer = None\n\n    if \"tiles\" in kwargs and isinstance(kwargs[\"tiles\"], str):\n        if kwargs[\"tiles\"].title() in google_tiles:\n            basemap_layer_name = google_tiles[kwargs[\"tiles\"].title()][\"name\"]\n            kwargs[\"tiles\"] = google_tiles[kwargs[\"tiles\"].title()][\"url\"]\n            kwargs[\"attr\"] = \"Google\"\n        elif kwargs[\"tiles\"].lower().endswith(\".tif\"):\n            if tiles_args is None:\n                tiles_args = {}\n            if kwargs[\"tiles\"].lower().startswith(\"http\"):\n                basemap_layer_name = \"Remote Raster\"\n                kwargs[\"tiles\"] = cog_tile(kwargs[\"tiles\"], **tiles_args)\n                kwargs[\"attr\"] = \"TiTiler\"\n            else:\n                basemap_layer_name = \"Local Raster\"\n                client = TileClient(kwargs[\"tiles\"])\n                raster_layer = get_folium_tile_layer(client, **tiles_args)\n                kwargs[\"tiles\"] = raster_layer.tiles\n                kwargs[\"attr\"] = \"localtileserver\"\n\n    if \"max_zoom\" not in kwargs:\n        kwargs[\"max_zoom\"] = 30\n\n    if isinstance(vector_data, str):\n        if vector_data.endswith(\".parquet\"):\n            vector_data = gpd.read_parquet(vector_data)\n        else:\n            vector_data = gpd.read_file(vector_data)\n\n    # Check if input is a GeoDataFrame\n    if not isinstance(vector_data, gpd.GeoDataFrame):\n        raise TypeError(\"Input data must be a GeoDataFrame\")\n\n    layer_control = kwargs.pop(\"layer_control\", True)\n    fullscreen_control = kwargs.pop(\"fullscreen_control\", True)\n\n    m = vector_data.explore(**kwargs)\n\n    # Change the layer name\n    for layer in m._children.values():\n        if isinstance(layer, folium.GeoJson):\n            layer.layer_name = layer_name\n        if isinstance(layer, folium.TileLayer) and basemap_layer_name:\n            layer.layer_name = basemap_layer_name\n\n    if layer_control:\n        m.add_child(folium.LayerControl())\n\n    if fullscreen_control:\n        plugins.Fullscreen().add_to(m)\n\n    return m\n</code></pre>"},{"location":"utils/#geoai.utils.visualize_vector_by_attribute","title":"<code>visualize_vector_by_attribute(vector_path, attribute_name, cmap='viridis', figsize=(10, 8))</code>","text":"<p>Create a thematic map visualization of vector data based on an attribute.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <code>attribute_name</code> <code>str</code> <p>Name of the attribute to visualize</p> required <code>cmap</code> <code>str</code> <p>Matplotlib colormap name. Defaults to 'viridis'.</p> <code>'viridis'</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (10, 8).</p> <code>(10, 8)</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if visualization was successful, False otherwise</p> Source code in <code>geoai/utils.py</code> <pre><code>def visualize_vector_by_attribute(\n    vector_path, attribute_name, cmap=\"viridis\", figsize=(10, 8)\n):\n    \"\"\"Create a thematic map visualization of vector data based on an attribute.\n\n    Args:\n        vector_path (str): Path to the vector file\n        attribute_name (str): Name of the attribute to visualize\n        cmap (str, optional): Matplotlib colormap name. Defaults to 'viridis'.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n\n    Returns:\n        bool: True if visualization was successful, False otherwise\n    \"\"\"\n    try:\n        # Read the vector data\n        gdf = gpd.read_file(vector_path)\n\n        # Check if attribute exists\n        if attribute_name not in gdf.columns:\n            print(f\"Attribute '{attribute_name}' not found in the dataset\")\n            return False\n\n        # Create the plot\n        fig, ax = plt.subplots(figsize=figsize)\n\n        # Determine plot type based on data type\n        if pd.api.types.is_numeric_dtype(gdf[attribute_name]):\n            # Continuous data\n            gdf.plot(column=attribute_name, cmap=cmap, legend=True, ax=ax)\n        else:\n            # Categorical data\n            gdf.plot(column=attribute_name, categorical=True, legend=True, ax=ax)\n\n        # Add title and labels\n        ax.set_title(f\"{os.path.basename(vector_path)} - {attribute_name}\")\n        ax.set_xlabel(\"Longitude\")\n        ax.set_ylabel(\"Latitude\")\n\n        # Add basemap or additional elements if available\n        # Note: Additional options could be added here for more complex maps\n\n        plt.tight_layout()\n        plt.show()\n\n    except Exception as e:\n        print(f\"Error visualizing data: {str(e)}\")\n</code></pre>"},{"location":"examples/_template/","title":"template","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/_template/#template","title":"Template\u00b6","text":""},{"location":"examples/_template/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/_template/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/_template/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/_template/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"examples/_template/#initialize-model","title":"Initialize model\u00b6","text":""},{"location":"examples/_template/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/_template/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/building_footprints_africa/","title":"Building footprints africa","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/buildings_africa.tif\"\n</pre> raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/buildings_africa.tif\" In\u00a0[\u00a0]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) In\u00a0[\u00a0]: Copied! <pre>extractor = geoai.BuildingFootprintExtractor(model_path=\"building_footprints_usa.pth\")\n</pre> extractor = geoai.BuildingFootprintExtractor(model_path=\"building_footprints_usa.pth\") In\u00a0[\u00a0]: Copied! <pre>masks_path = extractor.generate_masks(\n    raster_path,\n    output_dir=\"building_masks.tif\",\n    min_object_area=1000,\n    confidence_threshold=0.5,\n    threshold=0.5,\n)\n</pre> masks_path = extractor.generate_masks(     raster_path,     output_dir=\"building_masks.tif\",     min_object_area=1000,     confidence_threshold=0.5,     threshold=0.5, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(masks_path, opacity=0.7, colormap=\"tab20\", basemap=raster_url)\n</pre> geoai.view_raster(masks_path, opacity=0.7, colormap=\"tab20\", basemap=raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.orthogonalize(\n    input_path=masks_path, output_path=\"building_footprints.geojson\", epsilon=1.0\n)\n</pre> gdf = geoai.orthogonalize(     input_path=masks_path, output_path=\"building_footprints.geojson\", epsilon=1.0 ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.add_geometric_properties(gdf)\n</pre> gdf = geoai.add_geometric_properties(gdf) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf, style_kwds={\"color\": \"red\", \"fillOpacity\": 0.2}, tiles=raster_url\n)\n</pre> geoai.view_vector_interactive(     gdf, style_kwds={\"color\": \"red\", \"fillOpacity\": 0.2}, tiles=raster_url )"},{"location":"examples/building_footprints_africa/#building-footprint-extraction-for-africa","title":"Building Footprint Extraction for Africa\u00b6","text":""},{"location":"examples/building_footprints_africa/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/building_footprints_africa/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/building_footprints_africa/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/building_footprints_africa/#initialize-the-model","title":"Initialize the model\u00b6","text":""},{"location":"examples/building_footprints_africa/#extract-building-footprints","title":"Extract building footprints\u00b6","text":""},{"location":"examples/building_footprints_africa/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/building_footprints_africa/#add-geometric-attributes","title":"Add geometric attributes\u00b6","text":""},{"location":"examples/building_footprints_africa/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/building_footprints_china/","title":"Building footprints china","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/buildings_china.tif\"\n)\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/buildings_china.tif\" ) In\u00a0[\u00a0]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) In\u00a0[\u00a0]: Copied! <pre>extractor = geoai.BuildingFootprintExtractor(model_path=\"building_footprints_usa.pth\")\n</pre> extractor = geoai.BuildingFootprintExtractor(model_path=\"building_footprints_usa.pth\") In\u00a0[\u00a0]: Copied! <pre>masks_path = extractor.generate_masks(\n    raster_path,\n    output_dir=\"building_masks.tif\",\n    min_object_area=1000,\n    confidence_threshold=0.5,\n    mask_threshold=0.6,\n    threshold=0.5,\n)\n</pre> masks_path = extractor.generate_masks(     raster_path,     output_dir=\"building_masks.tif\",     min_object_area=1000,     confidence_threshold=0.5,     mask_threshold=0.6,     threshold=0.5, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(masks_path, opacity=0.7, colormap=\"tab20\", basemap=raster_path)\n</pre> geoai.view_raster(masks_path, opacity=0.7, colormap=\"tab20\", basemap=raster_path) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.orthogonalize(\n    input_path=masks_path, output_path=\"building_footprints.geojson\", epsilon=1.0\n)\n</pre> gdf = geoai.orthogonalize(     input_path=masks_path, output_path=\"building_footprints.geojson\", epsilon=1.0 ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.add_geometric_properties(gdf)\n</pre> gdf = geoai.add_geometric_properties(gdf) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf, style_kwds={\"color\": \"red\", \"fillOpacity\": 0.4}, tiles=raster_url\n)\n</pre> geoai.view_vector_interactive(     gdf, style_kwds={\"color\": \"red\", \"fillOpacity\": 0.4}, tiles=raster_url )"},{"location":"examples/building_footprints_china/#building-footprint-extraction-for-china","title":"Building Footprint Extraction for China\u00b6","text":""},{"location":"examples/building_footprints_china/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/building_footprints_china/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/building_footprints_china/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/building_footprints_china/#initialize-the-model","title":"Initialize the model\u00b6","text":""},{"location":"examples/building_footprints_china/#extract-building-footprints","title":"Extract building footprints\u00b6","text":""},{"location":"examples/building_footprints_china/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/building_footprints_china/#add-geometric-attributes","title":"Add geometric attributes\u00b6","text":""},{"location":"examples/building_footprints_china/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/building_footprints_usa/","title":"Building footprints usa","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\nvector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" In\u00a0[\u00a0]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) In\u00a0[\u00a0]: Copied! <pre>vector_path = geoai.download_file(vector_url)\n</pre> vector_path = geoai.download_file(vector_url) In\u00a0[\u00a0]: Copied! <pre>extractor = geoai.BuildingFootprintExtractor()\n</pre> extractor = geoai.BuildingFootprintExtractor() In\u00a0[\u00a0]: Copied! <pre>mask_path = extractor.save_masks_as_geotiff(\n    raster_path=raster_path,\n    output_path=\"building_masks.tif\",\n    confidence_threshold=0.5,\n    mask_threshold=0.5,\n)\n</pre> mask_path = extractor.save_masks_as_geotiff(     raster_path=raster_path,     output_path=\"building_masks.tif\",     confidence_threshold=0.5,     mask_threshold=0.5, ) <p>Convert raster to vector</p> In\u00a0[\u00a0]: Copied! <pre>gdf = extractor.masks_to_vector(\n    mask_path=mask_path,\n    output_path=\"building_masks.geojson\",\n    simplify_tolerance=1.0,\n)\n</pre> gdf = extractor.masks_to_vector(     mask_path=mask_path,     output_path=\"building_masks.geojson\",     simplify_tolerance=1.0, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"naip_buildings.geojson\"\ngdf = extractor.process_raster(\n    raster_path,\n    output_path=\"buildings.geojson\",\n    batch_size=4,\n    confidence_threshold=0.5,\n    overlap=0.25,\n    nms_iou_threshold=0.5,\n    min_object_area=100,\n    max_object_area=None,\n    mask_threshold=0.5,\n    simplify_tolerance=1.0,\n)\n</pre> output_path = \"naip_buildings.geojson\" gdf = extractor.process_raster(     raster_path,     output_path=\"buildings.geojson\",     batch_size=4,     confidence_threshold=0.5,     overlap=0.25,     nms_iou_threshold=0.5,     min_object_area=100,     max_object_area=None,     mask_threshold=0.5,     simplify_tolerance=1.0, ) In\u00a0[\u00a0]: Copied! <pre>gdf_regularized = extractor.regularize_buildings(\n    gdf=gdf,\n    min_area=100,\n    angle_threshold=15,\n    orthogonality_threshold=0.3,\n    rectangularity_threshold=0.7,\n)\n</pre> gdf_regularized = extractor.regularize_buildings(     gdf=gdf,     min_area=100,     angle_threshold=15,     orthogonality_threshold=0.3,     rectangularity_threshold=0.7, ) In\u00a0[\u00a0]: Copied! <pre>gdf.head()\n</pre> gdf.head() In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf, column=\"confidence\", layer_name=\"Building\", tiles=\"Satellite\"\n)\n</pre> geoai.view_vector_interactive(     gdf, column=\"confidence\", layer_name=\"Building\", tiles=\"Satellite\" ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf, column=\"confidence\", layer_name=\"Building\", tiles=raster_url\n)\n</pre> geoai.view_vector_interactive(     gdf, column=\"confidence\", layer_name=\"Building\", tiles=raster_url ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf_regularized, column=\"confidence\", layer_name=\"Building\", tiles=raster_url\n)\n</pre> geoai.view_vector_interactive(     gdf_regularized, column=\"confidence\", layer_name=\"Building\", tiles=raster_url ) In\u00a0[\u00a0]: Copied! <pre>extractor.visualize_results(raster_path, gdf, output_path=\"naip_buildings.png\")\n</pre> extractor.visualize_results(raster_path, gdf, output_path=\"naip_buildings.png\") In\u00a0[\u00a0]: Copied! <pre>extractor.visualize_results(\n    raster_path, gdf_regularized, output_path=\"naip_buildings_regularized.png\"\n)\n</pre> extractor.visualize_results(     raster_path, gdf_regularized, output_path=\"naip_buildings_regularized.png\" )"},{"location":"examples/building_footprints_usa/#building-footprint-extraction-for-the-usa","title":"Building Footprint Extraction for the USA\u00b6","text":""},{"location":"examples/building_footprints_usa/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/building_footprints_usa/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/building_footprints_usa/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/building_footprints_usa/#initialize-building-footprint-extraction-pretrained-model","title":"Initialize building footprint extraction pretrained model\u00b6","text":""},{"location":"examples/building_footprints_usa/#extract-building-footprints","title":"Extract building footprints\u00b6","text":""},{"location":"examples/building_footprints_usa/#option-1-extract-building-footprints-as-raster","title":"Option 1: Extract building footprints as raster\u00b6","text":""},{"location":"examples/building_footprints_usa/#option-2-extract-building-footprints-as-vector","title":"Option 2: Extract building footprints as vector\u00b6","text":""},{"location":"examples/building_footprints_usa/#regularize-building-footprints","title":"Regularize building footprints\u00b6","text":""},{"location":"examples/building_footprints_usa/#visualize-building-footprints","title":"Visualize building footprints\u00b6","text":""},{"location":"examples/building_regularization/","title":"Building regularization","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>naip_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\nmasks_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_building_masks.tif\"\n</pre> naip_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) masks_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_building_masks.tif\" In\u00a0[\u00a0]: Copied! <pre>masks_path = geoai.download_file(masks_url)\nnaip_path = geoai.download_file(naip_url)\n</pre> masks_path = geoai.download_file(masks_url) naip_path = geoai.download_file(naip_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(masks_path, figsize=(18, 10))\n</pre> geoai.view_image(masks_path, figsize=(18, 10)) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=masks_url,\n    right_layer=naip_url,\n    left_label=\"Building Masks\",\n    right_label=\"NAIP Imagery\",\n    left_args={\n        \"colormap\": {\"255\": \"#0000ff\"},\n        \"opacity\": 0.5,\n    },\n    basemap=naip_url,\n)\n</pre> geoai.create_split_map(     left_layer=masks_url,     right_layer=naip_url,     left_label=\"Building Masks\",     right_label=\"NAIP Imagery\",     left_args={         \"colormap\": {\"255\": \"#0000ff\"},         \"opacity\": 0.5,     },     basemap=naip_url, ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.raster_to_vector(masks_path, output_path=\"naip_building_masks.geojson\")\n</pre> gdf = geoai.raster_to_vector(masks_path, output_path=\"naip_building_masks.geojson\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf, style_kwds={\"color\": \"blue\", \"fillOpacity\": 0.4}, tiles=naip_url\n)\n</pre> geoai.view_vector_interactive(     gdf, style_kwds={\"color\": \"blue\", \"fillOpacity\": 0.4}, tiles=naip_url ) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf,\n    right_layer=naip_url,\n    left_label=\"Buildings\",\n    right_label=\"NAIP Imagery\",\n    left_args={\"style\": {\"color\": \"blue\"}, \"fillOpacity\": 0.4},\n    basemap=naip_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf,     right_layer=naip_url,     left_label=\"Buildings\",     right_label=\"NAIP Imagery\",     left_args={\"style\": {\"color\": \"blue\"}, \"fillOpacity\": 0.4},     basemap=naip_url, ) In\u00a0[\u00a0]: Copied! <pre>gdf_regularized = geoai.regularization(\n    building_polygons=gdf,\n    angle_tolerance=10,\n    simplify_tolerance=0.5,\n    orthogonalize=True,\n    preserve_topology=True,\n)\n</pre> gdf_regularized = geoai.regularization(     building_polygons=gdf,     angle_tolerance=10,     simplify_tolerance=0.5,     orthogonalize=True,     preserve_topology=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf_regularized, style_kwds={\"color\": \"red\", \"fillOpacity\": 0.4}, tiles=naip_url\n)\n</pre> geoai.view_vector_interactive(     gdf_regularized, style_kwds={\"color\": \"red\", \"fillOpacity\": 0.4}, tiles=naip_url ) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_regularized,\n    right_layer=naip_url,\n    left_label=\"Regularized Buildings\",\n    right_label=\"NAIP Imagery\",\n    left_args={\"style\": {\"color\": \"red\"}, \"fillOpacity\": 0.4},\n    basemap=naip_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_regularized,     right_layer=naip_url,     left_label=\"Regularized Buildings\",     right_label=\"NAIP Imagery\",     left_args={\"style\": {\"color\": \"red\"}, \"fillOpacity\": 0.4},     basemap=naip_url, ) In\u00a0[\u00a0]: Copied! <pre>gdf_hybrid = geoai.hybrid_regularization(gdf)\n</pre> gdf_hybrid = geoai.hybrid_regularization(gdf) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf_hybrid, style_kwds={\"color\": \"green\", \"fillOpacity\": 0.4}, tiles=naip_url\n)\n</pre> geoai.view_vector_interactive(     gdf_hybrid, style_kwds={\"color\": \"green\", \"fillOpacity\": 0.4}, tiles=naip_url ) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_regularized,\n    right_layer=naip_url,\n    left_label=\"Regularized Buildings\",\n    right_label=\"NAIP Imagery\",\n    left_args={\"style\": {\"color\": \"green\", \"fillOpacity\": 0.4}},\n    basemap=naip_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_regularized,     right_layer=naip_url,     left_label=\"Regularized Buildings\",     right_label=\"NAIP Imagery\",     left_args={\"style\": {\"color\": \"green\", \"fillOpacity\": 0.4}},     basemap=naip_url, ) In\u00a0[\u00a0]: Copied! <pre>gdf_adaptive = geoai.adaptive_regularization(\n    building_polygons=gdf,\n    simplify_tolerance=0.5,\n    area_threshold=0.9,\n    preserve_shape=True,\n)\n</pre> gdf_adaptive = geoai.adaptive_regularization(     building_polygons=gdf,     simplify_tolerance=0.5,     area_threshold=0.9,     preserve_shape=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf_adaptive, style_kwds={\"color\": \"yellow\", \"fillOpacity\": 0.4}, tiles=naip_url\n)\n</pre> geoai.view_vector_interactive(     gdf_adaptive, style_kwds={\"color\": \"yellow\", \"fillOpacity\": 0.4}, tiles=naip_url ) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_adaptive,\n    right_layer=naip_url,\n    left_label=\"Adaptive Regularization Buildings\",\n    right_label=\"NAIP Imagery\",\n    left_args={\"style\": {\"color\": \"yellow\", \"fillOpacity\": 0.4}},\n    basemap=naip_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_adaptive,     right_layer=naip_url,     left_label=\"Adaptive Regularization Buildings\",     right_label=\"NAIP Imagery\",     left_args={\"style\": {\"color\": \"yellow\", \"fillOpacity\": 0.4}},     basemap=naip_url, ) In\u00a0[\u00a0]: Copied! <pre>import leafmap.foliumap as leafmap\n</pre> import leafmap.foliumap as leafmap In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_basemap(\"SATELLITE\")\nm.add_gdf(gdf, layer_name=\"Original\")\nm.add_gdf(\n    gdf_regularized, style={\"color\": \"red\", \"fillOpacity\": 0}, layer_name=\"Regularized\"\n)\nm.add_gdf(gdf_hybrid, style={\"color\": \"green\", \"fillOpacity\": 0}, layer_name=\"Hybrid\")\nm.add_gdf(\n    gdf_adaptive, style={\"color\": \"yellow\", \"fillOpacity\": 0}, layer_name=\"Adaptive\"\n)\nlegend = {\n    \"Original\": \"blue\",\n    \"Regularized\": \"red\",\n    \"Hybrid\": \"green\",\n    \"Adaptive\": \"yellow\",\n}\nm.add_legend(title=\"Building Footprints\", legend_dict=legend)\nm\n</pre> m = leafmap.Map() m.add_basemap(\"SATELLITE\") m.add_gdf(gdf, layer_name=\"Original\") m.add_gdf(     gdf_regularized, style={\"color\": \"red\", \"fillOpacity\": 0}, layer_name=\"Regularized\" ) m.add_gdf(gdf_hybrid, style={\"color\": \"green\", \"fillOpacity\": 0}, layer_name=\"Hybrid\") m.add_gdf(     gdf_adaptive, style={\"color\": \"yellow\", \"fillOpacity\": 0}, layer_name=\"Adaptive\" ) legend = {     \"Original\": \"blue\",     \"Regularized\": \"red\",     \"Hybrid\": \"green\",     \"Adaptive\": \"yellow\", } m.add_legend(title=\"Building Footprints\", legend_dict=legend) m"},{"location":"examples/building_regularization/#building-regularization","title":"Building Regularization\u00b6","text":""},{"location":"examples/building_regularization/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/building_regularization/#import-package","title":"Import package\u00b6","text":""},{"location":"examples/building_regularization/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/building_regularization/#convert-raster-to-vector","title":"Convert raster to vector\u00b6","text":""},{"location":"examples/building_regularization/#building-regularization","title":"Building regularization\u00b6","text":""},{"location":"examples/building_regularization/#hybrid-regularization","title":"Hybrid regularization\u00b6","text":""},{"location":"examples/building_regularization/#adaptive-regularization","title":"Adaptive regularization\u00b6","text":""},{"location":"examples/building_regularization/#compare-regularization-methods","title":"Compare regularization methods\u00b6","text":""},{"location":"examples/car_detection/","title":"Car detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/cars_7cm.tif\"\n)\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/cars_7cm.tif\" ) In\u00a0[\u00a0]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(raster_url)\n</pre> geoai.view_raster(raster_url) In\u00a0[\u00a0]: Copied! <pre>detector = geoai.CarDetector()\n</pre> detector = geoai.CarDetector() In\u00a0[\u00a0]: Copied! <pre>mask_path = detector.generate_masks(\n    raster_path=raster_path,\n    output_path=\"cars_masks.tif\",\n    confidence_threshold=0.3,\n    mask_threshold=0.5,\n    overlap=0.25,\n    chip_size=(400, 400),\n)\n</pre> mask_path = detector.generate_masks(     raster_path=raster_path,     output_path=\"cars_masks.tif\",     confidence_threshold=0.3,     mask_threshold=0.5,     overlap=0.25,     chip_size=(400, 400), ) <p>Convert the image masks to polygons and save the output GeoJSON file.</p> In\u00a0[\u00a0]: Copied! <pre>gdf = detector.vectorize_masks(\n    masks_path=\"cars_masks.tif\",\n    output_path=\"cars.geojson\",\n    min_object_area=100,\n    max_object_area=2000,\n)\n</pre> gdf = detector.vectorize_masks(     masks_path=\"cars_masks.tif\",     output_path=\"cars.geojson\",     min_object_area=100,     max_object_area=2000, ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.add_geometric_properties(gdf)\n</pre> gdf = geoai.add_geometric_properties(gdf) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf_filter = gdf[\n    (gdf[\"area_m2\"] &gt; 8) &amp; (gdf[\"area_m2\"] &lt; 60) &amp; (gdf[\"minor_length_m\"] &gt; 1)\n]\n</pre> gdf_filter = gdf[     (gdf[\"area_m2\"] &gt; 8) &amp; (gdf[\"area_m2\"] &lt; 60) &amp; (gdf[\"minor_length_m\"] &gt; 1) ] In\u00a0[\u00a0]: Copied! <pre>len(gdf_filter)\n</pre> len(gdf_filter) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filter, column=\"confidence\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf_filter, column=\"confidence\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filter, tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf_filter, tiles=raster_url) <p></p>"},{"location":"examples/car_detection/#car-detection","title":"Car Detection\u00b6","text":""},{"location":"examples/car_detection/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/car_detection/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/car_detection/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We will download a sample image from Hugging Face Hub to use for car detection. You can find more high-resolution images from OpenAerialMap.</p>"},{"location":"examples/car_detection/#visualize-the-image","title":"Visualize the image\u00b6","text":""},{"location":"examples/car_detection/#initialize-the-model","title":"Initialize the model\u00b6","text":""},{"location":"examples/car_detection/#extract-cars","title":"Extract cars\u00b6","text":"<p>Extract cars from the image using the model and save the output image.</p>"},{"location":"examples/car_detection/#add-geometric-properties","title":"Add geometric properties\u00b6","text":""},{"location":"examples/car_detection/#visualize-initial-results","title":"Visualize initial results\u00b6","text":""},{"location":"examples/car_detection/#filter-cars-by-area","title":"Filter cars by area\u00b6","text":""},{"location":"examples/car_detection/#visualiza-final-results","title":"Visualiza final results\u00b6","text":""},{"location":"examples/data_visualization/","title":"Data visualization","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>from torchgeo.datasets import NAIP\nfrom torchgeo.samplers import RandomGeoSampler, GridGeoSampler\nfrom geoai.utils import view_image, view_raster, dict_to_image\nfrom geoai.download import download_naip\n</pre> from torchgeo.datasets import NAIP from torchgeo.samplers import RandomGeoSampler, GridGeoSampler from geoai.utils import view_image, view_raster, dict_to_image from geoai.download import download_naip In\u00a0[\u00a0]: Copied! <pre>root = \"naip_data\"\n</pre> root = \"naip_data\" In\u00a0[\u00a0]: Copied! <pre>bbox = (-117.6029, 47.65, -117.5936, 47.6563)\ndownloaded_files = download_naip(\n    bbox=bbox,\n    output_dir=root,\n    max_items=1,\n)\n</pre> bbox = (-117.6029, 47.65, -117.5936, 47.6563) downloaded_files = download_naip(     bbox=bbox,     output_dir=root,     max_items=1, ) <ul> <li>torchgeo.datasets.NAIP: Provides access to the National Agriculture Imagery Program (NAIP) dataset, which offers high-resolution aerial imagery across the United States.</li> <li>torchgeo.samplers: Contains sampling strategies for geospatial data:<ul> <li>RandomGeoSampler: Samples random patches from the dataset</li> <li>GridGeoSampler: Samples patches in a grid pattern with specified stride</li> </ul> </li> <li>geoai.utils: Custom utility functions for visualization:<ul> <li>view_image: Visualizes tensor images</li> <li>view_raster: Displays georeferenced data on an interactive map</li> <li>dict_to_image: Converts dictionary representation to image format</li> </ul> </li> </ul> <p>Load the NAIP dataset from the specified root directory:</p> In\u00a0[\u00a0]: Copied! <pre>dataset = NAIP(root)\n</pre> dataset = NAIP(root) <p>Examine the dataset object to understand its properties:</p> In\u00a0[\u00a0]: Copied! <pre>dataset\n</pre> dataset <p>This will display information about the NAIP dataset including available imagery dates, coverage area, and other metadata.</p> <p>Check the Coordinate Reference System (CRS) used by the dataset:</p> In\u00a0[\u00a0]: Copied! <pre>dataset.crs\n</pre> dataset.crs <p>The CRS defines how the geospatial data is projected onto a coordinate system, which is essential for accurate visualization and analysis.</p> In\u00a0[\u00a0]: Copied! <pre>train_sampler = RandomGeoSampler(dataset, size=256, length=1000)\n</pre> train_sampler = RandomGeoSampler(dataset, size=256, length=1000) <p>This creates a sampler that will randomly select 1000 patches, each 256x256 pixels in size. This sampling strategy is commonly used for training machine learning models where you need a diverse set of examples.</p> <p>Extract a bounding box from the random sampler:</p> In\u00a0[\u00a0]: Copied! <pre>train_bbox = next(iter(train_sampler))\ntrain_bbox\n</pre> train_bbox = next(iter(train_sampler)) train_bbox <p>The bounding box contains the coordinates defining the spatial extent of our randomly sampled patch.</p> <p>Load the actual image data corresponding to the randomly selected bounding box:</p> In\u00a0[\u00a0]: Copied! <pre>train_image = dataset[next(iter(train_sampler))][\"image\"]\n</pre> train_image = dataset[next(iter(train_sampler))][\"image\"] <p>Examine the complete data dictionary returned for a sample, which includes both the image and metadata:</p> In\u00a0[\u00a0]: Copied! <pre>dataset[next(iter(train_sampler))]\n</pre> dataset[next(iter(train_sampler))] <p>This returns a dictionary containing the image tensor and associated metadata such as the bounding box, CRS, and other properties.</p> In\u00a0[\u00a0]: Copied! <pre>view_image(\n    train_image, transpose=True, scale_factor=(1 / 250), title=\"Random GeoSampler\"\n)\n</pre> view_image(     train_image, transpose=True, scale_factor=(1 / 250), title=\"Random GeoSampler\" ) <ul> <li>transpose=True: Rearranges the dimensions for proper display (from [C,H,W] to [H,W,C])</li> <li>scale_factor=(1/250): Scales the pixel values for better visualization</li> <li>title=\"Random GeoSampler\": Adds a descriptive title to the plot</li> </ul> In\u00a0[\u00a0]: Copied! <pre>test_sampler = GridGeoSampler(dataset, size=256, stride=128)\n</pre> test_sampler = GridGeoSampler(dataset, size=256, stride=128) <p>This sampler extracts 256x256 pixel patches in a grid pattern with a stride of 128 pixels, meaning patches will overlap by 128 pixels. Grid sampling is typically used for inference or testing, where systematic coverage of the area is important.</p> <p>Extract a bounding box from the grid sampler:</p> In\u00a0[\u00a0]: Copied! <pre>test_bbox = next(iter(test_sampler))\ntest_bbox\n</pre> test_bbox = next(iter(test_sampler)) test_bbox <p>Load the image data for a patch selected by the grid sampler:</p> In\u00a0[\u00a0]: Copied! <pre>test_image = dataset[next(iter(test_sampler))][\"image\"]\n</pre> test_image = dataset[next(iter(test_sampler))][\"image\"] In\u00a0[\u00a0]: Copied! <pre>view_image(test_image, transpose=True, scale_factor=(1 / 250), title=\"Grid GeoSampler\")\n</pre> view_image(test_image, transpose=True, scale_factor=(1 / 250), title=\"Grid GeoSampler\") <p>The visualization shows a systematically sampled patch from the dataset.</p> In\u00a0[\u00a0]: Copied! <pre>data = dataset[next(iter(test_sampler))]\n</pre> data = dataset[next(iter(test_sampler))] <p>Visualize the raster data on an interactive map with Google Satellite imagery as the background:</p> In\u00a0[\u00a0]: Copied! <pre>view_raster(data, basemap=\"Google Satellite\")\n</pre> view_raster(data, basemap=\"Google Satellite\") <p>This interactive visualization places the sampled data in its real-world geographic context, allowing you to see how it aligns with the Google Satellite imagery.</p>"},{"location":"examples/data_visualization/#data-visualization","title":"Data visualization\u00b6","text":"<p>This notebook demonstrates how to work with geospatial imagery data using TorchGeo and GeoAI. We'll explore how to load data, sample it using different strategies, and visualize the results.</p>"},{"location":"examples/data_visualization/#install-package","title":"Install Package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/data_visualization/#importing-required-libraries","title":"Importing Required Libraries\u00b6","text":"<p>First, we import the necessary libraries for our geospatial data visualization workflow:</p>"},{"location":"examples/data_visualization/#download-naip-imagery","title":"Download NAIP imagery\u00b6","text":""},{"location":"examples/data_visualization/#setting-up-the-dataset","title":"Setting Up the Dataset\u00b6","text":""},{"location":"examples/data_visualization/#random-sampling-of-geospatial-data","title":"Random Sampling of Geospatial Data\u00b6","text":"<p>Create a random sampler to extract patches from the dataset:</p>"},{"location":"examples/data_visualization/#visualizing-randomly-sampled-data","title":"Visualizing Randomly Sampled Data\u00b6","text":"<p>Display the randomly sampled image:</p>"},{"location":"examples/data_visualization/#grid-sampling-of-geospatial-data","title":"Grid Sampling of Geospatial Data\u00b6","text":"<p>Create a grid sampler to extract patches in a systematic pattern:</p>"},{"location":"examples/data_visualization/#visualizing-grid-sampled-data","title":"Visualizing Grid Sampled Data\u00b6","text":"<p>Display the image from the grid sampler:</p>"},{"location":"examples/data_visualization/#advanced-visualization-with-geospatial-context","title":"Advanced Visualization with Geospatial Context\u00b6","text":"<p>Load a complete data sample including all metadata:</p>"},{"location":"examples/data_visualization/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ol> <li>TorchGeo provides a flexible framework for working with geospatial datasets like NAIP.</li> <li>Different sampling strategies (random vs. grid) serve different purposes in geospatial machine learning workflows.</li> <li>Visualization tools help understand the data in both pixel space (view_image) and geographic space (view_raster).</li> <li>Working with geospatial data requires attention to coordinate reference systems (CRS) and proper handling of georeferenced data.</li> </ol>"},{"location":"examples/download_data/","title":"Download data","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom geoai.download import (\n    download_naip,\n    download_overture_buildings,\n    extract_building_stats,\n)\n</pre> import leafmap from geoai.download import (     download_naip,     download_overture_buildings,     extract_building_stats, ) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[47.6526, -117.5923], zoom=16)\nm.add_basemap(\"Google Satellite\")\nm\n</pre> m = leafmap.Map(center=[47.6526, -117.5923], zoom=16) m.add_basemap(\"Google Satellite\") m <p>Use the drawing tools to draw a rectangle on the map. If no rectangle is drawn, the default ROI will be used.</p> In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = (-117.6029, 47.65, -117.5936, 47.6563)\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = (-117.6029, 47.65, -117.5936, 47.6563) In\u00a0[\u00a0]: Copied! <pre># Download NAIP imagery for the specified region\ndownloaded_files = download_naip(\n    bbox=bbox,\n    output_dir=\"naip_data\",\n    max_items=1,\n    # year=2020,\n)\n\nprint(f\"Downloaded {len(downloaded_files)} files.\")\n</pre> # Download NAIP imagery for the specified region downloaded_files = download_naip(     bbox=bbox,     output_dir=\"naip_data\",     max_items=1,     # year=2020, )  print(f\"Downloaded {len(downloaded_files)} files.\") In\u00a0[\u00a0]: Copied! <pre># Download buildings\ndata_file = download_overture_buildings(\n    bbox=bbox,\n    output=\"buildings.geojson\",\n)\n</pre> # Download buildings data_file = download_overture_buildings(     bbox=bbox,     output=\"buildings.geojson\", ) In\u00a0[\u00a0]: Copied! <pre>stats = extract_building_stats(data_file)\nprint(stats)\n</pre> stats = extract_building_stats(data_file) print(stats) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"naip_data/m_4711720_sw_11_060_20230701_20230911.tif\", layer_name=\"NAIP\")\nm.add_geojson(\"buildings.geojson\", layer_name=\"Buildings\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"naip_data/m_4711720_sw_11_060_20230701_20230911.tif\", layer_name=\"NAIP\") m.add_geojson(\"buildings.geojson\", layer_name=\"Buildings\") m <p></p>"},{"location":"examples/download_data/#download-data","title":"Download Data\u00b6","text":""},{"location":"examples/download_data/#install-package","title":"Install Package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/download_data/#import-libraries","title":"Import Libraries\u00b6","text":"<p>These modules allow downloading NAIP imagery and extracting building data statistics.</p>"},{"location":"examples/download_data/#define-bounding-box","title":"Define Bounding Box\u00b6","text":"<p>Define the geographic extent (longitude and latitude) for data downloads.</p>"},{"location":"examples/download_data/#download-naip-imagery","title":"Download NAIP Imagery\u00b6","text":"<p>Fetch NAIP aerial imagery for the specified bounding box. The <code>max_items</code> parameter limits the number of downloaded files.</p>"},{"location":"examples/download_data/#download-building-data","title":"Download Building Data\u00b6","text":"<p>Retrieve building footprint data in GeoJSON format within the bounding box. The <code>verbose</code> flag provides detailed output.</p>"},{"location":"examples/download_data/#extract-building-statistics","title":"Extract Building Statistics\u00b6","text":"<p>If the building data file is successfully downloaded, extract and display relevant statistics such as area, count, and footprint details.</p>"},{"location":"examples/download_data/#visualize-datasets","title":"Visualize Datasets\u00b6","text":""},{"location":"examples/download_sentinel2/","title":"Download sentinel2","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre># Example for Sentinel-2 item\nitem_url = \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/sentinel-2-l2a/items/S2B_MSIL2A_20250228T173149_R055_T14SLH_20250228T212633\"\n</pre> # Example for Sentinel-2 item item_url = \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/sentinel-2-l2a/items/S2B_MSIL2A_20250228T173149_R055_T14SLH_20250228T212633\" In\u00a0[\u00a0]: Copied! <pre># Specify which bands to download (Sentinel-2 bands)\nbands_to_download = [\n    \"B01\",\n    \"B02\",\n    \"B03\",\n    \"B04\",\n    \"B05\",\n    \"B06\",\n    \"B07\",\n    \"B08\",\n    \"B8A\",\n    \"B09\",\n    \"B11\",\n    \"B12\",\n    \"AOT\",\n    \"WVP\",\n    \"SCL\",\n]\n</pre> # Specify which bands to download (Sentinel-2 bands) bands_to_download = [     \"B01\",     \"B02\",     \"B03\",     \"B04\",     \"B05\",     \"B06\",     \"B07\",     \"B08\",     \"B8A\",     \"B09\",     \"B11\",     \"B12\",     \"AOT\",     \"WVP\",     \"SCL\", ] In\u00a0[\u00a0]: Copied! <pre># Create a directory for the downloaded bands\noutput_directory = \"sentinel2_bands\"\n</pre> # Create a directory for the downloaded bands output_directory = \"sentinel2_bands\" In\u00a0[\u00a0]: Copied! <pre># Download the bands, save them to the output directory, and create a merged GeoTIFF.\n# The download process may take a while. Please be patient.\ndownloaded_bands = geoai.download_pc_stac_item(\n    item_url=item_url,\n    bands=bands_to_download,\n    output_dir=output_directory,\n    show_progress=True,\n    merge_bands=True,\n    merged_filename=\"sentinel2_all_bands.tif\",\n    overwrite=False,  # Skip files that already exist\n    cell_size=10,  # Resample all bands to 10m resolution\n)\n</pre> # Download the bands, save them to the output directory, and create a merged GeoTIFF. # The download process may take a while. Please be patient. downloaded_bands = geoai.download_pc_stac_item(     item_url=item_url,     bands=bands_to_download,     output_dir=output_directory,     show_progress=True,     merge_bands=True,     merged_filename=\"sentinel2_all_bands.tif\",     overwrite=False,  # Skip files that already exist     cell_size=10,  # Resample all bands to 10m resolution ) In\u00a0[\u00a0]: Copied! <pre># Print the paths to the downloaded files\nfor band, path in downloaded_bands.items():\n    print(f\"Downloaded {band}: {path}\")\n</pre> # Print the paths to the downloaded files for band, path in downloaded_bands.items():     print(f\"Downloaded {band}: {path}\") In\u00a0[\u00a0]: Copied! <pre>raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/sentinel2a_kansas.tif\"\n</pre> raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/sentinel2a_kansas.tif\" In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(raster_url, bidx=[8, 4, 3], rescale=\"1000,4000\")\n</pre> geoai.view_raster(raster_url, bidx=[8, 4, 3], rescale=\"1000,4000\")"},{"location":"examples/download_sentinel2/#download-sentinel-2-data-from-planetary-computer","title":"Download Sentinel-2 Data from Planetary Computer\u00b6","text":"<p>The notebook demonstrates how to download Sentinel-2 data from the Planetary Computer using the <code>geoai</code> package.</p>"},{"location":"examples/download_sentinel2/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/download_sentinel2/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/download_sentinel2/#search-data","title":"Search data\u00b6","text":"<p>Go to the Planetary Computer and search for the data you are interested in. Copy the STAC item link and paste it to the code below.</p>"},{"location":"examples/download_sentinel2/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/download_sentinel2/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"examples/geometric_properties/","title":"Geometric properties","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_buildings_masks.geojson\"\nraster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\n</pre> vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_buildings_masks.geojson\" raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.read_vector(vector_url)\n</pre> gdf = geoai.read_vector(vector_url) In\u00a0[\u00a0]: Copied! <pre>gdf.head()\n</pre> gdf.head() In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\")\n</pre> gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\") In\u00a0[\u00a0]: Copied! <pre>gdf_props.head()\n</pre> gdf_props.head() In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_props, column=\"elongation\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf_props, column=\"elongation\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &lt; 2000) &amp; (gdf_props[\"elongation\"] &lt; 5)]\n</pre> gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &lt; 2000) &amp; (gdf_props[\"elongation\"] &lt; 5)] In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filtered, column=\"elongation\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf_filtered, column=\"elongation\", tiles=raster_url)"},{"location":"examples/geometric_properties/#geometric-properties","title":"Geometric Properties\u00b6","text":"<p>This notebook demonstrates how to calculate geometric properties of objects in a vector dataset and filter out unwanted objects based on these properties.</p>"},{"location":"examples/geometric_properties/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/geometric_properties/#import-package","title":"Import package\u00b6","text":""},{"location":"examples/geometric_properties/#load-data","title":"Load data\u00b6","text":""},{"location":"examples/geometric_properties/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"examples/geometric_properties/#add-geometric-properties","title":"Add geometric properties\u00b6","text":""},{"location":"examples/geometric_properties/#visualize-geometric-properties","title":"Visualize geometric properties\u00b6","text":""},{"location":"examples/geometric_properties/#filter-objects-based-on-geometric-properties","title":"Filter objects based on geometric properties\u00b6","text":""},{"location":"examples/geometric_properties/#visualize-filtered-objects","title":"Visualize filtered objects\u00b6","text":""},{"location":"examples/image_chips/","title":"Image chips","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\nvector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" In\u00a0[\u00a0]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) In\u00a0[\u00a0]: Copied! <pre>vector_path = geoai.download_file(vector_url)\n</pre> vector_path = geoai.download_file(vector_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(raster_path, figsize=(18, 10))\n</pre> geoai.view_image(raster_path, figsize=(18, 10)) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector(vector_path, basemap=True, alpha=0.5, figsize=(18, 10))\n</pre> geoai.view_vector(vector_path, basemap=True, alpha=0.5, figsize=(18, 10)) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(vector_path)\n</pre> geoai.view_vector_interactive(vector_path) In\u00a0[\u00a0]: Copied! <pre>output_path = vector_path.replace(\".geojson\", \".tif\")\ngeoai.vector_to_raster(vector_path, output_path, reference_raster=raster_path)\n</pre> output_path = vector_path.replace(\".geojson\", \".tif\") geoai.vector_to_raster(vector_path, output_path, reference_raster=raster_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(output_path, figsize=(18, 10))\n</pre> geoai.view_image(output_path, figsize=(18, 10)) In\u00a0[\u00a0]: Copied! <pre>tiles = geoai.export_geotiff_tiles(\n    in_raster=raster_path,\n    out_folder=\"output\",\n    in_class_data=vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n    create_overview=True,\n)\n</pre> tiles = geoai.export_geotiff_tiles(     in_raster=raster_path,     out_folder=\"output\",     in_class_data=vector_path,     tile_size=512,     stride=256,     buffer_radius=0,     create_overview=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(\"output/overview.png\", figsize=(18, 10))\n</pre> geoai.view_image(\"output/overview.png\", figsize=(18, 10)) In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(\"output/images/tile_000000.tif\")\n</pre> geoai.view_image(\"output/images/tile_000000.tif\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(\"output/labels/tile_000000.tif\")\n</pre> geoai.view_image(\"output/labels/tile_000000.tif\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(\"output/images/tile_000001.tif\")\n</pre> geoai.view_image(\"output/images/tile_000001.tif\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(\"output/labels/tile_000001.tif\")\n</pre> geoai.view_image(\"output/labels/tile_000001.tif\")"},{"location":"examples/image_chips/#generate-image-chips","title":"Generate Image Chips\u00b6","text":""},{"location":"examples/image_chips/#install-package","title":"Install Package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/image_chips/#import-packages","title":"Import Packages\u00b6","text":""},{"location":"examples/image_chips/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/image_chips/#preview-data","title":"Preview data\u00b6","text":""},{"location":"examples/image_chips/#convert-vector-to-raster","title":"Convert vector to raster\u00b6","text":""},{"location":"examples/image_chips/#generate-image-chips","title":"Generate image chips\u00b6","text":""},{"location":"examples/image_chips/#preview-image-chips","title":"Preview image chips\u00b6","text":""},{"location":"examples/parking_spot_detection/","title":"Parking spot detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/parking_spots.tif\"\n)\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/parking_spots.tif\" ) In\u00a0[\u00a0]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(raster_url)\n</pre> geoai.view_raster(raster_url) In\u00a0[\u00a0]: Copied! <pre>detector = geoai.ParkingSplotDetector()\n</pre> detector = geoai.ParkingSplotDetector() In\u00a0[\u00a0]: Copied! <pre>mask_path = detector.generate_masks(\n    raster_path=raster_path,\n    output_path=\"parking_masks.tif\",\n    confidence_threshold=0.5,\n    mask_threshold=0.5,\n    overlap=0.25,\n    chip_size=(256, 256),\n    min_object_area=10,\n    # max_object_area=5000,\n)\n</pre> mask_path = detector.generate_masks(     raster_path=raster_path,     output_path=\"parking_masks.tif\",     confidence_threshold=0.5,     mask_threshold=0.5,     overlap=0.25,     chip_size=(256, 256),     min_object_area=10,     # max_object_area=5000, ) <p>Convert the image masks to polygons and save the output GeoJSON file.</p> In\u00a0[\u00a0]: Copied! <pre>gdf = detector.vectorize_masks(\n    masks_path=\"parking_masks.tif\",\n    output_path=\"parking.geojson\",\n    min_object_area=300,\n    # max_object_area=5000,\n)\n</pre> gdf = detector.vectorize_masks(     masks_path=\"parking_masks.tif\",     output_path=\"parking.geojson\",     min_object_area=300,     # max_object_area=5000, ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.add_geometric_properties(gdf)\n</pre> gdf = geoai.add_geometric_properties(gdf) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url)"},{"location":"examples/parking_spot_detection/#parking-spot-detection","title":"Parking Spot Detection\u00b6","text":""},{"location":"examples/parking_spot_detection/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/parking_spot_detection/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/parking_spot_detection/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We will download a sample image from Hugging Face Hub to use for parking spot detection. You can find more high-resolution images from OpenAerialMap.</p>"},{"location":"examples/parking_spot_detection/#visualize-the-image","title":"Visualize the image\u00b6","text":""},{"location":"examples/parking_spot_detection/#initialize-the-model","title":"Initialize the model\u00b6","text":""},{"location":"examples/parking_spot_detection/#extract-parking-spots","title":"Extract parking spots\u00b6","text":"<p>Extract parking spots from the image using the model and save the output image.</p>"},{"location":"examples/parking_spot_detection/#add-geometric-properties","title":"Add geometric properties\u00b6","text":""},{"location":"examples/parking_spot_detection/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/planetary_computer/","title":"Planetary computer","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>collections = geoai.pc_collection_list()\ncollections\n</pre> collections = geoai.pc_collection_list() collections In\u00a0[\u00a0]: Copied! <pre>items = geoai.pc_stac_search(\n    collection=\"naip\",\n    bbox=[-76.6657, 39.2648, -76.6478, 39.2724],  # Baltimore area\n    time_range=\"2013-01-01/2014-12-31\",\n)\n</pre> items = geoai.pc_stac_search(     collection=\"naip\",     bbox=[-76.6657, 39.2648, -76.6478, 39.2724],  # Baltimore area     time_range=\"2013-01-01/2014-12-31\", ) In\u00a0[\u00a0]: Copied! <pre>items\n</pre> items In\u00a0[\u00a0]: Copied! <pre>geoai.pc_item_asset_list(items[0])\n</pre> geoai.pc_item_asset_list(items[0]) In\u00a0[\u00a0]: Copied! <pre>geoai.view_pc_item(item=items[0])\n</pre> geoai.view_pc_item(item=items[0]) In\u00a0[\u00a0]: Copied! <pre>downloaded = geoai.pc_stac_download(\n    items, output_dir=\"data\", assets=[\"image\", \"thumbnail\"]\n)\n</pre> downloaded = geoai.pc_stac_download(     items, output_dir=\"data\", assets=[\"image\", \"thumbnail\"] ) In\u00a0[\u00a0]: Copied! <pre>items = geoai.pc_stac_search(\n    collection=\"chesapeake-lc-13\",\n    bbox=[-76.6657, 39.2648, -76.6478, 39.2724],  # Baltimore area\n    time_range=\"2013-01-01/2014-12-31\",\n    max_items=10,\n)\n</pre> items = geoai.pc_stac_search(     collection=\"chesapeake-lc-13\",     bbox=[-76.6657, 39.2648, -76.6478, 39.2724],  # Baltimore area     time_range=\"2013-01-01/2014-12-31\",     max_items=10, ) In\u00a0[\u00a0]: Copied! <pre>items\n</pre> items In\u00a0[\u00a0]: Copied! <pre>geoai.pc_item_asset_list(items[0])\n</pre> geoai.pc_item_asset_list(items[0]) In\u00a0[\u00a0]: Copied! <pre>geoai.view_pc_item(item=items[0], colormap_name=\"tab10\", basemap=\"SATELLITE\")\n</pre> geoai.view_pc_item(item=items[0], colormap_name=\"tab10\", basemap=\"SATELLITE\") In\u00a0[\u00a0]: Copied! <pre>geoai.pc_stac_download(items[0], output_dir=\"data\", assets=[\"data\", \"rendered_preview\"])\n</pre> geoai.pc_stac_download(items[0], output_dir=\"data\", assets=[\"data\", \"rendered_preview\"]) In\u00a0[\u00a0]: Copied! <pre>ds = geoai.read_pc_item_asset(items[0], asset=\"data\")\n</pre> ds = geoai.read_pc_item_asset(items[0], asset=\"data\") In\u00a0[\u00a0]: Copied! <pre>ds\n</pre> ds In\u00a0[\u00a0]: Copied! <pre>items = geoai.pc_stac_search(\n    collection=\"landsat-c2-l2\",\n    bbox=[-76.6657, 39.2648, -76.6478, 39.2724],  # Baltimore area\n    time_range=\"2024-10-27/2024-12-31\",\n    query={\"eo:cloud_cover\": {\"lt\": 1}},\n    max_items=10,\n)\n</pre> items = geoai.pc_stac_search(     collection=\"landsat-c2-l2\",     bbox=[-76.6657, 39.2648, -76.6478, 39.2724],  # Baltimore area     time_range=\"2024-10-27/2024-12-31\",     query={\"eo:cloud_cover\": {\"lt\": 1}},     max_items=10, ) In\u00a0[\u00a0]: Copied! <pre>items\n</pre> items In\u00a0[\u00a0]: Copied! <pre>geoai.pc_item_asset_list(items[0])\n</pre> geoai.pc_item_asset_list(items[0]) In\u00a0[\u00a0]: Copied! <pre>geoai.view_pc_item(item=items[0], assets=[\"red\", \"green\", \"blue\"])\n</pre> geoai.view_pc_item(item=items[0], assets=[\"red\", \"green\", \"blue\"]) In\u00a0[\u00a0]: Copied! <pre>geoai.view_pc_item(item=items[0], assets=[\"nir08\", \"red\", \"green\"])\n</pre> geoai.view_pc_item(item=items[0], assets=[\"nir08\", \"red\", \"green\"]) In\u00a0[\u00a0]: Copied! <pre>geoai.view_pc_item(\n    item=items[0],\n    expression=\"(nir08-red)/(nir08+red)\",\n    rescale=\"-1,1\",\n    colormap_name=\"greens\",\n    name=\"NDVI Green\",\n)\n</pre> geoai.view_pc_item(     item=items[0],     expression=\"(nir08-red)/(nir08+red)\",     rescale=\"-1,1\",     colormap_name=\"greens\",     name=\"NDVI Green\", ) In\u00a0[\u00a0]: Copied! <pre>geoai.pc_stac_download(\n    items[0], output_dir=\"data\", assets=[\"nir08\", \"red\", \"green\", \"blue\"]\n)\n</pre> geoai.pc_stac_download(     items[0], output_dir=\"data\", assets=[\"nir08\", \"red\", \"green\", \"blue\"] )"},{"location":"examples/planetary_computer/#download-data-from-planetary-computer","title":"Download Data from Planetary Computer\u00b6","text":""},{"location":"examples/planetary_computer/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/planetary_computer/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/planetary_computer/#retrieve-collections","title":"Retrieve collections\u00b6","text":""},{"location":"examples/planetary_computer/#search-naip-imagery","title":"Search NAIP imagery\u00b6","text":""},{"location":"examples/planetary_computer/#visualize-naip-imagery","title":"Visualize NAIP imagery\u00b6","text":""},{"location":"examples/planetary_computer/#download-naip-imagery","title":"Download NAIP imagery\u00b6","text":""},{"location":"examples/planetary_computer/#search-land-cover-data","title":"Search land cover data\u00b6","text":""},{"location":"examples/planetary_computer/#visualize-land-cover-data","title":"Visualize land cover data\u00b6","text":""},{"location":"examples/planetary_computer/#download-land-cover-data","title":"Download land cover data\u00b6","text":""},{"location":"examples/planetary_computer/#search-landsat-data","title":"Search Landsat data\u00b6","text":""},{"location":"examples/planetary_computer/#visualize-landsat-data","title":"Visualize Landsat data\u00b6","text":""},{"location":"examples/planetary_computer/#download-landsat-data","title":"Download Landsat data\u00b6","text":""},{"location":"examples/ship_detection/","title":"Ship detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ships_dubai.tif\"\n)\nraster_path = geoai.download_file(raster_url, \"ships_dubai.tif\")\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ships_dubai.tif\" ) raster_path = geoai.download_file(raster_url, \"ships_dubai.tif\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(raster_url)\n</pre> geoai.view_raster(raster_url) In\u00a0[\u00a0]: Copied! <pre>detector = geoai.ShipDetector()\n</pre> detector = geoai.ShipDetector() In\u00a0[\u00a0]: Copied! <pre>output_path = \"ships_dubai_masks.tif\"\n</pre> output_path = \"ships_dubai_masks.tif\" In\u00a0[\u00a0]: Copied! <pre>masks_path = detector.generate_masks(\n    raster_path,\n    output_path=output_path,\n    confidence_threshold=0.9,\n    mask_threshold=0.7,\n    overlap=0.25,\n    chip_size=(256, 256),\n    batch_size=4,\n)\n</pre> masks_path = detector.generate_masks(     raster_path,     output_path=output_path,     confidence_threshold=0.9,     mask_threshold=0.7,     overlap=0.25,     chip_size=(256, 256),     batch_size=4, ) In\u00a0[\u00a0]: Copied! <pre>gdf = detector.vectorize_masks(\n    output_path,\n    output_path=\"ships_dubai_masks.geojson\",\n    confidence_threshold=0.8,\n    min_object_area=100,\n    max_object_size=10000,\n)\n</pre> gdf = detector.vectorize_masks(     output_path,     output_path=\"ships_dubai_masks.geojson\",     confidence_threshold=0.8,     min_object_area=100,     max_object_size=10000, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.add_geometric_properties(gdf)\ngdf.head()\n</pre> gdf = geoai.add_geometric_properties(gdf) gdf.head() In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>m = geoai.view_raster(raster_url, backend=\"ipyleaflet\")\nm\n</pre> m = geoai.view_raster(raster_url, backend=\"ipyleaflet\") m <p>Use the drawing tool to select the area of interest.</p> In\u00a0[\u00a0]: Copied! <pre>aoi = m.user_rois\n\nif aoi is None:\n\n    aoi = {\n        \"type\": \"FeatureCollection\",\n        \"features\": [\n            {\n                \"type\": \"Feature\",\n                \"properties\": {},\n                \"geometry\": {\n                    \"type\": \"Polygon\",\n                    \"coordinates\": [\n                        [\n                            [55.133729, 25.110277],\n                            [55.134072, 25.11393],\n                            [55.134823, 25.115601],\n                            [55.136025, 25.117116],\n                            [55.137677, 25.118127],\n                            [55.140145, 25.118787],\n                            [55.142248, 25.11902],\n                            [55.142012, 25.118243],\n                            [55.140831, 25.116728],\n                            [55.13948, 25.116903],\n                            [55.137956, 25.116825],\n                            [55.136132, 25.115543],\n                            [55.13566, 25.114416],\n                            [55.135467, 25.1136],\n                            [55.135939, 25.112609],\n                            [55.136218, 25.111657],\n                            [55.13551, 25.110685],\n                            [55.134373, 25.110102],\n                            [55.133729, 25.110277],\n                        ]\n                    ],\n                },\n            }\n        ],\n    }\n</pre> aoi = m.user_rois  if aoi is None:      aoi = {         \"type\": \"FeatureCollection\",         \"features\": [             {                 \"type\": \"Feature\",                 \"properties\": {},                 \"geometry\": {                     \"type\": \"Polygon\",                     \"coordinates\": [                         [                             [55.133729, 25.110277],                             [55.134072, 25.11393],                             [55.134823, 25.115601],                             [55.136025, 25.117116],                             [55.137677, 25.118127],                             [55.140145, 25.118787],                             [55.142248, 25.11902],                             [55.142012, 25.118243],                             [55.140831, 25.116728],                             [55.13948, 25.116903],                             [55.137956, 25.116825],                             [55.136132, 25.115543],                             [55.13566, 25.114416],                             [55.135467, 25.1136],                             [55.135939, 25.112609],                             [55.136218, 25.111657],                             [55.13551, 25.110685],                             [55.134373, 25.110102],                             [55.133729, 25.110277],                         ]                     ],                 },             }         ],     } In\u00a0[\u00a0]: Copied! <pre>import geopandas as gpd\n</pre> import geopandas as gpd In\u00a0[\u00a0]: Copied! <pre>aoi_gdf = gpd.GeoDataFrame.from_features(aoi[\"features\"], crs=\"EPSG:4326\").to_crs(\n    gdf.crs\n)\n</pre> aoi_gdf = gpd.GeoDataFrame.from_features(aoi[\"features\"], crs=\"EPSG:4326\").to_crs(     gdf.crs ) <p>Intersect the selected area with the vectorized masks to filter the results.</p> In\u00a0[\u00a0]: Copied! <pre>gdf_filter = gdf[gdf.intersects(aoi_gdf.geometry[0])]\ngdf_filter.head()\n</pre> gdf_filter = gdf[gdf.intersects(aoi_gdf.geometry[0])] gdf_filter.head() In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filter, column=\"confidence\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf_filter, column=\"confidence\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf_filter.to_file(\"ships_dubai.geojson\")\n</pre> gdf_filter.to_file(\"ships_dubai.geojson\") <p></p>"},{"location":"examples/ship_detection/#ship-detection","title":"Ship Detection\u00b6","text":"<p>This notebook demonstrates how to use the geoai package for ship detection using a pre-trained model.</p> <p></p>"},{"location":"examples/ship_detection/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/ship_detection/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/ship_detection/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/ship_detection/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"examples/ship_detection/#initialize-model","title":"Initialize model\u00b6","text":""},{"location":"examples/ship_detection/#generate-masks","title":"Generate masks\u00b6","text":""},{"location":"examples/ship_detection/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/ship_detection/#visualize-initial-results","title":"Visualize initial results\u00b6","text":""},{"location":"examples/ship_detection/#calculate-geometric-properties","title":"Calculate geometric properties\u00b6","text":""},{"location":"examples/ship_detection/#filter-results","title":"Filter results\u00b6","text":""},{"location":"examples/ship_detection/#visualize-final-results","title":"Visualize final results\u00b6","text":""},{"location":"examples/ship_detection/#save-results","title":"Save results\u00b6","text":""},{"location":"examples/solar_panel_detection/","title":"Solar panel detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.tif\"\nraster_path = geoai.download_file(raster_url)\n</pre> raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.tif\" raster_path = geoai.download_file(raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.print_raster_info(raster_path)\n</pre> geoai.print_raster_info(raster_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(raster_url)\n</pre> geoai.view_raster(raster_url) In\u00a0[\u00a0]: Copied! <pre>detector = geoai.SolarPanelDetector()\n</pre> detector = geoai.SolarPanelDetector() In\u00a0[\u00a0]: Copied! <pre>output_path = \"solar_panel_masks.tif\"\n</pre> output_path = \"solar_panel_masks.tif\" In\u00a0[\u00a0]: Copied! <pre>masks_path = detector.generate_masks(\n    raster_path,\n    output_path=output_path,\n    confidence_threshold=0.4,\n    mask_threshold=0.5,\n    min_object_area=100,\n    overlap=0.25,\n    chip_size=(400, 400),\n    batch_size=4,\n    verbose=False,\n)\n</pre> masks_path = detector.generate_masks(     raster_path,     output_path=output_path,     confidence_threshold=0.4,     mask_threshold=0.5,     min_object_area=100,     overlap=0.25,     chip_size=(400, 400),     batch_size=4,     verbose=False, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\n    output_path,\n    indexes=[2],\n    colormap=\"autumn\",\n    layer_name=\"Solar Panels\",\n    basemap=raster_url,\n)\n</pre> geoai.view_raster(     output_path,     indexes=[2],     colormap=\"autumn\",     layer_name=\"Solar Panels\",     basemap=raster_url, ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.orthogonalize(\n    input_path=masks_path, output_path=\"solar_panel_masks.geojson\", epsilon=0.2\n)\n</pre> gdf = geoai.orthogonalize(     input_path=masks_path, output_path=\"solar_panel_masks.geojson\", epsilon=0.2 ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf, tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.add_geometric_properties(gdf)\ngdf.head()\n</pre> gdf = geoai.add_geometric_properties(gdf) gdf.head() In\u00a0[\u00a0]: Copied! <pre>print(len(gdf))\n</pre> print(len(gdf)) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, column=\"elongation\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf, column=\"elongation\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf_filter = gdf[(gdf[\"elongation\"] &lt; 10) &amp; (gdf[\"area_m2\"] &gt; 5)]\nprint(len(gdf_filter))\n</pre> gdf_filter = gdf[(gdf[\"elongation\"] &lt; 10) &amp; (gdf[\"area_m2\"] &gt; 5)] print(len(gdf_filter)) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filter, column=\"area_m2\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf_filter, column=\"area_m2\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf_filter, style_kwds={\"color\": \"red\", \"fillOpacity\": 0}, tiles=raster_url\n)\n</pre> geoai.view_vector_interactive(     gdf_filter, style_kwds={\"color\": \"red\", \"fillOpacity\": 0}, tiles=raster_url ) In\u00a0[\u00a0]: Copied! <pre>gdf_filter[\"area_m2\"].hist()\n</pre> gdf_filter[\"area_m2\"].hist() In\u00a0[\u00a0]: Copied! <pre>gdf_filter[\"area_m2\"].describe()\n</pre> gdf_filter[\"area_m2\"].describe() In\u00a0[\u00a0]: Copied! <pre>gdf_filter[\"area_m2\"].sum()\n</pre> gdf_filter[\"area_m2\"].sum() In\u00a0[\u00a0]: Copied! <pre>gdf_filter.to_file(\"solar_panels.geojson\")\n</pre> gdf_filter.to_file(\"solar_panels.geojson\")"},{"location":"examples/solar_panel_detection/#solar-panel-detection","title":"Solar Panel Detection\u00b6","text":"<p>This notebook demonstrates how to use the geoai package for solar panel detection using a pre-trained model.</p> <p></p>"},{"location":"examples/solar_panel_detection/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/solar_panel_detection/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/solar_panel_detection/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/solar_panel_detection/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"examples/solar_panel_detection/#initialize-model","title":"Initialize model\u00b6","text":""},{"location":"examples/solar_panel_detection/#generate-masks","title":"Generate masks\u00b6","text":""},{"location":"examples/solar_panel_detection/#visualize-masks","title":"Visualize masks\u00b6","text":""},{"location":"examples/solar_panel_detection/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/solar_panel_detection/#visualize-initial-results","title":"Visualize initial results\u00b6","text":""},{"location":"examples/solar_panel_detection/#calculate-geometric-properties","title":"Calculate geometric properties\u00b6","text":""},{"location":"examples/solar_panel_detection/#filter-results","title":"Filter results\u00b6","text":""},{"location":"examples/solar_panel_detection/#visualize-final-results","title":"Visualize final results\u00b6","text":""},{"location":"examples/solar_panel_detection/#save-results","title":"Save results\u00b6","text":""},{"location":"examples/text_prompt_segmentation/","title":"Text prompt segmentation","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/trees_brazil.tif\"\n)\nraster_path = geoai.download_file(raster_url)\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/trees_brazil.tif\" ) raster_path = geoai.download_file(raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(raster_url)\n</pre> geoai.view_raster(raster_url) In\u00a0[\u00a0]: Copied! <pre>segmenter = geoai.CLIPSegmentation(tile_size=512, overlap=32)\n</pre> segmenter = geoai.CLIPSegmentation(tile_size=512, overlap=32) In\u00a0[\u00a0]: Copied! <pre>output_path = \"tree_masks.tif\"\ntext_prompt = \"trees\"\n</pre> output_path = \"tree_masks.tif\" text_prompt = \"trees\" In\u00a0[\u00a0]: Copied! <pre>segmenter.segment_image(\n    raster_path,\n    output_path=output_path,\n    text_prompt=text_prompt,\n    threshold=0.5,\n    smoothing_sigma=1.0,\n)\n</pre> segmenter.segment_image(     raster_path,     output_path=output_path,     text_prompt=text_prompt,     threshold=0.5,     smoothing_sigma=1.0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\n    output_path,\n    nodata=0,\n    opacity=0.8,\n    colormap=\"greens\",\n    layer_name=\"Trees\",\n    basemap=raster_url,\n)\n</pre> geoai.view_raster(     output_path,     nodata=0,     opacity=0.8,     colormap=\"greens\",     layer_name=\"Trees\",     basemap=raster_url, ) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=output_path,\n    right_layer=raster_url,\n    left_label=\"Trees\",\n    right_label=\"Satellite Image\",\n    left_args={\"nodata\": 0, \"opacity\": 0.8, \"colormap\": \"greens\"},\n    basemap=raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=output_path,     right_layer=raster_url,     left_label=\"Trees\",     right_label=\"Satellite Image\",     left_args={\"nodata\": 0, \"opacity\": 0.8, \"colormap\": \"greens\"},     basemap=raster_url, )"},{"location":"examples/text_prompt_segmentation/#text-prompt-segmentation","title":"Text Prompt Segmentation\u00b6","text":"<p>This example demonstrates how to use the <code>geoai-py</code> package for text prompt segmentation using CLIPSeg.</p> <p></p>"},{"location":"examples/text_prompt_segmentation/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/text_prompt_segmentation/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/text_prompt_segmentation/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/text_prompt_segmentation/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"examples/text_prompt_segmentation/#initialize-model","title":"Initialize model\u00b6","text":""},{"location":"examples/text_prompt_segmentation/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/text_prompt_segmentation/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/train_building_footprints_usa/","title":"Train building footprints usa","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>train_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\ntest_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\"\n)\n</pre> train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" test_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\" ) In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"output\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"output\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_MaskRCNN_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    num_channels=3,\n    pretrained=True,\n    batch_size=4,\n    num_epochs=100,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_MaskRCNN_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     num_channels=3,     pretrained=True,     batch_size=4,     num_epochs=100,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"naip_test_prediction.tif\"\nmodel_path = f\"{out_folder}/models/building_footprints_usa.pth\"\n</pre> masks_path = \"naip_test_prediction.tif\" model_path = f\"{out_folder}/models/building_footprints_usa.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path,\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.5,\n    batch_size=4,\n    num_channels=3,\n)\n</pre> geoai.object_detection(     test_raster_path,     masks_path,     model_path,     window_size=512,     overlap=256,     confidence_threshold=0.5,     batch_size=4,     num_channels=3, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"naip_test_prediction.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_path, epsilon=2)\n</pre> output_path = \"naip_test_prediction.geojson\" gdf = geoai.orthogonalize(masks_path, output_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(output_path, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(output_path, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=output_path,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=output_path,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, )"},{"location":"examples/train_building_footprints_usa/#train-a-building-footprints-detection-model-for-the-usa","title":"Train a Building Footprints Detection Model for the USA\u00b6","text":""},{"location":"examples/train_building_footprints_usa/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/train_building_footprints_usa/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/train_building_footprints_usa/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/train_building_footprints_usa/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/train_building_footprints_usa/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"examples/train_building_footprints_usa/#train-object-detection-model","title":"Train object detection model\u00b6","text":""},{"location":"examples/train_building_footprints_usa/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/train_building_footprints_usa/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/train_building_footprints_usa/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/train_car_detection/","title":"Train car detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>train_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/cars_7cm.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/car_detection.geojson\"\ntest_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/cars_test_7cm.tif\"\n)\n</pre> train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/cars_7cm.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/car_detection.geojson\" test_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/cars_test_7cm.tif\" ) In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"output\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"output\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_MaskRCNN_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    num_channels=3,\n    pretrained=True,\n    batch_size=4,\n    num_epochs=100,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_MaskRCNN_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     num_channels=3,     pretrained=True,     batch_size=4,     num_epochs=100,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"cars_prediction.tif\"\nmodel_path = f\"{out_folder}/models/best_model.pth\"\n</pre> masks_path = \"cars_prediction.tif\" model_path = f\"{out_folder}/models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path,\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.5,\n    batch_size=4,\n    num_channels=3,\n)\n</pre> geoai.object_detection(     test_raster_path,     masks_path,     model_path,     window_size=512,     overlap=256,     confidence_threshold=0.5,     batch_size=4,     num_channels=3, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"cars_prediction.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_path, epsilon=2)\n</pre> output_path = \"cars_prediction.geojson\" gdf = geoai.orthogonalize(masks_path, output_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(output_path, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(output_path, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=output_path,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=output_path,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, )"},{"location":"examples/train_car_detection/#train-a-model-for-detecting-cars","title":"Train a Model for Detecting Cars\u00b6","text":""},{"location":"examples/train_car_detection/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/train_car_detection/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/train_car_detection/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/train_car_detection/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/train_car_detection/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"examples/train_car_detection/#train-object-detection-model","title":"Train object detection model\u00b6","text":""},{"location":"examples/train_car_detection/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/train_car_detection/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/train_car_detection/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/train_object_detection_model/","title":"Train object detection model","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>train_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\ntest_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\"\n)\n</pre> train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" test_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\" ) In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"output\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"output\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_MaskRCNN_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    num_channels=4,\n    pretrained=True,\n    batch_size=4,\n    num_epochs=10,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_MaskRCNN_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     num_channels=4,     pretrained=True,     batch_size=4,     num_epochs=10,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"naip_test_prediction.tif\"\nmodel_path = f\"{out_folder}/models/best_model.pth\"\n</pre> masks_path = \"naip_test_prediction.tif\" model_path = f\"{out_folder}/models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path,\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.5,\n    batch_size=4,\n    num_channels=4,\n)\n</pre> geoai.object_detection(     test_raster_path,     masks_path,     model_path,     window_size=512,     overlap=256,     confidence_threshold=0.5,     batch_size=4,     num_channels=4, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"naip_test_prediction.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_path, epsilon=2)\n</pre> output_path = \"naip_test_prediction.geojson\" gdf = geoai.orthogonalize(masks_path, output_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(output_path, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(output_path, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=output_path,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=output_path,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, )"},{"location":"examples/train_object_detection_model/#train-an-object-detection-model-with-geoai","title":"Train an Object Detection Model with GeoAI\u00b6","text":""},{"location":"examples/train_object_detection_model/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/train_object_detection_model/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/train_object_detection_model/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/train_object_detection_model/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/train_object_detection_model/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"examples/train_object_detection_model/#train-object-detection-model","title":"Train object detection model\u00b6","text":""},{"location":"examples/train_object_detection_model/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/train_object_detection_model/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/train_object_detection_model/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/train_ship_detection/","title":"Train ship detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>train_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ships_sfo_15cm.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ship_detection.geojson\"\ntest_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ships_sfo_test_15cm.tif\"\n</pre> train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ships_sfo_15cm.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ship_detection.geojson\" test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ships_sfo_test_15cm.tif\" In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"output\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"output\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_MaskRCNN_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    num_channels=3,\n    pretrained=True,\n    batch_size=4,\n    num_epochs=100,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_MaskRCNN_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     num_channels=3,     pretrained=True,     batch_size=4,     num_epochs=100,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"ship_prediction.tif\"\nmodel_path = f\"{out_folder}/models/best_model.pth\"\n</pre> masks_path = \"ship_prediction.tif\" model_path = f\"{out_folder}/models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path,\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.5,\n    batch_size=4,\n    num_channels=3,\n)\n</pre> geoai.object_detection(     test_raster_path,     masks_path,     model_path,     window_size=512,     overlap=256,     confidence_threshold=0.5,     batch_size=4,     num_channels=3, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"ship_prediction.geojson\"\ngdf = geoai.raster_to_vector(masks_path, output_path)\n</pre> output_path = \"ship_prediction.geojson\" gdf = geoai.raster_to_vector(masks_path, output_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(output_path, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(output_path, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=output_path,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=output_path,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, )"},{"location":"examples/train_ship_detection/#train-a-model-for-detecting-ships","title":"Train a Model for Detecting Ships\u00b6","text":""},{"location":"examples/train_ship_detection/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/train_ship_detection/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/train_ship_detection/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/train_ship_detection/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/train_ship_detection/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"examples/train_ship_detection/#train-object-detection-model","title":"Train object detection model\u00b6","text":""},{"location":"examples/train_ship_detection/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/train_ship_detection/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/train_ship_detection/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/train_solar_panel_detection/","title":"Train solar panel detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.tif\"\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.geojson\"\ntest_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_test_davis_ca.tif\"\n</pre> train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.tif\" train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.geojson\" test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_test_davis_ca.tif\" In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"output\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"output\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_MaskRCNN_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    num_channels=3,\n    pretrained=True,\n    batch_size=4,\n    num_epochs=100,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_MaskRCNN_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     num_channels=3,     pretrained=True,     batch_size=4,     num_epochs=100,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"solar_panels_prediction.tif\"\nmodel_path = f\"{out_folder}/models/best_model.pth\"\n</pre> masks_path = \"solar_panels_prediction.tif\" model_path = f\"{out_folder}/models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path,\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.5,\n    batch_size=4,\n    num_channels=3,\n)\n</pre> geoai.object_detection(     test_raster_path,     masks_path,     model_path,     window_size=512,     overlap=256,     confidence_threshold=0.5,     batch_size=4,     num_channels=3, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"solar_panels_prediction.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_path, epsilon=2)\n</pre> output_path = \"solar_panels_prediction.geojson\" gdf = geoai.orthogonalize(masks_path, output_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(output_path, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(output_path, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=output_path,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=output_path,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, )"},{"location":"examples/train_solar_panel_detection/#train-a-model-for-detecting-solar-panels","title":"Train a Model for Detecting Solar Panels\u00b6","text":""},{"location":"examples/train_solar_panel_detection/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/train_solar_panel_detection/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/train_solar_panel_detection/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/train_solar_panel_detection/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/train_solar_panel_detection/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"examples/train_solar_panel_detection/#train-object-detection-model","title":"Train object detection model\u00b6","text":""},{"location":"examples/train_solar_panel_detection/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/train_solar_panel_detection/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/train_solar_panel_detection/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/view_metadata/","title":"View metadata","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py <p>Import the package</p> In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai <p>Define URLs for sample datasets: a NAIP imagery raster file and a building footprints vector file</p> In\u00a0[\u00a0]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\nvector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" <p>Download the raster file (NAIP imagery) and save it locally</p> In\u00a0[\u00a0]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) <p>Download the vector file (building footprints) and save it locally</p> In\u00a0[\u00a0]: Copied! <pre>vector_path = geoai.download_file(vector_url)\n</pre> vector_path = geoai.download_file(vector_url) <p>Display metadata about the raster file, including dimensions, resolution, projection, and bands</p> In\u00a0[\u00a0]: Copied! <pre>geoai.get_raster_info(raster_path)[\"band_stats\"]\n</pre> geoai.get_raster_info(raster_path)[\"band_stats\"] In\u00a0[\u00a0]: Copied! <pre>geoai.print_raster_info(raster_path, figsize=(18, 10))\n</pre> geoai.print_raster_info(raster_path, figsize=(18, 10)) <p>Display metadata about the vector file, including geometry type, feature count, extent, and attributes</p> In\u00a0[\u00a0]: Copied! <pre>geoai.print_vector_info(vector_path, figsize=(18, 10))\n</pre> geoai.print_vector_info(vector_path, figsize=(18, 10)) <p>Analyze the \"height\" attribute of buildings to obtain statistical information</p> In\u00a0[\u00a0]: Copied! <pre>geoai.analyze_vector_attributes(vector_path, \"height\")\n</pre> geoai.analyze_vector_attributes(vector_path, \"height\") <p>Create a visualization of building footprints colored by their height values</p> In\u00a0[\u00a0]: Copied! <pre>geoai.visualize_vector_by_attribute(vector_path, \"height\")\n</pre> geoai.visualize_vector_by_attribute(vector_path, \"height\") <p>Clip the raster file to a specified extent</p> In\u00a0[\u00a0]: Copied! <pre>clip_raster_path = \"naip_clip.tif\"\ngeoai.clip_raster_by_bbox(\n    raster_path,\n    clip_raster_path,\n    bbox=(0, 0, 500, 500),\n    bands=[1, 2, 3],\n    bbox_type=\"pixel\",\n)\n</pre> clip_raster_path = \"naip_clip.tif\" geoai.clip_raster_by_bbox(     raster_path,     clip_raster_path,     bbox=(0, 0, 500, 500),     bands=[1, 2, 3],     bbox_type=\"pixel\", ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(clip_raster_path)\n</pre> geoai.view_image(clip_raster_path)"},{"location":"examples/view_metadata/#view-metadata","title":"View Metadata\u00b6","text":"<p>Install Package</p> <p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/wetland_mapping/","title":"Wetland mapping","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import leafmap\nimport geoai\n</pre> import leafmap import geoai In\u00a0[\u00a0]: Copied! <pre>train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/m_4609931_ne_14_1_20100629.tif\"\ntest_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/m_4609932_nw_14_1_20100629.tif\"\n</pre> train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/m_4609931_ne_14_1_20100629.tif\" test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/m_4609932_nw_14_1_20100629.tif\" In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.print_raster_info(test_raster_path, show_preview=False)\n</pre> geoai.print_raster_info(test_raster_path, show_preview=False) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(train_raster_url)\n</pre> geoai.view_raster(train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_cog_layer(train_raster_url, bidx=[4, 1, 2], name=\"NAIP Imagery\")\nm\n</pre> m = leafmap.Map() m.add_cog_layer(train_raster_url, bidx=[4, 1, 2], name=\"NAIP Imagery\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-99.192, 46.56, -99.1206, 46.6283]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-99.192, 46.56, -99.1206, 46.6283] In\u00a0[\u00a0]: Copied! <pre>bbox_geometry = leafmap.bbox_to_gdf(bbox)\n</pre> bbox_geometry = leafmap.bbox_to_gdf(bbox) In\u00a0[\u00a0]: Copied! <pre>nwi_gdf = leafmap.get_nwi(bbox_geometry)\nnwi_gdf.head()\n</pre> nwi_gdf = leafmap.get_nwi(bbox_geometry) nwi_gdf.head() In\u00a0[\u00a0]: Copied! <pre>m.add_nwi(nwi_gdf, col_name=\"Wetlands.WETLAND_TYPE\")\n</pre> m.add_nwi(nwi_gdf, col_name=\"Wetlands.WETLAND_TYPE\") In\u00a0[\u00a0]: Copied! <pre>nwi_geojson = \"nwi_wetlands.geojson\"\nnwi_gdf.to_file(nwi_geojson)\n</pre> nwi_geojson = \"nwi_wetlands.geojson\" nwi_gdf.to_file(nwi_geojson) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"output\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=nwi_geojson,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"output\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=nwi_geojson,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_MaskRCNN_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    num_channels=4,\n    pretrained=True,\n    batch_size=4,\n    num_epochs=10,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_MaskRCNN_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     num_channels=4,     pretrained=True,     batch_size=4,     num_epochs=10,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"naip_test_prediction.tif\"\nmodel_path = f\"{out_folder}/models/best_model.pth\"\n</pre> masks_path = \"naip_test_prediction.tif\" model_path = f\"{out_folder}/models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path,\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.3,\n    batch_size=4,\n    num_channels=4,\n)\n</pre> geoai.object_detection(     test_raster_path,     masks_path,     model_path,     window_size=512,     overlap=256,     confidence_threshold=0.3,     batch_size=4,     num_channels=4, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"naip_test_prediction.geojson\"\ngdf = geoai.raster_to_vector(\n    masks_path, output_path, min_area=1000, simplify_tolerance=1\n)\n</pre> output_path = \"naip_test_prediction.geojson\" gdf = geoai.raster_to_vector(     masks_path, output_path, min_area=1000, simplify_tolerance=1 ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(output_path, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(output_path, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=output_path,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.4}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=output_path,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.4}},     basemap=test_raster_url, )"},{"location":"examples/wetland_mapping/#wetland-mapping-with-geoai","title":"Wetland Mapping with GeoAI\u00b6","text":""},{"location":"examples/wetland_mapping/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/wetland_mapping/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/wetland_mapping/#download-sample-naip-imagery","title":"Download sample NAIP imagery\u00b6","text":"<p>Sample NAIP imagery: https://huggingface.co/datasets/giswqs/geospatial/tree/main/naip</p>"},{"location":"examples/wetland_mapping/#download-sample-wetland-data","title":"Download sample wetland data\u00b6","text":"<p>National Wetlands Inventory: https://www.fws.gov/program/national-wetlands-inventory/wetlands-mapper</p>"},{"location":"examples/wetland_mapping/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"examples/wetland_mapping/#train-object-detection-model","title":"Train object detection model\u00b6","text":""},{"location":"examples/wetland_mapping/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/wetland_mapping/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/wetland_mapping/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/","title":"Lidar viz","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"leafmap[lidar]\" open3d\n</pre> # %pip install \"leafmap[lidar]\" open3d In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap <p>Download a sample LiDAR dataset from Google Drive. The zip file is 52.1 MB and the uncompressed LAS file is 109 MB.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://opengeos.org/data/lidar/madison.zip\"\nfilename = \"madison.las\"\n</pre> url = \"https://opengeos.org/data/lidar/madison.zip\" filename = \"madison.las\" In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(url, \"madison.zip\", unzip=True)\n</pre> leafmap.download_file(url, \"madison.zip\", unzip=True) <p>Read the LiDAR data</p> In\u00a0[\u00a0]: Copied! <pre>las = leafmap.read_lidar(filename)\n</pre> las = leafmap.read_lidar(filename) <p>The LAS header.</p> In\u00a0[\u00a0]: Copied! <pre>las.header\n</pre> las.header <p>The number of points.</p> In\u00a0[\u00a0]: Copied! <pre>las.header.point_count\n</pre> las.header.point_count <p>The list of features.</p> In\u00a0[\u00a0]: Copied! <pre>list(las.point_format.dimension_names)\n</pre> list(las.point_format.dimension_names) <p>Inspect data.</p> In\u00a0[\u00a0]: Copied! <pre>las.X\n</pre> las.X In\u00a0[\u00a0]: Copied! <pre>las.Y\n</pre> las.Y In\u00a0[\u00a0]: Copied! <pre>las.Z\n</pre> las.Z In\u00a0[\u00a0]: Copied! <pre>las.intensity\n</pre> las.intensity In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"pyvista\")\n</pre> leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"pyvista\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, backend=\"ipygany\", background=\"white\")\n</pre> leafmap.view_lidar(filename, backend=\"ipygany\", background=\"white\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"panel\", background=\"white\")\n</pre> leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"panel\", background=\"white\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, backend=\"open3d\")\n</pre> leafmap.view_lidar(filename, backend=\"open3d\") <p></p>"},{"location":"examples/dataviz/lidar_viz/#visualizing-lidar-data-with-leafmap","title":"Visualizing LiDAR Data with Leafmap\u00b6","text":"<p>This notebook demonstrates how to visualize LiDAR data using leafmap.</p>"},{"location":"examples/dataviz/lidar_viz/#installation","title":"Installation\u00b6","text":"<p>Uncomment and run the following cell to install the required Python packages.</p>"},{"location":"examples/dataviz/lidar_viz/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#download-data","title":"Download data\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#metadata","title":"Metadata\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#read-data","title":"Read data\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#pyvista","title":"PyVista\u00b6","text":"<p>Visualize LiDAR data using the pyvista backend.</p>"},{"location":"examples/dataviz/lidar_viz/#ipygany","title":"ipygany\u00b6","text":"<p>Visualize LiDAR data using the ipygany backend.</p>"},{"location":"examples/dataviz/lidar_viz/#panel","title":"Panel\u00b6","text":"<p>Visualize LiDAR data using the panel backend.</p>"},{"location":"examples/dataviz/lidar_viz/#open3d","title":"Open3D\u00b6","text":"<p>Visualize LiDAR data using the open3d backend.</p>"},{"location":"examples/dataviz/raster_viz/","title":"Raster viz","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"leafmap[raster]\"\n</pre> # %pip install \"leafmap[raster]\" In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nurl = \"https://opendata.digitalglobe.com/events/california-fire-2020/pre-event/2018-02-16/pine-gulch-fire20/1030010076004E00.tif\"\nm.add_cog_layer(url, name=\"Fire (pre-event)\")\nm\n</pre> m = leafmap.Map() url = \"https://opendata.digitalglobe.com/events/california-fire-2020/pre-event/2018-02-16/pine-gulch-fire20/1030010076004E00.tif\" m.add_cog_layer(url, name=\"Fire (pre-event)\") m <p>You can add multiple COGs to the map. Let's add another COG to the map.</p> In\u00a0[\u00a0]: Copied! <pre>url2 = \"https://opendata.digitalglobe.com/events/california-fire-2020/post-event/2020-08-14/pine-gulch-fire20/10300100AAC8DD00.tif\"\nm.add_cog_layer(url2, name=\"Fire (post-event)\")\nm\n</pre> url2 = \"https://opendata.digitalglobe.com/events/california-fire-2020/post-event/2020-08-14/pine-gulch-fire20/10300100AAC8DD00.tif\" m.add_cog_layer(url2, name=\"Fire (post-event)\") m <p></p> <p>Create a split map for comparing two COGs.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.split_map(left_layer=url, right_layer=url2)\nm\n</pre> m = leafmap.Map() m.split_map(left_layer=url, right_layer=url2) m <p></p> In\u00a0[\u00a0]: Copied! <pre>dem_url = \"https://opengeos.org/data/raster/srtm90.tif\"\nleafmap.download_file(dem_url, unzip=False)\n</pre> dem_url = \"https://opengeos.org/data/raster/srtm90.tif\" leafmap.download_file(dem_url, unzip=False) <p>Visualize a single-band raster.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"srtm90.tif\", cmap=\"terrain\", layer_name=\"DEM\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"srtm90.tif\", cmap=\"terrain\", layer_name=\"DEM\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>landsat_url = \"https://opengeos.org/data/raster/cog.tif\"\nleafmap.download_file(landsat_url)\n</pre> landsat_url = \"https://opengeos.org/data/raster/cog.tif\" leafmap.download_file(landsat_url) <p>Visualize a multi-band raster.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"cog.tif\", bands=[4, 3, 2], layer_name=\"Landsat\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"cog.tif\", bands=[4, 3, 2], layer_name=\"Landsat\") m <p></p> <p>Create an interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://canada-spot-ortho.s3.amazonaws.com/canada_spot_orthoimages/canada_spot5_orthoimages/S5_2007/S5_11055_6057_20070622/S5_11055_6057_20070622.json\"\nleafmap.stac_bands(url)\n</pre> url = \"https://canada-spot-ortho.s3.amazonaws.com/canada_spot_orthoimages/canada_spot5_orthoimages/S5_2007/S5_11055_6057_20070622/S5_11055_6057_20070622.json\" leafmap.stac_bands(url) <p>Add STAC layers to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_stac_layer(url, bands=[\"pan\"], name=\"Panchromatic\")\nm.add_stac_layer(url, bands=[\"B3\", \"B2\", \"B1\"], name=\"False color\")\nm\n</pre> m = leafmap.Map() m.add_stac_layer(url, bands=[\"pan\"], name=\"Panchromatic\") m.add_stac_layer(url, bands=[\"B3\", \"B2\", \"B1\"], name=\"False color\") m <p></p> <p>Provide custom STAC API endpoints as a dictionary in the format of <code>{\"name\": \"url\"}</code>. The name will show up in the dropdown menu, while the url is the STAC API endpoint that will be used to search for items.</p> In\u00a0[\u00a0]: Copied! <pre>catalogs = {\n    \"Element84 Earth Search\": \"https://earth-search.aws.element84.com/v1\",\n    \"Microsoft Planetary Computer\": \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n}\n</pre> catalogs = {     \"Element84 Earth Search\": \"https://earth-search.aws.element84.com/v1\",     \"Microsoft Planetary Computer\": \"https://planetarycomputer.microsoft.com/api/stac/v1\", } In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40, -100], zoom=4)\nm.set_catalog_source(catalogs)\nm.add_stac_gui()\nm\n</pre> m = leafmap.Map(center=[40, -100], zoom=4) m.set_catalog_source(catalogs) m.add_stac_gui() m <p>Once the catalog panel is open, you can search for items from the custom STAC API endpoints. Simply draw a bounding box on the map or zoom to a location of interest. Click on the Collections button to retrieve the collections from the custom STAC API endpoints. Next, select a collection from the dropdown menu. Then, click on the Items button to retrieve the items from the selected collection. Finally, click on the Display button to add the selected item to the map.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>m.stac_gdf  # The GeoDataFrame of the STAC search results\n</pre> m.stac_gdf  # The GeoDataFrame of the STAC search results In\u00a0[\u00a0]: Copied! <pre>m.stac_dict  # The STAC search results as a dictionary\n</pre> m.stac_dict  # The STAC search results as a dictionary In\u00a0[\u00a0]: Copied! <pre>m.stac_item  # The selected STAC item of the search result\n</pre> m.stac_item  # The selected STAC item of the search result <p>To Be able to run this notebook you'll need to have AWS credential available as environment variables. Uncomment the following lines to set the environment variables.</p> In\u00a0[\u00a0]: Copied! <pre># os.environ[\"AWS_ACCESS_KEY_ID\"] = \"YOUR AWS ACCESS ID HERE\"\n# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"YOUR AWS ACCESS KEY HERE\"\n</pre> # os.environ[\"AWS_ACCESS_KEY_ID\"] = \"YOUR AWS ACCESS ID HERE\" # os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"YOUR AWS ACCESS KEY HERE\" <p>In this example, we will use datasets from the Maxar Open Data Program on AWS.</p> In\u00a0[\u00a0]: Copied! <pre>BUCKET = \"maxar-opendata\"\nFOLDER = \"events/Kahramanmaras-turkey-earthquake-23/\"\n</pre> BUCKET = \"maxar-opendata\" FOLDER = \"events/Kahramanmaras-turkey-earthquake-23/\" <p>List all the datasets in the bucket. Specify a file extension to filter the results if needed.</p> In\u00a0[\u00a0]: Copied! <pre>items = leafmap.s3_list_objects(BUCKET, FOLDER, ext=\".tif\")\nitems[:10]\n</pre> items = leafmap.s3_list_objects(BUCKET, FOLDER, ext=\".tif\") items[:10] <p>Visualize raster datasets from the bucket.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_cog_layer(items[2], name=\"Maxar\")\nm\n</pre> m = leafmap.Map() m.add_cog_layer(items[2], name=\"Maxar\") m <p></p>"},{"location":"examples/dataviz/raster_viz/#visualizing-raster-data-with-leafmap","title":"Visualizing Raster Data with Leafmap\u00b6","text":"<p>This notebook demonstrates how to visualize raster data using leafmap. Leafmap can visualize raster data (e.g., Cloud Optimized GeoTIFF) stored in a local file or on the cloud (e.g., AWS S3). It can also visualize raster data stored in a STAC catalog.</p>"},{"location":"examples/dataviz/raster_viz/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install the required packages if needed.</p>"},{"location":"examples/dataviz/raster_viz/#import-packages","title":"Import packages\u00b6","text":""},{"location":"examples/dataviz/raster_viz/#cog","title":"COG\u00b6","text":"<p>A Cloud Optimized GeoTIFF (COG) is a regular GeoTIFF file, aimed at being hosted on a HTTP file server, with an internal organization that enables more efficient workflows on the cloud. It does this by leveraging the ability of clients issuing HTTP GET range requests to ask for just the parts of a file they need. More information about COG can be found at https://www.cogeo.org/in-depth.html</p> <p>For this demo, we will use data from https://www.maxar.com/open-data/california-colorado-fires for mapping California and Colorado fires. Let's create an interactive map and add the COG to the map.</p>"},{"location":"examples/dataviz/raster_viz/#local-raster","title":"Local Raster\u00b6","text":"<p>Leafmap can also visualize local GeoTIFF files. Let's download some sample data</p>"},{"location":"examples/dataviz/raster_viz/#stac","title":"STAC\u00b6","text":"<p>The SpatioTemporal Asset Catalog (STAC) specification provides a common language to describe a range of geospatial information so that it can more easily be indexed and discovered. A SpatioTemporal Asset is any file that represents information about the earth captured in a certain space and time. STAC aims to enable that next generation of geospatial search engines, while also supporting web best practices so geospatial information is more easily surfaced in traditional search engines. More information about STAC can be found at the STAC website. In this example, we will use a STAC item from the SPOT Orthoimages of Canada available through the link below:</p>"},{"location":"examples/dataviz/raster_viz/#custom-stac-catalog","title":"Custom STAC Catalog\u00b6","text":""},{"location":"examples/dataviz/raster_viz/#aws-s3","title":"AWS S3\u00b6","text":""},{"location":"examples/dataviz/vector_viz/","title":"Vector viz","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"leafmap[vector]\"\n</pre> # %pip install \"leafmap[vector]\" In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[0, 0], zoom=2)\ndata = \"https://opengeos.org/data/vector/cables.geojson\"\nm.add_vector(data, layer_name=\"Cable lines\", info_mode=\"on_hover\")\nm\n</pre> m = leafmap.Map(center=[0, 0], zoom=2) data = \"https://opengeos.org/data/vector/cables.geojson\" m.add_vector(data, layer_name=\"Cable lines\", info_mode=\"on_hover\") m <p>You can style the vector with custom style callback functions.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[20, 0], zoom=2)\nm.add_basemap(\"CartoDB.DarkMatter\")\ndata = \"https://opengeos.org/data/vector/cables.geojson\"\ncallback = lambda feat: {\"color\": feat[\"properties\"][\"color\"], \"weight\": 1}\nm.add_vector(data, layer_name=\"Cable lines\", style_callback=callback)\nm\n</pre> m = leafmap.Map(center=[20, 0], zoom=2) m.add_basemap(\"CartoDB.DarkMatter\") data = \"https://opengeos.org/data/vector/cables.geojson\" callback = lambda feat: {\"color\": feat[\"properties\"][\"color\"], \"weight\": 1} m.add_vector(data, layer_name=\"Cable lines\", style_callback=callback) m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\ndata = \"https://raw.githubusercontent.com/opengeos/leafmap/master/docs/examples/data/countries.geojson\"\nm.add_data(\n    data, column=\"POP_EST\", scheme=\"Quantiles\", cmap=\"Blues\", legend_title=\"Population\"\n)\nm\n</pre> m = leafmap.Map() data = \"https://raw.githubusercontent.com/opengeos/leafmap/master/docs/examples/data/countries.geojson\" m.add_data(     data, column=\"POP_EST\", scheme=\"Quantiles\", cmap=\"Blues\", legend_title=\"Population\" ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_data(\n    data,\n    column=\"POP_EST\",\n    scheme=\"EqualInterval\",\n    cmap=\"Blues\",\n    legend_title=\"Population\",\n)\nm\n</pre> m = leafmap.Map() m.add_data(     data,     column=\"POP_EST\",     scheme=\"EqualInterval\",     cmap=\"Blues\",     legend_title=\"Population\", ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://opengeos.org/data/duckdb/cities.parquet\"\ngdf = leafmap.read_parquet(url, return_type=\"gdf\", src_crs=\"EPSG:4326\")\ngdf.head()\n</pre> url = \"https://opengeos.org/data/duckdb/cities.parquet\" gdf = leafmap.read_parquet(url, return_type=\"gdf\", src_crs=\"EPSG:4326\") gdf.head() <p>Visualize point data.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_vector(\n    gdf,\n    get_radius=20000,\n    get_fill_color=\"blue\",\n    zoom_to_layer=False,\n    map_args={\"center\": (40, -100), \"zoom\": 3, \"height\": 500},\n)\n</pre> leafmap.view_vector(     gdf,     get_radius=20000,     get_fill_color=\"blue\",     zoom_to_layer=False,     map_args={\"center\": (40, -100), \"zoom\": 3, \"height\": 500}, ) <p></p> <p>Visualizing polygon data.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://data.source.coop/giswqs/nwi/wetlands/DC_Wetlands.parquet\"\ngdf = leafmap.read_parquet(\n    url, return_type=\"gdf\", src_crs=\"EPSG:5070\", dst_crs=\"EPSG:4326\"\n)\ngdf.head()\n</pre> url = \"https://data.source.coop/giswqs/nwi/wetlands/DC_Wetlands.parquet\" gdf = leafmap.read_parquet(     url, return_type=\"gdf\", src_crs=\"EPSG:5070\", dst_crs=\"EPSG:4326\" ) gdf.head() In\u00a0[\u00a0]: Copied! <pre>leafmap.view_vector(gdf, get_fill_color=[0, 0, 255, 128])\n</pre> leafmap.view_vector(gdf, get_fill_color=[0, 0, 255, 128]) <p></p> <p>Alternatively, you can specify a color map to visualize the data.</p> In\u00a0[\u00a0]: Copied! <pre>color_map = {\n    \"Freshwater Forested/Shrub Wetland\": (0, 136, 55),\n    \"Freshwater Emergent Wetland\": (127, 195, 28),\n    \"Freshwater Pond\": (104, 140, 192),\n    \"Estuarine and Marine Wetland\": (102, 194, 165),\n    \"Riverine\": (1, 144, 191),\n    \"Lake\": (19, 0, 124),\n    \"Estuarine and Marine Deepwater\": (0, 124, 136),\n    \"Other\": (178, 134, 86),\n}\n</pre> color_map = {     \"Freshwater Forested/Shrub Wetland\": (0, 136, 55),     \"Freshwater Emergent Wetland\": (127, 195, 28),     \"Freshwater Pond\": (104, 140, 192),     \"Estuarine and Marine Wetland\": (102, 194, 165),     \"Riverine\": (1, 144, 191),     \"Lake\": (19, 0, 124),     \"Estuarine and Marine Deepwater\": (0, 124, 136),     \"Other\": (178, 134, 86), } In\u00a0[\u00a0]: Copied! <pre>leafmap.view_vector(gdf, color_column=\"WETLAND_TYPE\", color_map=color_map, opacity=0.5)\n</pre> leafmap.view_vector(gdf, color_column=\"WETLAND_TYPE\", color_map=color_map, opacity=0.5) <p></p> <p>Display a legend for the data.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.Legend(title=\"Wetland Type\", legend_dict=color_map)\n</pre> leafmap.Legend(title=\"Wetland Type\", legend_dict=color_map) <p></p> In\u00a0[\u00a0]: Copied! <pre>import leafmap.foliumap as leafmap\n</pre> import leafmap.foliumap as leafmap <p>Check the metadata of the PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://storage.googleapis.com/ahp-research/overture/pmtiles/overture.pmtiles\"\nmetadata = leafmap.pmtiles_metadata(url)\nprint(f\"layer names: {metadata['layer_names']}\")\nprint(f\"bounds: {metadata['bounds']}\")\n</pre> url = \"https://storage.googleapis.com/ahp-research/overture/pmtiles/overture.pmtiles\" metadata = leafmap.pmtiles_metadata(url) print(f\"layer names: {metadata['layer_names']}\") print(f\"bounds: {metadata['bounds']}\") <p>Visualize the PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_basemap(\"CartoDB.DarkMatter\")\n\nstyle = {\n    \"version\": 8,\n    \"sources\": {\n        \"example_source\": {\n            \"type\": \"vector\",\n            \"url\": \"pmtiles://\" + url,\n            \"attribution\": \"PMTiles\",\n        }\n    },\n    \"layers\": [\n        {\n            \"id\": \"admins\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"admins\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#BDD3C7\", \"fill-opacity\": 0.1},\n        },\n        {\n            \"id\": \"buildings\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"buildings\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#FFFFB3\", \"fill-opacity\": 0.5},\n        },\n        {\n            \"id\": \"places\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"places\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#BEBADA\", \"fill-opacity\": 0.5},\n        },\n        {\n            \"id\": \"roads\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"roads\",\n            \"type\": \"line\",\n            \"paint\": {\"line-color\": \"#FB8072\"},\n        },\n    ],\n}\n\nm.add_pmtiles(\n    url, name=\"PMTiles\", style=style, overlay=True, show=True, zoom_to_layer=True\n)\n\nlegend_dict = {\n    \"admins\": \"BDD3C7\",\n    \"buildings\": \"FFFFB3\",\n    \"places\": \"BEBADA\",\n    \"roads\": \"FB8072\",\n}\n\nm.add_legend(legend_dict=legend_dict)\nm\n</pre> m = leafmap.Map() m.add_basemap(\"CartoDB.DarkMatter\")  style = {     \"version\": 8,     \"sources\": {         \"example_source\": {             \"type\": \"vector\",             \"url\": \"pmtiles://\" + url,             \"attribution\": \"PMTiles\",         }     },     \"layers\": [         {             \"id\": \"admins\",             \"source\": \"example_source\",             \"source-layer\": \"admins\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#BDD3C7\", \"fill-opacity\": 0.1},         },         {             \"id\": \"buildings\",             \"source\": \"example_source\",             \"source-layer\": \"buildings\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#FFFFB3\", \"fill-opacity\": 0.5},         },         {             \"id\": \"places\",             \"source\": \"example_source\",             \"source-layer\": \"places\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#BEBADA\", \"fill-opacity\": 0.5},         },         {             \"id\": \"roads\",             \"source\": \"example_source\",             \"source-layer\": \"roads\",             \"type\": \"line\",             \"paint\": {\"line-color\": \"#FB8072\"},         },     ], }  m.add_pmtiles(     url, name=\"PMTiles\", style=style, overlay=True, show=True, zoom_to_layer=True )  legend_dict = {     \"admins\": \"BDD3C7\",     \"buildings\": \"FFFFB3\",     \"places\": \"BEBADA\",     \"roads\": \"FB8072\", }  m.add_legend(legend_dict=legend_dict) m <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://data.source.coop/vida/google-microsoft-open-buildings/pmtiles/go_ms_building_footprints.pmtiles\"\nmetadata = leafmap.pmtiles_metadata(url)\nprint(f\"layer names: {metadata['layer_names']}\")\nprint(f\"bounds: {metadata['bounds']}\")\n</pre> url = \"https://data.source.coop/vida/google-microsoft-open-buildings/pmtiles/go_ms_building_footprints.pmtiles\" metadata = leafmap.pmtiles_metadata(url) print(f\"layer names: {metadata['layer_names']}\") print(f\"bounds: {metadata['bounds']}\") <p>Visualize the PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[20, 0], zoom=2)\nm.add_basemap(\"CartoDB.DarkMatter\")\nm.add_basemap(\"Esri.WorldImagery\", show=False)\n\nstyle = {\n    \"version\": 8,\n    \"sources\": {\n        \"example_source\": {\n            \"type\": \"vector\",\n            \"url\": \"pmtiles://\" + url,\n            \"attribution\": \"PMTiles\",\n        }\n    },\n    \"layers\": [\n        {\n            \"id\": \"buildings\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"building_footprints\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},\n        },\n    ],\n}\n\nm.add_pmtiles(\n    url, name=\"Buildings\", style=style, overlay=True, show=True, zoom_to_layer=False\n)\n\nm\n</pre> m = leafmap.Map(center=[20, 0], zoom=2) m.add_basemap(\"CartoDB.DarkMatter\") m.add_basemap(\"Esri.WorldImagery\", show=False)  style = {     \"version\": 8,     \"sources\": {         \"example_source\": {             \"type\": \"vector\",             \"url\": \"pmtiles://\" + url,             \"attribution\": \"PMTiles\",         }     },     \"layers\": [         {             \"id\": \"buildings\",             \"source\": \"example_source\",             \"source-layer\": \"building_footprints\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},         },     ], }  m.add_pmtiles(     url, name=\"Buildings\", style=style, overlay=True, show=True, zoom_to_layer=False )  m <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://raw.githubusercontent.com/opengeos/open-data/main/datasets/libya/Derna_buildings.geojson\"\nleafmap.download_file(url, \"buildings.geojson\")\n</pre> url = \"https://raw.githubusercontent.com/opengeos/open-data/main/datasets/libya/Derna_buildings.geojson\" leafmap.download_file(url, \"buildings.geojson\") <p>Convert vector to PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>pmtiles = \"buildings.pmtiles\"\nleafmap.geojson_to_pmtiles(\n    \"buildings.geojson\", pmtiles, layer_name=\"buildings\", overwrite=True, quiet=True\n)\n</pre> pmtiles = \"buildings.pmtiles\" leafmap.geojson_to_pmtiles(     \"buildings.geojson\", pmtiles, layer_name=\"buildings\", overwrite=True, quiet=True ) <p>Start a HTTP Sever</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.start_server(port=8000)\n</pre> leafmap.start_server(port=8000) In\u00a0[\u00a0]: Copied! <pre>url = f\"http://127.0.0.1:8000/{pmtiles}\"\nleafmap.pmtiles_metadata(url)\n</pre> url = f\"http://127.0.0.1:8000/{pmtiles}\" leafmap.pmtiles_metadata(url) <p>Display the PMTiles on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\n\nstyle = {\n    \"version\": 8,\n    \"sources\": {\n        \"example_source\": {\n            \"type\": \"vector\",\n            \"url\": \"pmtiles://\" + url,\n            \"attribution\": \"PMTiles\",\n        }\n    },\n    \"layers\": [\n        {\n            \"id\": \"buildings\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"buildings\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},\n        },\n    ],\n}\n\nm.add_pmtiles(url, name=\"Buildings\", show=True, zoom_to_layer=True, style=style)\nm\n</pre> m = leafmap.Map()  style = {     \"version\": 8,     \"sources\": {         \"example_source\": {             \"type\": \"vector\",             \"url\": \"pmtiles://\" + url,             \"attribution\": \"PMTiles\",         }     },     \"layers\": [         {             \"id\": \"buildings\",             \"source\": \"example_source\",             \"source-layer\": \"buildings\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},         },     ], }  m.add_pmtiles(url, name=\"Buildings\", show=True, zoom_to_layer=True, style=style) m <p></p>"},{"location":"examples/dataviz/vector_viz/#visualizing-vector-data-with-leafmap","title":"Visualizing Vector Data with Leafmap\u00b6","text":""},{"location":"examples/dataviz/vector_viz/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install leafmap if needed.</p>"},{"location":"examples/dataviz/vector_viz/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/dataviz/vector_viz/#visualize-vector-data","title":"Visualize vector data\u00b6","text":"<p>You can visualize vector data using the <code>add_vector</code> function. It supports common vector data formats, including GeoJSON, Shapefile, GeoPackage, and any other formats supported by geopandas.</p>"},{"location":"examples/dataviz/vector_viz/#choropleth-map","title":"Choropleth map\u00b6","text":"<p>You can create a choropleth map using the <code>add_data</code> function. It supports GeoJSON, Shapefile, GeoPackage, and any other formats supported by geopandas.</p>"},{"location":"examples/dataviz/vector_viz/#geoparquet","title":"GeoParquet\u00b6","text":"<p>Visualize GeoParquet data with leafmap and lonboard.</p>"},{"location":"examples/dataviz/vector_viz/#pmtiles","title":"PMTiles\u00b6","text":"<p>PMTiles is a single-file archive format for tiled data. A PMTiles archive can be hosted on a commodity storage platform such as S3, and enables low-cost, zero-maintenance map applications that are \"serverless\" - free of a custom tile backend or third party provider.</p>"},{"location":"examples/dataviz/vector_viz/#remote-pmtiles","title":"Remote PMTiles\u00b6","text":"<p>Leafmap can visualize PMTiles hosted locally or remotely.</p>"},{"location":"examples/dataviz/vector_viz/#overture-data","title":"Overture data\u00b6","text":""},{"location":"examples/dataviz/vector_viz/#source-cooperative","title":"Source Cooperative\u00b6","text":"<p>Visualize the Google-Microsoft Open Buildings data hosted on Source Cooperative.</p> <p>Check the metadata of the PMTiles.</p>"},{"location":"examples/dataviz/vector_viz/#local-pmtiles","title":"Local PMTiles\u00b6","text":"<p>tippecanoe is required to convert vector data to pmtiles. Install it with <code>conda install -c conda-forge tippecanoe</code>.</p> <p>Download building footprints of Derna, Libya.</p>"},{"location":"examples/rastervision/semantic_segmentation/","title":"Semantic segmentation","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport torch\nfrom matplotlib import pyplot as plt\n\nfrom rastervision.core.data import ClassConfig, SemanticSegmentationLabels\n\nimport albumentations as A\n\nfrom rastervision.pytorch_learner import (\n    SemanticSegmentationRandomWindowGeoDataset,\n    SemanticSegmentationSlidingWindowGeoDataset,\n    SemanticSegmentationVisualizer,\n    SemanticSegmentationGeoDataConfig,\n    SemanticSegmentationLearnerConfig,\n    SolverConfig,\n    SemanticSegmentationLearner,\n)\n</pre> import os import torch from matplotlib import pyplot as plt  from rastervision.core.data import ClassConfig, SemanticSegmentationLabels  import albumentations as A  from rastervision.pytorch_learner import (     SemanticSegmentationRandomWindowGeoDataset,     SemanticSegmentationSlidingWindowGeoDataset,     SemanticSegmentationVisualizer,     SemanticSegmentationGeoDataConfig,     SemanticSegmentationLearnerConfig,     SolverConfig,     SemanticSegmentationLearner, ) In\u00a0[\u00a0]: Copied! <pre>os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\"\n</pre> os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\" In\u00a0[\u00a0]: Copied! <pre>class_config = ClassConfig(\n    names=[\"background\", \"building\"],\n    colors=[\"lightgray\", \"darkred\"],\n    null_class=\"background\",\n)\n\nviz = SemanticSegmentationVisualizer(\n    class_names=class_config.names, class_colors=class_config.colors\n)\n</pre> class_config = ClassConfig(     names=[\"background\", \"building\"],     colors=[\"lightgray\", \"darkred\"],     null_class=\"background\", )  viz = SemanticSegmentationVisualizer(     class_names=class_config.names, class_colors=class_config.colors ) In\u00a0[\u00a0]: Copied! <pre>train_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/images/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13.tif\"\ntrain_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/labels/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13_Buildings.geojson\"\n\nval_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\"\nval_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\"\n</pre> train_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/images/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13.tif\" train_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/labels/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13_Buildings.geojson\"  val_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\" val_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\" In\u00a0[\u00a0]: Copied! <pre>pred_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\"\npred_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\"\n</pre> pred_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\" pred_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\" In\u00a0[\u00a0]: Copied! <pre>data_augmentation_transform = A.Compose(\n    [\n        A.Flip(),\n        A.ShiftScaleRotate(),\n        A.OneOf(\n            [\n                A.HueSaturationValue(hue_shift_limit=10),\n                A.RGBShift(),\n                A.ToGray(),\n                A.ToSepia(),\n                A.RandomBrightness(),\n                A.RandomGamma(),\n            ]\n        ),\n        A.CoarseDropout(max_height=32, max_width=32, max_holes=5),\n    ]\n)\n</pre> data_augmentation_transform = A.Compose(     [         A.Flip(),         A.ShiftScaleRotate(),         A.OneOf(             [                 A.HueSaturationValue(hue_shift_limit=10),                 A.RGBShift(),                 A.ToGray(),                 A.ToSepia(),                 A.RandomBrightness(),                 A.RandomGamma(),             ]         ),         A.CoarseDropout(max_height=32, max_width=32, max_holes=5),     ] ) In\u00a0[\u00a0]: Copied! <pre>train_ds = SemanticSegmentationRandomWindowGeoDataset.from_uris(\n    class_config=class_config,\n    image_uri=train_image_uri,\n    label_vector_uri=train_label_uri,\n    label_vector_default_class_id=class_config.get_class_id(\"building\"),\n    size_lims=(150, 200),\n    out_size=256,\n    max_windows=400,\n    transform=data_augmentation_transform,\n)\n\nlen(train_ds)\n</pre> train_ds = SemanticSegmentationRandomWindowGeoDataset.from_uris(     class_config=class_config,     image_uri=train_image_uri,     label_vector_uri=train_label_uri,     label_vector_default_class_id=class_config.get_class_id(\"building\"),     size_lims=(150, 200),     out_size=256,     max_windows=400,     transform=data_augmentation_transform, )  len(train_ds) In\u00a0[\u00a0]: Copied! <pre>x, y = viz.get_batch(train_ds, 4)\nviz.plot_batch(x, y, show=True)\n</pre> x, y = viz.get_batch(train_ds, 4) viz.plot_batch(x, y, show=True) In\u00a0[\u00a0]: Copied! <pre>val_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(\n    class_config=class_config,\n    image_uri=val_image_uri,\n    label_vector_uri=val_label_uri,\n    label_vector_default_class_id=class_config.get_class_id(\"building\"),\n    size=200,\n    stride=100,\n    transform=A.Resize(256, 256),\n)\nlen(val_ds)\n</pre> val_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(     class_config=class_config,     image_uri=val_image_uri,     label_vector_uri=val_label_uri,     label_vector_default_class_id=class_config.get_class_id(\"building\"),     size=200,     stride=100,     transform=A.Resize(256, 256), ) len(val_ds) In\u00a0[\u00a0]: Copied! <pre>x, y = viz.get_batch(val_ds, 4)\nviz.plot_batch(x, y, show=True)\n</pre> x, y = viz.get_batch(val_ds, 4) viz.plot_batch(x, y, show=True) In\u00a0[\u00a0]: Copied! <pre>pred_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(\n    class_config=class_config,\n    image_uri=pred_image_uri,\n    size=200,\n    stride=100,\n    transform=A.Resize(256, 256),\n)\nlen(pred_ds)\n</pre> pred_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(     class_config=class_config,     image_uri=pred_image_uri,     size=200,     stride=100,     transform=A.Resize(256, 256), ) len(pred_ds) In\u00a0[\u00a0]: Copied! <pre>model = torch.hub.load(\n    \"AdeelH/pytorch-fpn:0.3\",\n    \"make_fpn_resnet\",\n    name=\"resnet18\",\n    fpn_type=\"panoptic\",\n    num_classes=len(class_config),\n    fpn_channels=128,\n    in_channels=3,\n    out_size=(256, 256),\n    pretrained=True,\n)\n</pre> model = torch.hub.load(     \"AdeelH/pytorch-fpn:0.3\",     \"make_fpn_resnet\",     name=\"resnet18\",     fpn_type=\"panoptic\",     num_classes=len(class_config),     fpn_channels=128,     in_channels=3,     out_size=(256, 256),     pretrained=True, ) In\u00a0[\u00a0]: Copied! <pre>data_cfg = SemanticSegmentationGeoDataConfig(\n    class_names=class_config.names,\n    class_colors=class_config.colors,\n    num_workers=0,  # increase to use multi-processing\n)\n</pre> data_cfg = SemanticSegmentationGeoDataConfig(     class_names=class_config.names,     class_colors=class_config.colors,     num_workers=0,  # increase to use multi-processing ) In\u00a0[\u00a0]: Copied! <pre>solver_cfg = SolverConfig(batch_sz=8, lr=3e-2, class_loss_weights=[1.0, 10.0])\n</pre> solver_cfg = SolverConfig(batch_sz=8, lr=3e-2, class_loss_weights=[1.0, 10.0]) In\u00a0[\u00a0]: Copied! <pre>learner_cfg = SemanticSegmentationLearnerConfig(data=data_cfg, solver=solver_cfg)\n</pre> learner_cfg = SemanticSegmentationLearnerConfig(data=data_cfg, solver=solver_cfg) In\u00a0[\u00a0]: Copied! <pre>learner = SemanticSegmentationLearner(\n    cfg=learner_cfg,\n    output_dir=\"./train-demo/\",\n    model=model,\n    train_ds=train_ds,\n    valid_ds=val_ds,\n)\n</pre> learner = SemanticSegmentationLearner(     cfg=learner_cfg,     output_dir=\"./train-demo/\",     model=model,     train_ds=train_ds,     valid_ds=val_ds, ) In\u00a0[\u00a0]: Copied! <pre>learner.log_data_stats()\n</pre> learner.log_data_stats() In\u00a0[\u00a0]: Copied! <pre>%load_ext tensorboard\n</pre> %load_ext tensorboard In\u00a0[\u00a0]: Copied! <pre>%tensorboard --bind_all --logdir \"./train-demo/tb-logs\" --reload_interval 10\n</pre> %tensorboard --bind_all --logdir \"./train-demo/tb-logs\" --reload_interval 10 In\u00a0[\u00a0]: Copied! <pre>learner.train(epochs=3)\n</pre> learner.train(epochs=3) In\u00a0[\u00a0]: Copied! <pre>learner.train(epochs=1)\n</pre> learner.train(epochs=1) In\u00a0[\u00a0]: Copied! <pre>learner.plot_predictions(split=\"valid\", show=True)\n</pre> learner.plot_predictions(split=\"valid\", show=True) In\u00a0[\u00a0]: Copied! <pre>learner.save_model_bundle()\n</pre> learner.save_model_bundle() In\u00a0[\u00a0]: Copied! <pre>learner = SemanticSegmentationLearner.from_model_bundle(\n    model_bundle_uri=\"./train-demo/model-bundle.zip\",\n    output_dir=\"./train-demo/\",\n    model=model,\n)\n</pre> learner = SemanticSegmentationLearner.from_model_bundle(     model_bundle_uri=\"./train-demo/model-bundle.zip\",     output_dir=\"./train-demo/\",     model=model, ) In\u00a0[\u00a0]: Copied! <pre>learner = SemanticSegmentationLearner.from_model_bundle(\n    model_bundle_uri=\"./train-demo/model-bundle.zip\",\n    output_dir=\"./train-demo/\",\n    model=model,\n    train_ds=train_ds,\n    valid_ds=val_ds,\n    training=True,\n)\n</pre> learner = SemanticSegmentationLearner.from_model_bundle(     model_bundle_uri=\"./train-demo/model-bundle.zip\",     output_dir=\"./train-demo/\",     model=model,     train_ds=train_ds,     valid_ds=val_ds,     training=True, ) In\u00a0[\u00a0]: Copied! <pre>learner.train(epochs=1)\n</pre> learner.train(epochs=1) In\u00a0[\u00a0]: Copied! <pre>learner.plot_predictions(split=\"valid\", show=True)\n</pre> learner.plot_predictions(split=\"valid\", show=True) In\u00a0[\u00a0]: Copied! <pre>predictions = learner.predict_dataset(\n    pred_ds,\n    raw_out=True,\n    numpy_out=True,\n    predict_kw=dict(out_shape=(325, 325)),\n    progress_bar=True,\n)\n</pre> predictions = learner.predict_dataset(     pred_ds,     raw_out=True,     numpy_out=True,     predict_kw=dict(out_shape=(325, 325)),     progress_bar=True, ) In\u00a0[\u00a0]: Copied! <pre>pred_labels = SemanticSegmentationLabels.from_predictions(\n    pred_ds.windows,\n    predictions,\n    smooth=True,\n    extent=pred_ds.scene.extent,\n    num_classes=len(class_config),\n)\n</pre> pred_labels = SemanticSegmentationLabels.from_predictions(     pred_ds.windows,     predictions,     smooth=True,     extent=pred_ds.scene.extent,     num_classes=len(class_config), ) In\u00a0[\u00a0]: Copied! <pre>scores = pred_labels.get_score_arr(pred_labels.extent)\n</pre> scores = pred_labels.get_score_arr(pred_labels.extent) In\u00a0[\u00a0]: Copied! <pre>pred_labels.save(\n    uri=f\"predict\",\n    crs_transformer=pred_ds.scene.raster_source.crs_transformer,\n    class_config=class_config,\n)\n</pre> pred_labels.save(     uri=f\"predict\",     crs_transformer=pred_ds.scene.raster_source.crs_transformer,     class_config=class_config, )"},{"location":"examples/samgeo/arcgis/","title":"Arcgis","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo\n\n%matplotlib inline\n</pre> import os import leafmap from samgeo import SamGeo  %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>workspace = os.path.dirname(arcpy.env.workspace)\nos.chdir(workspace)\narcpy.env.overwriteOutput = True\n</pre> workspace = os.path.dirname(arcpy.env.workspace) os.chdir(workspace) arcpy.env.overwriteOutput = True In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/buildings.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/buildings.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/agriculture.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/agriculture.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/water.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/water.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>image = \"agriculture.tif\"\n</pre> image = \"agriculture.tif\" <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p> In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"ag_masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"ag_masks.tif\", foreground=True, unique=True) <p>If you run into GPU memory errors, uncomment the following code block and run it to empty cuda cache then rerun the code block above.</p> In\u00a0[\u00a0]: Copied! <pre># sam.clear_cuda_cache()\n</pre> # sam.clear_cuda_cache() <p>Show the segmentation result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"ag_annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"ag_annotations.tif\") <p>Add layers to ArcGIS Pro.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.arc_active_map()\n</pre> m = leafmap.arc_active_map() In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"agriculture.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"agriculture.tif\")) In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"ag_annotations.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"ag_annotations.tif\")) <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>in_raster = os.path.join(workspace, \"ag_masks.tif\")\nout_shp = os.path.join(workspace, \"ag_masks.shp\")\n</pre> in_raster = os.path.join(workspace, \"ag_masks.tif\") out_shp = os.path.join(workspace, \"ag_masks.shp\") In\u00a0[\u00a0]: Copied! <pre>arcpy.conversion.RasterToPolygon(in_raster, out_shp)\n</pre> arcpy.conversion.RasterToPolygon(in_raster, out_shp) In\u00a0[\u00a0]: Copied! <pre>image = \"water.tif\"\n</pre> image = \"water.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"water_masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"water_masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre># sam.clear_cuda_cache()\n</pre> # sam.clear_cuda_cache() In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"water_annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"water_annotations.tif\") In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"water.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"water.tif\")) In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"water_annotations.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"water_annotations.tif\")) In\u00a0[\u00a0]: Copied! <pre>in_raster = os.path.join(workspace, \"water_masks.tif\")\nout_shp = os.path.join(workspace, \"water_masks.shp\")\n</pre> in_raster = os.path.join(workspace, \"water_masks.tif\") out_shp = os.path.join(workspace, \"water_masks.shp\") In\u00a0[\u00a0]: Copied! <pre>arcpy.conversion.RasterToPolygon(in_raster, out_shp)\n</pre> arcpy.conversion.RasterToPolygon(in_raster, out_shp) In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(\"agriculture.tif\", output=\"ag_masks2.tif\", foreground=True)\n</pre> sam.generate(\"agriculture.tif\", output=\"ag_masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=0.5, output=\"ag_annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=0.5, output=\"ag_annotations2.tif\")"},{"location":"examples/samgeo/arcgis/#using-the-segment-geospatial-python-package-with-arcgis-pro","title":"Using the Segment-Geospatial Python Package with ArcGIS Pro\u00b6","text":"<p>The notebook shows step-by-step instructions for using the Segment Anything Model (SAM) with ArcGIS Pro. Check out the YouTube tutorial here and the Resources for Unlocking the Power of Deep Learning Applications Using ArcGIS. Credit goes to Esri.</p> <p></p>"},{"location":"examples/samgeo/arcgis/#installation","title":"Installation\u00b6","text":"<ol> <li><p>Open Windows Registry Editor (<code>regedit.exe</code>) and navigate to <code>Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem</code>. Change the value of <code>LongPathsEnabled</code> to <code>1</code>. See this screenshot. This is a known issue with the deep learning libraries for ArcGIS Pro 3.1. A future release might fix this issue.</p> </li> <li><p>Navigate to the Start Menu -&gt; All apps -&gt; ArcGIS folder, then open the Python Command Prompt.</p> </li> <li><p>Create a new conda environment and install mamba and Python 3.9.x from the Esri Anaconda channel. Mamba is a drop-in replacement for conda that is mach faster for installing Python packages and their dependencies.</p> <p><code>conda create conda-forge::mamba esri::python --name samgeo</code></p> </li> <li><p>Activate the new conda environment.</p> <p><code>conda activate samgeo</code></p> </li> <li><p>Install arcpy, deep-learning-essentials, segment-geospatial, and other dependencies (~4GB download).</p> <p><code>mamba install arcpy deep-learning-essentials leafmap localtileserver segment-geospatial -c esri -c conda-forge</code></p> </li> <li><p>Activate the new environment in ArcGIS Pro.</p> <p><code>proswap samgeo</code></p> </li> <li><p>Close the Python Command Prompt and open ArcGIS Pro.</p> </li> <li><p>Download this notebook and run it in ArcGIS Pro.</p> </li> </ol>"},{"location":"examples/samgeo/arcgis/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/samgeo/arcgis/#download-sample-data","title":"Download sample data\u00b6","text":"<p>In this example, we will use the high-resolution aerial imagery from the USDA National Agricultural Imagery Program (NAIP). You can download NAIP imagery using the USDA Data Gateway or the USDA NCRS Box Drive. I have downloaded some NAIP imagery and clipped them to a smaller area, which are available here.</p>"},{"location":"examples/samgeo/arcgis/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/arcgis/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Specify the file path to the image we downloaded earlier.</p>"},{"location":"examples/samgeo/arcgis/#segment-waterbodies","title":"Segment waterbodies\u00b6","text":""},{"location":"examples/samgeo/arcgis/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/samgeo/automatic_mask_generator/","title":"Automatic mask generator","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, show_image, download_file, overlay_images, tms_to_geotiff\n</pre> import os import leafmap from samgeo import SamGeo, show_image, download_file, overlay_images, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.2659, 37.8682, -122.2521, 37.8741]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.2659, 37.8682, -122.2521, 37.8741] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\") m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\")\n</pre> overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\") <p></p>"},{"location":"examples/samgeo/automatic_mask_generator/#automatically-generating-object-masks-with-sam","title":"Automatically generating object masks with SAM\u00b6","text":"<p>This notebook shows how to segment objects from an image using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p> <p>The notebook is adapted from segment-anything/notebooks/automatic_mask_generator_example.ipynb, but I have made it much easier to save the segmentation results and visualize them.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/","title":"Automatic mask generator hq","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo.hq_sam import (\n    SamGeo,\n    show_image,\n    download_file,\n    overlay_images,\n    tms_to_geotiff,\n)\n</pre> import os import leafmap from samgeo.hq_sam import (     SamGeo,     show_image,     download_file,     overlay_images,     tms_to_geotiff, ) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.2659, 37.8682, -122.2521, 37.8741]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.2659, 37.8682, -122.2521, 37.8741] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\") m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\")\n</pre> overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\") <p></p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#automatically-generating-object-masks-with-hq-sam","title":"Automatically generating object masks with HQ-SAM\u00b6","text":"<p>This notebook shows how to segment objects from an image using the High-Quality Segment Anything Model (HQ-SAM) with a few lines of code.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator_hq/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator_hq/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/samgeo/box_prompts/","title":"Box prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo import SamGeo\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo import SamGeo In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) <p>Display the map. Use the drawing tools to draw some rectangles around the features you want to extract, such as trees, buildings.</p> In\u00a0[\u00a0]: Copied! <pre>m\n</pre> m In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-51.2546, -22.1771, -51.2541, -22.1767],\n        [-51.2538, -22.1764, -51.2535, -22.1761],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-51.2546, -22.1771, -51.2541, -22.1767],         [-51.2538, -22.1764, -51.2535, -22.1761],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m In\u00a0[\u00a0]: Copied! <pre>url = \"https://opengeos.github.io/data/sam/tree_boxes.geojson\"\ngeojson = \"tree_boxes.geojson\"\nleafmap.download_file(url, geojson)\n</pre> url = \"https://opengeos.github.io/data/sam/tree_boxes.geojson\" geojson = \"tree_boxes.geojson\" leafmap.download_file(url, geojson) <p>Display the vector data on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"Image.tif\", layer_name=\"image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"Image.tif\", layer_name=\"image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\") m In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=\"mask2.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=\"mask2.tif\", dtype=\"uint8\") <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask2.tif\", cmap=\"Greens\", nodata=0, opacity=0.5, layer_name=\"Tree masks\")\nm\n</pre> m.add_raster(\"mask2.tif\", cmap=\"Greens\", nodata=0, opacity=0.5, layer_name=\"Tree masks\") m <p></p>"},{"location":"examples/samgeo/box_prompts/#segmenting-remote-sensing-imagery-with-box-prompts","title":"Segmenting remote sensing imagery with box prompts\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/box_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/box_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/box_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/box_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p> <p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p> <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p>"},{"location":"examples/samgeo/box_prompts/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"examples/samgeo/box_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"examples/samgeo/box_prompts/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"examples/samgeo/box_prompts/#use-an-existing-vector-file-as-box-prompts","title":"Use an existing vector file as box prompts\u00b6","text":"<p>Alternatively, you can specify a file path to a vector file. Let's download a sample vector file from GitHub.</p>"},{"location":"examples/samgeo/box_prompts/#segment-image-with-box-prompts","title":"Segment image with box prompts\u00b6","text":"<p>Segment the image using the specified file path to the vector mask.</p>"},{"location":"examples/samgeo/fast_sam/","title":"Fast sam","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial segment-anything-fast\n</pre> # %pip install segment-geospatial segment-anything-fast In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.fast_sam import SamGeo\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.fast_sam import SamGeo In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>from samgeo.fast_sam import SamGeo\n\nsam = SamGeo(model=\"FastSAM-x.pt\")\n</pre> from samgeo.fast_sam import SamGeo  sam = SamGeo(model=\"FastSAM-x.pt\") <p>Set the image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(\"Image.tif\")\n</pre> sam.set_image(\"Image.tif\") <p>Segment the image with <code>everything_prompt</code>. You can also try <code>point_prompt</code>, <code>box_prompt</code>, or <code>text_prompt</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam.everything_prompt(output=\"mask.tif\")\n</pre> sam.everything_prompt(output=\"mask.tif\") <p>Show the annotated image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\"mask.png\")\n</pre> sam.show_anns(\"mask.png\") <p></p> <p>Convert the segmentation results from GeoTIFF to vector.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"mask.tif\", \"mask.geojson\")\n</pre> sam.raster_to_vector(\"mask.tif\", \"mask.geojson\") <p>Show the segmentation results on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", opacity=0.5, layer_name=\"Mask\")\nm.add_vector(\"mask.geojson\", layer_name=\"Mask Vector\")\nm\n</pre> m.add_raster(\"mask.tif\", opacity=0.5, layer_name=\"Mask\") m.add_vector(\"mask.geojson\", layer_name=\"Mask Vector\") m <p></p>"},{"location":"examples/samgeo/fast_sam/#segmenting-remote-sensing-imagery-with-fastsam","title":"Segmenting remote sensing imagery with FastSAM\u00b6","text":"<p>FastSAM: https://github.com/CASIA-IVA-Lab/FastSAM</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/fast_sam/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/fast_sam/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/fast_sam/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/fast_sam/#initialize-samgeo-class","title":"Initialize SamGeo class\u00b6","text":"<p>The initialization of the SamGeo class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/input_prompts/","title":"Input prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, tms_to_geotiff\n</pre> import os import leafmap from samgeo import SamGeo, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p>"},{"location":"examples/samgeo/input_prompts/#generating-object-masks-from-input-prompts-with-sam","title":"Generating object masks from input prompts with SAM\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p> <p>The notebook is adapted from segment-anything/notebooks/predictor_example.ipynb, but I have made it much easier to save the segmentation results and visualize them.</p>"},{"location":"examples/samgeo/input_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/input_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/input_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/input_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/input_prompts/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/samgeo/input_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/samgeo/input_prompts_hq/","title":"Input prompts hq","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo.hq_sam import SamGeo, tms_to_geotiff\n</pre> import os import leafmap from samgeo.hq_sam import SamGeo, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p>"},{"location":"examples/samgeo/input_prompts_hq/#generating-object-masks-from-input-prompts-with-hq-sam","title":"Generating object masks from input prompts with HQ-SAM\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the High-Quality Segment Anything Model (HQ-SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/input_prompts_hq/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/input_prompts_hq/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/input_prompts_hq/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/input_prompts_hq/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/input_prompts_hq/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/samgeo/input_prompts_hq/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/samgeo/maxar_open_data/","title":"Maxar open data","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, raster_to_vector, overlay_images\n</pre> import os import leafmap from samgeo import SamGeo, raster_to_vector, overlay_images In\u00a0[\u00a0]: Copied! <pre>url = (\n    \"https://drive.google.com/file/d/1jIIC5hvSPeJEC0fbDhtxVWk2XV9AxsQD/view?usp=sharing\"\n)\n</pre> url = (     \"https://drive.google.com/file/d/1jIIC5hvSPeJEC0fbDhtxVWk2XV9AxsQD/view?usp=sharing\" ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(url, output=\"image.tif\")\n</pre> leafmap.download_file(url, output=\"image.tif\") In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(height=\"600px\")\nm.add_basemap(\"SATELLITE\")\nm.add_raster(\"image.tif\", layer_name=\"Image\")\nm.add_layer_manager()\nm\n</pre> m = leafmap.Map(height=\"600px\") m.add_basemap(\"SATELLITE\") m.add_raster(\"image.tif\", layer_name=\"Image\") m.add_layer_manager() m <p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p> In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 80,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 80, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(\"image.tif\", output=\"mask.tif\", foreground=True)\n</pre> sam.generate(\"image.tif\", output=\"mask.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>raster_to_vector(\"mask.tif\", output=\"mask.shp\")\n</pre> raster_to_vector(\"mask.tif\", output=\"mask.shp\") In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Display the annotations (each mask with a random color).</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotation.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotation.tif\") In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"image.tif\",\n    \"annotation.tif\",\n    label1=\"Image\",\n    label2=\"Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"image.tif\",     \"annotation.tif\",     label1=\"Image\",     label2=\"Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(\"image.tif\", \"annotation.tif\", backend=\"TkAgg\")\n</pre> overlay_images(\"image.tif\", \"annotation.tif\", backend=\"TkAgg\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", layer_name=\"Mask\", nodata=0)\nm.add_raster(\"annotation.tif\", layer_name=\"Annotation\")\nm\n</pre> m.add_raster(\"mask.tif\", layer_name=\"Mask\", nodata=0) m.add_raster(\"annotation.tif\", layer_name=\"Annotation\") m In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"mask.shp\", layer_name=\"Vector\", info_mode=None)\n</pre> m.add_vector(\"mask.shp\", layer_name=\"Vector\", info_mode=None) <p></p>"},{"location":"examples/samgeo/maxar_open_data/#segmenting-satellite-imagery-from-the-maxar-open-data-program","title":"Segmenting satellite imagery from the Maxar Open Data Program\u00b6","text":"<p>This notebook shows how to segment satellite imagery from the Maxar Open Data program for Libya floods.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/maxar_open_data/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/maxar_open_data/#download-sample-data","title":"Download sample data\u00b6","text":"<p>First, let's download a sample image of Derna, Libya from here.</p>"},{"location":"examples/samgeo/maxar_open_data/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#segment-the-image","title":"Segment the image\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#convert-raster-to-vector","title":"Convert raster to vector\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#display-the-segmentation-result","title":"Display the segmentation result\u00b6","text":"<p>First, let's show the result as a binary image.</p>"},{"location":"examples/samgeo/maxar_open_data/#compare-images-with-a-slider","title":"Compare images with a slider\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#display-images-on-an-interactive-map","title":"Display images on an interactive map.\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/","title":"Satellite predictor","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeoPredictor, tms_to_geotiff, get_basemaps\nfrom segment_anything import sam_model_registry\n</pre> import os import leafmap from samgeo import SamGeoPredictor, tms_to_geotiff, get_basemaps from segment_anything import sam_model_registry In\u00a0[\u00a0]: Copied! <pre>zoom = 16\nm = leafmap.Map(center=[45, -123], zoom=zoom)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> zoom = 16 m = leafmap.Map(center=[45, -123], zoom=zoom) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-123.0127, 44.9957, -122.9874, 45.0045]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-123.0127, 44.9957, -122.9874, 45.0045] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\n# image = '/path/to/your/own/image.tif'\n</pre> image = \"satellite.tif\" # image = '/path/to/your/own/image.tif' <p>Besides the <code>satellite</code> basemap, you can use any of the following basemaps returned by the <code>get_basemaps()</code> function:</p> In\u00a0[\u00a0]: Copied! <pre># get_basemaps().keys()\n</pre> # get_basemaps().keys() <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>tms_to_geotiff(\n    output=image, bbox=bbox, zoom=zoom + 1, source=\"Satellite\", overwrite=True\n)\n</pre> tms_to_geotiff(     output=image, bbox=bbox, zoom=zoom + 1, source=\"Satellite\", overwrite=True ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.add_raster(image, layer_name=\"Image\") m <p>Use the draw tools to draw a rectangle from which to subset segmentations on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    clip_box = m.user_roi_bounds()\nelse:\n    clip_box = [-123.0064, 44.9988, -123.0005, 45.0025]\n</pre> if m.user_roi_bounds() is not None:     clip_box = m.user_roi_bounds() else:     clip_box = [-123.0064, 44.9988, -123.0005, 45.0025] In\u00a0[\u00a0]: Copied! <pre>clip_box\n</pre> clip_box In\u00a0[\u00a0]: Copied! <pre>out_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\ncheckpoint = os.path.join(out_dir, \"sam_vit_h_4b8939.pth\")\n</pre> out_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\") checkpoint = os.path.join(out_dir, \"sam_vit_h_4b8939.pth\") In\u00a0[\u00a0]: Copied! <pre>import cv2\n\nimg_arr = cv2.imread(image)\n\nmodel_type = \"vit_h\"\n\nsam = sam_model_registry[model_type](checkpoint=checkpoint)\n\npredictor = SamGeoPredictor(sam)\n\npredictor.set_image(img_arr)\n\nmasks, _, _ = predictor.predict(src_fp=image, geo_box=clip_box)\n</pre> import cv2  img_arr = cv2.imread(image)  model_type = \"vit_h\"  sam = sam_model_registry[model_type](checkpoint=checkpoint)  predictor = SamGeoPredictor(sam)  predictor.set_image(img_arr)  masks, _, _ = predictor.predict(src_fp=image, geo_box=clip_box) In\u00a0[\u00a0]: Copied! <pre>masks_img = \"preds.tif\"\npredictor.masks_to_geotiff(image, masks_img, masks.astype(\"uint8\"))\n</pre> masks_img = \"preds.tif\" predictor.masks_to_geotiff(image, masks_img, masks.astype(\"uint8\")) In\u00a0[\u00a0]: Copied! <pre>vector = \"feats.geojson\"\ngdf = predictor.geotiff_to_geojson(masks_img, vector, bidx=1)\ngdf.plot()\n</pre> vector = \"feats.geojson\" gdf = predictor.geotiff_to_geojson(masks_img, vector, bidx=1) gdf.plot() In\u00a0[\u00a0]: Copied! <pre>style = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(vector, layer_name=\"Vector\", style=style)\nm\n</pre> style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(vector, layer_name=\"Vector\", style=style) m"},{"location":"examples/samgeo/satellite-predictor/#segment-anything-model-for-geospatial-data","title":"Segment Anything Model for Geospatial Data\u00b6","text":"<p>This notebook shows how to use segment satellite imagery using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/satellite-predictor/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/satellite-predictor/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"examples/samgeo/satellite-predictor/#initialize-samgeopredictor-class","title":"Initialize SamGeoPredictor class\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/samgeo/satellite/","title":"Satellite","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, tms_to_geotiff, get_basemaps\n</pre> import os import leafmap from samgeo import SamGeo, tms_to_geotiff, get_basemaps In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[29.676840, -95.369222], zoom=19)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[29.676840, -95.369222], zoom=19) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-95.3704, 29.6762, -95.368, 29.6775]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-95.3704, 29.6762, -95.368, 29.6775] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\n</pre> image = \"satellite.tif\" <p>Besides the <code>satellite</code> basemap, you can use any of the following basemaps returned by the <code>get_basemaps()</code> function:</p> In\u00a0[\u00a0]: Copied! <pre># get_basemaps().keys()\n</pre> # get_basemaps().keys() <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>tms_to_geotiff(output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True)\n</pre> tms_to_geotiff(output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False  # turn off the basemap\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False  # turn off the basemap m.add_raster(image, layer_name=\"Image\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    checkpoint=\"sam_vit_h_4b8939.pth\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     checkpoint=\"sam_vit_h_4b8939.pth\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>mask = \"segment.tif\"\nsam.generate(\n    image, mask, batch=True, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255\n)\n</pre> mask = \"segment.tif\" sam.generate(     image, mask, batch=True, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255 ) In\u00a0[\u00a0]: Copied! <pre>vector = \"segment.gpkg\"\nsam.tiff_to_gpkg(mask, vector, simplify_tolerance=None)\n</pre> vector = \"segment.gpkg\" sam.tiff_to_gpkg(mask, vector, simplify_tolerance=None) <p>You can also save the segmentation results as any vector data format supported by GeoPandas.</p> In\u00a0[\u00a0]: Copied! <pre>shapefile = \"segment.shp\"\nsam.tiff_to_vector(mask, shapefile)\n</pre> shapefile = \"segment.shp\" sam.tiff_to_vector(mask, shapefile) In\u00a0[\u00a0]: Copied! <pre>style = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(vector, layer_name=\"Vector\", style=style)\nm\n</pre> style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(vector, layer_name=\"Vector\", style=style) m <p></p>"},{"location":"examples/samgeo/satellite/#segment-anything-model-for-geospatial-data","title":"Segment Anything Model for Geospatial Data\u00b6","text":"<p>This notebook shows how to use segment satellite imagery using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/satellite/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/satellite/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/samgeo/satellite/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/satellite/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"examples/samgeo/satellite/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/samgeo/satellite/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Set <code>batch=True</code> to segment the image in batches. This is useful for large images that cannot fit in memory.</p>"},{"location":"examples/samgeo/satellite/#polygonize-the-raster-data","title":"Polygonize the raster data\u00b6","text":"<p>Save the segmentation results as a GeoPackage file.</p>"},{"location":"examples/samgeo/satellite/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/samgeo/swimming_pools/","title":"Swimming pools","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[34.040984, -118.491668], zoom=19, height=\"600px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[34.040984, -118.491668], zoom=19, height=\"600px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-118.4932, 34.0404, -118.4903, 34.0417]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-118.4932, 34.0404, -118.4903, 34.0417] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"swimming pool\"\n</pre> text_prompt = \"swimming pool\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Blues\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Swimming Pools\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Blues\",     box_color=\"red\",     title=\"Automatic Segmentation of Swimming Pools\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Blues\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Swimming Pools\",\n)\n</pre> sam.show_anns(     cmap=\"Blues\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Swimming Pools\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Swimming Pools\",\n    blend=False,\n    output=\"pools.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Swimming Pools\",     blend=False,     output=\"pools.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"pools.tif\", \"pools.shp\")\n</pre> sam.raster_to_vector(\"pools.tif\", \"pools.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"pools.tif\", layer_name=\"Pools\", palette=\"Blues\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"pools.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"pools.tif\", layer_name=\"Pools\", palette=\"Blues\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"pools.shp\", layer_name=\"Vector\", style=style) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/samgeo/swimming_pools/#mapping-swimming-pools-with-text-prompts","title":"Mapping swimming pools with text prompts\u00b6","text":"<p>This notebook shows how to map swimming pools with text prompts and the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/swimming_pools/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/swimming_pools/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/swimming_pools/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/swimming_pools/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/swimming_pools/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/samgeo/swimming_pools/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/samgeo/swimming_pools/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/samgeo/swimming_pools/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/samgeo/text_prompts/","title":"Text prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"trees.tif\", \"trees.shp\")\n</pre> sam.raster_to_vector(\"trees.tif\", \"trees.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style) m In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/samgeo/text_prompts/#segmenting-remote-sensing-imagery-with-text-prompts-and-the-segment-anything-model-sam","title":"Segmenting remote sensing imagery with text prompts and the Segment Anything Model (SAM)\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/text_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/text_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/text_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/text_prompts/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/text_prompts/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/samgeo/text_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/samgeo/text_prompts/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/samgeo/text_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/","title":"Text prompts batch","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff, split_raster\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff, split_raster from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.1278, -51.4430], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.1278, -51.4430], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.4494, -22.1307, -51.4371, -22.1244]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.4494, -22.1307, -51.4371, -22.1244] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>split_raster(image, out_dir=\"tiles\", tile_size=(1000, 1000), overlap=0)\n</pre> split_raster(image, out_dir=\"tiles\", tile_size=(1000, 1000), overlap=0) In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict_batch(\n    images=\"tiles\",\n    out_dir=\"masks\",\n    text_prompt=text_prompt,\n    box_threshold=0.24,\n    text_threshold=0.24,\n    mask_multiplier=255,\n    dtype=\"uint8\",\n    merge=True,\n    verbose=True,\n)\n</pre> sam.predict_batch(     images=\"tiles\",     out_dir=\"masks\",     text_prompt=text_prompt,     box_threshold=0.24,     text_threshold=0.24,     mask_multiplier=255,     dtype=\"uint8\",     merge=True,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"masks/merged.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm.add_layer_manager()\nm\n</pre> m.add_raster(\"masks/merged.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m.add_layer_manager() m <p></p>"},{"location":"examples/samgeo/text_prompts_batch/#batch-segmentation-with-text-prompts","title":"Batch segmentation with text prompts\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/text_prompts_batch/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/text_prompts_batch/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/text_prompts_batch/#split-the-image-into-tiles","title":"Split the image into tiles\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/text_prompts_batch/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/#segment-images","title":"Segment images\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/samgeo/text_prompts_batch/#visualize-the-results","title":"Visualize the results\u00b6","text":""}]}