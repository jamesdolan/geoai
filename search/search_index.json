{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to geoai","text":"<p>A Python package for using Artificial Intelligence (AI) with geospatial data</p> <ul> <li>Free software: MIT license</li> <li>Documentation: https://geoai.gishub.org</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Visualizing geospatial data, including vector, raster, and LiDAR data</li> <li>Segmenting remote sensing imagery with the Segment Anything Model</li> <li>Classifying remote sensing imagery with deep learning models</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-aug-11-2023","title":"v0.0.1 - Aug 11, 2023","text":"<p>Initial release</p>"},{"location":"common/","title":"common module","text":"<p>The common module contains common functions and classes used by the other modules.</p>"},{"location":"common/#geoai.common.calc_stats","title":"<code>calc_stats(dataset, divide_by=1.0)</code>","text":"<p>Calculate the statistics (mean and std) for the entire dataset.</p> <p>This function is adapted from the plot_batch() function in the torchgeo library at https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html. Credit to the torchgeo developers for the original implementation.</p> <p>Warning: This is an approximation. The correct value should take into account the mean for the whole dataset for computing individual stds.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>RasterDataset</code> <p>The dataset to calculate statistics for.</p> required <code>divide_by</code> <code>float</code> <p>The value to divide the image data by. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tuple[np.ndarray, np.ndarray]</code> <p>The mean and standard deviation for each band.</p> Source code in <code>geoai/common.py</code> <pre><code>def calc_stats(\n    dataset: RasterDataset, divide_by: float = 1.0\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate the statistics (mean and std) for the entire dataset.\n\n    This function is adapted from the plot_batch() function in the torchgeo library at\n    https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html.\n    Credit to the torchgeo developers for the original implementation.\n\n    Warning: This is an approximation. The correct value should take into account the\n    mean for the whole dataset for computing individual stds.\n\n    Args:\n        dataset (RasterDataset): The dataset to calculate statistics for.\n        divide_by (float, optional): The value to divide the image data by. Defaults to 1.0.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: The mean and standard deviation for each band.\n    \"\"\"\n    import rasterio as rio\n\n    # To avoid loading the entire dataset in memory, we will loop through each img\n    # The filenames will be retrieved from the dataset's rtree index\n    files = [\n        item.object\n        for item in dataset.index.intersection(dataset.index.bounds, objects=True)\n    ]\n\n    # Resetting statistics\n    accum_mean = 0\n    accum_std = 0\n\n    for file in files:\n        img = rio.open(file).read() / divide_by  # type: ignore\n        accum_mean += img.reshape((img.shape[0], -1)).mean(axis=1)\n        accum_std += img.reshape((img.shape[0], -1)).std(axis=1)\n\n    # at the end, we shall have 2 vectors with length n=chnls\n    # we will average them considering the number of images\n    return accum_mean / len(files), accum_std / len(files)\n</code></pre>"},{"location":"common/#geoai.common.dict_to_image","title":"<code>dict_to_image(data_dict, output=None, **kwargs)</code>","text":"<p>Convert a dictionary containing spatial data to a rasterio dataset or save it to a file. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo dataset sampler.</p> <p>This function transforms a dictionary with CRS, bounding box, and image data into a rasterio DatasetReader using leafmap's array_to_image utility after first converting to a rioxarray DataArray.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>Dict[str, Any]</code> <p>A dictionary containing: - 'crs': A pyproj CRS object - 'bounds': A BoundingBox object with minx, maxx, miny, maxy attributes   and optionally mint, maxt for temporal bounds - 'image': A tensor or array-like object with image data</p> required <code>output</code> <code>Optional[str]</code> <p>Optional path to save the image to a file. If not provided, the image will be returned as a rasterio DatasetReader object.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to leafmap.array_to_image. Common options include: - colormap: str, name of the colormap (e.g., 'viridis', 'terrain') - vmin: float, minimum value for colormap scaling - vmax: float, maximum value for colormap scaling</p> <code>{}</code> <p>Returns:</p> Type Description <code>DatasetReader</code> <p>A rasterio DatasetReader object that can be used for visualization or further processing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image = dict_to_image(\n...     {'crs': CRS.from_epsg(26911), 'bounds': bbox, 'image': tensor},\n...     colormap='terrain'\n... )\n&gt;&gt;&gt; fig, ax = plt.subplots(figsize=(10, 10))\n&gt;&gt;&gt; show(image, ax=ax)\n</code></pre> Source code in <code>geoai/common.py</code> <pre><code>def dict_to_image(\n    data_dict: Dict[str, Any], output: Optional[str] = None, **kwargs\n) -&gt; rio.DatasetReader:\n    \"\"\"Convert a dictionary containing spatial data to a rasterio dataset or save it to\n    a file. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\".\n    It can be generated from a TorchGeo dataset sampler.\n\n    This function transforms a dictionary with CRS, bounding box, and image data\n    into a rasterio DatasetReader using leafmap's array_to_image utility after\n    first converting to a rioxarray DataArray.\n\n    Args:\n        data_dict: A dictionary containing:\n            - 'crs': A pyproj CRS object\n            - 'bounds': A BoundingBox object with minx, maxx, miny, maxy attributes\n              and optionally mint, maxt for temporal bounds\n            - 'image': A tensor or array-like object with image data\n        output: Optional path to save the image to a file. If not provided, the image\n            will be returned as a rasterio DatasetReader object.\n        **kwargs: Additional keyword arguments to pass to leafmap.array_to_image.\n            Common options include:\n            - colormap: str, name of the colormap (e.g., 'viridis', 'terrain')\n            - vmin: float, minimum value for colormap scaling\n            - vmax: float, maximum value for colormap scaling\n\n    Returns:\n        A rasterio DatasetReader object that can be used for visualization or\n        further processing.\n\n    Examples:\n        &gt;&gt;&gt; image = dict_to_image(\n        ...     {'crs': CRS.from_epsg(26911), 'bounds': bbox, 'image': tensor},\n        ...     colormap='terrain'\n        ... )\n        &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(10, 10))\n        &gt;&gt;&gt; show(image, ax=ax)\n    \"\"\"\n    da = dict_to_rioxarray(data_dict)\n\n    if output is not None:\n        out_dir = os.path.abspath(os.path.dirname(output))\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir, exist_ok=True)\n        da.rio.to_raster(output)\n        return output\n    else:\n        image = leafmap.array_to_image(da, **kwargs)\n        return image\n</code></pre>"},{"location":"common/#geoai.common.dict_to_rioxarray","title":"<code>dict_to_rioxarray(data_dict)</code>","text":"<p>Convert a dictionary to a xarray DataArray. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo dataset sampler.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>Dict</code> <p>The dictionary containing the data.</p> required <p>Returns:</p> Type Description <code>xr.DataArray</code> <p>The xarray DataArray.</p> Source code in <code>geoai/common.py</code> <pre><code>def dict_to_rioxarray(data_dict: Dict) -&gt; xr.DataArray:\n    \"\"\"Convert a dictionary to a xarray DataArray. The dictionary should contain the\n    following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo\n    dataset sampler.\n\n    Args:\n        data_dict (Dict): The dictionary containing the data.\n\n    Returns:\n        xr.DataArray: The xarray DataArray.\n    \"\"\"\n\n    from affine import Affine\n\n    # Extract components from the dictionary\n    crs = data_dict[\"crs\"]\n    bounds = data_dict[\"bounds\"]\n    image_tensor = data_dict[\"image\"]\n\n    # Convert tensor to numpy array if needed\n    if hasattr(image_tensor, \"numpy\"):\n        # For PyTorch tensors\n        image_array = image_tensor.numpy()\n    else:\n        # If it's already a numpy array or similar\n        image_array = np.array(image_tensor)\n\n    # Calculate pixel resolution\n    width = image_array.shape[2]  # Width is the size of the last dimension\n    height = image_array.shape[1]  # Height is the size of the middle dimension\n\n    res_x = (bounds.maxx - bounds.minx) / width\n    res_y = (bounds.maxy - bounds.miny) / height\n\n    # Create the transform matrix\n    transform = Affine(res_x, 0.0, bounds.minx, 0.0, -res_y, bounds.maxy)\n\n    # Create dimensions\n    x_coords = np.linspace(bounds.minx + res_x / 2, bounds.maxx - res_x / 2, width)\n    y_coords = np.linspace(bounds.maxy - res_y / 2, bounds.miny + res_y / 2, height)\n\n    # If time dimension exists in the bounds\n    if hasattr(bounds, \"mint\") and hasattr(bounds, \"maxt\"):\n        # Create a single time value or range if needed\n        t_coords = [\n            bounds.mint\n        ]  # Or np.linspace(bounds.mint, bounds.maxt, num_time_steps)\n\n        # Create DataArray with time dimension\n        dims = (\n            (\"band\", \"y\", \"x\")\n            if image_array.shape[0] &lt;= 10\n            else (\"time\", \"band\", \"y\", \"x\")\n        )\n\n        if dims[0] == \"band\":\n            # For multi-band single time\n            da = xr.DataArray(\n                image_array,\n                dims=dims,\n                coords={\n                    \"band\": np.arange(1, image_array.shape[0] + 1),\n                    \"y\": y_coords,\n                    \"x\": x_coords,\n                },\n            )\n        else:\n            # For multi-time multi-band\n            da = xr.DataArray(\n                image_array,\n                dims=dims,\n                coords={\n                    \"time\": t_coords,\n                    \"band\": np.arange(1, image_array.shape[1] + 1),\n                    \"y\": y_coords,\n                    \"x\": x_coords,\n                },\n            )\n    else:\n        # Create DataArray without time dimension\n        da = xr.DataArray(\n            image_array,\n            dims=(\"band\", \"y\", \"x\"),\n            coords={\n                \"band\": np.arange(1, image_array.shape[0] + 1),\n                \"y\": y_coords,\n                \"x\": x_coords,\n            },\n        )\n\n    # Set spatial attributes\n    da.rio.write_crs(crs, inplace=True)\n    da.rio.write_transform(transform, inplace=True)\n\n    return da\n</code></pre>"},{"location":"common/#geoai.common.plot_batch","title":"<code>plot_batch(batch, bright=1.0, cols=4, width=5, chnls=[2, 1, 0], cmap='Blues')</code>","text":"<p>Plot a batch of images and masks. This function is adapted from the plot_batch() function in the torchgeo library at https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html Credit to the torchgeo developers for the original implementation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch containing images and masks.</p> required <code>bright</code> <code>float</code> <p>The brightness factor. Defaults to 1.0.</p> <code>1.0</code> <code>cols</code> <code>int</code> <p>The number of columns in the plot grid. Defaults to 4.</p> <code>4</code> <code>width</code> <code>int</code> <p>The width of each plot. Defaults to 5.</p> <code>5</code> <code>chnls</code> <code>List[int]</code> <p>The channels to use for RGB. Defaults to [2, 1, 0].</p> <code>[2, 1, 0]</code> <code>cmap</code> <code>str</code> <p>The colormap to use for masks. Defaults to \"Blues\".</p> <code>'Blues'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/common.py</code> <pre><code>def plot_batch(\n    batch: Dict[str, Any],\n    bright: float = 1.0,\n    cols: int = 4,\n    width: int = 5,\n    chnls: List[int] = [2, 1, 0],\n    cmap: str = \"Blues\",\n) -&gt; None:\n    \"\"\"\n    Plot a batch of images and masks. This function is adapted from the plot_batch()\n    function in the torchgeo library at\n    https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html\n    Credit to the torchgeo developers for the original implementation.\n\n    Args:\n        batch (Dict[str, Any]): The batch containing images and masks.\n        bright (float, optional): The brightness factor. Defaults to 1.0.\n        cols (int, optional): The number of columns in the plot grid. Defaults to 4.\n        width (int, optional): The width of each plot. Defaults to 5.\n        chnls (List[int], optional): The channels to use for RGB. Defaults to [2, 1, 0].\n        cmap (str, optional): The colormap to use for masks. Defaults to \"Blues\".\n\n    Returns:\n        None\n    \"\"\"\n    # Get the samples and the number of items in the batch\n    samples = unbind_samples(batch.copy())\n\n    # if batch contains images and masks, the number of images will be doubled\n    n = 2 * len(samples) if (\"image\" in batch) and (\"mask\" in batch) else len(samples)\n\n    # calculate the number of rows in the grid\n    rows = n // cols + (1 if n % cols != 0 else 0)\n\n    # create a grid\n    _, axs = plt.subplots(rows, cols, figsize=(cols * width, rows * width))\n\n    if (\"image\" in batch) and (\"mask\" in batch):\n        # plot the images on the even axis\n        plot_images(\n            images=map(lambda x: x[\"image\"], samples),\n            axs=axs.reshape(-1)[::2],\n            chnls=chnls,\n            bright=bright,\n        )\n\n        # plot the masks on the odd axis\n        plot_masks(masks=map(lambda x: x[\"mask\"], samples), axs=axs.reshape(-1)[1::2])\n\n    else:\n        if \"image\" in batch:\n            plot_images(\n                images=map(lambda x: x[\"image\"], samples),\n                axs=axs.reshape(-1),\n                chnls=chnls,\n                bright=bright,\n            )\n\n        elif \"mask\" in batch:\n            plot_masks(\n                masks=map(lambda x: x[\"mask\"], samples), axs=axs.reshape(-1), cmap=cmap\n            )\n</code></pre>"},{"location":"common/#geoai.common.plot_images","title":"<code>plot_images(images, axs, chnls=[2, 1, 0], bright=1.0)</code>","text":"<p>Plot a list of images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Iterable[torch.Tensor]</code> <p>The images to plot.</p> required <code>axs</code> <code>Iterable[plt.Axes]</code> <p>The axes to plot the images on.</p> required <code>chnls</code> <code>List[int]</code> <p>The channels to use for RGB. Defaults to [2, 1, 0].</p> <code>[2, 1, 0]</code> <code>bright</code> <code>float</code> <p>The brightness factor. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/common.py</code> <pre><code>def plot_images(\n    images: Iterable[torch.Tensor],\n    axs: Iterable[plt.Axes],\n    chnls: List[int] = [2, 1, 0],\n    bright: float = 1.0,\n) -&gt; None:\n    \"\"\"\n    Plot a list of images.\n\n    Args:\n        images (Iterable[torch.Tensor]): The images to plot.\n        axs (Iterable[plt.Axes]): The axes to plot the images on.\n        chnls (List[int], optional): The channels to use for RGB. Defaults to [2, 1, 0].\n        bright (float, optional): The brightness factor. Defaults to 1.0.\n\n    Returns:\n        None\n    \"\"\"\n    for img, ax in zip(images, axs):\n        arr = torch.clamp(bright * img, min=0, max=1).numpy()\n        rgb = arr.transpose(1, 2, 0)[:, :, chnls]\n        ax.imshow(rgb)\n        ax.axis(\"off\")\n</code></pre>"},{"location":"common/#geoai.common.plot_masks","title":"<code>plot_masks(masks, axs, cmap='Blues')</code>","text":"<p>Plot a list of masks.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>Iterable[torch.Tensor]</code> <p>The masks to plot.</p> required <code>axs</code> <code>Iterable[plt.Axes]</code> <p>The axes to plot the masks on.</p> required <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \"Blues\".</p> <code>'Blues'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/common.py</code> <pre><code>def plot_masks(\n    masks: Iterable[torch.Tensor], axs: Iterable[plt.Axes], cmap: str = \"Blues\"\n) -&gt; None:\n    \"\"\"\n    Plot a list of masks.\n\n    Args:\n        masks (Iterable[torch.Tensor]): The masks to plot.\n        axs (Iterable[plt.Axes]): The axes to plot the masks on.\n        cmap (str, optional): The colormap to use. Defaults to \"Blues\".\n\n    Returns:\n        None\n    \"\"\"\n    for mask, ax in zip(masks, axs):\n        ax.imshow(mask.squeeze().numpy(), cmap=cmap)\n        ax.axis(\"off\")\n</code></pre>"},{"location":"common/#geoai.common.viz_image","title":"<code>viz_image(image, transpose=False, bdx=None, scale_factor=1.0, figsize=(10, 5), axis_off=True, title=None, **kwargs)</code>","text":"<p>Visualize an image using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[np.ndarray, torch.Tensor]</code> <p>The image to visualize.</p> required <code>transpose</code> <code>bool</code> <p>Whether to transpose the image. Defaults to False.</p> <code>False</code> <code>bdx</code> <code>Optional[int]</code> <p>The band index to visualize. Defaults to None.</p> <code>None</code> <code>scale_factor</code> <code>float</code> <p>The scale factor to apply to the image. Defaults to 1.0.</p> <code>1.0</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>The size of the figure. Defaults to (10, 5).</p> <code>(10, 5)</code> <code>axis_off</code> <code>bool</code> <p>Whether to turn off the axis. Defaults to True.</p> <code>True</code> <code>title</code> <code>Optional[str]</code> <p>The title of the plot. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for plt.imshow().</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/common.py</code> <pre><code>def viz_image(\n    image: Union[np.ndarray, torch.Tensor],\n    transpose: bool = False,\n    bdx: Optional[int] = None,\n    scale_factor: float = 1.0,\n    figsize: Tuple[int, int] = (10, 5),\n    axis_off: bool = True,\n    title: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Visualize an image using matplotlib.\n\n    Args:\n        image (Union[np.ndarray, torch.Tensor]): The image to visualize.\n        transpose (bool, optional): Whether to transpose the image. Defaults to False.\n        bdx (Optional[int], optional): The band index to visualize. Defaults to None.\n        scale_factor (float, optional): The scale factor to apply to the image. Defaults to 1.0.\n        figsize (Tuple[int, int], optional): The size of the figure. Defaults to (10, 5).\n        axis_off (bool, optional): Whether to turn off the axis. Defaults to True.\n        title (Optional[str], optional): The title of the plot. Defaults to None.\n        **kwargs (Any): Additional keyword arguments for plt.imshow().\n\n    Returns:\n        None\n    \"\"\"\n\n    if isinstance(image, torch.Tensor):\n        image = image.cpu().numpy()\n\n    plt.figure(figsize=figsize)\n\n    if transpose:\n        image = image.transpose(1, 2, 0)\n\n    if bdx is not None:\n        image = image[:, :, bdx]\n\n    if len(image.shape) &gt; 2 and image.shape[2] &gt; 3:\n        image = image[:, :, 0:3]\n\n    if scale_factor != 1.0:\n        image = np.clip(image * scale_factor, 0, 1)\n\n    plt.imshow(image, **kwargs)\n    if axis_off:\n        plt.axis(\"off\")\n    if title is not None:\n        plt.title(title)\n    plt.show()\n    plt.close()\n</code></pre>"},{"location":"common/#geoai.common.viz_raster","title":"<code>viz_raster(source, indexes=None, colormap=None, vmin=None, vmax=None, nodata=None, attribution=None, layer_name='Raster', layer_index=None, zoom_to_layer=True, visible=True, opacity=1.0, array_args={}, client_args={'cors_all': False}, basemap='OpenStreetMap', **kwargs)</code>","text":"<p>Visualize a raster using leafmap.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the raster.</p> required <code>indexes</code> <code>Optional[int]</code> <p>The band indexes to visualize. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>Optional[str]</code> <p>The colormap to apply. Defaults to None.</p> <code>None</code> <code>vmin</code> <code>Optional[float]</code> <p>The minimum value for colormap scaling. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>Optional[float]</code> <p>The maximum value for colormap scaling. Defaults to None.</p> <code>None</code> <code>nodata</code> <code>Optional[float]</code> <p>The nodata value. Defaults to None.</p> <code>None</code> <code>attribution</code> <code>Optional[str]</code> <p>The attribution for the raster. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>Optional[str]</code> <p>The name of the layer. Defaults to \"Raster\".</p> <code>'Raster'</code> <code>layer_index</code> <code>Optional[int]</code> <p>The index of the layer. Defaults to None.</p> <code>None</code> <code>zoom_to_layer</code> <code>Optional[bool]</code> <p>Whether to zoom to the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>Optional[bool]</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>opacity</code> <code>Optional[float]</code> <p>The opacity of the layer. Defaults to 1.0.</p> <code>1.0</code> <code>array_args</code> <code>Optional[Dict]</code> <p>Additional arguments for array processing. Defaults to {}.</p> <code>{}</code> <code>client_args</code> <code>Optional[Dict]</code> <p>Additional arguments for the client. Defaults to {\"cors_all\": False}.</p> <code>{'cors_all': False}</code> <code>basemap</code> <code>Optional[str]</code> <p>The basemap to use. Defaults to \"OpenStreetMap\".</p> <code>'OpenStreetMap'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>leafmap.Map</code> <p>The map object with the raster layer added.</p> Source code in <code>geoai/common.py</code> <pre><code>def viz_raster(\n    source: str,\n    indexes: Optional[int] = None,\n    colormap: Optional[str] = None,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n    nodata: Optional[float] = None,\n    attribution: Optional[str] = None,\n    layer_name: Optional[str] = \"Raster\",\n    layer_index: Optional[int] = None,\n    zoom_to_layer: Optional[bool] = True,\n    visible: Optional[bool] = True,\n    opacity: Optional[float] = 1.0,\n    array_args: Optional[Dict] = {},\n    client_args: Optional[Dict] = {\"cors_all\": False},\n    basemap: Optional[str] = \"OpenStreetMap\",\n    **kwargs,\n):\n    \"\"\"\n    Visualize a raster using leafmap.\n\n    Args:\n        source (str): The source of the raster.\n        indexes (Optional[int], optional): The band indexes to visualize. Defaults to None.\n        colormap (Optional[str], optional): The colormap to apply. Defaults to None.\n        vmin (Optional[float], optional): The minimum value for colormap scaling. Defaults to None.\n        vmax (Optional[float], optional): The maximum value for colormap scaling. Defaults to None.\n        nodata (Optional[float], optional): The nodata value. Defaults to None.\n        attribution (Optional[str], optional): The attribution for the raster. Defaults to None.\n        layer_name (Optional[str], optional): The name of the layer. Defaults to \"Raster\".\n        layer_index (Optional[int], optional): The index of the layer. Defaults to None.\n        zoom_to_layer (Optional[bool], optional): Whether to zoom to the layer. Defaults to True.\n        visible (Optional[bool], optional): Whether the layer is visible. Defaults to True.\n        opacity (Optional[float], optional): The opacity of the layer. Defaults to 1.0.\n        array_args (Optional[Dict], optional): Additional arguments for array processing. Defaults to {}.\n        client_args (Optional[Dict], optional): Additional arguments for the client. Defaults to {\"cors_all\": False}.\n        basemap (Optional[str], optional): The basemap to use. Defaults to \"OpenStreetMap\".\n        **kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        leafmap.Map: The map object with the raster layer added.\n    \"\"\"\n\n    m = leafmap.Map(basemap=basemap)\n\n    if isinstance(source, dict):\n        source = dict_to_image(source)\n\n    m.add_raster(\n        source=source,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        attribution=attribution,\n        layer_name=layer_name,\n        layer_index=layer_index,\n        zoom_to_layer=zoom_to_layer,\n        visible=visible,\n        opacity=opacity,\n        array_args=array_args,\n        client_args=client_args,\n        **kwargs,\n    )\n    return m\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/opengeos/geoai/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>geoai could always use more documentation, whether as part of the official geoai docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/opengeos/geoai/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up geoai for local development.</p> <ol> <li> <p>Fork the geoai repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/geoai.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv geoai\n$ cd geoai/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 geoai tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and     for PyPy. Check https://github.com/opengeos/geoai/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"download/","title":"download module","text":"<p>This module provides functions to download data, including NAIP imagery and building data from Overture Maps.</p>"},{"location":"download/#geoai.download.convert_vector_format","title":"<code>convert_vector_format(input_file, output_format='geojson', filter_expression=None)</code>","text":"<p>Convert the downloaded data to a different format or filter it.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>Path to the input file.</p> required <code>output_format</code> <code>str</code> <p>Format to convert to, one of \"geojson\", \"parquet\", \"shapefile\", \"csv\".</p> <code>'geojson'</code> <code>filter_expression</code> <code>Optional[str]</code> <p>Optional GeoDataFrame query expression to filter the data.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the converted file.</p> Source code in <code>geoai/download.py</code> <pre><code>def convert_vector_format(\n    input_file: str,\n    output_format: str = \"geojson\",\n    filter_expression: Optional[str] = None,\n) -&gt; str:\n    \"\"\"Convert the downloaded data to a different format or filter it.\n\n    Args:\n        input_file: Path to the input file.\n        output_format: Format to convert to, one of \"geojson\", \"parquet\", \"shapefile\", \"csv\".\n        filter_expression: Optional GeoDataFrame query expression to filter the data.\n\n    Returns:\n        Path to the converted file.\n    \"\"\"\n    try:\n        # Read the input file\n        logger.info(f\"Reading {input_file}\")\n        gdf = gpd.read_file(input_file)\n\n        # Apply filter if specified\n        if filter_expression:\n            logger.info(f\"Filtering data using expression: {filter_expression}\")\n            gdf = gdf.query(filter_expression)\n            logger.info(f\"After filtering: {len(gdf)} features\")\n\n        # Define output file path\n        base_path = os.path.splitext(input_file)[0]\n\n        if output_format == \"geojson\":\n            output_file = f\"{base_path}.geojson\"\n            logger.info(f\"Converting to GeoJSON: {output_file}\")\n            gdf.to_file(output_file, driver=\"GeoJSON\")\n        elif output_format == \"parquet\":\n            output_file = f\"{base_path}.parquet\"\n            logger.info(f\"Converting to Parquet: {output_file}\")\n            gdf.to_parquet(output_file)\n        elif output_format == \"shapefile\":\n            output_file = f\"{base_path}.shp\"\n            logger.info(f\"Converting to Shapefile: {output_file}\")\n            gdf.to_file(output_file)\n        elif output_format == \"csv\":\n            output_file = f\"{base_path}.csv\"\n            logger.info(f\"Converting to CSV: {output_file}\")\n\n            # For CSV, we need to convert geometry to WKT\n            gdf[\"geometry_wkt\"] = gdf.geometry.apply(lambda g: g.wkt)\n\n            # Save to CSV with geometry as WKT\n            gdf.drop(columns=[\"geometry\"]).to_csv(output_file, index=False)\n        else:\n            raise ValueError(f\"Unsupported output format: {output_format}\")\n\n        return output_file\n\n    except Exception as e:\n        logger.error(f\"Error converting data: {str(e)}\")\n        raise\n</code></pre>"},{"location":"download/#geoai.download.download_naip","title":"<code>download_naip(bbox, output_dir, year=None, max_items=10, overwrite=False, preview=False, **kwargs)</code>","text":"<p>Download NAIP imagery from Planetary Computer based on a bounding box.</p> <p>This function searches for NAIP (National Agriculture Imagery Program) imagery from Microsoft's Planetary Computer that intersects with the specified bounding box. It downloads the imagery and saves it as GeoTIFF files.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save the downloaded imagery.</p> required <code>year</code> <code>Optional[int]</code> <p>Specific year of NAIP imagery to download (e.g., 2020). If None, returns imagery from all available years.</p> <code>None</code> <code>max_items</code> <code>int</code> <p>Maximum number of items to download.</p> <code>10</code> <code>overwrite</code> <code>bool</code> <p>If True, overwrite existing files with the same name.</p> <code>False</code> <code>preview</code> <code>bool</code> <p>If True, display a preview of the downloaded imagery.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of downloaded file paths.</p> <p>Exceptions:</p> Type Description <code>Exception</code> <p>If there is an error downloading or saving the imagery.</p> Source code in <code>geoai/download.py</code> <pre><code>def download_naip(\n    bbox: Tuple[float, float, float, float],\n    output_dir: str,\n    year: Optional[int] = None,\n    max_items: int = 10,\n    overwrite: bool = False,\n    preview: bool = False,\n    **kwargs: Any,\n) -&gt; List[str]:\n    \"\"\"Download NAIP imagery from Planetary Computer based on a bounding box.\n\n    This function searches for NAIP (National Agriculture Imagery Program) imagery\n    from Microsoft's Planetary Computer that intersects with the specified bounding box.\n    It downloads the imagery and saves it as GeoTIFF files.\n\n    Args:\n        bbox: Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.\n        output_dir: Directory to save the downloaded imagery.\n        year: Specific year of NAIP imagery to download (e.g., 2020). If None, returns imagery from all available years.\n        max_items: Maximum number of items to download.\n        overwrite: If True, overwrite existing files with the same name.\n        preview: If True, display a preview of the downloaded imagery.\n\n    Returns:\n        List of downloaded file paths.\n\n    Raises:\n        Exception: If there is an error downloading or saving the imagery.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Create a geometry from the bounding box\n    geometry = box(*bbox)\n\n    # Connect to Planetary Computer STAC API\n    catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n\n    # Build query for NAIP data\n    search_params = {\n        \"collections\": [\"naip\"],\n        \"intersects\": geometry,\n        \"limit\": max_items,\n    }\n\n    # Add year filter if specified\n    if year:\n        search_params[\"query\"] = {\"naip:year\": {\"eq\": year}}\n\n    for key, value in kwargs.items():\n        search_params[key] = value\n\n    # Search for NAIP imagery\n    search_results = catalog.search(**search_params)\n    items = list(search_results.items())\n\n    if len(items) &gt; max_items:\n        items = items[:max_items]\n\n    if not items:\n        print(\"No NAIP imagery found for the specified region and parameters.\")\n        return []\n\n    print(f\"Found {len(items)} NAIP items.\")\n\n    # Download and save each item\n    downloaded_files = []\n    for i, item in enumerate(items):\n        # Sign the assets (required for Planetary Computer)\n        signed_item = pc.sign(item)\n\n        # Get the RGB asset URL\n        rgb_asset = signed_item.assets.get(\"image\")\n        if not rgb_asset:\n            print(f\"No RGB asset found for item {i+1}\")\n            continue\n\n        # Use the original filename from the asset\n        original_filename = os.path.basename(\n            rgb_asset.href.split(\"?\")[0]\n        )  # Remove query parameters\n        output_path = os.path.join(output_dir, original_filename)\n        if not overwrite and os.path.exists(output_path):\n            print(f\"Skipping existing file: {output_path}\")\n            downloaded_files.append(output_path)\n            continue\n\n        print(f\"Downloading item {i+1}/{len(items)}: {original_filename}\")\n\n        try:\n            # Open and save the data with progress bar\n            # For direct file download with progress bar\n            if rgb_asset.href.startswith(\"http\"):\n                download_with_progress(rgb_asset.href, output_path)\n                #\n            else:\n                # Fallback to direct rioxarray opening (less common case)\n                data = rioxarray.open_rasterio(rgb_asset.href)\n                data.rio.to_raster(output_path)\n\n            downloaded_files.append(output_path)\n            print(f\"Successfully saved to {output_path}\")\n\n            # Optional: Display a preview (uncomment if needed)\n            if preview:\n                data = rioxarray.open_rasterio(output_path)\n                preview_raster(data)\n\n        except Exception as e:\n            print(f\"Error downloading item {i+1}: {str(e)}\")\n\n    return downloaded_files\n</code></pre>"},{"location":"download/#geoai.download.download_overture_buildings","title":"<code>download_overture_buildings(bbox, output_file, output_format='geojson', data_type='building', verbose=True)</code>","text":"<p>Download building data from Overture Maps for a given bounding box using the overturemaps CLI tool.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.</p> required <code>output_file</code> <code>str</code> <p>Path to save the output file.</p> required <code>output_format</code> <code>str</code> <p>Format to save the output, one of \"geojson\", \"geojsonseq\", or \"geoparquet\".</p> <code>'geojson'</code> <code>data_type</code> <code>str</code> <p>The Overture Maps data type to download (building, place, etc.).</p> <code>'building'</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the output file.</p> Source code in <code>geoai/download.py</code> <pre><code>def download_overture_buildings(\n    bbox: Tuple[float, float, float, float],\n    output_file: str,\n    output_format: str = \"geojson\",\n    data_type: str = \"building\",\n    verbose: bool = True,\n) -&gt; str:\n    \"\"\"Download building data from Overture Maps for a given bounding box using the overturemaps CLI tool.\n\n    Args:\n        bbox: Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.\n        output_file: Path to save the output file.\n        output_format: Format to save the output, one of \"geojson\", \"geojsonseq\", or \"geoparquet\".\n        data_type: The Overture Maps data type to download (building, place, etc.).\n        verbose: Whether to print verbose output.\n\n    Returns:\n        Path to the output file.\n    \"\"\"\n    # Create output directory if needed\n    output_dir = os.path.dirname(output_file)\n    if output_dir and not os.path.exists(output_dir):\n        os.makedirs(output_dir, exist_ok=True)\n\n    # Format the bounding box string for the command\n    west, south, east, north = bbox\n    bbox_str = f\"{west},{south},{east},{north}\"\n\n    # Build the command\n    cmd = [\n        \"overturemaps\",\n        \"download\",\n        \"--bbox\",\n        bbox_str,\n        \"-f\",\n        output_format,\n        \"--type\",\n        data_type,\n        \"--output\",\n        output_file,\n    ]\n\n    if verbose:\n        logger.info(f\"Running command: {' '.join(cmd)}\")\n        logger.info(\"Downloading %s data for area: %s\", data_type, bbox_str)\n\n    try:\n        # Run the command\n        result = subprocess.run(\n            cmd,\n            check=True,\n            stdout=subprocess.PIPE if not verbose else None,\n            stderr=subprocess.PIPE,\n            text=True,\n        )\n\n        # Check if the file was created\n        if os.path.exists(output_file):\n            file_size = os.path.getsize(output_file) / (1024 * 1024)  # Size in MB\n            logger.info(\n                f\"Successfully downloaded data to {output_file} ({file_size:.2f} MB)\"\n            )\n\n            # Optionally show some stats about the downloaded data\n            if output_format == \"geojson\" and os.path.getsize(output_file) &gt; 0:\n                try:\n                    gdf = gpd.read_file(output_file)\n                    logger.info(f\"Downloaded {len(gdf)} features\")\n\n                    if len(gdf) &gt; 0 and verbose:\n                        # Show a sample of the attribute names\n                        attrs = list(gdf.columns)\n                        attrs.remove(\"geometry\")\n                        logger.info(f\"Available attributes: {', '.join(attrs[:10])}...\")\n                except Exception as e:\n                    logger.warning(f\"Could not read the GeoJSON file: {str(e)}\")\n\n            return output_file\n        else:\n            logger.error(f\"Command completed but file {output_file} was not created\")\n            if result.stderr:\n                logger.error(f\"Command error output: {result.stderr}\")\n            return None\n\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Error running overturemaps command: {str(e)}\")\n        if e.stderr:\n            logger.error(f\"Command error output: {e.stderr}\")\n        raise RuntimeError(f\"Failed to download Overture Maps data: {str(e)}\")\n    except Exception as e:\n        logger.error(f\"Unexpected error: {str(e)}\")\n        raise\n</code></pre>"},{"location":"download/#geoai.download.download_with_progress","title":"<code>download_with_progress(url, output_path)</code>","text":"<p>Download a file with a progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of the file to download.</p> required <code>output_path</code> <code>str</code> <p>Path where the file will be saved.</p> required Source code in <code>geoai/download.py</code> <pre><code>def download_with_progress(url: str, output_path: str) -&gt; None:\n    \"\"\"Download a file with a progress bar.\n\n    Args:\n        url: URL of the file to download.\n        output_path: Path where the file will be saved.\n    \"\"\"\n    response = requests.get(url, stream=True)\n    total_size = int(response.headers.get(\"content-length\", 0))\n    block_size = 1024  # 1 Kibibyte\n\n    with (\n        open(output_path, \"wb\") as file,\n        tqdm(\n            desc=os.path.basename(output_path),\n            total=total_size,\n            unit=\"iB\",\n            unit_scale=True,\n            unit_divisor=1024,\n        ) as bar,\n    ):\n        for data in response.iter_content(block_size):\n            size = file.write(data)\n            bar.update(size)\n</code></pre>"},{"location":"download/#geoai.download.extract_building_stats","title":"<code>extract_building_stats(geojson_file)</code>","text":"<p>Extract statistics from the building data.</p> <p>Parameters:</p> Name Type Description Default <code>geojson_file</code> <code>str</code> <p>Path to the GeoJSON file.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with statistics.</p> Source code in <code>geoai/download.py</code> <pre><code>def extract_building_stats(geojson_file: str) -&gt; Dict[str, Any]:\n    \"\"\"Extract statistics from the building data.\n\n    Args:\n        geojson_file: Path to the GeoJSON file.\n\n    Returns:\n        Dictionary with statistics.\n    \"\"\"\n    try:\n        # Read the GeoJSON file\n        gdf = gpd.read_file(geojson_file)\n\n        # Calculate statistics\n        bbox = gdf.total_bounds.tolist()\n        # Convert numpy values to Python native types\n        bbox = [float(x) for x in bbox]\n\n        stats = {\n            \"total_buildings\": int(len(gdf)),\n            \"has_height\": (\n                int(gdf[\"height\"].notna().sum()) if \"height\" in gdf.columns else 0\n            ),\n            \"has_name\": (\n                int(gdf[\"names.common.value\"].notna().sum())\n                if \"names.common.value\" in gdf.columns\n                else 0\n            ),\n            \"bbox\": bbox,\n        }\n\n        return stats\n\n    except Exception as e:\n        logger.error(f\"Error extracting statistics: {str(e)}\")\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"download/#geoai.download.json_serializable","title":"<code>json_serializable(obj)</code>","text":"<p>Convert NumPy types to native Python types for JSON serialization.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Any object to convert.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>JSON serializable version of the object.</p> Source code in <code>geoai/download.py</code> <pre><code>def json_serializable(obj: Any) -&gt; Any:\n    \"\"\"Convert NumPy types to native Python types for JSON serialization.\n\n    Args:\n        obj: Any object to convert.\n\n    Returns:\n        JSON serializable version of the object.\n    \"\"\"\n    if isinstance(obj, np.integer):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    else:\n        return obj\n</code></pre>"},{"location":"download/#geoai.download.preview_raster","title":"<code>preview_raster(data, title=None)</code>","text":"<p>Display a preview of the downloaded imagery.</p> <p>This function creates a visualization of the downloaded NAIP imagery by converting it to an RGB array and displaying it with matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The raster data as a rioxarray object.</p> required <code>title</code> <code>str</code> <p>The title for the preview plot.</p> <code>None</code> Source code in <code>geoai/download.py</code> <pre><code>def preview_raster(data: Any, title: str = None) -&gt; None:\n    \"\"\"Display a preview of the downloaded imagery.\n\n    This function creates a visualization of the downloaded NAIP imagery\n    by converting it to an RGB array and displaying it with matplotlib.\n\n    Args:\n        data: The raster data as a rioxarray object.\n        title: The title for the preview plot.\n    \"\"\"\n    # Convert to 8-bit RGB for display\n    rgb_data = data.transpose(\"y\", \"x\", \"band\").values[:, :, 0:3]\n    rgb_data = np.where(rgb_data &gt; 255, 255, rgb_data).astype(np.uint8)\n\n    plt.figure(figsize=(10, 10))\n    plt.imshow(rgb_data)\n    if title is not None:\n        plt.title(title)\n    plt.axis(\"off\")\n    plt.show()\n</code></pre>"},{"location":"extract/","title":"extract module","text":""},{"location":"extract/#geoai.extract.BuildingFootprintDataset","title":"<code> BuildingFootprintDataset            (NonGeoDataset)         </code>","text":"<p>A TorchGeo dataset for building footprint extraction. Using NonGeoDataset to avoid spatial indexing issues.</p> Source code in <code>geoai/extract.py</code> <pre><code>class BuildingFootprintDataset(NonGeoDataset):\n    \"\"\"\n    A TorchGeo dataset for building footprint extraction.\n    Using NonGeoDataset to avoid spatial indexing issues.\n    \"\"\"\n\n    def __init__(self, raster_path, chip_size=(512, 512), transforms=None):\n        \"\"\"\n        Initialize the dataset.\n\n        Args:\n            raster_path: Path to the input raster file\n            chip_size: Size of image chips to extract (height, width)\n            transforms: Transforms to apply to the image\n        \"\"\"\n        super().__init__()\n\n        # Initialize parameters\n        self.raster_path = raster_path\n        self.chip_size = chip_size\n        self.transforms = transforms\n\n        # Open raster and get metadata\n        with rasterio.open(self.raster_path) as src:\n            self.crs = src.crs\n            self.transform = src.transform\n            self.height = src.height\n            self.width = src.width\n            self.count = src.count\n\n            # Define the bounds of the dataset\n            west, south, east, north = src.bounds\n            self.bounds = (west, south, east, north)\n\n            # Define the ROI for the dataset\n            self.roi = box(*self.bounds)\n\n            # Calculate number of chips in each dimension\n            self.rows = self.height // self.chip_size[0]\n            self.cols = self.width // self.chip_size[1]\n\n            print(\n                f\"Dataset initialized with {self.rows} rows and {self.cols} columns of chips\"\n            )\n            if src.crs:\n                print(f\"CRS: {src.crs}\")\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Get an image chip from the dataset by index.\n\n        Args:\n            idx: Index of the chip\n\n        Returns:\n            Dict containing image tensor\n        \"\"\"\n        # Convert flat index to grid position\n        row = idx // self.cols\n        col = idx % self.cols\n\n        # Calculate pixel coordinates\n        i = col * self.chip_size[1]\n        j = row * self.chip_size[0]\n\n        # Read window from raster\n        with rasterio.open(self.raster_path) as src:\n            # Make sure we don't read outside the image\n            width = min(self.chip_size[1], self.width - i)\n            height = min(self.chip_size[0], self.height - j)\n\n            window = Window(i, j, width, height)\n            image = src.read(window=window)\n\n            # Handle RGBA or multispectral images - keep only first 3 bands\n            if image.shape[0] &gt; 3:\n                print(f\"Image has {image.shape[0]} bands, using first 3 bands only\")\n                image = image[:3]\n            elif image.shape[0] &lt; 3:\n                # If image has fewer than 3 bands, duplicate the last band to make 3\n                print(f\"Image has {image.shape[0]} bands, duplicating bands to make 3\")\n                temp = np.zeros((3, image.shape[1], image.shape[2]), dtype=image.dtype)\n                for c in range(3):\n                    temp[c] = image[min(c, image.shape[0] - 1)]\n                image = temp\n\n            # Handle partial windows at edges by padding\n            if (\n                image.shape[1] != self.chip_size[0]\n                or image.shape[2] != self.chip_size[1]\n            ):\n                temp = np.zeros(\n                    (image.shape[0], self.chip_size[0], self.chip_size[1]),\n                    dtype=image.dtype,\n                )\n                temp[:, : image.shape[1], : image.shape[2]] = image\n                image = temp\n\n        # Convert to format expected by model (C,H,W)\n        image = torch.from_numpy(image).float()\n\n        # Normalize to [0, 1]\n        if image.max() &gt; 1:\n            image = image / 255.0\n\n        # Apply transforms if any\n        if self.transforms is not None:\n            image = self.transforms(image)\n\n        # Create geographic bounding box for the window\n        minx, miny = self.transform * (i, j + height)\n        maxx, maxy = self.transform * (i + width, j)\n        bbox = box(minx, miny, maxx, maxy)\n\n        return {\n            \"image\": image,\n            \"bbox\": bbox,\n            \"coords\": torch.tensor([i, j], dtype=torch.long),  # Consistent format\n            \"window_size\": torch.tensor(\n                [width, height], dtype=torch.long\n            ),  # Consistent format\n        }\n\n    def __len__(self):\n        \"\"\"Return the number of samples in the dataset.\"\"\"\n        return self.rows * self.cols\n</code></pre>"},{"location":"extract/#geoai.extract.BuildingFootprintDataset.__getitem__","title":"<code>__getitem__(self, idx)</code>  <code>special</code>","text":"<p>Get an image chip from the dataset by index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <p>Index of the chip</p> required <p>Returns:</p> Type Description <p>Dict containing image tensor</p> Source code in <code>geoai/extract.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Get an image chip from the dataset by index.\n\n    Args:\n        idx: Index of the chip\n\n    Returns:\n        Dict containing image tensor\n    \"\"\"\n    # Convert flat index to grid position\n    row = idx // self.cols\n    col = idx % self.cols\n\n    # Calculate pixel coordinates\n    i = col * self.chip_size[1]\n    j = row * self.chip_size[0]\n\n    # Read window from raster\n    with rasterio.open(self.raster_path) as src:\n        # Make sure we don't read outside the image\n        width = min(self.chip_size[1], self.width - i)\n        height = min(self.chip_size[0], self.height - j)\n\n        window = Window(i, j, width, height)\n        image = src.read(window=window)\n\n        # Handle RGBA or multispectral images - keep only first 3 bands\n        if image.shape[0] &gt; 3:\n            print(f\"Image has {image.shape[0]} bands, using first 3 bands only\")\n            image = image[:3]\n        elif image.shape[0] &lt; 3:\n            # If image has fewer than 3 bands, duplicate the last band to make 3\n            print(f\"Image has {image.shape[0]} bands, duplicating bands to make 3\")\n            temp = np.zeros((3, image.shape[1], image.shape[2]), dtype=image.dtype)\n            for c in range(3):\n                temp[c] = image[min(c, image.shape[0] - 1)]\n            image = temp\n\n        # Handle partial windows at edges by padding\n        if (\n            image.shape[1] != self.chip_size[0]\n            or image.shape[2] != self.chip_size[1]\n        ):\n            temp = np.zeros(\n                (image.shape[0], self.chip_size[0], self.chip_size[1]),\n                dtype=image.dtype,\n            )\n            temp[:, : image.shape[1], : image.shape[2]] = image\n            image = temp\n\n    # Convert to format expected by model (C,H,W)\n    image = torch.from_numpy(image).float()\n\n    # Normalize to [0, 1]\n    if image.max() &gt; 1:\n        image = image / 255.0\n\n    # Apply transforms if any\n    if self.transforms is not None:\n        image = self.transforms(image)\n\n    # Create geographic bounding box for the window\n    minx, miny = self.transform * (i, j + height)\n    maxx, maxy = self.transform * (i + width, j)\n    bbox = box(minx, miny, maxx, maxy)\n\n    return {\n        \"image\": image,\n        \"bbox\": bbox,\n        \"coords\": torch.tensor([i, j], dtype=torch.long),  # Consistent format\n        \"window_size\": torch.tensor(\n            [width, height], dtype=torch.long\n        ),  # Consistent format\n    }\n</code></pre>"},{"location":"extract/#geoai.extract.BuildingFootprintDataset.__init__","title":"<code>__init__(self, raster_path, chip_size=(512, 512), transforms=None)</code>  <code>special</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <p>Path to the input raster file</p> required <code>chip_size</code> <p>Size of image chips to extract (height, width)</p> <code>(512, 512)</code> <code>transforms</code> <p>Transforms to apply to the image</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(self, raster_path, chip_size=(512, 512), transforms=None):\n    \"\"\"\n    Initialize the dataset.\n\n    Args:\n        raster_path: Path to the input raster file\n        chip_size: Size of image chips to extract (height, width)\n        transforms: Transforms to apply to the image\n    \"\"\"\n    super().__init__()\n\n    # Initialize parameters\n    self.raster_path = raster_path\n    self.chip_size = chip_size\n    self.transforms = transforms\n\n    # Open raster and get metadata\n    with rasterio.open(self.raster_path) as src:\n        self.crs = src.crs\n        self.transform = src.transform\n        self.height = src.height\n        self.width = src.width\n        self.count = src.count\n\n        # Define the bounds of the dataset\n        west, south, east, north = src.bounds\n        self.bounds = (west, south, east, north)\n\n        # Define the ROI for the dataset\n        self.roi = box(*self.bounds)\n\n        # Calculate number of chips in each dimension\n        self.rows = self.height // self.chip_size[0]\n        self.cols = self.width // self.chip_size[1]\n\n        print(\n            f\"Dataset initialized with {self.rows} rows and {self.cols} columns of chips\"\n        )\n        if src.crs:\n            print(f\"CRS: {src.crs}\")\n</code></pre>"},{"location":"extract/#geoai.extract.BuildingFootprintDataset.__len__","title":"<code>__len__(self)</code>  <code>special</code>","text":"<p>Return the number of samples in the dataset.</p> Source code in <code>geoai/extract.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the number of samples in the dataset.\"\"\"\n    return self.rows * self.cols\n</code></pre>"},{"location":"extract/#geoai.extract.BuildingFootprintExtractor","title":"<code> BuildingFootprintExtractor        </code>","text":"<p>Building footprint extraction using Mask R-CNN with TorchGeo.</p> Source code in <code>geoai/extract.py</code> <pre><code>class BuildingFootprintExtractor:\n    \"\"\"\n    Building footprint extraction using Mask R-CNN with TorchGeo.\n    \"\"\"\n\n    def __init__(self, model_path=None, device=None):\n        \"\"\"\n        Initialize the building footprint extractor.\n\n        Args:\n            model_path: Path to the .pth model file\n            device: Device to use for inference ('cuda:0', 'cpu', etc.)\n        \"\"\"\n        # Set device\n        if device is None:\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = torch.device(device)\n\n        # Default parameters for building detection - these can be overridden in process_raster\n        self.chip_size = (512, 512)  # Size of image chips for processing\n        self.overlap = 0.25  # Default overlap between tiles\n        self.confidence_threshold = 0.5  # Default confidence threshold\n        self.nms_iou_threshold = 0.5  # IoU threshold for non-maximum suppression\n        self.small_building_area = 100  # Minimum area in pixels to keep a building\n        self.mask_threshold = 0.5  # Threshold for mask binarization\n        self.simplify_tolerance = 1.0  # Tolerance for polygon simplification\n\n        # Initialize model\n        self.model = self._initialize_model()\n\n        # Download model if needed\n        if model_path is None:\n            model_path = self._download_model_from_hf()\n\n        # Load model weights\n        self._load_weights(model_path)\n\n        # Set model to evaluation mode\n        self.model.eval()\n\n    def _download_model_from_hf(self):\n        \"\"\"\n        Download the USA building footprints model from Hugging Face.\n\n        Returns:\n            Path to the downloaded model file\n        \"\"\"\n        try:\n\n            print(\"Model path not specified, downloading from Hugging Face...\")\n\n            # Define the repository ID and model filename\n            repo_id = \"giswqs/geoai\"  # Update with your actual username/repo\n            filename = \"usa_building_footprints.pth\"\n\n            # Ensure cache directory exists\n            # cache_dir = os.path.join(\n            #     os.path.expanduser(\"~\"), \".cache\", \"building_footprints\"\n            # )\n            # os.makedirs(cache_dir, exist_ok=True)\n\n            # Download the model\n            model_path = hf_hub_download(repo_id=repo_id, filename=filename)\n            print(f\"Model downloaded to: {model_path}\")\n\n            return model_path\n\n        except Exception as e:\n            print(f\"Error downloading model from Hugging Face: {e}\")\n            print(\"Please specify a local model path or ensure internet connectivity.\")\n            raise\n\n    def _initialize_model(self):\n        \"\"\"Initialize Mask R-CNN model with ResNet50 backbone.\"\"\"\n        # Standard image mean and std for pre-trained models\n        # Note: This would normally come from your config file\n        image_mean = [0.485, 0.456, 0.406]\n        image_std = [0.229, 0.224, 0.225]\n\n        # Create model with explicit normalization parameters\n        model = maskrcnn_resnet50_fpn(\n            weights=None,\n            progress=False,\n            num_classes=2,  # Background + building\n            weights_backbone=None,\n            # These parameters ensure consistent normalization\n            image_mean=image_mean,\n            image_std=image_std,\n        )\n\n        model.to(self.device)\n        return model\n\n    def _load_weights(self, model_path):\n        \"\"\"\n        Load weights from file with error handling for different formats.\n\n        Args:\n            model_path: Path to model weights\n        \"\"\"\n        if not os.path.exists(model_path):\n            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n\n        try:\n            state_dict = torch.load(model_path, map_location=self.device)\n\n            # Handle different state dict formats\n            if isinstance(state_dict, dict):\n                if \"model\" in state_dict:\n                    state_dict = state_dict[\"model\"]\n                elif \"state_dict\" in state_dict:\n                    state_dict = state_dict[\"state_dict\"]\n\n            # Try to load state dict\n            try:\n                self.model.load_state_dict(state_dict)\n                print(\"Model loaded successfully\")\n            except Exception as e:\n                print(f\"Error loading model: {e}\")\n                print(\"Attempting to fix state_dict keys...\")\n\n                # Try to fix state_dict keys (remove module prefix if needed)\n                new_state_dict = {}\n                for k, v in state_dict.items():\n                    if k.startswith(\"module.\"):\n                        new_state_dict[k[7:]] = v\n                    else:\n                        new_state_dict[k] = v\n\n                self.model.load_state_dict(new_state_dict)\n                print(\"Model loaded successfully after key fixing\")\n\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load model: {e}\")\n\n    def _mask_to_polygons(self, mask, **kwargs):\n        \"\"\"\n        Convert binary mask to polygon contours using OpenCV.\n\n        Args:\n            mask: Binary mask as numpy array\n            **kwargs: Optional parameters:\n                simplify_tolerance: Tolerance for polygon simplification\n                mask_threshold: Threshold for mask binarization\n                small_building_area: Minimum area in pixels to keep a building\n\n        Returns:\n            List of polygons as lists of (x, y) coordinates\n        \"\"\"\n\n        # Get parameters from kwargs or use instance defaults\n        simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n        mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n        small_building_area = kwargs.get(\n            \"small_building_area\", self.small_building_area\n        )\n\n        # Ensure binary mask\n        mask = (mask &gt; mask_threshold).astype(np.uint8)\n\n        # Optional: apply morphological operations to improve mask quality\n        kernel = np.ones((3, 3), np.uint8)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Convert to list of [x, y] coordinates\n        polygons = []\n        for contour in contours:\n            # Filter out too small contours\n            if contour.shape[0] &lt; 3 or cv2.contourArea(contour) &lt; small_building_area:\n                continue\n\n            # Simplify contour if it has many points\n            if contour.shape[0] &gt; 50:\n                epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                contour = cv2.approxPolyDP(contour, epsilon, True)\n\n            # Convert to list of [x, y] coordinates\n            polygon = contour.reshape(-1, 2).tolist()\n            polygons.append(polygon)\n\n        return polygons\n\n    def _filter_overlapping_polygons(self, gdf, **kwargs):\n        \"\"\"\n        Filter overlapping polygons using non-maximum suppression.\n\n        Args:\n            gdf: GeoDataFrame with polygons\n            **kwargs: Optional parameters:\n                nms_iou_threshold: IoU threshold for filtering\n\n        Returns:\n            Filtered GeoDataFrame\n        \"\"\"\n        if len(gdf) &lt;= 1:\n            return gdf\n\n        # Get parameters from kwargs or use instance defaults\n        iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n\n        # Sort by confidence\n        gdf = gdf.sort_values(\"confidence\", ascending=False)\n\n        # Fix any invalid geometries\n        gdf[\"geometry\"] = gdf[\"geometry\"].apply(\n            lambda geom: geom.buffer(0) if not geom.is_valid else geom\n        )\n\n        keep_indices = []\n        polygons = gdf.geometry.values\n\n        for i in range(len(polygons)):\n            if i in keep_indices:\n                continue\n\n            keep = True\n            for j in keep_indices:\n                # Skip invalid geometries\n                if not polygons[i].is_valid or not polygons[j].is_valid:\n                    continue\n\n                # Calculate IoU\n                try:\n                    intersection = polygons[i].intersection(polygons[j]).area\n                    union = polygons[i].area + polygons[j].area - intersection\n                    iou = intersection / union if union &gt; 0 else 0\n\n                    if iou &gt; iou_threshold:\n                        keep = False\n                        break\n                except Exception:\n                    # Skip on topology exceptions\n                    continue\n\n            if keep:\n                keep_indices.append(i)\n\n        return gdf.iloc[keep_indices]\n\n    @torch.no_grad()\n    def process_raster(self, raster_path, output_path=None, batch_size=4, **kwargs):\n        \"\"\"\n        Process a raster file to extract building footprints with customizable parameters.\n\n        Args:\n            raster_path: Path to input raster file\n            output_path: Path to output GeoJSON file (optional)\n            batch_size: Batch size for processing\n            **kwargs: Additional parameters:\n                confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n                overlap: Overlap between adjacent tiles (0.0-1.0)\n                chip_size: Size of image chips for processing (height, width)\n                nms_iou_threshold: IoU threshold for non-maximum suppression (0.0-1.0)\n                mask_threshold: Threshold for mask binarization (0.0-1.0)\n                small_building_area: Minimum area in pixels to keep a building\n                simplify_tolerance: Tolerance for polygon simplification\n\n        Returns:\n            GeoDataFrame with building footprints\n        \"\"\"\n        # Get parameters from kwargs or use instance defaults\n        confidence_threshold = kwargs.get(\n            \"confidence_threshold\", self.confidence_threshold\n        )\n        overlap = kwargs.get(\"overlap\", self.overlap)\n        chip_size = kwargs.get(\"chip_size\", self.chip_size)\n        nms_iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n        mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n        small_building_area = kwargs.get(\n            \"small_building_area\", self.small_building_area\n        )\n        simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n\n        # Print parameters being used\n        print(f\"Processing with parameters:\")\n        print(f\"- Confidence threshold: {confidence_threshold}\")\n        print(f\"- Tile overlap: {overlap}\")\n        print(f\"- Chip size: {chip_size}\")\n        print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n        print(f\"- Mask threshold: {mask_threshold}\")\n        print(f\"- Min building area: {small_building_area}\")\n        print(f\"- Simplify tolerance: {simplify_tolerance}\")\n\n        # Create dataset\n        dataset = BuildingFootprintDataset(raster_path=raster_path, chip_size=chip_size)\n\n        # Custom collate function to handle Shapely objects\n        def custom_collate(batch):\n            \"\"\"\n            Custom collate function that handles Shapely geometries\n            by keeping them as Python objects rather than trying to collate them.\n            \"\"\"\n            elem = batch[0]\n            if isinstance(elem, dict):\n                result = {}\n                for key in elem:\n                    if key == \"bbox\":\n                        # Don't collate shapely objects, keep as list\n                        result[key] = [d[key] for d in batch]\n                    else:\n                        # For tensors and other collatable types\n                        try:\n                            result[key] = (\n                                torch.utils.data._utils.collate.default_collate(\n                                    [d[key] for d in batch]\n                                )\n                            )\n                        except TypeError:\n                            # Fall back to list for non-collatable types\n                            result[key] = [d[key] for d in batch]\n                return result\n            else:\n                # Default collate for non-dict types\n                return torch.utils.data._utils.collate.default_collate(batch)\n\n        # Create dataloader with simple indexing and custom collate\n        dataloader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=0,\n            collate_fn=custom_collate,\n        )\n\n        # Process batches\n        all_polygons = []\n        all_scores = []\n\n        print(f\"Processing raster with {len(dataloader)} batches\")\n        for batch in tqdm(dataloader):\n            # Move images to device\n            images = batch[\"image\"].to(self.device)\n            coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n            bboxes = batch[\n                \"bbox\"\n            ]  # Geographic bounding boxes - now a list, not a tensor\n\n            # Run inference\n            predictions = self.model(images)\n\n            # Process predictions\n            for idx, prediction in enumerate(predictions):\n                masks = prediction[\"masks\"].cpu().numpy()\n                scores = prediction[\"scores\"].cpu().numpy()\n                labels = prediction[\"labels\"].cpu().numpy()\n\n                # Skip if no predictions\n                if len(scores) == 0:\n                    continue\n\n                # Filter by confidence threshold\n                valid_indices = scores &gt;= confidence_threshold\n                masks = masks[valid_indices]\n                scores = scores[valid_indices]\n                labels = labels[valid_indices]\n\n                # Skip if no valid predictions\n                if len(scores) == 0:\n                    continue\n\n                # Get window coordinates\n                # The coords might be in different formats depending on batch handling\n                if isinstance(coords, list):\n                    # If coords is a list of tuples\n                    coord_item = coords[idx]\n                    if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                        i, j = coord_item\n                    elif isinstance(coord_item, torch.Tensor):\n                        i, j = coord_item.cpu().numpy().tolist()\n                    else:\n                        print(f\"Unexpected coords format: {type(coord_item)}\")\n                        continue\n                elif isinstance(coords, torch.Tensor):\n                    # If coords is a tensor of shape [batch_size, 2]\n                    i, j = coords[idx].cpu().numpy().tolist()\n                else:\n                    print(f\"Unexpected coords type: {type(coords)}\")\n                    continue\n\n                # Get window size\n                if isinstance(batch[\"window_size\"], list):\n                    window_item = batch[\"window_size\"][idx]\n                    if isinstance(window_item, tuple) and len(window_item) == 2:\n                        window_width, window_height = window_item\n                    elif isinstance(window_item, torch.Tensor):\n                        window_width, window_height = window_item.cpu().numpy().tolist()\n                    else:\n                        print(f\"Unexpected window_size format: {type(window_item)}\")\n                        continue\n                elif isinstance(batch[\"window_size\"], torch.Tensor):\n                    window_width, window_height = (\n                        batch[\"window_size\"][idx].cpu().numpy().tolist()\n                    )\n                else:\n                    print(f\"Unexpected window_size type: {type(batch['window_size'])}\")\n                    continue\n\n                # Process masks to polygons\n                for mask_idx, mask in enumerate(masks):\n                    # Get binary mask\n                    binary_mask = mask[0]  # Get binary mask\n\n                    # Convert mask to polygon with custom parameters\n                    contours = self._mask_to_polygons(\n                        binary_mask,\n                        simplify_tolerance=simplify_tolerance,\n                        mask_threshold=mask_threshold,\n                        small_building_area=small_building_area,\n                    )\n\n                    # Skip if no valid polygons\n                    if not contours:\n                        continue\n\n                    # Transform polygons to geographic coordinates\n                    with rasterio.open(raster_path) as src:\n                        transform = src.transform\n\n                        for contour in contours:\n                            # Convert polygon to global coordinates\n                            global_polygon = []\n                            for x, y in contour:\n                                # Adjust coordinates based on window position\n                                gx, gy = transform * (i + x, j + y)\n                                global_polygon.append((gx, gy))\n\n                            # Create Shapely polygon\n                            if len(global_polygon) &gt;= 3:\n                                try:\n                                    shapely_poly = Polygon(global_polygon)\n                                    if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                                        all_polygons.append(shapely_poly)\n                                        all_scores.append(float(scores[mask_idx]))\n                                except Exception as e:\n                                    print(f\"Error creating polygon: {e}\")\n\n        # Create GeoDataFrame\n        if not all_polygons:\n            print(\"No valid polygons found\")\n            return None\n\n        gdf = gpd.GeoDataFrame(\n            {\n                \"geometry\": all_polygons,\n                \"confidence\": all_scores,\n                \"class\": 1,  # Building class\n            },\n            crs=dataset.crs,\n        )\n\n        # Remove overlapping polygons with custom threshold\n        gdf = self._filter_overlapping_polygons(\n            gdf, nms_iou_threshold=nms_iou_threshold\n        )\n\n        # Save to file if requested\n        if output_path:\n            gdf.to_file(output_path, driver=\"GeoJSON\")\n            print(f\"Saved {len(gdf)} building footprints to {output_path}\")\n\n        return gdf\n\n    def visualize_results(\n        self, raster_path, gdf=None, output_path=None, figsize=(12, 12)\n    ):\n        \"\"\"\n        Visualize building detection results.\n\n        Args:\n            raster_path: Path to input raster\n            gdf: GeoDataFrame with building polygons (optional)\n            output_path: Path to save visualization (optional)\n            figsize: Figure size (width, height) in inches\n        \"\"\"\n        # Check if raster file exists\n        if not os.path.exists(raster_path):\n            print(f\"Error: Raster file '{raster_path}' not found.\")\n            return\n\n        # Process raster if GeoDataFrame not provided\n        if gdf is None:\n            gdf = self.process_raster(raster_path)\n\n        if gdf is None or len(gdf) == 0:\n            print(\"No buildings to visualize\")\n            return\n\n        # Read raster for visualization\n        with rasterio.open(raster_path) as src:\n            # Read the entire image or a subset if it's very large\n            if src.height &gt; 2000 or src.width &gt; 2000:\n                # Calculate scale factor to reduce size\n                scale = min(2000 / src.height, 2000 / src.width)\n                out_shape = (\n                    int(src.count),\n                    int(src.height * scale),\n                    int(src.width * scale),\n                )\n\n                # Read and resample\n                image = src.read(\n                    out_shape=out_shape, resampling=rasterio.enums.Resampling.bilinear\n                )\n            else:\n                image = src.read()\n\n            # Convert to RGB for display\n            if image.shape[0] &gt; 3:\n                image = image[:3]\n            elif image.shape[0] == 1:\n                image = np.repeat(image, 3, axis=0)\n\n            # Normalize image for display\n            image = image.transpose(1, 2, 0)  # CHW to HWC\n            image = image.astype(np.float32)\n\n            if image.max() &gt; 10:  # Likely 0-255 range\n                image = image / 255.0\n\n            image = np.clip(image, 0, 1)\n\n            # Get image bounds\n            bounds = src.bounds\n\n        # Create figure with appropriate aspect ratio\n        aspect_ratio = image.shape[1] / image.shape[0]  # width / height\n        plt.figure(figsize=(figsize[0], figsize[0] / aspect_ratio))\n\n        # Create axis with the right projection if CRS is available\n        ax = plt.gca()\n\n        # Display image\n        ax.imshow(image)\n\n        # Convert GeoDataFrame to pixel coordinates for plotting\n        with rasterio.open(raster_path) as src:\n\n            def geo_to_pixel(x, y):\n                return ~src.transform * (x, y)\n\n            # Plot each building footprint\n            for _, row in gdf.iterrows():\n                # Convert polygon to pixel coordinates\n                geom = row.geometry\n                if geom.is_empty:\n                    continue\n\n                try:\n                    # Get polygon exterior coordinates\n                    x, y = geom.exterior.xy\n\n                    # Convert to pixel coordinates\n                    pixel_coords = [geo_to_pixel(x[i], y[i]) for i in range(len(x))]\n                    pixel_x = [coord[0] for coord in pixel_coords]\n                    pixel_y = [coord[1] for coord in pixel_coords]\n\n                    # Plot polygon\n                    ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n                except Exception as e:\n                    print(f\"Error plotting polygon: {e}\")\n\n        # Remove axes\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"Building Footprints (Found: {len(gdf)})\")\n\n        # Add colorbar for confidence if available\n        if \"confidence\" in gdf.columns:\n            # Create a colorbar legend\n            sm = plt.cm.ScalarMappable(\n                cmap=plt.cm.viridis,\n                norm=plt.Normalize(gdf.confidence.min(), gdf.confidence.max()),\n            )\n            sm.set_array([])\n            cbar = plt.colorbar(sm, ax=ax, orientation=\"vertical\", shrink=0.7)\n            cbar.set_label(\"Confidence\")\n\n        # Save if requested\n        if output_path:\n            plt.tight_layout()\n            plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n            print(f\"Visualization saved to {output_path}\")\n\n        plt.close()\n\n        # Create a simpler visualization focused just on a subset of buildings\n        # This helps when the raster is very large\n        plt.figure(figsize=figsize)\n        ax = plt.gca()\n\n        # Choose a subset of the image to show\n        with rasterio.open(raster_path) as src:\n            # Get a sample window based on the first few buildings\n            if len(gdf) &gt; 0:\n                # Get centroid of first building\n                sample_geom = gdf.iloc[0].geometry\n                centroid = sample_geom.centroid\n\n                # Convert to pixel coordinates\n                center_x, center_y = ~src.transform * (centroid.x, centroid.y)\n\n                # Define a window around this building\n                window_size = 500  # pixels\n                window = rasterio.windows.Window(\n                    max(0, int(center_x - window_size / 2)),\n                    max(0, int(center_y - window_size / 2)),\n                    min(window_size, src.width - int(center_x - window_size / 2)),\n                    min(window_size, src.height - int(center_y - window_size / 2)),\n                )\n\n                # Read this window\n                sample_image = src.read(window=window)\n\n                # Convert to RGB for display\n                if sample_image.shape[0] &gt; 3:\n                    sample_image = sample_image[:3]\n                elif sample_image.shape[0] == 1:\n                    sample_image = np.repeat(sample_image, 3, axis=0)\n\n                # Normalize image for display\n                sample_image = sample_image.transpose(1, 2, 0)  # CHW to HWC\n                sample_image = sample_image.astype(np.float32)\n\n                if sample_image.max() &gt; 10:  # Likely 0-255 range\n                    sample_image = sample_image / 255.0\n\n                sample_image = np.clip(sample_image, 0, 1)\n\n                # Get transform for this window\n                window_transform = src.window_transform(window)\n\n                # Display sample image\n                ax.imshow(sample_image)\n\n                # Filter buildings that intersect with this window\n                window_bounds = rasterio.windows.bounds(window, src.transform)\n                window_box = box(*window_bounds)\n                visible_gdf = gdf[gdf.intersects(window_box)]\n\n                # Plot building footprints in this view\n                for _, row in visible_gdf.iterrows():\n                    try:\n                        # Get polygon exterior coordinates\n                        geom = row.geometry\n                        if geom.is_empty:\n                            continue\n\n                        x, y = geom.exterior.xy\n\n                        # Convert to pixel coordinates relative to window\n                        pixel_coords = [\n                            ~window_transform * (x[i], y[i]) for i in range(len(x))\n                        ]\n                        pixel_x = [coord[0] for coord in pixel_coords]\n                        pixel_y = [coord[1] for coord in pixel_coords]\n\n                        # Plot polygon\n                        ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                    except Exception as e:\n                        print(f\"Error plotting polygon in sample view: {e}\")\n\n                # Set title\n                ax.set_title(\n                    f\"Sample Area - Building Footprints (Showing: {len(visible_gdf)})\"\n                )\n\n                # Remove axes\n                ax.set_xticks([])\n                ax.set_yticks([])\n\n                # Save if requested\n                if output_path:\n                    sample_output = (\n                        os.path.splitext(output_path)[0]\n                        + \"_sample\"\n                        + os.path.splitext(output_path)[1]\n                    )\n                    plt.tight_layout()\n                    plt.savefig(sample_output, dpi=300, bbox_inches=\"tight\")\n                    print(f\"Sample visualization saved to {sample_output}\")\n\n        return True\n</code></pre>"},{"location":"extract/#geoai.extract.BuildingFootprintExtractor.__init__","title":"<code>__init__(self, model_path=None, device=None)</code>  <code>special</code>","text":"<p>Initialize the building footprint extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <p>Path to the .pth model file</p> <code>None</code> <code>device</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.)</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(self, model_path=None, device=None):\n    \"\"\"\n    Initialize the building footprint extractor.\n\n    Args:\n        model_path: Path to the .pth model file\n        device: Device to use for inference ('cuda:0', 'cpu', etc.)\n    \"\"\"\n    # Set device\n    if device is None:\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    else:\n        self.device = torch.device(device)\n\n    # Default parameters for building detection - these can be overridden in process_raster\n    self.chip_size = (512, 512)  # Size of image chips for processing\n    self.overlap = 0.25  # Default overlap between tiles\n    self.confidence_threshold = 0.5  # Default confidence threshold\n    self.nms_iou_threshold = 0.5  # IoU threshold for non-maximum suppression\n    self.small_building_area = 100  # Minimum area in pixels to keep a building\n    self.mask_threshold = 0.5  # Threshold for mask binarization\n    self.simplify_tolerance = 1.0  # Tolerance for polygon simplification\n\n    # Initialize model\n    self.model = self._initialize_model()\n\n    # Download model if needed\n    if model_path is None:\n        model_path = self._download_model_from_hf()\n\n    # Load model weights\n    self._load_weights(model_path)\n\n    # Set model to evaluation mode\n    self.model.eval()\n</code></pre>"},{"location":"extract/#geoai.extract.BuildingFootprintExtractor.process_raster","title":"<code>process_raster(self, raster_path, output_path=None, batch_size=4, **kwargs)</code>","text":"<p>Process a raster file to extract building footprints with customizable parameters.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <p>Path to input raster file</p> required <code>output_path</code> <p>Path to output GeoJSON file (optional)</p> <code>None</code> <code>batch_size</code> <p>Batch size for processing</p> <code>4</code> <code>**kwargs</code> <p>Additional parameters: confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0) overlap: Overlap between adjacent tiles (0.0-1.0) chip_size: Size of image chips for processing (height, width) nms_iou_threshold: IoU threshold for non-maximum suppression (0.0-1.0) mask_threshold: Threshold for mask binarization (0.0-1.0) small_building_area: Minimum area in pixels to keep a building simplify_tolerance: Tolerance for polygon simplification</p> <code>{}</code> <p>Returns:</p> Type Description <p>GeoDataFrame with building footprints</p> Source code in <code>geoai/extract.py</code> <pre><code>@torch.no_grad()\ndef process_raster(self, raster_path, output_path=None, batch_size=4, **kwargs):\n    \"\"\"\n    Process a raster file to extract building footprints with customizable parameters.\n\n    Args:\n        raster_path: Path to input raster file\n        output_path: Path to output GeoJSON file (optional)\n        batch_size: Batch size for processing\n        **kwargs: Additional parameters:\n            confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n            overlap: Overlap between adjacent tiles (0.0-1.0)\n            chip_size: Size of image chips for processing (height, width)\n            nms_iou_threshold: IoU threshold for non-maximum suppression (0.0-1.0)\n            mask_threshold: Threshold for mask binarization (0.0-1.0)\n            small_building_area: Minimum area in pixels to keep a building\n            simplify_tolerance: Tolerance for polygon simplification\n\n    Returns:\n        GeoDataFrame with building footprints\n    \"\"\"\n    # Get parameters from kwargs or use instance defaults\n    confidence_threshold = kwargs.get(\n        \"confidence_threshold\", self.confidence_threshold\n    )\n    overlap = kwargs.get(\"overlap\", self.overlap)\n    chip_size = kwargs.get(\"chip_size\", self.chip_size)\n    nms_iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n    mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n    small_building_area = kwargs.get(\n        \"small_building_area\", self.small_building_area\n    )\n    simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n\n    # Print parameters being used\n    print(f\"Processing with parameters:\")\n    print(f\"- Confidence threshold: {confidence_threshold}\")\n    print(f\"- Tile overlap: {overlap}\")\n    print(f\"- Chip size: {chip_size}\")\n    print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n    print(f\"- Min building area: {small_building_area}\")\n    print(f\"- Simplify tolerance: {simplify_tolerance}\")\n\n    # Create dataset\n    dataset = BuildingFootprintDataset(raster_path=raster_path, chip_size=chip_size)\n\n    # Custom collate function to handle Shapely objects\n    def custom_collate(batch):\n        \"\"\"\n        Custom collate function that handles Shapely geometries\n        by keeping them as Python objects rather than trying to collate them.\n        \"\"\"\n        elem = batch[0]\n        if isinstance(elem, dict):\n            result = {}\n            for key in elem:\n                if key == \"bbox\":\n                    # Don't collate shapely objects, keep as list\n                    result[key] = [d[key] for d in batch]\n                else:\n                    # For tensors and other collatable types\n                    try:\n                        result[key] = (\n                            torch.utils.data._utils.collate.default_collate(\n                                [d[key] for d in batch]\n                            )\n                        )\n                    except TypeError:\n                        # Fall back to list for non-collatable types\n                        result[key] = [d[key] for d in batch]\n            return result\n        else:\n            # Default collate for non-dict types\n            return torch.utils.data._utils.collate.default_collate(batch)\n\n    # Create dataloader with simple indexing and custom collate\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=0,\n        collate_fn=custom_collate,\n    )\n\n    # Process batches\n    all_polygons = []\n    all_scores = []\n\n    print(f\"Processing raster with {len(dataloader)} batches\")\n    for batch in tqdm(dataloader):\n        # Move images to device\n        images = batch[\"image\"].to(self.device)\n        coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n        bboxes = batch[\n            \"bbox\"\n        ]  # Geographic bounding boxes - now a list, not a tensor\n\n        # Run inference\n        predictions = self.model(images)\n\n        # Process predictions\n        for idx, prediction in enumerate(predictions):\n            masks = prediction[\"masks\"].cpu().numpy()\n            scores = prediction[\"scores\"].cpu().numpy()\n            labels = prediction[\"labels\"].cpu().numpy()\n\n            # Skip if no predictions\n            if len(scores) == 0:\n                continue\n\n            # Filter by confidence threshold\n            valid_indices = scores &gt;= confidence_threshold\n            masks = masks[valid_indices]\n            scores = scores[valid_indices]\n            labels = labels[valid_indices]\n\n            # Skip if no valid predictions\n            if len(scores) == 0:\n                continue\n\n            # Get window coordinates\n            # The coords might be in different formats depending on batch handling\n            if isinstance(coords, list):\n                # If coords is a list of tuples\n                coord_item = coords[idx]\n                if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                    i, j = coord_item\n                elif isinstance(coord_item, torch.Tensor):\n                    i, j = coord_item.cpu().numpy().tolist()\n                else:\n                    print(f\"Unexpected coords format: {type(coord_item)}\")\n                    continue\n            elif isinstance(coords, torch.Tensor):\n                # If coords is a tensor of shape [batch_size, 2]\n                i, j = coords[idx].cpu().numpy().tolist()\n            else:\n                print(f\"Unexpected coords type: {type(coords)}\")\n                continue\n\n            # Get window size\n            if isinstance(batch[\"window_size\"], list):\n                window_item = batch[\"window_size\"][idx]\n                if isinstance(window_item, tuple) and len(window_item) == 2:\n                    window_width, window_height = window_item\n                elif isinstance(window_item, torch.Tensor):\n                    window_width, window_height = window_item.cpu().numpy().tolist()\n                else:\n                    print(f\"Unexpected window_size format: {type(window_item)}\")\n                    continue\n            elif isinstance(batch[\"window_size\"], torch.Tensor):\n                window_width, window_height = (\n                    batch[\"window_size\"][idx].cpu().numpy().tolist()\n                )\n            else:\n                print(f\"Unexpected window_size type: {type(batch['window_size'])}\")\n                continue\n\n            # Process masks to polygons\n            for mask_idx, mask in enumerate(masks):\n                # Get binary mask\n                binary_mask = mask[0]  # Get binary mask\n\n                # Convert mask to polygon with custom parameters\n                contours = self._mask_to_polygons(\n                    binary_mask,\n                    simplify_tolerance=simplify_tolerance,\n                    mask_threshold=mask_threshold,\n                    small_building_area=small_building_area,\n                )\n\n                # Skip if no valid polygons\n                if not contours:\n                    continue\n\n                # Transform polygons to geographic coordinates\n                with rasterio.open(raster_path) as src:\n                    transform = src.transform\n\n                    for contour in contours:\n                        # Convert polygon to global coordinates\n                        global_polygon = []\n                        for x, y in contour:\n                            # Adjust coordinates based on window position\n                            gx, gy = transform * (i + x, j + y)\n                            global_polygon.append((gx, gy))\n\n                        # Create Shapely polygon\n                        if len(global_polygon) &gt;= 3:\n                            try:\n                                shapely_poly = Polygon(global_polygon)\n                                if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                                    all_polygons.append(shapely_poly)\n                                    all_scores.append(float(scores[mask_idx]))\n                            except Exception as e:\n                                print(f\"Error creating polygon: {e}\")\n\n    # Create GeoDataFrame\n    if not all_polygons:\n        print(\"No valid polygons found\")\n        return None\n\n    gdf = gpd.GeoDataFrame(\n        {\n            \"geometry\": all_polygons,\n            \"confidence\": all_scores,\n            \"class\": 1,  # Building class\n        },\n        crs=dataset.crs,\n    )\n\n    # Remove overlapping polygons with custom threshold\n    gdf = self._filter_overlapping_polygons(\n        gdf, nms_iou_threshold=nms_iou_threshold\n    )\n\n    # Save to file if requested\n    if output_path:\n        gdf.to_file(output_path, driver=\"GeoJSON\")\n        print(f\"Saved {len(gdf)} building footprints to {output_path}\")\n\n    return gdf\n</code></pre>"},{"location":"extract/#geoai.extract.BuildingFootprintExtractor.visualize_results","title":"<code>visualize_results(self, raster_path, gdf=None, output_path=None, figsize=(12, 12))</code>","text":"<p>Visualize building detection results.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <p>Path to input raster</p> required <code>gdf</code> <p>GeoDataFrame with building polygons (optional)</p> <code>None</code> <code>output_path</code> <p>Path to save visualization (optional)</p> <code>None</code> <code>figsize</code> <p>Figure size (width, height) in inches</p> <code>(12, 12)</code> Source code in <code>geoai/extract.py</code> <pre><code>def visualize_results(\n    self, raster_path, gdf=None, output_path=None, figsize=(12, 12)\n):\n    \"\"\"\n    Visualize building detection results.\n\n    Args:\n        raster_path: Path to input raster\n        gdf: GeoDataFrame with building polygons (optional)\n        output_path: Path to save visualization (optional)\n        figsize: Figure size (width, height) in inches\n    \"\"\"\n    # Check if raster file exists\n    if not os.path.exists(raster_path):\n        print(f\"Error: Raster file '{raster_path}' not found.\")\n        return\n\n    # Process raster if GeoDataFrame not provided\n    if gdf is None:\n        gdf = self.process_raster(raster_path)\n\n    if gdf is None or len(gdf) == 0:\n        print(\"No buildings to visualize\")\n        return\n\n    # Read raster for visualization\n    with rasterio.open(raster_path) as src:\n        # Read the entire image or a subset if it's very large\n        if src.height &gt; 2000 or src.width &gt; 2000:\n            # Calculate scale factor to reduce size\n            scale = min(2000 / src.height, 2000 / src.width)\n            out_shape = (\n                int(src.count),\n                int(src.height * scale),\n                int(src.width * scale),\n            )\n\n            # Read and resample\n            image = src.read(\n                out_shape=out_shape, resampling=rasterio.enums.Resampling.bilinear\n            )\n        else:\n            image = src.read()\n\n        # Convert to RGB for display\n        if image.shape[0] &gt; 3:\n            image = image[:3]\n        elif image.shape[0] == 1:\n            image = np.repeat(image, 3, axis=0)\n\n        # Normalize image for display\n        image = image.transpose(1, 2, 0)  # CHW to HWC\n        image = image.astype(np.float32)\n\n        if image.max() &gt; 10:  # Likely 0-255 range\n            image = image / 255.0\n\n        image = np.clip(image, 0, 1)\n\n        # Get image bounds\n        bounds = src.bounds\n\n    # Create figure with appropriate aspect ratio\n    aspect_ratio = image.shape[1] / image.shape[0]  # width / height\n    plt.figure(figsize=(figsize[0], figsize[0] / aspect_ratio))\n\n    # Create axis with the right projection if CRS is available\n    ax = plt.gca()\n\n    # Display image\n    ax.imshow(image)\n\n    # Convert GeoDataFrame to pixel coordinates for plotting\n    with rasterio.open(raster_path) as src:\n\n        def geo_to_pixel(x, y):\n            return ~src.transform * (x, y)\n\n        # Plot each building footprint\n        for _, row in gdf.iterrows():\n            # Convert polygon to pixel coordinates\n            geom = row.geometry\n            if geom.is_empty:\n                continue\n\n            try:\n                # Get polygon exterior coordinates\n                x, y = geom.exterior.xy\n\n                # Convert to pixel coordinates\n                pixel_coords = [geo_to_pixel(x[i], y[i]) for i in range(len(x))]\n                pixel_x = [coord[0] for coord in pixel_coords]\n                pixel_y = [coord[1] for coord in pixel_coords]\n\n                # Plot polygon\n                ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n            except Exception as e:\n                print(f\"Error plotting polygon: {e}\")\n\n    # Remove axes\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(f\"Building Footprints (Found: {len(gdf)})\")\n\n    # Add colorbar for confidence if available\n    if \"confidence\" in gdf.columns:\n        # Create a colorbar legend\n        sm = plt.cm.ScalarMappable(\n            cmap=plt.cm.viridis,\n            norm=plt.Normalize(gdf.confidence.min(), gdf.confidence.max()),\n        )\n        sm.set_array([])\n        cbar = plt.colorbar(sm, ax=ax, orientation=\"vertical\", shrink=0.7)\n        cbar.set_label(\"Confidence\")\n\n    # Save if requested\n    if output_path:\n        plt.tight_layout()\n        plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n        print(f\"Visualization saved to {output_path}\")\n\n    plt.close()\n\n    # Create a simpler visualization focused just on a subset of buildings\n    # This helps when the raster is very large\n    plt.figure(figsize=figsize)\n    ax = plt.gca()\n\n    # Choose a subset of the image to show\n    with rasterio.open(raster_path) as src:\n        # Get a sample window based on the first few buildings\n        if len(gdf) &gt; 0:\n            # Get centroid of first building\n            sample_geom = gdf.iloc[0].geometry\n            centroid = sample_geom.centroid\n\n            # Convert to pixel coordinates\n            center_x, center_y = ~src.transform * (centroid.x, centroid.y)\n\n            # Define a window around this building\n            window_size = 500  # pixels\n            window = rasterio.windows.Window(\n                max(0, int(center_x - window_size / 2)),\n                max(0, int(center_y - window_size / 2)),\n                min(window_size, src.width - int(center_x - window_size / 2)),\n                min(window_size, src.height - int(center_y - window_size / 2)),\n            )\n\n            # Read this window\n            sample_image = src.read(window=window)\n\n            # Convert to RGB for display\n            if sample_image.shape[0] &gt; 3:\n                sample_image = sample_image[:3]\n            elif sample_image.shape[0] == 1:\n                sample_image = np.repeat(sample_image, 3, axis=0)\n\n            # Normalize image for display\n            sample_image = sample_image.transpose(1, 2, 0)  # CHW to HWC\n            sample_image = sample_image.astype(np.float32)\n\n            if sample_image.max() &gt; 10:  # Likely 0-255 range\n                sample_image = sample_image / 255.0\n\n            sample_image = np.clip(sample_image, 0, 1)\n\n            # Get transform for this window\n            window_transform = src.window_transform(window)\n\n            # Display sample image\n            ax.imshow(sample_image)\n\n            # Filter buildings that intersect with this window\n            window_bounds = rasterio.windows.bounds(window, src.transform)\n            window_box = box(*window_bounds)\n            visible_gdf = gdf[gdf.intersects(window_box)]\n\n            # Plot building footprints in this view\n            for _, row in visible_gdf.iterrows():\n                try:\n                    # Get polygon exterior coordinates\n                    geom = row.geometry\n                    if geom.is_empty:\n                        continue\n\n                    x, y = geom.exterior.xy\n\n                    # Convert to pixel coordinates relative to window\n                    pixel_coords = [\n                        ~window_transform * (x[i], y[i]) for i in range(len(x))\n                    ]\n                    pixel_x = [coord[0] for coord in pixel_coords]\n                    pixel_y = [coord[1] for coord in pixel_coords]\n\n                    # Plot polygon\n                    ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                except Exception as e:\n                    print(f\"Error plotting polygon in sample view: {e}\")\n\n            # Set title\n            ax.set_title(\n                f\"Sample Area - Building Footprints (Showing: {len(visible_gdf)})\"\n            )\n\n            # Remove axes\n            ax.set_xticks([])\n            ax.set_yticks([])\n\n            # Save if requested\n            if output_path:\n                sample_output = (\n                    os.path.splitext(output_path)[0]\n                    + \"_sample\"\n                    + os.path.splitext(output_path)[1]\n                )\n                plt.tight_layout()\n                plt.savefig(sample_output, dpi=300, bbox_inches=\"tight\")\n                print(f\"Sample visualization saved to {sample_output}\")\n\n    return True\n</code></pre>"},{"location":"faq/","title":"FAQ","text":""},{"location":"geoai/","title":"geoai module","text":"<p>Main module.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install geoai, run this command in your terminal:</p> <pre><code>pip install geoai-py\n</code></pre> <p>This is the preferred method to install geoai, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>To install geoai from sources, run this command in your terminal:</p> <pre><code>pip install git+https://github.com/opengeos/geoai\n</code></pre>"},{"location":"segmentation/","title":"segmentation module","text":""},{"location":"segmentation/#geoai.segmentation.CustomDataset","title":"<code> CustomDataset            (Dataset)         </code>","text":"<p>Custom Dataset for loading images and masks.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>class CustomDataset(Dataset):\n    \"\"\"Custom Dataset for loading images and masks.\"\"\"\n\n    def __init__(\n        self,\n        images_dir: str,\n        masks_dir: str,\n        transform: A.Compose = None,\n        target_size: tuple = (256, 256),\n        num_classes: int = 2,\n    ):\n        \"\"\"\n        Args:\n            images_dir (str): Directory containing images.\n            masks_dir (str): Directory containing masks.\n            transform (A.Compose, optional): Transformations to be applied on the images and masks.\n            target_size (tuple, optional): Target size for resizing images and masks.\n            num_classes (int, optional): Number of classes in the masks.\n        \"\"\"\n        self.images_dir = images_dir\n        self.masks_dir = masks_dir\n        self.transform = transform\n        self.target_size = target_size\n        self.num_classes = num_classes\n        self.images = sorted(os.listdir(images_dir))\n        self.masks = sorted(os.listdir(masks_dir))\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the total number of samples.\"\"\"\n        return len(self.images)\n\n    def __getitem__(self, idx: int) -&gt; dict:\n        \"\"\"\n        Args:\n            idx (int): Index of the sample to fetch.\n\n        Returns:\n            dict: A dictionary with 'pixel_values' and 'labels'.\n        \"\"\"\n        img_path = os.path.join(self.images_dir, self.images[idx])\n        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n        image = Image.open(img_path).convert(\"RGB\")\n        mask = Image.open(mask_path).convert(\"L\")\n\n        image = image.resize(self.target_size)\n        mask = mask.resize(self.target_size)\n\n        image = np.array(image)\n        mask = np.array(mask)\n\n        mask = (mask &gt; 127).astype(np.uint8)\n\n        if self.transform:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed[\"image\"]\n            mask = transformed[\"mask\"]\n\n        assert (\n            mask.max() &lt; self.num_classes\n        ), f\"Mask values should be less than {self.num_classes}, but found {mask.max()}\"\n        assert (\n            mask.min() &gt;= 0\n        ), f\"Mask values should be greater than or equal to 0, but found {mask.min()}\"\n\n        mask = mask.clone().detach().long()\n\n        return {\"pixel_values\": image, \"labels\": mask}\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.CustomDataset.__getitem__","title":"<code>__getitem__(self, idx)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the sample to fetch.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with 'pixel_values' and 'labels'.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; dict:\n    \"\"\"\n    Args:\n        idx (int): Index of the sample to fetch.\n\n    Returns:\n        dict: A dictionary with 'pixel_values' and 'labels'.\n    \"\"\"\n    img_path = os.path.join(self.images_dir, self.images[idx])\n    mask_path = os.path.join(self.masks_dir, self.masks[idx])\n    image = Image.open(img_path).convert(\"RGB\")\n    mask = Image.open(mask_path).convert(\"L\")\n\n    image = image.resize(self.target_size)\n    mask = mask.resize(self.target_size)\n\n    image = np.array(image)\n    mask = np.array(mask)\n\n    mask = (mask &gt; 127).astype(np.uint8)\n\n    if self.transform:\n        transformed = self.transform(image=image, mask=mask)\n        image = transformed[\"image\"]\n        mask = transformed[\"mask\"]\n\n    assert (\n        mask.max() &lt; self.num_classes\n    ), f\"Mask values should be less than {self.num_classes}, but found {mask.max()}\"\n    assert (\n        mask.min() &gt;= 0\n    ), f\"Mask values should be greater than or equal to 0, but found {mask.min()}\"\n\n    mask = mask.clone().detach().long()\n\n    return {\"pixel_values\": image, \"labels\": mask}\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.CustomDataset.__init__","title":"<code>__init__(self, images_dir, masks_dir, transform=None, target_size=(256, 256), num_classes=2)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Directory containing images.</p> required <code>masks_dir</code> <code>str</code> <p>Directory containing masks.</p> required <code>transform</code> <code>A.Compose</code> <p>Transformations to be applied on the images and masks.</p> <code>None</code> <code>target_size</code> <code>tuple</code> <p>Target size for resizing images and masks.</p> <code>(256, 256)</code> <code>num_classes</code> <code>int</code> <p>Number of classes in the masks.</p> <code>2</code> Source code in <code>geoai/segmentation.py</code> <pre><code>def __init__(\n    self,\n    images_dir: str,\n    masks_dir: str,\n    transform: A.Compose = None,\n    target_size: tuple = (256, 256),\n    num_classes: int = 2,\n):\n    \"\"\"\n    Args:\n        images_dir (str): Directory containing images.\n        masks_dir (str): Directory containing masks.\n        transform (A.Compose, optional): Transformations to be applied on the images and masks.\n        target_size (tuple, optional): Target size for resizing images and masks.\n        num_classes (int, optional): Number of classes in the masks.\n    \"\"\"\n    self.images_dir = images_dir\n    self.masks_dir = masks_dir\n    self.transform = transform\n    self.target_size = target_size\n    self.num_classes = num_classes\n    self.images = sorted(os.listdir(images_dir))\n    self.masks = sorted(os.listdir(masks_dir))\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.CustomDataset.__len__","title":"<code>__len__(self)</code>  <code>special</code>","text":"<p>Returns the total number of samples.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the total number of samples.\"\"\"\n    return len(self.images)\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.get_transform","title":"<code>get_transform()</code>","text":"<p>Returns:</p> Type Description <code>A.Compose</code> <p>A composition of image transformations.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def get_transform() -&gt; A.Compose:\n    \"\"\"\n    Returns:\n        A.Compose: A composition of image transformations.\n    \"\"\"\n    return A.Compose(\n        [\n            A.Resize(256, 256),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.RandomRotate90(p=0.5),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            ToTensorV2(),\n        ]\n    )\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.load_model","title":"<code>load_model(model_path, device)</code>","text":"<p>Loads the fine-tuned model from the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model.</p> required <code>device</code> <code>torch.device</code> <p>Device to load the model on.</p> required <p>Returns:</p> Type Description <code>SegformerForSemanticSegmentation</code> <p>Loaded model.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def load_model(\n    model_path: str, device: torch.device\n) -&gt; SegformerForSemanticSegmentation:\n    \"\"\"\n    Loads the fine-tuned model from the specified path.\n\n    Args:\n        model_path (str): Path to the model.\n        device (torch.device): Device to load the model on.\n\n    Returns:\n        SegformerForSemanticSegmentation: Loaded model.\n    \"\"\"\n    model = SegformerForSemanticSegmentation.from_pretrained(model_path)\n    model.to(device)\n    model.eval()\n    return model\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.predict_image","title":"<code>predict_image(model, image_tensor, original_size, device)</code>","text":"<p>Predicts the segmentation mask for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>SegformerForSemanticSegmentation</code> <p>Fine-tuned model.</p> required <code>image_tensor</code> <code>torch.Tensor</code> <p>Preprocessed image tensor.</p> required <code>original_size</code> <code>tuple</code> <p>Original size of the image (width, height).</p> required <code>device</code> <code>torch.device</code> <p>Device to perform inference on.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Predicted segmentation mask.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def predict_image(\n    model: SegformerForSemanticSegmentation,\n    image_tensor: torch.Tensor,\n    original_size: tuple,\n    device: torch.device,\n) -&gt; np.ndarray:\n    \"\"\"\n    Predicts the segmentation mask for the input image.\n\n    Args:\n        model (SegformerForSemanticSegmentation): Fine-tuned model.\n        image_tensor (torch.Tensor): Preprocessed image tensor.\n        original_size (tuple): Original size of the image (width, height).\n        device (torch.device): Device to perform inference on.\n\n    Returns:\n        np.ndarray: Predicted segmentation mask.\n    \"\"\"\n    with torch.no_grad():\n        image_tensor = image_tensor.to(device)\n        outputs = model(pixel_values=image_tensor)\n        logits = outputs.logits\n        upsampled_logits = F.interpolate(\n            logits, size=original_size[::-1], mode=\"bilinear\", align_corners=False\n        )\n        predictions = torch.argmax(upsampled_logits, dim=1).cpu().numpy()\n    return predictions[0]\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.prepare_datasets","title":"<code>prepare_datasets(images_dir, masks_dir, transform, test_size=0.2, random_state=42)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Directory containing images.</p> required <code>masks_dir</code> <code>str</code> <p>Directory containing masks.</p> required <code>transform</code> <code>A.Compose</code> <p>Transformations to be applied.</p> required <code>test_size</code> <code>float</code> <p>Proportion of the dataset to include in the validation split.</p> <code>0.2</code> <code>random_state</code> <code>int</code> <p>Random seed for shuffling the dataset.</p> <code>42</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Training and validation datasets.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def prepare_datasets(\n    images_dir: str,\n    masks_dir: str,\n    transform: A.Compose,\n    test_size: float = 0.2,\n    random_state: int = 42,\n) -&gt; tuple:\n    \"\"\"\n    Args:\n        images_dir (str): Directory containing images.\n        masks_dir (str): Directory containing masks.\n        transform (A.Compose): Transformations to be applied.\n        test_size (float, optional): Proportion of the dataset to include in the validation split.\n        random_state (int, optional): Random seed for shuffling the dataset.\n\n    Returns:\n        tuple: Training and validation datasets.\n    \"\"\"\n    dataset = CustomDataset(images_dir, masks_dir, transform)\n    train_indices, val_indices = train_test_split(\n        list(range(len(dataset))), test_size=test_size, random_state=random_state\n    )\n    train_dataset = Subset(dataset, train_indices)\n    val_dataset = Subset(dataset, val_indices)\n    return train_dataset, val_dataset\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.preprocess_image","title":"<code>preprocess_image(image_path, target_size=(256, 256))</code>","text":"<p>Preprocesses the input image for prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the input image.</p> required <code>target_size</code> <code>tuple</code> <p>Target size for resizing the image.</p> <code>(256, 256)</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Preprocessed image tensor.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def preprocess_image(image_path: str, target_size: tuple = (256, 256)) -&gt; torch.Tensor:\n    \"\"\"\n    Preprocesses the input image for prediction.\n\n    Args:\n        image_path (str): Path to the input image.\n        target_size (tuple, optional): Target size for resizing the image.\n\n    Returns:\n        torch.Tensor: Preprocessed image tensor.\n    \"\"\"\n    image = Image.open(image_path).convert(\"RGB\")\n    transform = A.Compose(\n        [\n            A.Resize(target_size[0], target_size[1]),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            ToTensorV2(),\n        ]\n    )\n    image = np.array(image)\n    transformed = transform(image=image)\n    return transformed[\"image\"].unsqueeze(0)\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.segment_image","title":"<code>segment_image(image_path, model_path, target_size=(256, 256), device=device(type='cpu'))</code>","text":"<p>Segments the input image using the fine-tuned model.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the input image.</p> required <code>model_path</code> <code>str</code> <p>Path to the fine-tuned model.</p> required <code>target_size</code> <code>tuple</code> <p>Target size for resizing the image.</p> <code>(256, 256)</code> <code>device</code> <code>torch.device</code> <p>Device to perform inference on.</p> <code>device(type='cpu')</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Predicted segmentation mask.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def segment_image(\n    image_path: str,\n    model_path: str,\n    target_size: tuple = (256, 256),\n    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n) -&gt; np.ndarray:\n    \"\"\"\n    Segments the input image using the fine-tuned model.\n\n    Args:\n        image_path (str): Path to the input image.\n        model_path (str): Path to the fine-tuned model.\n        target_size (tuple, optional): Target size for resizing the image.\n        device (torch.device, optional): Device to perform inference on.\n\n    Returns:\n        np.ndarray: Predicted segmentation mask.\n    \"\"\"\n    model = load_model(model_path, device)\n    image = Image.open(image_path).convert(\"RGB\")\n    original_size = image.size\n    image_tensor = preprocess_image(image_path, target_size)\n    predictions = predict_image(model, image_tensor, original_size, device)\n    return predictions\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.train_model","title":"<code>train_model(train_dataset, val_dataset, pretrained_model='nvidia/segformer-b0-finetuned-ade-512-512', model_save_path='./model', output_dir='./results', num_epochs=10, batch_size=8, learning_rate=5e-05)</code>","text":"<p>Trains the model and saves the fine-tuned model to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataset</code> <code>Dataset</code> <p>Training dataset.</p> required <code>val_dataset</code> <code>Dataset</code> <p>Validation dataset.</p> required <code>pretrained_model</code> <code>str</code> <p>Pretrained model to fine-tune.</p> <code>'nvidia/segformer-b0-finetuned-ade-512-512'</code> <code>model_save_path</code> <code>str</code> <p>Path to save the fine-tuned model. Defaults to './model'.</p> <code>'./model'</code> <code>output_dir</code> <code>str</code> <p>Directory to save training outputs.</p> <code>'./results'</code> <code>num_epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>Batch size for training and evaluation.</p> <code>8</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for training.</p> <code>5e-05</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the saved fine-tuned model.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def train_model(\n    train_dataset: Dataset,\n    val_dataset: Dataset,\n    pretrained_model: str = \"nvidia/segformer-b0-finetuned-ade-512-512\",\n    model_save_path: str = \"./model\",\n    output_dir: str = \"./results\",\n    num_epochs: int = 10,\n    batch_size: int = 8,\n    learning_rate: float = 5e-5,\n) -&gt; str:\n    \"\"\"\n    Trains the model and saves the fine-tuned model to the specified path.\n\n    Args:\n        train_dataset (Dataset): Training dataset.\n        val_dataset (Dataset): Validation dataset.\n        pretrained_model (str, optional): Pretrained model to fine-tune.\n        model_save_path (str): Path to save the fine-tuned model. Defaults to './model'.\n        output_dir (str, optional): Directory to save training outputs.\n        num_epochs (int, optional): Number of training epochs.\n        batch_size (int, optional): Batch size for training and evaluation.\n        learning_rate (float, optional): Learning rate for training.\n\n    Returns:\n        str: Path to the saved fine-tuned model.\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = SegformerForSemanticSegmentation.from_pretrained(pretrained_model).to(\n        device\n    )\n    data_collator = DefaultDataCollator(return_tensors=\"pt\")\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=num_epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_dir=\"./logs\",\n        learning_rate=learning_rate,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n    )\n\n    trainer.train()\n    model.save_pretrained(model_save_path)\n    print(f\"Model saved to {model_save_path}\")\n    return model_save_path\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.visualize_predictions","title":"<code>visualize_predictions(image_path, segmented_mask, target_size=(256, 256), reference_image_path=None)</code>","text":"<p>Visualizes the original image, segmented mask, and optionally the reference image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the original image.</p> required <code>segmented_mask</code> <code>np.ndarray</code> <p>Predicted segmentation mask.</p> required <code>target_size</code> <code>tuple</code> <p>Target size for resizing images.</p> <code>(256, 256)</code> <code>reference_image_path</code> <code>str</code> <p>Path to the reference image.</p> <code>None</code> Source code in <code>geoai/segmentation.py</code> <pre><code>def visualize_predictions(\n    image_path: str,\n    segmented_mask: np.ndarray,\n    target_size: tuple = (256, 256),\n    reference_image_path: str = None,\n) -&gt; None:\n    \"\"\"\n    Visualizes the original image, segmented mask, and optionally the reference image.\n\n    Args:\n        image_path (str): Path to the original image.\n        segmented_mask (np.ndarray): Predicted segmentation mask.\n        target_size (tuple, optional): Target size for resizing images.\n        reference_image_path (str, optional): Path to the reference image.\n    \"\"\"\n    original_image = Image.open(image_path).convert(\"RGB\")\n    original_image = original_image.resize(target_size)\n    segmented_image = Image.fromarray((segmented_mask * 255).astype(np.uint8))\n\n    if reference_image_path:\n        reference_image = Image.open(reference_image_path).convert(\"RGB\")\n        reference_image = reference_image.resize(target_size)\n        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n        axes[1].imshow(reference_image)\n        axes[1].set_title(\"Reference Image\")\n        axes[1].axis(\"off\")\n    else:\n        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n    axes[0].imshow(original_image)\n    axes[0].set_title(\"Original Image\")\n    axes[0].axis(\"off\")\n\n    if reference_image_path:\n        axes[2].imshow(segmented_image, cmap=\"gray\")\n        axes[2].set_title(\"Segmented Image\")\n        axes[2].axis(\"off\")\n    else:\n        axes[1].imshow(segmented_image, cmap=\"gray\")\n        axes[1].set_title(\"Segmented Image\")\n        axes[1].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use geoai in a project:</p> <pre><code>import geoai\n</code></pre>"},{"location":"examples/data_viz/","title":"Data viz","text":"In\u00a0[\u00a0]: Copied! <pre>from torchgeo.datasets import NAIP\nfrom torchgeo.samplers import RandomGeoSampler, GridGeoSampler\nfrom geoai.common import viz_image, viz_raster, dict_to_image\nfrom geoai.download import download_naip\n</pre> from torchgeo.datasets import NAIP from torchgeo.samplers import RandomGeoSampler, GridGeoSampler from geoai.common import viz_image, viz_raster, dict_to_image from geoai.download import download_naip In\u00a0[\u00a0]: Copied! <pre>root = \"naip_data\"\n</pre> root = \"naip_data\" In\u00a0[\u00a0]: Copied! <pre>bbox = (-117.6029, 47.65, -117.5936, 47.6563)\ndownloaded_files = download_naip(\n    bbox=bbox,\n    output_dir=root,\n    max_items=1,\n)\n</pre> bbox = (-117.6029, 47.65, -117.5936, 47.6563) downloaded_files = download_naip(     bbox=bbox,     output_dir=root,     max_items=1, ) <ul> <li>torchgeo.datasets.NAIP: Provides access to the National Agriculture Imagery Program (NAIP) dataset, which offers high-resolution aerial imagery across the United States.</li> <li>torchgeo.samplers: Contains sampling strategies for geospatial data:<ul> <li>RandomGeoSampler: Samples random patches from the dataset</li> <li>GridGeoSampler: Samples patches in a grid pattern with specified stride</li> </ul> </li> <li>geoai.common: Custom utility functions for visualization:<ul> <li>viz_image: Visualizes tensor images</li> <li>viz_raster: Displays georeferenced data on an interactive map</li> <li>dict_to_image: Converts dictionary representation to image format</li> </ul> </li> </ul> <p>Load the NAIP dataset from the specified root directory:</p> In\u00a0[\u00a0]: Copied! <pre>dataset = NAIP(root)\n</pre> dataset = NAIP(root) <p>Examine the dataset object to understand its properties:</p> In\u00a0[\u00a0]: Copied! <pre>dataset\n</pre> dataset <p>This will display information about the NAIP dataset including available imagery dates, coverage area, and other metadata.</p> <p>Check the Coordinate Reference System (CRS) used by the dataset:</p> In\u00a0[\u00a0]: Copied! <pre>dataset.crs\n</pre> dataset.crs <p>The CRS defines how the geospatial data is projected onto a coordinate system, which is essential for accurate visualization and analysis.</p> In\u00a0[\u00a0]: Copied! <pre>train_sampler = RandomGeoSampler(dataset, size=256, length=1000)\n</pre> train_sampler = RandomGeoSampler(dataset, size=256, length=1000) <p>This creates a sampler that will randomly select 1000 patches, each 256x256 pixels in size. This sampling strategy is commonly used for training machine learning models where you need a diverse set of examples.</p> <p>Extract a bounding box from the random sampler:</p> In\u00a0[\u00a0]: Copied! <pre>train_bbox = next(iter(train_sampler))\ntrain_bbox\n</pre> train_bbox = next(iter(train_sampler)) train_bbox <p>The bounding box contains the coordinates defining the spatial extent of our randomly sampled patch.</p> <p>Load the actual image data corresponding to the randomly selected bounding box:</p> In\u00a0[\u00a0]: Copied! <pre>train_image = dataset[next(iter(train_sampler))][\"image\"]\n</pre> train_image = dataset[next(iter(train_sampler))][\"image\"] <p>Examine the complete data dictionary returned for a sample, which includes both the image and metadata:</p> In\u00a0[\u00a0]: Copied! <pre>dataset[next(iter(train_sampler))]\n</pre> dataset[next(iter(train_sampler))] <p>This returns a dictionary containing the image tensor and associated metadata such as the bounding box, CRS, and other properties.</p> In\u00a0[\u00a0]: Copied! <pre>viz_image(\n    train_image, transpose=True, scale_factor=(1 / 250), title=\"Random GeoSampler\"\n)\n</pre> viz_image(     train_image, transpose=True, scale_factor=(1 / 250), title=\"Random GeoSampler\" ) <ul> <li>transpose=True: Rearranges the dimensions for proper display (from [C,H,W] to [H,W,C])</li> <li>scale_factor=(1/250): Scales the pixel values for better visualization</li> <li>title=\"Random GeoSampler\": Adds a descriptive title to the plot</li> </ul> In\u00a0[\u00a0]: Copied! <pre>test_sampler = GridGeoSampler(dataset, size=256, stride=128)\n</pre> test_sampler = GridGeoSampler(dataset, size=256, stride=128) <p>This sampler extracts 256x256 pixel patches in a grid pattern with a stride of 128 pixels, meaning patches will overlap by 128 pixels. Grid sampling is typically used for inference or testing, where systematic coverage of the area is important.</p> <p>Extract a bounding box from the grid sampler:</p> In\u00a0[\u00a0]: Copied! <pre>test_bbox = next(iter(test_sampler))\ntest_bbox\n</pre> test_bbox = next(iter(test_sampler)) test_bbox <p>Load the image data for a patch selected by the grid sampler:</p> In\u00a0[\u00a0]: Copied! <pre>test_image = dataset[next(iter(test_sampler))][\"image\"]\n</pre> test_image = dataset[next(iter(test_sampler))][\"image\"] In\u00a0[\u00a0]: Copied! <pre>viz_image(test_image, transpose=True, scale_factor=(1 / 250), title=\"Grid GeoSampler\")\n</pre> viz_image(test_image, transpose=True, scale_factor=(1 / 250), title=\"Grid GeoSampler\") <p>The visualization shows a systematically sampled patch from the dataset.</p> In\u00a0[\u00a0]: Copied! <pre>data = dataset[next(iter(test_sampler))]\n</pre> data = dataset[next(iter(test_sampler))] <p>Visualize the raster data on an interactive map with Google Satellite imagery as the background:</p> In\u00a0[\u00a0]: Copied! <pre>viz_raster(data, basemap=\"Google Satellite\")\n</pre> viz_raster(data, basemap=\"Google Satellite\") <p>This interactive visualization places the sampled data in its real-world geographic context, allowing you to see how it aligns with the Google Satellite imagery.</p>"},{"location":"examples/data_viz/#data-visualization","title":"Data visualization\u00b6","text":"<p>This notebook demonstrates how to work with geospatial imagery data using TorchGeo and GeoAI. We'll explore how to load data, sample it using different strategies, and visualize the results.</p>"},{"location":"examples/data_viz/#importing-required-libraries","title":"Importing Required Libraries\u00b6","text":"<p>First, we import the necessary libraries for our geospatial data visualization workflow:</p>"},{"location":"examples/data_viz/#download-naip-imagery","title":"Download NAIP imagery\u00b6","text":""},{"location":"examples/data_viz/#setting-up-the-dataset","title":"Setting Up the Dataset\u00b6","text":""},{"location":"examples/data_viz/#random-sampling-of-geospatial-data","title":"Random Sampling of Geospatial Data\u00b6","text":"<p>Create a random sampler to extract patches from the dataset:</p>"},{"location":"examples/data_viz/#visualizing-randomly-sampled-data","title":"Visualizing Randomly Sampled Data\u00b6","text":"<p>Display the randomly sampled image:</p>"},{"location":"examples/data_viz/#grid-sampling-of-geospatial-data","title":"Grid Sampling of Geospatial Data\u00b6","text":"<p>Create a grid sampler to extract patches in a systematic pattern:</p>"},{"location":"examples/data_viz/#visualizing-grid-sampled-data","title":"Visualizing Grid Sampled Data\u00b6","text":"<p>Display the image from the grid sampler:</p>"},{"location":"examples/data_viz/#advanced-visualization-with-geospatial-context","title":"Advanced Visualization with Geospatial Context\u00b6","text":"<p>Load a complete data sample including all metadata:</p>"},{"location":"examples/data_viz/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ol> <li>TorchGeo provides a flexible framework for working with geospatial datasets like NAIP.</li> <li>Different sampling strategies (random vs. grid) serve different purposes in geospatial machine learning workflows.</li> <li>Visualization tools help understand the data in both pixel space (viz_image) and geographic space (viz_raster).</li> <li>Working with geospatial data requires attention to coordinate reference systems (CRS) and proper handling of georeferenced data.</li> </ol>"},{"location":"examples/download_data/","title":"Download data","text":"In\u00a0[\u00a0]: Copied! <pre>from geoai.download import (\n    download_naip,\n    download_overture_buildings,\n    extract_building_stats,\n)\n</pre> from geoai.download import (     download_naip,     download_overture_buildings,     extract_building_stats, ) In\u00a0[\u00a0]: Copied! <pre>bbox = (-117.6029, 47.65, -117.5936, 47.6563)\n\n# Download NAIP imagery for the specified region\ndownloaded_files = download_naip(\n    bbox=bbox,\n    output_dir=\"naip_data\",\n    max_items=5,\n    # year=2020,\n)\n\nprint(f\"Downloaded {len(downloaded_files)} files.\")\n</pre> bbox = (-117.6029, 47.65, -117.5936, 47.6563)  # Download NAIP imagery for the specified region downloaded_files = download_naip(     bbox=bbox,     output_dir=\"naip_data\",     max_items=5,     # year=2020, )  print(f\"Downloaded {len(downloaded_files)} files.\") In\u00a0[\u00a0]: Copied! <pre># Format: (min_lon, min_lat, max_lon, max_lat)\nbbox = (-117.6029, 47.65, -117.5936, 47.6563)\n\n# Download buildings\noutput_file = download_overture_buildings(\n    bbox=bbox,\n    output_file=\"buildings.geojson\",\n    output_format=\"geojson\",\n    data_type=\"building\",\n    verbose=True,\n)\n\nif output_file:\n    # Extract statistics\n    stats = extract_building_stats(output_file)\n</pre> # Format: (min_lon, min_lat, max_lon, max_lat) bbox = (-117.6029, 47.65, -117.5936, 47.6563)  # Download buildings output_file = download_overture_buildings(     bbox=bbox,     output_file=\"buildings.geojson\",     output_format=\"geojson\",     data_type=\"building\",     verbose=True, )  if output_file:     # Extract statistics     stats = extract_building_stats(output_file)"},{"location":"examples/download_data/#download-data","title":"Download Data\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/","title":"Lidar viz","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"leafmap[lidar]\" open3d\n</pre> # %pip install \"leafmap[lidar]\" open3d In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap <p>Download a sample LiDAR dataset from Google Drive. The zip file is 52.1 MB and the uncompressed LAS file is 109 MB.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://open.gishub.org/data/lidar/madison.zip\"\nfilename = \"madison.las\"\n</pre> url = \"https://open.gishub.org/data/lidar/madison.zip\" filename = \"madison.las\" In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(url, \"madison.zip\", unzip=True)\n</pre> leafmap.download_file(url, \"madison.zip\", unzip=True) <p>Read the LiDAR data</p> In\u00a0[\u00a0]: Copied! <pre>las = leafmap.read_lidar(filename)\n</pre> las = leafmap.read_lidar(filename) <p>The LAS header.</p> In\u00a0[\u00a0]: Copied! <pre>las.header\n</pre> las.header <p>The number of points.</p> In\u00a0[\u00a0]: Copied! <pre>las.header.point_count\n</pre> las.header.point_count <p>The list of features.</p> In\u00a0[\u00a0]: Copied! <pre>list(las.point_format.dimension_names)\n</pre> list(las.point_format.dimension_names) <p>Inspect data.</p> In\u00a0[\u00a0]: Copied! <pre>las.X\n</pre> las.X In\u00a0[\u00a0]: Copied! <pre>las.Y\n</pre> las.Y In\u00a0[\u00a0]: Copied! <pre>las.Z\n</pre> las.Z In\u00a0[\u00a0]: Copied! <pre>las.intensity\n</pre> las.intensity In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"pyvista\")\n</pre> leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"pyvista\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, backend=\"ipygany\", background=\"white\")\n</pre> leafmap.view_lidar(filename, backend=\"ipygany\", background=\"white\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"panel\", background=\"white\")\n</pre> leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"panel\", background=\"white\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, backend=\"open3d\")\n</pre> leafmap.view_lidar(filename, backend=\"open3d\") <p></p>"},{"location":"examples/dataviz/lidar_viz/#visualizing-lidar-data-with-leafmap","title":"Visualizing LiDAR Data with Leafmap\u00b6","text":"<p>This notebook demonstrates how to visualize LiDAR data using leafmap.</p>"},{"location":"examples/dataviz/lidar_viz/#installation","title":"Installation\u00b6","text":"<p>Uncomment and run the following cell to install the required Python packages.</p>"},{"location":"examples/dataviz/lidar_viz/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#download-data","title":"Download data\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#metadata","title":"Metadata\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#read-data","title":"Read data\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#pyvista","title":"PyVista\u00b6","text":"<p>Visualize LiDAR data using the pyvista backend.</p>"},{"location":"examples/dataviz/lidar_viz/#ipygany","title":"ipygany\u00b6","text":"<p>Visualize LiDAR data using the ipygany backend.</p>"},{"location":"examples/dataviz/lidar_viz/#panel","title":"Panel\u00b6","text":"<p>Visualize LiDAR data using the panel backend.</p>"},{"location":"examples/dataviz/lidar_viz/#open3d","title":"Open3D\u00b6","text":"<p>Visualize LiDAR data using the open3d backend.</p>"},{"location":"examples/dataviz/raster_viz/","title":"Raster viz","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"leafmap[raster]\"\n</pre> # %pip install \"leafmap[raster]\" In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nurl = \"https://opendata.digitalglobe.com/events/california-fire-2020/pre-event/2018-02-16/pine-gulch-fire20/1030010076004E00.tif\"\nm.add_cog_layer(url, name=\"Fire (pre-event)\")\nm\n</pre> m = leafmap.Map() url = \"https://opendata.digitalglobe.com/events/california-fire-2020/pre-event/2018-02-16/pine-gulch-fire20/1030010076004E00.tif\" m.add_cog_layer(url, name=\"Fire (pre-event)\") m <p>You can add multiple COGs to the map. Let's add another COG to the map.</p> In\u00a0[\u00a0]: Copied! <pre>url2 = \"https://opendata.digitalglobe.com/events/california-fire-2020/post-event/2020-08-14/pine-gulch-fire20/10300100AAC8DD00.tif\"\nm.add_cog_layer(url2, name=\"Fire (post-event)\")\nm\n</pre> url2 = \"https://opendata.digitalglobe.com/events/california-fire-2020/post-event/2020-08-14/pine-gulch-fire20/10300100AAC8DD00.tif\" m.add_cog_layer(url2, name=\"Fire (post-event)\") m <p></p> <p>Create a split map for comparing two COGs.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.split_map(left_layer=url, right_layer=url2)\nm\n</pre> m = leafmap.Map() m.split_map(left_layer=url, right_layer=url2) m <p></p> In\u00a0[\u00a0]: Copied! <pre>dem_url = \"https://open.gishub.org/data/raster/srtm90.tif\"\nleafmap.download_file(dem_url, unzip=False)\n</pre> dem_url = \"https://open.gishub.org/data/raster/srtm90.tif\" leafmap.download_file(dem_url, unzip=False) <p>Visualize a single-band raster.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"srtm90.tif\", cmap=\"terrain\", layer_name=\"DEM\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"srtm90.tif\", cmap=\"terrain\", layer_name=\"DEM\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>landsat_url = \"https://open.gishub.org/data/raster/cog.tif\"\nleafmap.download_file(landsat_url)\n</pre> landsat_url = \"https://open.gishub.org/data/raster/cog.tif\" leafmap.download_file(landsat_url) <p>Visualize a multi-band raster.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"cog.tif\", bands=[4, 3, 2], layer_name=\"Landsat\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"cog.tif\", bands=[4, 3, 2], layer_name=\"Landsat\") m <p></p> <p>Create an interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://canada-spot-ortho.s3.amazonaws.com/canada_spot_orthoimages/canada_spot5_orthoimages/S5_2007/S5_11055_6057_20070622/S5_11055_6057_20070622.json\"\nleafmap.stac_bands(url)\n</pre> url = \"https://canada-spot-ortho.s3.amazonaws.com/canada_spot_orthoimages/canada_spot5_orthoimages/S5_2007/S5_11055_6057_20070622/S5_11055_6057_20070622.json\" leafmap.stac_bands(url) <p>Add STAC layers to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_stac_layer(url, bands=[\"pan\"], name=\"Panchromatic\")\nm.add_stac_layer(url, bands=[\"B3\", \"B2\", \"B1\"], name=\"False color\")\nm\n</pre> m = leafmap.Map() m.add_stac_layer(url, bands=[\"pan\"], name=\"Panchromatic\") m.add_stac_layer(url, bands=[\"B3\", \"B2\", \"B1\"], name=\"False color\") m <p></p> <p>Provide custom STAC API endpoints as a dictionary in the format of <code>{\"name\": \"url\"}</code>. The name will show up in the dropdown menu, while the url is the STAC API endpoint that will be used to search for items.</p> In\u00a0[\u00a0]: Copied! <pre>catalogs = {\n    \"Element84 Earth Search\": \"https://earth-search.aws.element84.com/v1\",\n    \"Microsoft Planetary Computer\": \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n}\n</pre> catalogs = {     \"Element84 Earth Search\": \"https://earth-search.aws.element84.com/v1\",     \"Microsoft Planetary Computer\": \"https://planetarycomputer.microsoft.com/api/stac/v1\", } In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40, -100], zoom=4)\nm.set_catalog_source(catalogs)\nm.add_stac_gui()\nm\n</pre> m = leafmap.Map(center=[40, -100], zoom=4) m.set_catalog_source(catalogs) m.add_stac_gui() m <p>Once the catalog panel is open, you can search for items from the custom STAC API endpoints. Simply draw a bounding box on the map or zoom to a location of interest. Click on the Collections button to retrieve the collections from the custom STAC API endpoints. Next, select a collection from the dropdown menu. Then, click on the Items button to retrieve the items from the selected collection. Finally, click on the Display button to add the selected item to the map.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>m.stac_gdf  # The GeoDataFrame of the STAC search results\n</pre> m.stac_gdf  # The GeoDataFrame of the STAC search results In\u00a0[\u00a0]: Copied! <pre>m.stac_dict  # The STAC search results as a dictionary\n</pre> m.stac_dict  # The STAC search results as a dictionary In\u00a0[\u00a0]: Copied! <pre>m.stac_item  # The selected STAC item of the search result\n</pre> m.stac_item  # The selected STAC item of the search result <p>To Be able to run this notebook you'll need to have AWS credential available as environment variables. Uncomment the following lines to set the environment variables.</p> In\u00a0[\u00a0]: Copied! <pre># os.environ[\"AWS_ACCESS_KEY_ID\"] = \"YOUR AWS ACCESS ID HERE\"\n# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"YOUR AWS ACCESS KEY HERE\"\n</pre> # os.environ[\"AWS_ACCESS_KEY_ID\"] = \"YOUR AWS ACCESS ID HERE\" # os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"YOUR AWS ACCESS KEY HERE\" <p>In this example, we will use datasets from the Maxar Open Data Program on AWS.</p> In\u00a0[\u00a0]: Copied! <pre>BUCKET = \"maxar-opendata\"\nFOLDER = \"events/Kahramanmaras-turkey-earthquake-23/\"\n</pre> BUCKET = \"maxar-opendata\" FOLDER = \"events/Kahramanmaras-turkey-earthquake-23/\" <p>List all the datasets in the bucket. Specify a file extension to filter the results if needed.</p> In\u00a0[\u00a0]: Copied! <pre>items = leafmap.s3_list_objects(BUCKET, FOLDER, ext=\".tif\")\nitems[:10]\n</pre> items = leafmap.s3_list_objects(BUCKET, FOLDER, ext=\".tif\") items[:10] <p>Visualize raster datasets from the bucket.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_cog_layer(items[2], name=\"Maxar\")\nm\n</pre> m = leafmap.Map() m.add_cog_layer(items[2], name=\"Maxar\") m <p></p>"},{"location":"examples/dataviz/raster_viz/#visualizing-raster-data-with-leafmap","title":"Visualizing Raster Data with Leafmap\u00b6","text":"<p>This notebook demonstrates how to visualize raster data using leafmap. Leafmap can visualize raster data (e.g., Cloud Optimized GeoTIFF) stored in a local file or on the cloud (e.g., AWS S3). It can also visualize raster data stored in a STAC catalog.</p>"},{"location":"examples/dataviz/raster_viz/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install the required packages if needed.</p>"},{"location":"examples/dataviz/raster_viz/#import-packages","title":"Import packages\u00b6","text":""},{"location":"examples/dataviz/raster_viz/#cog","title":"COG\u00b6","text":"<p>A Cloud Optimized GeoTIFF (COG) is a regular GeoTIFF file, aimed at being hosted on a HTTP file server, with an internal organization that enables more efficient workflows on the cloud. It does this by leveraging the ability of clients issuing HTTP GET range requests to ask for just the parts of a file they need. More information about COG can be found at https://www.cogeo.org/in-depth.html</p> <p>For this demo, we will use data from https://www.maxar.com/open-data/california-colorado-fires for mapping California and Colorado fires. Let's create an interactive map and add the COG to the map.</p>"},{"location":"examples/dataviz/raster_viz/#local-raster","title":"Local Raster\u00b6","text":"<p>Leafmap can also visualize local GeoTIFF files. Let's download some sample data</p>"},{"location":"examples/dataviz/raster_viz/#stac","title":"STAC\u00b6","text":"<p>The SpatioTemporal Asset Catalog (STAC) specification provides a common language to describe a range of geospatial information so that it can more easily be indexed and discovered. A SpatioTemporal Asset is any file that represents information about the earth captured in a certain space and time. STAC aims to enable that next generation of geospatial search engines, while also supporting web best practices so geospatial information is more easily surfaced in traditional search engines. More information about STAC can be found at the STAC website. In this example, we will use a STAC item from the SPOT Orthoimages of Canada available through the link below:</p>"},{"location":"examples/dataviz/raster_viz/#custom-stac-catalog","title":"Custom STAC Catalog\u00b6","text":""},{"location":"examples/dataviz/raster_viz/#aws-s3","title":"AWS S3\u00b6","text":""},{"location":"examples/dataviz/vector_viz/","title":"Vector viz","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"leafmap[vector]\"\n</pre> # %pip install \"leafmap[vector]\" In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[0, 0], zoom=2)\ndata = \"https://open.gishub.org/data/vector/cables.geojson\"\nm.add_vector(data, layer_name=\"Cable lines\", info_mode=\"on_hover\")\nm\n</pre> m = leafmap.Map(center=[0, 0], zoom=2) data = \"https://open.gishub.org/data/vector/cables.geojson\" m.add_vector(data, layer_name=\"Cable lines\", info_mode=\"on_hover\") m <p>You can style the vector with custom style callback functions.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[20, 0], zoom=2)\nm.add_basemap(\"CartoDB.DarkMatter\")\ndata = \"https://open.gishub.org/data/vector/cables.geojson\"\ncallback = lambda feat: {\"color\": feat[\"properties\"][\"color\"], \"weight\": 1}\nm.add_vector(data, layer_name=\"Cable lines\", style_callback=callback)\nm\n</pre> m = leafmap.Map(center=[20, 0], zoom=2) m.add_basemap(\"CartoDB.DarkMatter\") data = \"https://open.gishub.org/data/vector/cables.geojson\" callback = lambda feat: {\"color\": feat[\"properties\"][\"color\"], \"weight\": 1} m.add_vector(data, layer_name=\"Cable lines\", style_callback=callback) m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\ndata = \"https://raw.githubusercontent.com/opengeos/leafmap/master/docs/examples/data/countries.geojson\"\nm.add_data(\n    data, column=\"POP_EST\", scheme=\"Quantiles\", cmap=\"Blues\", legend_title=\"Population\"\n)\nm\n</pre> m = leafmap.Map() data = \"https://raw.githubusercontent.com/opengeos/leafmap/master/docs/examples/data/countries.geojson\" m.add_data(     data, column=\"POP_EST\", scheme=\"Quantiles\", cmap=\"Blues\", legend_title=\"Population\" ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_data(\n    data,\n    column=\"POP_EST\",\n    scheme=\"EqualInterval\",\n    cmap=\"Blues\",\n    legend_title=\"Population\",\n)\nm\n</pre> m = leafmap.Map() m.add_data(     data,     column=\"POP_EST\",     scheme=\"EqualInterval\",     cmap=\"Blues\",     legend_title=\"Population\", ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://open.gishub.org/data/duckdb/cities.parquet\"\ngdf = leafmap.read_parquet(url, return_type=\"gdf\", src_crs=\"EPSG:4326\")\ngdf.head()\n</pre> url = \"https://open.gishub.org/data/duckdb/cities.parquet\" gdf = leafmap.read_parquet(url, return_type=\"gdf\", src_crs=\"EPSG:4326\") gdf.head() <p>Visualize point data.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_vector(\n    gdf,\n    get_radius=20000,\n    get_fill_color=\"blue\",\n    zoom_to_layer=False,\n    map_args={\"center\": (40, -100), \"zoom\": 3, \"height\": 500},\n)\n</pre> leafmap.view_vector(     gdf,     get_radius=20000,     get_fill_color=\"blue\",     zoom_to_layer=False,     map_args={\"center\": (40, -100), \"zoom\": 3, \"height\": 500}, ) <p></p> <p>Visualizing polygon data.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://data.source.coop/giswqs/nwi/wetlands/DC_Wetlands.parquet\"\ngdf = leafmap.read_parquet(\n    url, return_type=\"gdf\", src_crs=\"EPSG:5070\", dst_crs=\"EPSG:4326\"\n)\ngdf.head()\n</pre> url = \"https://data.source.coop/giswqs/nwi/wetlands/DC_Wetlands.parquet\" gdf = leafmap.read_parquet(     url, return_type=\"gdf\", src_crs=\"EPSG:5070\", dst_crs=\"EPSG:4326\" ) gdf.head() In\u00a0[\u00a0]: Copied! <pre>leafmap.view_vector(gdf, get_fill_color=[0, 0, 255, 128])\n</pre> leafmap.view_vector(gdf, get_fill_color=[0, 0, 255, 128]) <p></p> <p>Alternatively, you can specify a color map to visualize the data.</p> In\u00a0[\u00a0]: Copied! <pre>color_map = {\n    \"Freshwater Forested/Shrub Wetland\": (0, 136, 55),\n    \"Freshwater Emergent Wetland\": (127, 195, 28),\n    \"Freshwater Pond\": (104, 140, 192),\n    \"Estuarine and Marine Wetland\": (102, 194, 165),\n    \"Riverine\": (1, 144, 191),\n    \"Lake\": (19, 0, 124),\n    \"Estuarine and Marine Deepwater\": (0, 124, 136),\n    \"Other\": (178, 134, 86),\n}\n</pre> color_map = {     \"Freshwater Forested/Shrub Wetland\": (0, 136, 55),     \"Freshwater Emergent Wetland\": (127, 195, 28),     \"Freshwater Pond\": (104, 140, 192),     \"Estuarine and Marine Wetland\": (102, 194, 165),     \"Riverine\": (1, 144, 191),     \"Lake\": (19, 0, 124),     \"Estuarine and Marine Deepwater\": (0, 124, 136),     \"Other\": (178, 134, 86), } In\u00a0[\u00a0]: Copied! <pre>leafmap.view_vector(gdf, color_column=\"WETLAND_TYPE\", color_map=color_map, opacity=0.5)\n</pre> leafmap.view_vector(gdf, color_column=\"WETLAND_TYPE\", color_map=color_map, opacity=0.5) <p></p> <p>Display a legend for the data.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.Legend(title=\"Wetland Type\", legend_dict=color_map)\n</pre> leafmap.Legend(title=\"Wetland Type\", legend_dict=color_map) <p></p> In\u00a0[\u00a0]: Copied! <pre>import leafmap.foliumap as leafmap\n</pre> import leafmap.foliumap as leafmap <p>Check the metadata of the PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://storage.googleapis.com/ahp-research/overture/pmtiles/overture.pmtiles\"\nmetadata = leafmap.pmtiles_metadata(url)\nprint(f\"layer names: {metadata['layer_names']}\")\nprint(f\"bounds: {metadata['bounds']}\")\n</pre> url = \"https://storage.googleapis.com/ahp-research/overture/pmtiles/overture.pmtiles\" metadata = leafmap.pmtiles_metadata(url) print(f\"layer names: {metadata['layer_names']}\") print(f\"bounds: {metadata['bounds']}\") <p>Visualize the PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_basemap(\"CartoDB.DarkMatter\")\n\nstyle = {\n    \"version\": 8,\n    \"sources\": {\n        \"example_source\": {\n            \"type\": \"vector\",\n            \"url\": \"pmtiles://\" + url,\n            \"attribution\": \"PMTiles\",\n        }\n    },\n    \"layers\": [\n        {\n            \"id\": \"admins\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"admins\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#BDD3C7\", \"fill-opacity\": 0.1},\n        },\n        {\n            \"id\": \"buildings\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"buildings\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#FFFFB3\", \"fill-opacity\": 0.5},\n        },\n        {\n            \"id\": \"places\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"places\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#BEBADA\", \"fill-opacity\": 0.5},\n        },\n        {\n            \"id\": \"roads\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"roads\",\n            \"type\": \"line\",\n            \"paint\": {\"line-color\": \"#FB8072\"},\n        },\n    ],\n}\n\nm.add_pmtiles(\n    url, name=\"PMTiles\", style=style, overlay=True, show=True, zoom_to_layer=True\n)\n\nlegend_dict = {\n    \"admins\": \"BDD3C7\",\n    \"buildings\": \"FFFFB3\",\n    \"places\": \"BEBADA\",\n    \"roads\": \"FB8072\",\n}\n\nm.add_legend(legend_dict=legend_dict)\nm\n</pre> m = leafmap.Map() m.add_basemap(\"CartoDB.DarkMatter\")  style = {     \"version\": 8,     \"sources\": {         \"example_source\": {             \"type\": \"vector\",             \"url\": \"pmtiles://\" + url,             \"attribution\": \"PMTiles\",         }     },     \"layers\": [         {             \"id\": \"admins\",             \"source\": \"example_source\",             \"source-layer\": \"admins\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#BDD3C7\", \"fill-opacity\": 0.1},         },         {             \"id\": \"buildings\",             \"source\": \"example_source\",             \"source-layer\": \"buildings\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#FFFFB3\", \"fill-opacity\": 0.5},         },         {             \"id\": \"places\",             \"source\": \"example_source\",             \"source-layer\": \"places\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#BEBADA\", \"fill-opacity\": 0.5},         },         {             \"id\": \"roads\",             \"source\": \"example_source\",             \"source-layer\": \"roads\",             \"type\": \"line\",             \"paint\": {\"line-color\": \"#FB8072\"},         },     ], }  m.add_pmtiles(     url, name=\"PMTiles\", style=style, overlay=True, show=True, zoom_to_layer=True )  legend_dict = {     \"admins\": \"BDD3C7\",     \"buildings\": \"FFFFB3\",     \"places\": \"BEBADA\",     \"roads\": \"FB8072\", }  m.add_legend(legend_dict=legend_dict) m <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://data.source.coop/vida/google-microsoft-open-buildings/pmtiles/go_ms_building_footprints.pmtiles\"\nmetadata = leafmap.pmtiles_metadata(url)\nprint(f\"layer names: {metadata['layer_names']}\")\nprint(f\"bounds: {metadata['bounds']}\")\n</pre> url = \"https://data.source.coop/vida/google-microsoft-open-buildings/pmtiles/go_ms_building_footprints.pmtiles\" metadata = leafmap.pmtiles_metadata(url) print(f\"layer names: {metadata['layer_names']}\") print(f\"bounds: {metadata['bounds']}\") <p>Visualize the PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[20, 0], zoom=2)\nm.add_basemap(\"CartoDB.DarkMatter\")\nm.add_basemap(\"Esri.WorldImagery\", show=False)\n\nstyle = {\n    \"version\": 8,\n    \"sources\": {\n        \"example_source\": {\n            \"type\": \"vector\",\n            \"url\": \"pmtiles://\" + url,\n            \"attribution\": \"PMTiles\",\n        }\n    },\n    \"layers\": [\n        {\n            \"id\": \"buildings\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"building_footprints\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},\n        },\n    ],\n}\n\nm.add_pmtiles(\n    url, name=\"Buildings\", style=style, overlay=True, show=True, zoom_to_layer=False\n)\n\nm\n</pre> m = leafmap.Map(center=[20, 0], zoom=2) m.add_basemap(\"CartoDB.DarkMatter\") m.add_basemap(\"Esri.WorldImagery\", show=False)  style = {     \"version\": 8,     \"sources\": {         \"example_source\": {             \"type\": \"vector\",             \"url\": \"pmtiles://\" + url,             \"attribution\": \"PMTiles\",         }     },     \"layers\": [         {             \"id\": \"buildings\",             \"source\": \"example_source\",             \"source-layer\": \"building_footprints\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},         },     ], }  m.add_pmtiles(     url, name=\"Buildings\", style=style, overlay=True, show=True, zoom_to_layer=False )  m <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://raw.githubusercontent.com/opengeos/open-data/main/datasets/libya/Derna_buildings.geojson\"\nleafmap.download_file(url, \"buildings.geojson\")\n</pre> url = \"https://raw.githubusercontent.com/opengeos/open-data/main/datasets/libya/Derna_buildings.geojson\" leafmap.download_file(url, \"buildings.geojson\") <p>Convert vector to PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>pmtiles = \"buildings.pmtiles\"\nleafmap.geojson_to_pmtiles(\n    \"buildings.geojson\", pmtiles, layer_name=\"buildings\", overwrite=True, quiet=True\n)\n</pre> pmtiles = \"buildings.pmtiles\" leafmap.geojson_to_pmtiles(     \"buildings.geojson\", pmtiles, layer_name=\"buildings\", overwrite=True, quiet=True ) <p>Start a HTTP Sever</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.start_server(port=8000)\n</pre> leafmap.start_server(port=8000) In\u00a0[\u00a0]: Copied! <pre>url = f\"http://127.0.0.1:8000/{pmtiles}\"\nleafmap.pmtiles_metadata(url)\n</pre> url = f\"http://127.0.0.1:8000/{pmtiles}\" leafmap.pmtiles_metadata(url) <p>Display the PMTiles on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\n\nstyle = {\n    \"version\": 8,\n    \"sources\": {\n        \"example_source\": {\n            \"type\": \"vector\",\n            \"url\": \"pmtiles://\" + url,\n            \"attribution\": \"PMTiles\",\n        }\n    },\n    \"layers\": [\n        {\n            \"id\": \"buildings\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"buildings\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},\n        },\n    ],\n}\n\nm.add_pmtiles(url, name=\"Buildings\", show=True, zoom_to_layer=True, style=style)\nm\n</pre> m = leafmap.Map()  style = {     \"version\": 8,     \"sources\": {         \"example_source\": {             \"type\": \"vector\",             \"url\": \"pmtiles://\" + url,             \"attribution\": \"PMTiles\",         }     },     \"layers\": [         {             \"id\": \"buildings\",             \"source\": \"example_source\",             \"source-layer\": \"buildings\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},         },     ], }  m.add_pmtiles(url, name=\"Buildings\", show=True, zoom_to_layer=True, style=style) m <p></p>"},{"location":"examples/dataviz/vector_viz/#visualizing-vector-data-with-leafmap","title":"Visualizing Vector Data with Leafmap\u00b6","text":""},{"location":"examples/dataviz/vector_viz/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install leafmap if needed.</p>"},{"location":"examples/dataviz/vector_viz/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/dataviz/vector_viz/#visualize-vector-data","title":"Visualize vector data\u00b6","text":"<p>You can visualize vector data using the <code>add_vector</code> function. It supports common vector data formats, including GeoJSON, Shapefile, GeoPackage, and any other formats supported by geopandas.</p>"},{"location":"examples/dataviz/vector_viz/#choropleth-map","title":"Choropleth map\u00b6","text":"<p>You can create a choropleth map using the <code>add_data</code> function. It supports GeoJSON, Shapefile, GeoPackage, and any other formats supported by geopandas.</p>"},{"location":"examples/dataviz/vector_viz/#geoparquet","title":"GeoParquet\u00b6","text":"<p>Visualize GeoParquet data with leafmap and lonboard.</p>"},{"location":"examples/dataviz/vector_viz/#pmtiles","title":"PMTiles\u00b6","text":"<p>PMTiles is a single-file archive format for tiled data. A PMTiles archive can be hosted on a commodity storage platform such as S3, and enables low-cost, zero-maintenance map applications that are \"serverless\" - free of a custom tile backend or third party provider.</p>"},{"location":"examples/dataviz/vector_viz/#remote-pmtiles","title":"Remote PMTiles\u00b6","text":"<p>Leafmap can visualize PMTiles hosted locally or remotely.</p>"},{"location":"examples/dataviz/vector_viz/#overture-data","title":"Overture data\u00b6","text":""},{"location":"examples/dataviz/vector_viz/#source-cooperative","title":"Source Cooperative\u00b6","text":"<p>Visualize the Google-Microsoft Open Buildings data hosted on Source Cooperative.</p> <p>Check the metadata of the PMTiles.</p>"},{"location":"examples/dataviz/vector_viz/#local-pmtiles","title":"Local PMTiles\u00b6","text":"<p>tippecanoe is required to convert vector data to pmtiles. Install it with <code>conda install -c conda-forge tippecanoe</code>.</p> <p>Download building footprints of Derna, Libya.</p>"},{"location":"examples/rastervision/semantic_segmentation/","title":"Semantic segmentation","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport torch\nfrom matplotlib import pyplot as plt\n\nfrom rastervision.core.data import ClassConfig, SemanticSegmentationLabels\n\nimport albumentations as A\n\nfrom rastervision.pytorch_learner import (\n    SemanticSegmentationRandomWindowGeoDataset,\n    SemanticSegmentationSlidingWindowGeoDataset,\n    SemanticSegmentationVisualizer,\n    SemanticSegmentationGeoDataConfig,\n    SemanticSegmentationLearnerConfig,\n    SolverConfig,\n    SemanticSegmentationLearner,\n)\n</pre> import os import torch from matplotlib import pyplot as plt  from rastervision.core.data import ClassConfig, SemanticSegmentationLabels  import albumentations as A  from rastervision.pytorch_learner import (     SemanticSegmentationRandomWindowGeoDataset,     SemanticSegmentationSlidingWindowGeoDataset,     SemanticSegmentationVisualizer,     SemanticSegmentationGeoDataConfig,     SemanticSegmentationLearnerConfig,     SolverConfig,     SemanticSegmentationLearner, ) In\u00a0[\u00a0]: Copied! <pre>os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\"\n</pre> os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\" In\u00a0[\u00a0]: Copied! <pre>class_config = ClassConfig(\n    names=[\"background\", \"building\"],\n    colors=[\"lightgray\", \"darkred\"],\n    null_class=\"background\",\n)\n\nviz = SemanticSegmentationVisualizer(\n    class_names=class_config.names, class_colors=class_config.colors\n)\n</pre> class_config = ClassConfig(     names=[\"background\", \"building\"],     colors=[\"lightgray\", \"darkred\"],     null_class=\"background\", )  viz = SemanticSegmentationVisualizer(     class_names=class_config.names, class_colors=class_config.colors ) In\u00a0[\u00a0]: Copied! <pre>train_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/images/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13.tif\"\ntrain_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/labels/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13_Buildings.geojson\"\n\nval_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\"\nval_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\"\n</pre> train_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/images/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13.tif\" train_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/labels/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13_Buildings.geojson\"  val_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\" val_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\" In\u00a0[\u00a0]: Copied! <pre>pred_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\"\npred_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\"\n</pre> pred_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\" pred_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\" In\u00a0[\u00a0]: Copied! <pre>data_augmentation_transform = A.Compose(\n    [\n        A.Flip(),\n        A.ShiftScaleRotate(),\n        A.OneOf(\n            [\n                A.HueSaturationValue(hue_shift_limit=10),\n                A.RGBShift(),\n                A.ToGray(),\n                A.ToSepia(),\n                A.RandomBrightness(),\n                A.RandomGamma(),\n            ]\n        ),\n        A.CoarseDropout(max_height=32, max_width=32, max_holes=5),\n    ]\n)\n</pre> data_augmentation_transform = A.Compose(     [         A.Flip(),         A.ShiftScaleRotate(),         A.OneOf(             [                 A.HueSaturationValue(hue_shift_limit=10),                 A.RGBShift(),                 A.ToGray(),                 A.ToSepia(),                 A.RandomBrightness(),                 A.RandomGamma(),             ]         ),         A.CoarseDropout(max_height=32, max_width=32, max_holes=5),     ] ) In\u00a0[\u00a0]: Copied! <pre>train_ds = SemanticSegmentationRandomWindowGeoDataset.from_uris(\n    class_config=class_config,\n    image_uri=train_image_uri,\n    label_vector_uri=train_label_uri,\n    label_vector_default_class_id=class_config.get_class_id(\"building\"),\n    size_lims=(150, 200),\n    out_size=256,\n    max_windows=400,\n    transform=data_augmentation_transform,\n)\n\nlen(train_ds)\n</pre> train_ds = SemanticSegmentationRandomWindowGeoDataset.from_uris(     class_config=class_config,     image_uri=train_image_uri,     label_vector_uri=train_label_uri,     label_vector_default_class_id=class_config.get_class_id(\"building\"),     size_lims=(150, 200),     out_size=256,     max_windows=400,     transform=data_augmentation_transform, )  len(train_ds) In\u00a0[\u00a0]: Copied! <pre>x, y = viz.get_batch(train_ds, 4)\nviz.plot_batch(x, y, show=True)\n</pre> x, y = viz.get_batch(train_ds, 4) viz.plot_batch(x, y, show=True) In\u00a0[\u00a0]: Copied! <pre>val_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(\n    class_config=class_config,\n    image_uri=val_image_uri,\n    label_vector_uri=val_label_uri,\n    label_vector_default_class_id=class_config.get_class_id(\"building\"),\n    size=200,\n    stride=100,\n    transform=A.Resize(256, 256),\n)\nlen(val_ds)\n</pre> val_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(     class_config=class_config,     image_uri=val_image_uri,     label_vector_uri=val_label_uri,     label_vector_default_class_id=class_config.get_class_id(\"building\"),     size=200,     stride=100,     transform=A.Resize(256, 256), ) len(val_ds) In\u00a0[\u00a0]: Copied! <pre>x, y = viz.get_batch(val_ds, 4)\nviz.plot_batch(x, y, show=True)\n</pre> x, y = viz.get_batch(val_ds, 4) viz.plot_batch(x, y, show=True) In\u00a0[\u00a0]: Copied! <pre>pred_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(\n    class_config=class_config,\n    image_uri=pred_image_uri,\n    size=200,\n    stride=100,\n    transform=A.Resize(256, 256),\n)\nlen(pred_ds)\n</pre> pred_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(     class_config=class_config,     image_uri=pred_image_uri,     size=200,     stride=100,     transform=A.Resize(256, 256), ) len(pred_ds) In\u00a0[\u00a0]: Copied! <pre>model = torch.hub.load(\n    \"AdeelH/pytorch-fpn:0.3\",\n    \"make_fpn_resnet\",\n    name=\"resnet18\",\n    fpn_type=\"panoptic\",\n    num_classes=len(class_config),\n    fpn_channels=128,\n    in_channels=3,\n    out_size=(256, 256),\n    pretrained=True,\n)\n</pre> model = torch.hub.load(     \"AdeelH/pytorch-fpn:0.3\",     \"make_fpn_resnet\",     name=\"resnet18\",     fpn_type=\"panoptic\",     num_classes=len(class_config),     fpn_channels=128,     in_channels=3,     out_size=(256, 256),     pretrained=True, ) In\u00a0[\u00a0]: Copied! <pre>data_cfg = SemanticSegmentationGeoDataConfig(\n    class_names=class_config.names,\n    class_colors=class_config.colors,\n    num_workers=0,  # increase to use multi-processing\n)\n</pre> data_cfg = SemanticSegmentationGeoDataConfig(     class_names=class_config.names,     class_colors=class_config.colors,     num_workers=0,  # increase to use multi-processing ) In\u00a0[\u00a0]: Copied! <pre>solver_cfg = SolverConfig(batch_sz=8, lr=3e-2, class_loss_weights=[1.0, 10.0])\n</pre> solver_cfg = SolverConfig(batch_sz=8, lr=3e-2, class_loss_weights=[1.0, 10.0]) In\u00a0[\u00a0]: Copied! <pre>learner_cfg = SemanticSegmentationLearnerConfig(data=data_cfg, solver=solver_cfg)\n</pre> learner_cfg = SemanticSegmentationLearnerConfig(data=data_cfg, solver=solver_cfg) In\u00a0[\u00a0]: Copied! <pre>learner = SemanticSegmentationLearner(\n    cfg=learner_cfg,\n    output_dir=\"./train-demo/\",\n    model=model,\n    train_ds=train_ds,\n    valid_ds=val_ds,\n)\n</pre> learner = SemanticSegmentationLearner(     cfg=learner_cfg,     output_dir=\"./train-demo/\",     model=model,     train_ds=train_ds,     valid_ds=val_ds, ) In\u00a0[\u00a0]: Copied! <pre>learner.log_data_stats()\n</pre> learner.log_data_stats() In\u00a0[\u00a0]: Copied! <pre>%load_ext tensorboard\n</pre> %load_ext tensorboard In\u00a0[\u00a0]: Copied! <pre>%tensorboard --bind_all --logdir \"./train-demo/tb-logs\" --reload_interval 10\n</pre> %tensorboard --bind_all --logdir \"./train-demo/tb-logs\" --reload_interval 10 In\u00a0[\u00a0]: Copied! <pre>learner.train(epochs=3)\n</pre> learner.train(epochs=3) In\u00a0[\u00a0]: Copied! <pre>learner.train(epochs=1)\n</pre> learner.train(epochs=1) In\u00a0[\u00a0]: Copied! <pre>learner.plot_predictions(split=\"valid\", show=True)\n</pre> learner.plot_predictions(split=\"valid\", show=True) In\u00a0[\u00a0]: Copied! <pre>learner.save_model_bundle()\n</pre> learner.save_model_bundle() In\u00a0[\u00a0]: Copied! <pre>learner = SemanticSegmentationLearner.from_model_bundle(\n    model_bundle_uri=\"./train-demo/model-bundle.zip\",\n    output_dir=\"./train-demo/\",\n    model=model,\n)\n</pre> learner = SemanticSegmentationLearner.from_model_bundle(     model_bundle_uri=\"./train-demo/model-bundle.zip\",     output_dir=\"./train-demo/\",     model=model, ) In\u00a0[\u00a0]: Copied! <pre>learner = SemanticSegmentationLearner.from_model_bundle(\n    model_bundle_uri=\"./train-demo/model-bundle.zip\",\n    output_dir=\"./train-demo/\",\n    model=model,\n    train_ds=train_ds,\n    valid_ds=val_ds,\n    training=True,\n)\n</pre> learner = SemanticSegmentationLearner.from_model_bundle(     model_bundle_uri=\"./train-demo/model-bundle.zip\",     output_dir=\"./train-demo/\",     model=model,     train_ds=train_ds,     valid_ds=val_ds,     training=True, ) In\u00a0[\u00a0]: Copied! <pre>learner.train(epochs=1)\n</pre> learner.train(epochs=1) In\u00a0[\u00a0]: Copied! <pre>learner.plot_predictions(split=\"valid\", show=True)\n</pre> learner.plot_predictions(split=\"valid\", show=True) In\u00a0[\u00a0]: Copied! <pre>predictions = learner.predict_dataset(\n    pred_ds,\n    raw_out=True,\n    numpy_out=True,\n    predict_kw=dict(out_shape=(325, 325)),\n    progress_bar=True,\n)\n</pre> predictions = learner.predict_dataset(     pred_ds,     raw_out=True,     numpy_out=True,     predict_kw=dict(out_shape=(325, 325)),     progress_bar=True, ) In\u00a0[\u00a0]: Copied! <pre>pred_labels = SemanticSegmentationLabels.from_predictions(\n    pred_ds.windows,\n    predictions,\n    smooth=True,\n    extent=pred_ds.scene.extent,\n    num_classes=len(class_config),\n)\n</pre> pred_labels = SemanticSegmentationLabels.from_predictions(     pred_ds.windows,     predictions,     smooth=True,     extent=pred_ds.scene.extent,     num_classes=len(class_config), ) In\u00a0[\u00a0]: Copied! <pre>scores = pred_labels.get_score_arr(pred_labels.extent)\n</pre> scores = pred_labels.get_score_arr(pred_labels.extent) In\u00a0[\u00a0]: Copied! <pre>pred_labels.save(\n    uri=f\"predict\",\n    crs_transformer=pred_ds.scene.raster_source.crs_transformer,\n    class_config=class_config,\n)\n</pre> pred_labels.save(     uri=f\"predict\",     crs_transformer=pred_ds.scene.raster_source.crs_transformer,     class_config=class_config, )"},{"location":"examples/samgeo/arcgis/","title":"Arcgis","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo\n\n%matplotlib inline\n</pre> import os import leafmap from samgeo import SamGeo  %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>workspace = os.path.dirname(arcpy.env.workspace)\nos.chdir(workspace)\narcpy.env.overwriteOutput = True\n</pre> workspace = os.path.dirname(arcpy.env.workspace) os.chdir(workspace) arcpy.env.overwriteOutput = True In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/buildings.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/buildings.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/agriculture.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/agriculture.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/water.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/water.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>image = \"agriculture.tif\"\n</pre> image = \"agriculture.tif\" <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p> In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"ag_masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"ag_masks.tif\", foreground=True, unique=True) <p>If you run into GPU memory errors, uncomment the following code block and run it to empty cuda cache then rerun the code block above.</p> In\u00a0[\u00a0]: Copied! <pre># sam.clear_cuda_cache()\n</pre> # sam.clear_cuda_cache() <p>Show the segmentation result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"ag_annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"ag_annotations.tif\") <p>Add layers to ArcGIS Pro.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.arc_active_map()\n</pre> m = leafmap.arc_active_map() In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"agriculture.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"agriculture.tif\")) In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"ag_annotations.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"ag_annotations.tif\")) <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>in_raster = os.path.join(workspace, \"ag_masks.tif\")\nout_shp = os.path.join(workspace, \"ag_masks.shp\")\n</pre> in_raster = os.path.join(workspace, \"ag_masks.tif\") out_shp = os.path.join(workspace, \"ag_masks.shp\") In\u00a0[\u00a0]: Copied! <pre>arcpy.conversion.RasterToPolygon(in_raster, out_shp)\n</pre> arcpy.conversion.RasterToPolygon(in_raster, out_shp) In\u00a0[\u00a0]: Copied! <pre>image = \"water.tif\"\n</pre> image = \"water.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"water_masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"water_masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre># sam.clear_cuda_cache()\n</pre> # sam.clear_cuda_cache() In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"water_annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"water_annotations.tif\") In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"water.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"water.tif\")) In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"water_annotations.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"water_annotations.tif\")) In\u00a0[\u00a0]: Copied! <pre>in_raster = os.path.join(workspace, \"water_masks.tif\")\nout_shp = os.path.join(workspace, \"water_masks.shp\")\n</pre> in_raster = os.path.join(workspace, \"water_masks.tif\") out_shp = os.path.join(workspace, \"water_masks.shp\") In\u00a0[\u00a0]: Copied! <pre>arcpy.conversion.RasterToPolygon(in_raster, out_shp)\n</pre> arcpy.conversion.RasterToPolygon(in_raster, out_shp) In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(\"agriculture.tif\", output=\"ag_masks2.tif\", foreground=True)\n</pre> sam.generate(\"agriculture.tif\", output=\"ag_masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=0.5, output=\"ag_annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=0.5, output=\"ag_annotations2.tif\")"},{"location":"examples/samgeo/arcgis/#using-the-segment-geospatial-python-package-with-arcgis-pro","title":"Using the Segment-Geospatial Python Package with ArcGIS Pro\u00b6","text":"<p>The notebook shows step-by-step instructions for using the Segment Anything Model (SAM) with ArcGIS Pro. Check out the YouTube tutorial here and the Resources for Unlocking the Power of Deep Learning Applications Using ArcGIS. Credit goes to Esri.</p> <p></p>"},{"location":"examples/samgeo/arcgis/#installation","title":"Installation\u00b6","text":"<ol> <li><p>Open Windows Registry Editor (<code>regedit.exe</code>) and navigate to <code>Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem</code>. Change the value of <code>LongPathsEnabled</code> to <code>1</code>. See this screenshot. This is a known issue with the deep learning libraries for ArcGIS Pro 3.1. A future release might fix this issue.</p> </li> <li><p>Navigate to the Start Menu -&gt; All apps -&gt; ArcGIS folder, then open the Python Command Prompt.</p> </li> <li><p>Create a new conda environment and install mamba and Python 3.9.x from the Esri Anaconda channel. Mamba is a drop-in replacement for conda that is mach faster for installing Python packages and their dependencies.</p> <p><code>conda create conda-forge::mamba esri::python --name samgeo</code></p> </li> <li><p>Activate the new conda environment.</p> <p><code>conda activate samgeo</code></p> </li> <li><p>Install arcpy, deep-learning-essentials, segment-geospatial, and other dependencies (~4GB download).</p> <p><code>mamba install arcpy deep-learning-essentials leafmap localtileserver segment-geospatial -c esri -c conda-forge</code></p> </li> <li><p>Activate the new environment in ArcGIS Pro.</p> <p><code>proswap samgeo</code></p> </li> <li><p>Close the Python Command Prompt and open ArcGIS Pro.</p> </li> <li><p>Download this notebook and run it in ArcGIS Pro.</p> </li> </ol>"},{"location":"examples/samgeo/arcgis/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/samgeo/arcgis/#download-sample-data","title":"Download sample data\u00b6","text":"<p>In this example, we will use the high-resolution aerial imagery from the USDA National Agricultural Imagery Program (NAIP). You can download NAIP imagery using the USDA Data Gateway or the USDA NCRS Box Drive. I have downloaded some NAIP imagery and clipped them to a smaller area, which are available here.</p>"},{"location":"examples/samgeo/arcgis/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/arcgis/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Specify the file path to the image we downloaded earlier.</p>"},{"location":"examples/samgeo/arcgis/#segment-waterbodies","title":"Segment waterbodies\u00b6","text":""},{"location":"examples/samgeo/arcgis/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/samgeo/automatic_mask_generator/","title":"Automatic mask generator","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, show_image, download_file, overlay_images, tms_to_geotiff\n</pre> import os import leafmap from samgeo import SamGeo, show_image, download_file, overlay_images, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.2659, 37.8682, -122.2521, 37.8741]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.2659, 37.8682, -122.2521, 37.8741] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\") m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\")\n</pre> overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\") <p></p>"},{"location":"examples/samgeo/automatic_mask_generator/#automatically-generating-object-masks-with-sam","title":"Automatically generating object masks with SAM\u00b6","text":"<p>This notebook shows how to segment objects from an image using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p> <p>The notebook is adapted from segment-anything/notebooks/automatic_mask_generator_example.ipynb, but I have made it much easier to save the segmentation results and visualize them.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/","title":"Automatic mask generator hq","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo.hq_sam import (\n    SamGeo,\n    show_image,\n    download_file,\n    overlay_images,\n    tms_to_geotiff,\n)\n</pre> import os import leafmap from samgeo.hq_sam import (     SamGeo,     show_image,     download_file,     overlay_images,     tms_to_geotiff, ) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.2659, 37.8682, -122.2521, 37.8741]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.2659, 37.8682, -122.2521, 37.8741] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\") m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\")\n</pre> overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\") <p></p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#automatically-generating-object-masks-with-hq-sam","title":"Automatically generating object masks with HQ-SAM\u00b6","text":"<p>This notebook shows how to segment objects from an image using the High-Quality Segment Anything Model (HQ-SAM) with a few lines of code.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator_hq/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator_hq/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/samgeo/box_prompts/","title":"Box prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo import SamGeo\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo import SamGeo In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) <p>Display the map. Use the drawing tools to draw some rectangles around the features you want to extract, such as trees, buildings.</p> In\u00a0[\u00a0]: Copied! <pre>m\n</pre> m In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-51.2546, -22.1771, -51.2541, -22.1767],\n        [-51.2538, -22.1764, -51.2535, -22.1761],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-51.2546, -22.1771, -51.2541, -22.1767],         [-51.2538, -22.1764, -51.2535, -22.1761],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m In\u00a0[\u00a0]: Copied! <pre>url = \"https://opengeos.github.io/data/sam/tree_boxes.geojson\"\ngeojson = \"tree_boxes.geojson\"\nleafmap.download_file(url, geojson)\n</pre> url = \"https://opengeos.github.io/data/sam/tree_boxes.geojson\" geojson = \"tree_boxes.geojson\" leafmap.download_file(url, geojson) <p>Display the vector data on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"Image.tif\", layer_name=\"image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"Image.tif\", layer_name=\"image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\") m In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=\"mask2.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=\"mask2.tif\", dtype=\"uint8\") <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask2.tif\", cmap=\"Greens\", nodata=0, opacity=0.5, layer_name=\"Tree masks\")\nm\n</pre> m.add_raster(\"mask2.tif\", cmap=\"Greens\", nodata=0, opacity=0.5, layer_name=\"Tree masks\") m <p></p>"},{"location":"examples/samgeo/box_prompts/#segmenting-remote-sensing-imagery-with-box-prompts","title":"Segmenting remote sensing imagery with box prompts\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/box_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/box_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/box_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/box_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p> <p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p> <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p>"},{"location":"examples/samgeo/box_prompts/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"examples/samgeo/box_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"examples/samgeo/box_prompts/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"examples/samgeo/box_prompts/#use-an-existing-vector-file-as-box-prompts","title":"Use an existing vector file as box prompts\u00b6","text":"<p>Alternatively, you can specify a file path to a vector file. Let's download a sample vector file from GitHub.</p>"},{"location":"examples/samgeo/box_prompts/#segment-image-with-box-prompts","title":"Segment image with box prompts\u00b6","text":"<p>Segment the image using the specified file path to the vector mask.</p>"},{"location":"examples/samgeo/fast_sam/","title":"Fast sam","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial segment-anything-fast\n</pre> # %pip install segment-geospatial segment-anything-fast In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.fast_sam import SamGeo\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.fast_sam import SamGeo In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>from samgeo.fast_sam import SamGeo\n\nsam = SamGeo(model=\"FastSAM-x.pt\")\n</pre> from samgeo.fast_sam import SamGeo  sam = SamGeo(model=\"FastSAM-x.pt\") <p>Set the image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(\"Image.tif\")\n</pre> sam.set_image(\"Image.tif\") <p>Segment the image with <code>everything_prompt</code>. You can also try <code>point_prompt</code>, <code>box_prompt</code>, or <code>text_prompt</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam.everything_prompt(output=\"mask.tif\")\n</pre> sam.everything_prompt(output=\"mask.tif\") <p>Show the annotated image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\"mask.png\")\n</pre> sam.show_anns(\"mask.png\") <p></p> <p>Convert the segmentation results from GeoTIFF to vector.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"mask.tif\", \"mask.geojson\")\n</pre> sam.raster_to_vector(\"mask.tif\", \"mask.geojson\") <p>Show the segmentation results on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", opacity=0.5, layer_name=\"Mask\")\nm.add_vector(\"mask.geojson\", layer_name=\"Mask Vector\")\nm\n</pre> m.add_raster(\"mask.tif\", opacity=0.5, layer_name=\"Mask\") m.add_vector(\"mask.geojson\", layer_name=\"Mask Vector\") m <p></p>"},{"location":"examples/samgeo/fast_sam/#segmenting-remote-sensing-imagery-with-fastsam","title":"Segmenting remote sensing imagery with FastSAM\u00b6","text":"<p>FastSAM: https://github.com/CASIA-IVA-Lab/FastSAM</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/fast_sam/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/fast_sam/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/fast_sam/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/fast_sam/#initialize-samgeo-class","title":"Initialize SamGeo class\u00b6","text":"<p>The initialization of the SamGeo class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/input_prompts/","title":"Input prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, tms_to_geotiff\n</pre> import os import leafmap from samgeo import SamGeo, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p>"},{"location":"examples/samgeo/input_prompts/#generating-object-masks-from-input-prompts-with-sam","title":"Generating object masks from input prompts with SAM\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p> <p>The notebook is adapted from segment-anything/notebooks/predictor_example.ipynb, but I have made it much easier to save the segmentation results and visualize them.</p>"},{"location":"examples/samgeo/input_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/input_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/input_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/input_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/input_prompts/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/samgeo/input_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/samgeo/input_prompts_hq/","title":"Input prompts hq","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo.hq_sam import SamGeo, tms_to_geotiff\n</pre> import os import leafmap from samgeo.hq_sam import SamGeo, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p>"},{"location":"examples/samgeo/input_prompts_hq/#generating-object-masks-from-input-prompts-with-hq-sam","title":"Generating object masks from input prompts with HQ-SAM\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the High-Quality Segment Anything Model (HQ-SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/input_prompts_hq/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/input_prompts_hq/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/input_prompts_hq/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/input_prompts_hq/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/input_prompts_hq/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/samgeo/input_prompts_hq/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/samgeo/maxar_open_data/","title":"Maxar open data","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, raster_to_vector, overlay_images\n</pre> import os import leafmap from samgeo import SamGeo, raster_to_vector, overlay_images In\u00a0[\u00a0]: Copied! <pre>url = (\n    \"https://drive.google.com/file/d/1jIIC5hvSPeJEC0fbDhtxVWk2XV9AxsQD/view?usp=sharing\"\n)\n</pre> url = (     \"https://drive.google.com/file/d/1jIIC5hvSPeJEC0fbDhtxVWk2XV9AxsQD/view?usp=sharing\" ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(url, output=\"image.tif\")\n</pre> leafmap.download_file(url, output=\"image.tif\") In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(height=\"600px\")\nm.add_basemap(\"SATELLITE\")\nm.add_raster(\"image.tif\", layer_name=\"Image\")\nm.add_layer_manager()\nm\n</pre> m = leafmap.Map(height=\"600px\") m.add_basemap(\"SATELLITE\") m.add_raster(\"image.tif\", layer_name=\"Image\") m.add_layer_manager() m <p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p> In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 80,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 80, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(\"image.tif\", output=\"mask.tif\", foreground=True)\n</pre> sam.generate(\"image.tif\", output=\"mask.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>raster_to_vector(\"mask.tif\", output=\"mask.shp\")\n</pre> raster_to_vector(\"mask.tif\", output=\"mask.shp\") In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Display the annotations (each mask with a random color).</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotation.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotation.tif\") In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"image.tif\",\n    \"annotation.tif\",\n    label1=\"Image\",\n    label2=\"Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"image.tif\",     \"annotation.tif\",     label1=\"Image\",     label2=\"Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(\"image.tif\", \"annotation.tif\", backend=\"TkAgg\")\n</pre> overlay_images(\"image.tif\", \"annotation.tif\", backend=\"TkAgg\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", layer_name=\"Mask\", nodata=0)\nm.add_raster(\"annotation.tif\", layer_name=\"Annotation\")\nm\n</pre> m.add_raster(\"mask.tif\", layer_name=\"Mask\", nodata=0) m.add_raster(\"annotation.tif\", layer_name=\"Annotation\") m In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"mask.shp\", layer_name=\"Vector\", info_mode=None)\n</pre> m.add_vector(\"mask.shp\", layer_name=\"Vector\", info_mode=None) <p></p>"},{"location":"examples/samgeo/maxar_open_data/#segmenting-satellite-imagery-from-the-maxar-open-data-program","title":"Segmenting satellite imagery from the Maxar Open Data Program\u00b6","text":"<p>This notebook shows how to segment satellite imagery from the Maxar Open Data program for Libya floods.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/maxar_open_data/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/maxar_open_data/#download-sample-data","title":"Download sample data\u00b6","text":"<p>First, let's download a sample image of Derna, Libya from here.</p>"},{"location":"examples/samgeo/maxar_open_data/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#segment-the-image","title":"Segment the image\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#convert-raster-to-vector","title":"Convert raster to vector\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#display-the-segmentation-result","title":"Display the segmentation result\u00b6","text":"<p>First, let's show the result as a binary image.</p>"},{"location":"examples/samgeo/maxar_open_data/#compare-images-with-a-slider","title":"Compare images with a slider\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#display-images-on-an-interactive-map","title":"Display images on an interactive map.\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/","title":"Satellite predictor","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeoPredictor, tms_to_geotiff, get_basemaps\nfrom segment_anything import sam_model_registry\n</pre> import os import leafmap from samgeo import SamGeoPredictor, tms_to_geotiff, get_basemaps from segment_anything import sam_model_registry In\u00a0[\u00a0]: Copied! <pre>zoom = 16\nm = leafmap.Map(center=[45, -123], zoom=zoom)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> zoom = 16 m = leafmap.Map(center=[45, -123], zoom=zoom) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-123.0127, 44.9957, -122.9874, 45.0045]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-123.0127, 44.9957, -122.9874, 45.0045] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\n# image = '/path/to/your/own/image.tif'\n</pre> image = \"satellite.tif\" # image = '/path/to/your/own/image.tif' <p>Besides the <code>satellite</code> basemap, you can use any of the following basemaps returned by the <code>get_basemaps()</code> function:</p> In\u00a0[\u00a0]: Copied! <pre># get_basemaps().keys()\n</pre> # get_basemaps().keys() <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>tms_to_geotiff(\n    output=image, bbox=bbox, zoom=zoom + 1, source=\"Satellite\", overwrite=True\n)\n</pre> tms_to_geotiff(     output=image, bbox=bbox, zoom=zoom + 1, source=\"Satellite\", overwrite=True ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.add_raster(image, layer_name=\"Image\") m <p>Use the draw tools to draw a rectangle from which to subset segmentations on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    clip_box = m.user_roi_bounds()\nelse:\n    clip_box = [-123.0064, 44.9988, -123.0005, 45.0025]\n</pre> if m.user_roi_bounds() is not None:     clip_box = m.user_roi_bounds() else:     clip_box = [-123.0064, 44.9988, -123.0005, 45.0025] In\u00a0[\u00a0]: Copied! <pre>clip_box\n</pre> clip_box In\u00a0[\u00a0]: Copied! <pre>out_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\ncheckpoint = os.path.join(out_dir, \"sam_vit_h_4b8939.pth\")\n</pre> out_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\") checkpoint = os.path.join(out_dir, \"sam_vit_h_4b8939.pth\") In\u00a0[\u00a0]: Copied! <pre>import cv2\n\nimg_arr = cv2.imread(image)\n\nmodel_type = \"vit_h\"\n\nsam = sam_model_registry[model_type](checkpoint=checkpoint)\n\npredictor = SamGeoPredictor(sam)\n\npredictor.set_image(img_arr)\n\nmasks, _, _ = predictor.predict(src_fp=image, geo_box=clip_box)\n</pre> import cv2  img_arr = cv2.imread(image)  model_type = \"vit_h\"  sam = sam_model_registry[model_type](checkpoint=checkpoint)  predictor = SamGeoPredictor(sam)  predictor.set_image(img_arr)  masks, _, _ = predictor.predict(src_fp=image, geo_box=clip_box) In\u00a0[\u00a0]: Copied! <pre>masks_img = \"preds.tif\"\npredictor.masks_to_geotiff(image, masks_img, masks.astype(\"uint8\"))\n</pre> masks_img = \"preds.tif\" predictor.masks_to_geotiff(image, masks_img, masks.astype(\"uint8\")) In\u00a0[\u00a0]: Copied! <pre>vector = \"feats.geojson\"\ngdf = predictor.geotiff_to_geojson(masks_img, vector, bidx=1)\ngdf.plot()\n</pre> vector = \"feats.geojson\" gdf = predictor.geotiff_to_geojson(masks_img, vector, bidx=1) gdf.plot() In\u00a0[\u00a0]: Copied! <pre>style = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(vector, layer_name=\"Vector\", style=style)\nm\n</pre> style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(vector, layer_name=\"Vector\", style=style) m"},{"location":"examples/samgeo/satellite-predictor/#segment-anything-model-for-geospatial-data","title":"Segment Anything Model for Geospatial Data\u00b6","text":"<p>This notebook shows how to use segment satellite imagery using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/satellite-predictor/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/satellite-predictor/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"examples/samgeo/satellite-predictor/#initialize-samgeopredictor-class","title":"Initialize SamGeoPredictor class\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/samgeo/satellite/","title":"Satellite","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, tms_to_geotiff, get_basemaps\n</pre> import os import leafmap from samgeo import SamGeo, tms_to_geotiff, get_basemaps In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[29.676840, -95.369222], zoom=19)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[29.676840, -95.369222], zoom=19) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-95.3704, 29.6762, -95.368, 29.6775]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-95.3704, 29.6762, -95.368, 29.6775] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\n</pre> image = \"satellite.tif\" <p>Besides the <code>satellite</code> basemap, you can use any of the following basemaps returned by the <code>get_basemaps()</code> function:</p> In\u00a0[\u00a0]: Copied! <pre># get_basemaps().keys()\n</pre> # get_basemaps().keys() <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>tms_to_geotiff(output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True)\n</pre> tms_to_geotiff(output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False  # turn off the basemap\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False  # turn off the basemap m.add_raster(image, layer_name=\"Image\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    checkpoint=\"sam_vit_h_4b8939.pth\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     checkpoint=\"sam_vit_h_4b8939.pth\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>mask = \"segment.tif\"\nsam.generate(\n    image, mask, batch=True, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255\n)\n</pre> mask = \"segment.tif\" sam.generate(     image, mask, batch=True, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255 ) In\u00a0[\u00a0]: Copied! <pre>vector = \"segment.gpkg\"\nsam.tiff_to_gpkg(mask, vector, simplify_tolerance=None)\n</pre> vector = \"segment.gpkg\" sam.tiff_to_gpkg(mask, vector, simplify_tolerance=None) <p>You can also save the segmentation results as any vector data format supported by GeoPandas.</p> In\u00a0[\u00a0]: Copied! <pre>shapefile = \"segment.shp\"\nsam.tiff_to_vector(mask, shapefile)\n</pre> shapefile = \"segment.shp\" sam.tiff_to_vector(mask, shapefile) In\u00a0[\u00a0]: Copied! <pre>style = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(vector, layer_name=\"Vector\", style=style)\nm\n</pre> style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(vector, layer_name=\"Vector\", style=style) m <p></p>"},{"location":"examples/samgeo/satellite/#segment-anything-model-for-geospatial-data","title":"Segment Anything Model for Geospatial Data\u00b6","text":"<p>This notebook shows how to use segment satellite imagery using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/satellite/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/satellite/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/samgeo/satellite/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/satellite/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"examples/samgeo/satellite/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/samgeo/satellite/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Set <code>batch=True</code> to segment the image in batches. This is useful for large images that cannot fit in memory.</p>"},{"location":"examples/samgeo/satellite/#polygonize-the-raster-data","title":"Polygonize the raster data\u00b6","text":"<p>Save the segmentation results as a GeoPackage file.</p>"},{"location":"examples/samgeo/satellite/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/samgeo/swimming_pools/","title":"Swimming pools","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[34.040984, -118.491668], zoom=19, height=\"600px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[34.040984, -118.491668], zoom=19, height=\"600px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-118.4932, 34.0404, -118.4903, 34.0417]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-118.4932, 34.0404, -118.4903, 34.0417] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"swimming pool\"\n</pre> text_prompt = \"swimming pool\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Blues\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Swimming Pools\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Blues\",     box_color=\"red\",     title=\"Automatic Segmentation of Swimming Pools\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Blues\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Swimming Pools\",\n)\n</pre> sam.show_anns(     cmap=\"Blues\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Swimming Pools\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Swimming Pools\",\n    blend=False,\n    output=\"pools.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Swimming Pools\",     blend=False,     output=\"pools.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"pools.tif\", \"pools.shp\")\n</pre> sam.raster_to_vector(\"pools.tif\", \"pools.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"pools.tif\", layer_name=\"Pools\", palette=\"Blues\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"pools.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"pools.tif\", layer_name=\"Pools\", palette=\"Blues\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"pools.shp\", layer_name=\"Vector\", style=style) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/samgeo/swimming_pools/#mapping-swimming-pools-with-text-prompts","title":"Mapping swimming pools with text prompts\u00b6","text":"<p>This notebook shows how to map swimming pools with text prompts and the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/swimming_pools/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/swimming_pools/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/swimming_pools/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/swimming_pools/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/swimming_pools/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/samgeo/swimming_pools/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/samgeo/swimming_pools/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/samgeo/swimming_pools/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/samgeo/text_prompts/","title":"Text prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"trees.tif\", \"trees.shp\")\n</pre> sam.raster_to_vector(\"trees.tif\", \"trees.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style) m In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/samgeo/text_prompts/#segmenting-remote-sensing-imagery-with-text-prompts-and-the-segment-anything-model-sam","title":"Segmenting remote sensing imagery with text prompts and the Segment Anything Model (SAM)\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/text_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/text_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/text_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/text_prompts/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/text_prompts/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/samgeo/text_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/samgeo/text_prompts/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/samgeo/text_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/","title":"Text prompts batch","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff, split_raster\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff, split_raster from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.1278, -51.4430], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.1278, -51.4430], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.4494, -22.1307, -51.4371, -22.1244]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.4494, -22.1307, -51.4371, -22.1244] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>split_raster(image, out_dir=\"tiles\", tile_size=(1000, 1000), overlap=0)\n</pre> split_raster(image, out_dir=\"tiles\", tile_size=(1000, 1000), overlap=0) In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict_batch(\n    images=\"tiles\",\n    out_dir=\"masks\",\n    text_prompt=text_prompt,\n    box_threshold=0.24,\n    text_threshold=0.24,\n    mask_multiplier=255,\n    dtype=\"uint8\",\n    merge=True,\n    verbose=True,\n)\n</pre> sam.predict_batch(     images=\"tiles\",     out_dir=\"masks\",     text_prompt=text_prompt,     box_threshold=0.24,     text_threshold=0.24,     mask_multiplier=255,     dtype=\"uint8\",     merge=True,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"masks/merged.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm.add_layer_manager()\nm\n</pre> m.add_raster(\"masks/merged.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m.add_layer_manager() m <p></p>"},{"location":"examples/samgeo/text_prompts_batch/#batch-segmentation-with-text-prompts","title":"Batch segmentation with text prompts\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/text_prompts_batch/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/text_prompts_batch/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/text_prompts_batch/#split-the-image-into-tiles","title":"Split the image into tiles\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/text_prompts_batch/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/#segment-images","title":"Segment images\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/samgeo/text_prompts_batch/#visualize-the-results","title":"Visualize the results\u00b6","text":""}]}