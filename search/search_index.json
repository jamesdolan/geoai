{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to geoai","text":"<p>A Python package for using Artificial Intelligence (AI) with geospatial data</p> <ul> <li>Free software: MIT license</li> <li>Documentation: https://geoai.gishub.org</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Visualizing geospatial data, including vector, raster, and LiDAR data</li> <li>Segmenting remote sensing imagery with the Segment Anything Model</li> <li>Classifying remote sensing imagery with deep learning models</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-aug-11-2023","title":"v0.0.1 - Aug 11, 2023","text":"<p>Initial release</p>"},{"location":"common/","title":"common module","text":"<p>The common module contains common functions and classes used by the other modules.</p>"},{"location":"common/#geoai.common.calc_stats","title":"<code>calc_stats(dataset, divide_by=1.0)</code>","text":"<p>Calculate the statistics (mean and std) for the entire dataset.</p> <p>This function is adapted from the plot_batch() function in the torchgeo library at https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html. Credit to the torchgeo developers for the original implementation.</p> <p>Warning: This is an approximation. The correct value should take into account the mean for the whole dataset for computing individual stds.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>RasterDataset</code> <p>The dataset to calculate statistics for.</p> required <code>divide_by</code> <code>float</code> <p>The value to divide the image data by. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tuple[np.ndarray, np.ndarray]</code> <p>The mean and standard deviation for each band.</p> Source code in <code>geoai/common.py</code> <pre><code>def calc_stats(\n    dataset: RasterDataset, divide_by: float = 1.0\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate the statistics (mean and std) for the entire dataset.\n\n    This function is adapted from the plot_batch() function in the torchgeo library at\n    https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html.\n    Credit to the torchgeo developers for the original implementation.\n\n    Warning: This is an approximation. The correct value should take into account the\n    mean for the whole dataset for computing individual stds.\n\n    Args:\n        dataset (RasterDataset): The dataset to calculate statistics for.\n        divide_by (float, optional): The value to divide the image data by. Defaults to 1.0.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: The mean and standard deviation for each band.\n    \"\"\"\n    import rasterio as rio\n\n    # To avoid loading the entire dataset in memory, we will loop through each img\n    # The filenames will be retrieved from the dataset's rtree index\n    files = [\n        item.object\n        for item in dataset.index.intersection(dataset.index.bounds, objects=True)\n    ]\n\n    # Resetting statistics\n    accum_mean = 0\n    accum_std = 0\n\n    for file in files:\n        img = rio.open(file).read() / divide_by  # type: ignore\n        accum_mean += img.reshape((img.shape[0], -1)).mean(axis=1)\n        accum_std += img.reshape((img.shape[0], -1)).std(axis=1)\n\n    # at the end, we shall have 2 vectors with length n=chnls\n    # we will average them considering the number of images\n    return accum_mean / len(files), accum_std / len(files)\n</code></pre>"},{"location":"common/#geoai.common.plot_batch","title":"<code>plot_batch(batch, bright=1.0, cols=4, width=5, chnls=[2, 1, 0], cmap='Blues')</code>","text":"<p>Plot a batch of images and masks. This function is adapted from the plot_batch() function in the torchgeo library at https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html Credit to the torchgeo developers for the original implementation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch containing images and masks.</p> required <code>bright</code> <code>float</code> <p>The brightness factor. Defaults to 1.0.</p> <code>1.0</code> <code>cols</code> <code>int</code> <p>The number of columns in the plot grid. Defaults to 4.</p> <code>4</code> <code>width</code> <code>int</code> <p>The width of each plot. Defaults to 5.</p> <code>5</code> <code>chnls</code> <code>List[int]</code> <p>The channels to use for RGB. Defaults to [2, 1, 0].</p> <code>[2, 1, 0]</code> <code>cmap</code> <code>str</code> <p>The colormap to use for masks. Defaults to \"Blues\".</p> <code>'Blues'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/common.py</code> <pre><code>def plot_batch(\n    batch: Dict[str, Any],\n    bright: float = 1.0,\n    cols: int = 4,\n    width: int = 5,\n    chnls: List[int] = [2, 1, 0],\n    cmap: str = \"Blues\",\n) -&gt; None:\n    \"\"\"\n    Plot a batch of images and masks. This function is adapted from the plot_batch()\n    function in the torchgeo library at\n    https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html\n    Credit to the torchgeo developers for the original implementation.\n\n    Args:\n        batch (Dict[str, Any]): The batch containing images and masks.\n        bright (float, optional): The brightness factor. Defaults to 1.0.\n        cols (int, optional): The number of columns in the plot grid. Defaults to 4.\n        width (int, optional): The width of each plot. Defaults to 5.\n        chnls (List[int], optional): The channels to use for RGB. Defaults to [2, 1, 0].\n        cmap (str, optional): The colormap to use for masks. Defaults to \"Blues\".\n\n    Returns:\n        None\n    \"\"\"\n    # Get the samples and the number of items in the batch\n    samples = unbind_samples(batch.copy())\n\n    # if batch contains images and masks, the number of images will be doubled\n    n = 2 * len(samples) if (\"image\" in batch) and (\"mask\" in batch) else len(samples)\n\n    # calculate the number of rows in the grid\n    rows = n // cols + (1 if n % cols != 0 else 0)\n\n    # create a grid\n    _, axs = plt.subplots(rows, cols, figsize=(cols * width, rows * width))\n\n    if (\"image\" in batch) and (\"mask\" in batch):\n        # plot the images on the even axis\n        plot_images(\n            images=map(lambda x: x[\"image\"], samples),\n            axs=axs.reshape(-1)[::2],\n            chnls=chnls,\n            bright=bright,\n        )\n\n        # plot the masks on the odd axis\n        plot_masks(masks=map(lambda x: x[\"mask\"], samples), axs=axs.reshape(-1)[1::2])\n\n    else:\n        if \"image\" in batch:\n            plot_images(\n                images=map(lambda x: x[\"image\"], samples),\n                axs=axs.reshape(-1),\n                chnls=chnls,\n                bright=bright,\n            )\n\n        elif \"mask\" in batch:\n            plot_masks(\n                masks=map(lambda x: x[\"mask\"], samples), axs=axs.reshape(-1), cmap=cmap\n            )\n</code></pre>"},{"location":"common/#geoai.common.plot_images","title":"<code>plot_images(images, axs, chnls=[2, 1, 0], bright=1.0)</code>","text":"<p>Plot a list of images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Iterable[torch.Tensor]</code> <p>The images to plot.</p> required <code>axs</code> <code>Iterable[plt.Axes]</code> <p>The axes to plot the images on.</p> required <code>chnls</code> <code>List[int]</code> <p>The channels to use for RGB. Defaults to [2, 1, 0].</p> <code>[2, 1, 0]</code> <code>bright</code> <code>float</code> <p>The brightness factor. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/common.py</code> <pre><code>def plot_images(\n    images: Iterable[torch.Tensor],\n    axs: Iterable[plt.Axes],\n    chnls: List[int] = [2, 1, 0],\n    bright: float = 1.0,\n) -&gt; None:\n    \"\"\"\n    Plot a list of images.\n\n    Args:\n        images (Iterable[torch.Tensor]): The images to plot.\n        axs (Iterable[plt.Axes]): The axes to plot the images on.\n        chnls (List[int], optional): The channels to use for RGB. Defaults to [2, 1, 0].\n        bright (float, optional): The brightness factor. Defaults to 1.0.\n\n    Returns:\n        None\n    \"\"\"\n    for img, ax in zip(images, axs):\n        arr = torch.clamp(bright * img, min=0, max=1).numpy()\n        rgb = arr.transpose(1, 2, 0)[:, :, chnls]\n        ax.imshow(rgb)\n        ax.axis(\"off\")\n</code></pre>"},{"location":"common/#geoai.common.plot_masks","title":"<code>plot_masks(masks, axs, cmap='Blues')</code>","text":"<p>Plot a list of masks.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>Iterable[torch.Tensor]</code> <p>The masks to plot.</p> required <code>axs</code> <code>Iterable[plt.Axes]</code> <p>The axes to plot the masks on.</p> required <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \"Blues\".</p> <code>'Blues'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/common.py</code> <pre><code>def plot_masks(\n    masks: Iterable[torch.Tensor], axs: Iterable[plt.Axes], cmap: str = \"Blues\"\n) -&gt; None:\n    \"\"\"\n    Plot a list of masks.\n\n    Args:\n        masks (Iterable[torch.Tensor]): The masks to plot.\n        axs (Iterable[plt.Axes]): The axes to plot the masks on.\n        cmap (str, optional): The colormap to use. Defaults to \"Blues\".\n\n    Returns:\n        None\n    \"\"\"\n    for mask, ax in zip(masks, axs):\n        ax.imshow(mask.squeeze().numpy(), cmap=cmap)\n        ax.axis(\"off\")\n</code></pre>"},{"location":"common/#geoai.common.viz_image","title":"<code>viz_image(image, transpose=False, bdx=None, scale_factor=1.0, figsize=(10, 5), axis_off=True, **kwargs)</code>","text":"<p>Visualize an image using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[np.ndarray, torch.Tensor]</code> <p>The image to visualize.</p> required <code>transpose</code> <code>bool</code> <p>Whether to transpose the image. Defaults to False.</p> <code>False</code> <code>bdx</code> <code>Optional[int]</code> <p>The band index to visualize. Defaults to None.</p> <code>None</code> <code>scale_factor</code> <code>float</code> <p>The scale factor to apply to the image. Defaults to 1.0.</p> <code>1.0</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>The size of the figure. Defaults to (10, 5).</p> <code>(10, 5)</code> <code>axis_off</code> <code>bool</code> <p>Whether to turn off the axis. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for plt.imshow().</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/common.py</code> <pre><code>def viz_image(\n    image: Union[np.ndarray, torch.Tensor],\n    transpose: bool = False,\n    bdx: Optional[int] = None,\n    scale_factor: float = 1.0,\n    figsize: Tuple[int, int] = (10, 5),\n    axis_off: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Visualize an image using matplotlib.\n\n    Args:\n        image (Union[np.ndarray, torch.Tensor]): The image to visualize.\n        transpose (bool, optional): Whether to transpose the image. Defaults to False.\n        bdx (Optional[int], optional): The band index to visualize. Defaults to None.\n        scale_factor (float, optional): The scale factor to apply to the image. Defaults to 1.0.\n        figsize (Tuple[int, int], optional): The size of the figure. Defaults to (10, 5).\n        axis_off (bool, optional): Whether to turn off the axis. Defaults to True.\n        **kwargs (Any): Additional keyword arguments for plt.imshow().\n\n    Returns:\n        None\n    \"\"\"\n\n    if isinstance(image, torch.Tensor):\n        image = image.cpu().numpy()\n\n    plt.figure(figsize=figsize)\n\n    if transpose:\n        image = image.transpose(1, 2, 0)\n\n    if bdx is not None:\n        image = image[:, :, bdx]\n\n    if len(image.shape) &gt; 2 and image.shape[2] &gt; 3:\n        image = image[:, :, 0:3]\n\n    if scale_factor != 1.0:\n        image = np.clip(image * scale_factor, 0, 1)\n\n    plt.imshow(image, **kwargs)\n    if axis_off:\n        plt.axis(\"off\")\n    plt.show()\n    plt.close()\n</code></pre>"},{"location":"common/#geoai.common.viz_raster","title":"<code>viz_raster(source, indexes=None, colormap=None, vmin=None, vmax=None, nodata=None, attribution=None, layer_name='Raster', layer_index=None, zoom_to_layer=True, visible=True, opacity=1.0, array_args={}, client_args={'cors_all': False}, basemap='OpenStreetMap', **kwargs)</code>","text":"<p>Visualize a raster using leafmap.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the raster.</p> required <code>indexes</code> <code>Optional[int]</code> <p>The band indexes to visualize. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>Optional[str]</code> <p>The colormap to apply. Defaults to None.</p> <code>None</code> <code>vmin</code> <code>Optional[float]</code> <p>The minimum value for colormap scaling. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>Optional[float]</code> <p>The maximum value for colormap scaling. Defaults to None.</p> <code>None</code> <code>nodata</code> <code>Optional[float]</code> <p>The nodata value. Defaults to None.</p> <code>None</code> <code>attribution</code> <code>Optional[str]</code> <p>The attribution for the raster. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>Optional[str]</code> <p>The name of the layer. Defaults to \"Raster\".</p> <code>'Raster'</code> <code>layer_index</code> <code>Optional[int]</code> <p>The index of the layer. Defaults to None.</p> <code>None</code> <code>zoom_to_layer</code> <code>Optional[bool]</code> <p>Whether to zoom to the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>Optional[bool]</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>opacity</code> <code>Optional[float]</code> <p>The opacity of the layer. Defaults to 1.0.</p> <code>1.0</code> <code>array_args</code> <code>Optional[Dict]</code> <p>Additional arguments for array processing. Defaults to {}.</p> <code>{}</code> <code>client_args</code> <code>Optional[Dict]</code> <p>Additional arguments for the client. Defaults to {\"cors_all\": False}.</p> <code>{'cors_all': False}</code> <code>basemap</code> <code>Optional[str]</code> <p>The basemap to use. Defaults to \"OpenStreetMap\".</p> <code>'OpenStreetMap'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>leafmap.Map</code> <p>The map object with the raster layer added.</p> Source code in <code>geoai/common.py</code> <pre><code>def viz_raster(\n    source: str,\n    indexes: Optional[int] = None,\n    colormap: Optional[str] = None,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n    nodata: Optional[float] = None,\n    attribution: Optional[str] = None,\n    layer_name: Optional[str] = \"Raster\",\n    layer_index: Optional[int] = None,\n    zoom_to_layer: Optional[bool] = True,\n    visible: Optional[bool] = True,\n    opacity: Optional[float] = 1.0,\n    array_args: Optional[Dict] = {},\n    client_args: Optional[Dict] = {\"cors_all\": False},\n    basemap: Optional[str] = \"OpenStreetMap\",\n    **kwargs,\n):\n    \"\"\"\n    Visualize a raster using leafmap.\n\n    Args:\n        source (str): The source of the raster.\n        indexes (Optional[int], optional): The band indexes to visualize. Defaults to None.\n        colormap (Optional[str], optional): The colormap to apply. Defaults to None.\n        vmin (Optional[float], optional): The minimum value for colormap scaling. Defaults to None.\n        vmax (Optional[float], optional): The maximum value for colormap scaling. Defaults to None.\n        nodata (Optional[float], optional): The nodata value. Defaults to None.\n        attribution (Optional[str], optional): The attribution for the raster. Defaults to None.\n        layer_name (Optional[str], optional): The name of the layer. Defaults to \"Raster\".\n        layer_index (Optional[int], optional): The index of the layer. Defaults to None.\n        zoom_to_layer (Optional[bool], optional): Whether to zoom to the layer. Defaults to True.\n        visible (Optional[bool], optional): Whether the layer is visible. Defaults to True.\n        opacity (Optional[float], optional): The opacity of the layer. Defaults to 1.0.\n        array_args (Optional[Dict], optional): Additional arguments for array processing. Defaults to {}.\n        client_args (Optional[Dict], optional): Additional arguments for the client. Defaults to {\"cors_all\": False}.\n        basemap (Optional[str], optional): The basemap to use. Defaults to \"OpenStreetMap\".\n        **kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        leafmap.Map: The map object with the raster layer added.\n    \"\"\"\n    import leafmap\n\n    m = leafmap.Map(basemap=basemap)\n\n    m.add_raster(\n        source=source,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        attribution=attribution,\n        layer_name=layer_name,\n        layer_index=layer_index,\n        zoom_to_layer=zoom_to_layer,\n        visible=visible,\n        opacity=opacity,\n        array_args=array_args,\n        client_args=client_args,\n        **kwargs,\n    )\n    return m\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/opengeos/geoai/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>geoai could always use more documentation, whether as part of the official geoai docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/opengeos/geoai/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up geoai for local development.</p> <ol> <li> <p>Fork the geoai repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/geoai.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv geoai\n$ cd geoai/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 geoai tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and     for PyPy. Check https://github.com/opengeos/geoai/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"download/","title":"download module","text":"<p>This module provides functions to download data, including NAIP imagery and building data from Overture Maps.</p>"},{"location":"download/#geoai.download.convert_vector_format","title":"<code>convert_vector_format(input_file, output_format='geojson', filter_expression=None)</code>","text":"<p>Convert the downloaded data to a different format or filter it.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>Path to the input file.</p> required <code>output_format</code> <code>str</code> <p>Format to convert to, one of \"geojson\", \"parquet\", \"shapefile\", \"csv\".</p> <code>'geojson'</code> <code>filter_expression</code> <code>Optional[str]</code> <p>Optional GeoDataFrame query expression to filter the data.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the converted file.</p> Source code in <code>geoai/download.py</code> <pre><code>def convert_vector_format(\n    input_file: str,\n    output_format: str = \"geojson\",\n    filter_expression: Optional[str] = None,\n) -&gt; str:\n    \"\"\"Convert the downloaded data to a different format or filter it.\n\n    Args:\n        input_file: Path to the input file.\n        output_format: Format to convert to, one of \"geojson\", \"parquet\", \"shapefile\", \"csv\".\n        filter_expression: Optional GeoDataFrame query expression to filter the data.\n\n    Returns:\n        Path to the converted file.\n    \"\"\"\n    try:\n        # Read the input file\n        logger.info(f\"Reading {input_file}\")\n        gdf = gpd.read_file(input_file)\n\n        # Apply filter if specified\n        if filter_expression:\n            logger.info(f\"Filtering data using expression: {filter_expression}\")\n            gdf = gdf.query(filter_expression)\n            logger.info(f\"After filtering: {len(gdf)} features\")\n\n        # Define output file path\n        base_path = os.path.splitext(input_file)[0]\n\n        if output_format == \"geojson\":\n            output_file = f\"{base_path}.geojson\"\n            logger.info(f\"Converting to GeoJSON: {output_file}\")\n            gdf.to_file(output_file, driver=\"GeoJSON\")\n        elif output_format == \"parquet\":\n            output_file = f\"{base_path}.parquet\"\n            logger.info(f\"Converting to Parquet: {output_file}\")\n            gdf.to_parquet(output_file)\n        elif output_format == \"shapefile\":\n            output_file = f\"{base_path}.shp\"\n            logger.info(f\"Converting to Shapefile: {output_file}\")\n            gdf.to_file(output_file)\n        elif output_format == \"csv\":\n            output_file = f\"{base_path}.csv\"\n            logger.info(f\"Converting to CSV: {output_file}\")\n\n            # For CSV, we need to convert geometry to WKT\n            gdf[\"geometry_wkt\"] = gdf.geometry.apply(lambda g: g.wkt)\n\n            # Save to CSV with geometry as WKT\n            gdf.drop(columns=[\"geometry\"]).to_csv(output_file, index=False)\n        else:\n            raise ValueError(f\"Unsupported output format: {output_format}\")\n\n        return output_file\n\n    except Exception as e:\n        logger.error(f\"Error converting data: {str(e)}\")\n        raise\n</code></pre>"},{"location":"download/#geoai.download.download_naip","title":"<code>download_naip(bbox, output_dir, year=None, max_items=10, preview=False, **kwargs)</code>","text":"<p>Download NAIP imagery from Planetary Computer based on a bounding box.</p> <p>This function searches for NAIP (National Agriculture Imagery Program) imagery from Microsoft's Planetary Computer that intersects with the specified bounding box. It downloads the imagery and saves it as GeoTIFF files.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save the downloaded imagery.</p> required <code>year</code> <code>Optional[int]</code> <p>Specific year of NAIP imagery to download (e.g., 2020). If None, returns imagery from all available years.</p> <code>None</code> <code>max_items</code> <code>int</code> <p>Maximum number of items to download.</p> <code>10</code> <code>preview</code> <code>bool</code> <p>If True, display a preview of the downloaded imagery.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of downloaded file paths.</p> <p>Exceptions:</p> Type Description <code>Exception</code> <p>If there is an error downloading or saving the imagery.</p> Source code in <code>geoai/download.py</code> <pre><code>def download_naip(\n    bbox: Tuple[float, float, float, float],\n    output_dir: str,\n    year: Optional[int] = None,\n    max_items: int = 10,\n    preview: bool = False,\n    **kwargs: Any,\n) -&gt; List[str]:\n    \"\"\"Download NAIP imagery from Planetary Computer based on a bounding box.\n\n    This function searches for NAIP (National Agriculture Imagery Program) imagery\n    from Microsoft's Planetary Computer that intersects with the specified bounding box.\n    It downloads the imagery and saves it as GeoTIFF files.\n\n    Args:\n        bbox: Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.\n        output_dir: Directory to save the downloaded imagery.\n        year: Specific year of NAIP imagery to download (e.g., 2020). If None, returns imagery from all available years.\n        max_items: Maximum number of items to download.\n        preview: If True, display a preview of the downloaded imagery.\n\n    Returns:\n        List of downloaded file paths.\n\n    Raises:\n        Exception: If there is an error downloading or saving the imagery.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Create a geometry from the bounding box\n    geometry = box(*bbox)\n\n    # Connect to Planetary Computer STAC API\n    catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n\n    # Build query for NAIP data\n    search_params = {\n        \"collections\": [\"naip\"],\n        \"intersects\": geometry,\n        \"limit\": max_items,\n    }\n\n    # Add year filter if specified\n    if year:\n        search_params[\"query\"] = {\"naip:year\": {\"eq\": year}}\n\n    for key, value in kwargs.items():\n        search_params[key] = value\n\n    # Search for NAIP imagery\n    search_results = catalog.search(**search_params)\n    items = list(search_results.items())\n\n    if not items:\n        print(\"No NAIP imagery found for the specified region and parameters.\")\n        return []\n\n    print(f\"Found {len(items)} NAIP items.\")\n\n    # Download and save each item\n    downloaded_files = []\n    for i, item in enumerate(items):\n        # Sign the assets (required for Planetary Computer)\n        signed_item = pc.sign(item)\n\n        # Get the RGB asset URL\n        rgb_asset = signed_item.assets.get(\"image\")\n        if not rgb_asset:\n            print(f\"No RGB asset found for item {i+1}\")\n            continue\n\n        # Use the original filename from the asset\n        original_filename = os.path.basename(\n            rgb_asset.href.split(\"?\")[0]\n        )  # Remove query parameters\n        output_path = os.path.join(output_dir, original_filename)\n\n        print(f\"Downloading item {i+1}/{len(items)}: {original_filename}\")\n\n        try:\n            # Open and save the data with progress bar\n            # For direct file download with progress bar\n            if rgb_asset.href.startswith(\"http\"):\n                download_with_progress(rgb_asset.href, output_path)\n                #\n            else:\n                # Fallback to direct rioxarray opening (less common case)\n                data = rioxarray.open_rasterio(rgb_asset.href)\n                data.rio.to_raster(output_path)\n\n            downloaded_files.append(output_path)\n            print(f\"Successfully saved to {output_path}\")\n\n            # Optional: Display a preview (uncomment if needed)\n            if preview:\n                data = rioxarray.open_rasterio(output_path)\n                preview_raster(data)\n\n        except Exception as e:\n            print(f\"Error downloading item {i+1}: {str(e)}\")\n\n    return downloaded_files\n</code></pre>"},{"location":"download/#geoai.download.download_overture_buildings","title":"<code>download_overture_buildings(bbox, output_file, output_format='geojson', data_type='building', verbose=True)</code>","text":"<p>Download building data from Overture Maps for a given bounding box using the overturemaps CLI tool.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.</p> required <code>output_file</code> <code>str</code> <p>Path to save the output file.</p> required <code>output_format</code> <code>str</code> <p>Format to save the output, one of \"geojson\", \"geojsonseq\", or \"geoparquet\".</p> <code>'geojson'</code> <code>data_type</code> <code>str</code> <p>The Overture Maps data type to download (building, place, etc.).</p> <code>'building'</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the output file.</p> Source code in <code>geoai/download.py</code> <pre><code>def download_overture_buildings(\n    bbox: Tuple[float, float, float, float],\n    output_file: str,\n    output_format: str = \"geojson\",\n    data_type: str = \"building\",\n    verbose: bool = True,\n) -&gt; str:\n    \"\"\"Download building data from Overture Maps for a given bounding box using the overturemaps CLI tool.\n\n    Args:\n        bbox: Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.\n        output_file: Path to save the output file.\n        output_format: Format to save the output, one of \"geojson\", \"geojsonseq\", or \"geoparquet\".\n        data_type: The Overture Maps data type to download (building, place, etc.).\n        verbose: Whether to print verbose output.\n\n    Returns:\n        Path to the output file.\n    \"\"\"\n    # Create output directory if needed\n    output_dir = os.path.dirname(output_file)\n    if output_dir and not os.path.exists(output_dir):\n        os.makedirs(output_dir, exist_ok=True)\n\n    # Format the bounding box string for the command\n    west, south, east, north = bbox\n    bbox_str = f\"{west},{south},{east},{north}\"\n\n    # Build the command\n    cmd = [\n        \"overturemaps\",\n        \"download\",\n        \"--bbox\",\n        bbox_str,\n        \"-f\",\n        output_format,\n        \"--type\",\n        data_type,\n        \"--output\",\n        output_file,\n    ]\n\n    if verbose:\n        logger.info(f\"Running command: {' '.join(cmd)}\")\n        logger.info(\"Downloading %s data for area: %s\", data_type, bbox_str)\n\n    try:\n        # Run the command\n        result = subprocess.run(\n            cmd,\n            check=True,\n            stdout=subprocess.PIPE if not verbose else None,\n            stderr=subprocess.PIPE,\n            text=True,\n        )\n\n        # Check if the file was created\n        if os.path.exists(output_file):\n            file_size = os.path.getsize(output_file) / (1024 * 1024)  # Size in MB\n            logger.info(\n                f\"Successfully downloaded data to {output_file} ({file_size:.2f} MB)\"\n            )\n\n            # Optionally show some stats about the downloaded data\n            if output_format == \"geojson\" and os.path.getsize(output_file) &gt; 0:\n                try:\n                    gdf = gpd.read_file(output_file)\n                    logger.info(f\"Downloaded {len(gdf)} features\")\n\n                    if len(gdf) &gt; 0 and verbose:\n                        # Show a sample of the attribute names\n                        attrs = list(gdf.columns)\n                        attrs.remove(\"geometry\")\n                        logger.info(f\"Available attributes: {', '.join(attrs[:10])}...\")\n                except Exception as e:\n                    logger.warning(f\"Could not read the GeoJSON file: {str(e)}\")\n\n            return output_file\n        else:\n            logger.error(f\"Command completed but file {output_file} was not created\")\n            if result.stderr:\n                logger.error(f\"Command error output: {result.stderr}\")\n            return None\n\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Error running overturemaps command: {str(e)}\")\n        if e.stderr:\n            logger.error(f\"Command error output: {e.stderr}\")\n        raise RuntimeError(f\"Failed to download Overture Maps data: {str(e)}\")\n    except Exception as e:\n        logger.error(f\"Unexpected error: {str(e)}\")\n        raise\n</code></pre>"},{"location":"download/#geoai.download.download_with_progress","title":"<code>download_with_progress(url, output_path)</code>","text":"<p>Download a file with a progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of the file to download.</p> required <code>output_path</code> <code>str</code> <p>Path where the file will be saved.</p> required Source code in <code>geoai/download.py</code> <pre><code>def download_with_progress(url: str, output_path: str) -&gt; None:\n    \"\"\"Download a file with a progress bar.\n\n    Args:\n        url: URL of the file to download.\n        output_path: Path where the file will be saved.\n    \"\"\"\n    response = requests.get(url, stream=True)\n    total_size = int(response.headers.get(\"content-length\", 0))\n    block_size = 1024  # 1 Kibibyte\n\n    with (\n        open(output_path, \"wb\") as file,\n        tqdm(\n            desc=os.path.basename(output_path),\n            total=total_size,\n            unit=\"iB\",\n            unit_scale=True,\n            unit_divisor=1024,\n        ) as bar,\n    ):\n        for data in response.iter_content(block_size):\n            size = file.write(data)\n            bar.update(size)\n</code></pre>"},{"location":"download/#geoai.download.extract_building_stats","title":"<code>extract_building_stats(geojson_file)</code>","text":"<p>Extract statistics from the building data.</p> <p>Parameters:</p> Name Type Description Default <code>geojson_file</code> <code>str</code> <p>Path to the GeoJSON file.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with statistics.</p> Source code in <code>geoai/download.py</code> <pre><code>def extract_building_stats(geojson_file: str) -&gt; Dict[str, Any]:\n    \"\"\"Extract statistics from the building data.\n\n    Args:\n        geojson_file: Path to the GeoJSON file.\n\n    Returns:\n        Dictionary with statistics.\n    \"\"\"\n    try:\n        # Read the GeoJSON file\n        gdf = gpd.read_file(geojson_file)\n\n        # Calculate statistics\n        bbox = gdf.total_bounds.tolist()\n        # Convert numpy values to Python native types\n        bbox = [float(x) for x in bbox]\n\n        stats = {\n            \"total_buildings\": int(len(gdf)),\n            \"has_height\": (\n                int(gdf[\"height\"].notna().sum()) if \"height\" in gdf.columns else 0\n            ),\n            \"has_name\": (\n                int(gdf[\"names.common.value\"].notna().sum())\n                if \"names.common.value\" in gdf.columns\n                else 0\n            ),\n            \"bbox\": bbox,\n        }\n\n        return stats\n\n    except Exception as e:\n        logger.error(f\"Error extracting statistics: {str(e)}\")\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"download/#geoai.download.json_serializable","title":"<code>json_serializable(obj)</code>","text":"<p>Convert NumPy types to native Python types for JSON serialization.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Any object to convert.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>JSON serializable version of the object.</p> Source code in <code>geoai/download.py</code> <pre><code>def json_serializable(obj: Any) -&gt; Any:\n    \"\"\"Convert NumPy types to native Python types for JSON serialization.\n\n    Args:\n        obj: Any object to convert.\n\n    Returns:\n        JSON serializable version of the object.\n    \"\"\"\n    if isinstance(obj, np.integer):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    else:\n        return obj\n</code></pre>"},{"location":"download/#geoai.download.preview_raster","title":"<code>preview_raster(data, title=None)</code>","text":"<p>Display a preview of the downloaded imagery.</p> <p>This function creates a visualization of the downloaded NAIP imagery by converting it to an RGB array and displaying it with matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The raster data as a rioxarray object.</p> required <code>title</code> <code>str</code> <p>The title for the preview plot.</p> <code>None</code> Source code in <code>geoai/download.py</code> <pre><code>def preview_raster(data: Any, title: str = None) -&gt; None:\n    \"\"\"Display a preview of the downloaded imagery.\n\n    This function creates a visualization of the downloaded NAIP imagery\n    by converting it to an RGB array and displaying it with matplotlib.\n\n    Args:\n        data: The raster data as a rioxarray object.\n        title: The title for the preview plot.\n    \"\"\"\n    # Convert to 8-bit RGB for display\n    rgb_data = data.transpose(\"y\", \"x\", \"band\").values[:, :, 0:3]\n    rgb_data = np.where(rgb_data &gt; 255, 255, rgb_data).astype(np.uint8)\n\n    plt.figure(figsize=(10, 10))\n    plt.imshow(rgb_data)\n    if title is not None:\n        plt.title(title)\n    plt.axis(\"off\")\n    plt.show()\n</code></pre>"},{"location":"faq/","title":"FAQ","text":""},{"location":"geoai/","title":"geoai module","text":"<p>Main module.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install geoai, run this command in your terminal:</p> <pre><code>pip install geoai-py\n</code></pre> <p>This is the preferred method to install geoai, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>To install geoai from sources, run this command in your terminal:</p> <pre><code>pip install git+https://github.com/opengeos/geoai\n</code></pre>"},{"location":"segmentation/","title":"segmentation module","text":""},{"location":"segmentation/#geoai.segmentation.CustomDataset","title":"<code> CustomDataset            (Dataset)         </code>","text":"<p>Custom Dataset for loading images and masks.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>class CustomDataset(Dataset):\n    \"\"\"Custom Dataset for loading images and masks.\"\"\"\n\n    def __init__(\n        self,\n        images_dir: str,\n        masks_dir: str,\n        transform: A.Compose = None,\n        target_size: tuple = (256, 256),\n        num_classes: int = 2,\n    ):\n        \"\"\"\n        Args:\n            images_dir (str): Directory containing images.\n            masks_dir (str): Directory containing masks.\n            transform (A.Compose, optional): Transformations to be applied on the images and masks.\n            target_size (tuple, optional): Target size for resizing images and masks.\n            num_classes (int, optional): Number of classes in the masks.\n        \"\"\"\n        self.images_dir = images_dir\n        self.masks_dir = masks_dir\n        self.transform = transform\n        self.target_size = target_size\n        self.num_classes = num_classes\n        self.images = sorted(os.listdir(images_dir))\n        self.masks = sorted(os.listdir(masks_dir))\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the total number of samples.\"\"\"\n        return len(self.images)\n\n    def __getitem__(self, idx: int) -&gt; dict:\n        \"\"\"\n        Args:\n            idx (int): Index of the sample to fetch.\n\n        Returns:\n            dict: A dictionary with 'pixel_values' and 'labels'.\n        \"\"\"\n        img_path = os.path.join(self.images_dir, self.images[idx])\n        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n        image = Image.open(img_path).convert(\"RGB\")\n        mask = Image.open(mask_path).convert(\"L\")\n\n        image = image.resize(self.target_size)\n        mask = mask.resize(self.target_size)\n\n        image = np.array(image)\n        mask = np.array(mask)\n\n        mask = (mask &gt; 127).astype(np.uint8)\n\n        if self.transform:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed[\"image\"]\n            mask = transformed[\"mask\"]\n\n        assert (\n            mask.max() &lt; self.num_classes\n        ), f\"Mask values should be less than {self.num_classes}, but found {mask.max()}\"\n        assert (\n            mask.min() &gt;= 0\n        ), f\"Mask values should be greater than or equal to 0, but found {mask.min()}\"\n\n        mask = mask.clone().detach().long()\n\n        return {\"pixel_values\": image, \"labels\": mask}\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.CustomDataset.__getitem__","title":"<code>__getitem__(self, idx)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the sample to fetch.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with 'pixel_values' and 'labels'.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; dict:\n    \"\"\"\n    Args:\n        idx (int): Index of the sample to fetch.\n\n    Returns:\n        dict: A dictionary with 'pixel_values' and 'labels'.\n    \"\"\"\n    img_path = os.path.join(self.images_dir, self.images[idx])\n    mask_path = os.path.join(self.masks_dir, self.masks[idx])\n    image = Image.open(img_path).convert(\"RGB\")\n    mask = Image.open(mask_path).convert(\"L\")\n\n    image = image.resize(self.target_size)\n    mask = mask.resize(self.target_size)\n\n    image = np.array(image)\n    mask = np.array(mask)\n\n    mask = (mask &gt; 127).astype(np.uint8)\n\n    if self.transform:\n        transformed = self.transform(image=image, mask=mask)\n        image = transformed[\"image\"]\n        mask = transformed[\"mask\"]\n\n    assert (\n        mask.max() &lt; self.num_classes\n    ), f\"Mask values should be less than {self.num_classes}, but found {mask.max()}\"\n    assert (\n        mask.min() &gt;= 0\n    ), f\"Mask values should be greater than or equal to 0, but found {mask.min()}\"\n\n    mask = mask.clone().detach().long()\n\n    return {\"pixel_values\": image, \"labels\": mask}\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.CustomDataset.__init__","title":"<code>__init__(self, images_dir, masks_dir, transform=None, target_size=(256, 256), num_classes=2)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Directory containing images.</p> required <code>masks_dir</code> <code>str</code> <p>Directory containing masks.</p> required <code>transform</code> <code>A.Compose</code> <p>Transformations to be applied on the images and masks.</p> <code>None</code> <code>target_size</code> <code>tuple</code> <p>Target size for resizing images and masks.</p> <code>(256, 256)</code> <code>num_classes</code> <code>int</code> <p>Number of classes in the masks.</p> <code>2</code> Source code in <code>geoai/segmentation.py</code> <pre><code>def __init__(\n    self,\n    images_dir: str,\n    masks_dir: str,\n    transform: A.Compose = None,\n    target_size: tuple = (256, 256),\n    num_classes: int = 2,\n):\n    \"\"\"\n    Args:\n        images_dir (str): Directory containing images.\n        masks_dir (str): Directory containing masks.\n        transform (A.Compose, optional): Transformations to be applied on the images and masks.\n        target_size (tuple, optional): Target size for resizing images and masks.\n        num_classes (int, optional): Number of classes in the masks.\n    \"\"\"\n    self.images_dir = images_dir\n    self.masks_dir = masks_dir\n    self.transform = transform\n    self.target_size = target_size\n    self.num_classes = num_classes\n    self.images = sorted(os.listdir(images_dir))\n    self.masks = sorted(os.listdir(masks_dir))\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.CustomDataset.__len__","title":"<code>__len__(self)</code>  <code>special</code>","text":"<p>Returns the total number of samples.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the total number of samples.\"\"\"\n    return len(self.images)\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.get_transform","title":"<code>get_transform()</code>","text":"<p>Returns:</p> Type Description <code>A.Compose</code> <p>A composition of image transformations.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def get_transform() -&gt; A.Compose:\n    \"\"\"\n    Returns:\n        A.Compose: A composition of image transformations.\n    \"\"\"\n    return A.Compose(\n        [\n            A.Resize(256, 256),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.RandomRotate90(p=0.5),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            ToTensorV2(),\n        ]\n    )\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.load_model","title":"<code>load_model(model_path, device)</code>","text":"<p>Loads the fine-tuned model from the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model.</p> required <code>device</code> <code>torch.device</code> <p>Device to load the model on.</p> required <p>Returns:</p> Type Description <code>SegformerForSemanticSegmentation</code> <p>Loaded model.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def load_model(\n    model_path: str, device: torch.device\n) -&gt; SegformerForSemanticSegmentation:\n    \"\"\"\n    Loads the fine-tuned model from the specified path.\n\n    Args:\n        model_path (str): Path to the model.\n        device (torch.device): Device to load the model on.\n\n    Returns:\n        SegformerForSemanticSegmentation: Loaded model.\n    \"\"\"\n    model = SegformerForSemanticSegmentation.from_pretrained(model_path)\n    model.to(device)\n    model.eval()\n    return model\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.predict_image","title":"<code>predict_image(model, image_tensor, original_size, device)</code>","text":"<p>Predicts the segmentation mask for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>SegformerForSemanticSegmentation</code> <p>Fine-tuned model.</p> required <code>image_tensor</code> <code>torch.Tensor</code> <p>Preprocessed image tensor.</p> required <code>original_size</code> <code>tuple</code> <p>Original size of the image (width, height).</p> required <code>device</code> <code>torch.device</code> <p>Device to perform inference on.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Predicted segmentation mask.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def predict_image(\n    model: SegformerForSemanticSegmentation,\n    image_tensor: torch.Tensor,\n    original_size: tuple,\n    device: torch.device,\n) -&gt; np.ndarray:\n    \"\"\"\n    Predicts the segmentation mask for the input image.\n\n    Args:\n        model (SegformerForSemanticSegmentation): Fine-tuned model.\n        image_tensor (torch.Tensor): Preprocessed image tensor.\n        original_size (tuple): Original size of the image (width, height).\n        device (torch.device): Device to perform inference on.\n\n    Returns:\n        np.ndarray: Predicted segmentation mask.\n    \"\"\"\n    with torch.no_grad():\n        image_tensor = image_tensor.to(device)\n        outputs = model(pixel_values=image_tensor)\n        logits = outputs.logits\n        upsampled_logits = F.interpolate(\n            logits, size=original_size[::-1], mode=\"bilinear\", align_corners=False\n        )\n        predictions = torch.argmax(upsampled_logits, dim=1).cpu().numpy()\n    return predictions[0]\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.prepare_datasets","title":"<code>prepare_datasets(images_dir, masks_dir, transform, test_size=0.2, random_state=42)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Directory containing images.</p> required <code>masks_dir</code> <code>str</code> <p>Directory containing masks.</p> required <code>transform</code> <code>A.Compose</code> <p>Transformations to be applied.</p> required <code>test_size</code> <code>float</code> <p>Proportion of the dataset to include in the validation split.</p> <code>0.2</code> <code>random_state</code> <code>int</code> <p>Random seed for shuffling the dataset.</p> <code>42</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Training and validation datasets.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def prepare_datasets(\n    images_dir: str,\n    masks_dir: str,\n    transform: A.Compose,\n    test_size: float = 0.2,\n    random_state: int = 42,\n) -&gt; tuple:\n    \"\"\"\n    Args:\n        images_dir (str): Directory containing images.\n        masks_dir (str): Directory containing masks.\n        transform (A.Compose): Transformations to be applied.\n        test_size (float, optional): Proportion of the dataset to include in the validation split.\n        random_state (int, optional): Random seed for shuffling the dataset.\n\n    Returns:\n        tuple: Training and validation datasets.\n    \"\"\"\n    dataset = CustomDataset(images_dir, masks_dir, transform)\n    train_indices, val_indices = train_test_split(\n        list(range(len(dataset))), test_size=test_size, random_state=random_state\n    )\n    train_dataset = Subset(dataset, train_indices)\n    val_dataset = Subset(dataset, val_indices)\n    return train_dataset, val_dataset\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.preprocess_image","title":"<code>preprocess_image(image_path, target_size=(256, 256))</code>","text":"<p>Preprocesses the input image for prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the input image.</p> required <code>target_size</code> <code>tuple</code> <p>Target size for resizing the image.</p> <code>(256, 256)</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Preprocessed image tensor.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def preprocess_image(image_path: str, target_size: tuple = (256, 256)) -&gt; torch.Tensor:\n    \"\"\"\n    Preprocesses the input image for prediction.\n\n    Args:\n        image_path (str): Path to the input image.\n        target_size (tuple, optional): Target size for resizing the image.\n\n    Returns:\n        torch.Tensor: Preprocessed image tensor.\n    \"\"\"\n    image = Image.open(image_path).convert(\"RGB\")\n    transform = A.Compose(\n        [\n            A.Resize(target_size[0], target_size[1]),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            ToTensorV2(),\n        ]\n    )\n    image = np.array(image)\n    transformed = transform(image=image)\n    return transformed[\"image\"].unsqueeze(0)\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.segment_image","title":"<code>segment_image(image_path, model_path, target_size=(256, 256), device=device(type='cpu'))</code>","text":"<p>Segments the input image using the fine-tuned model.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the input image.</p> required <code>model_path</code> <code>str</code> <p>Path to the fine-tuned model.</p> required <code>target_size</code> <code>tuple</code> <p>Target size for resizing the image.</p> <code>(256, 256)</code> <code>device</code> <code>torch.device</code> <p>Device to perform inference on.</p> <code>device(type='cpu')</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Predicted segmentation mask.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def segment_image(\n    image_path: str,\n    model_path: str,\n    target_size: tuple = (256, 256),\n    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n) -&gt; np.ndarray:\n    \"\"\"\n    Segments the input image using the fine-tuned model.\n\n    Args:\n        image_path (str): Path to the input image.\n        model_path (str): Path to the fine-tuned model.\n        target_size (tuple, optional): Target size for resizing the image.\n        device (torch.device, optional): Device to perform inference on.\n\n    Returns:\n        np.ndarray: Predicted segmentation mask.\n    \"\"\"\n    model = load_model(model_path, device)\n    image = Image.open(image_path).convert(\"RGB\")\n    original_size = image.size\n    image_tensor = preprocess_image(image_path, target_size)\n    predictions = predict_image(model, image_tensor, original_size, device)\n    return predictions\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.train_model","title":"<code>train_model(train_dataset, val_dataset, pretrained_model='nvidia/segformer-b0-finetuned-ade-512-512', model_save_path='./model', output_dir='./results', num_epochs=10, batch_size=8, learning_rate=5e-05)</code>","text":"<p>Trains the model and saves the fine-tuned model to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataset</code> <code>Dataset</code> <p>Training dataset.</p> required <code>val_dataset</code> <code>Dataset</code> <p>Validation dataset.</p> required <code>pretrained_model</code> <code>str</code> <p>Pretrained model to fine-tune.</p> <code>'nvidia/segformer-b0-finetuned-ade-512-512'</code> <code>model_save_path</code> <code>str</code> <p>Path to save the fine-tuned model. Defaults to './model'.</p> <code>'./model'</code> <code>output_dir</code> <code>str</code> <p>Directory to save training outputs.</p> <code>'./results'</code> <code>num_epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>Batch size for training and evaluation.</p> <code>8</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for training.</p> <code>5e-05</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the saved fine-tuned model.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def train_model(\n    train_dataset: Dataset,\n    val_dataset: Dataset,\n    pretrained_model: str = \"nvidia/segformer-b0-finetuned-ade-512-512\",\n    model_save_path: str = \"./model\",\n    output_dir: str = \"./results\",\n    num_epochs: int = 10,\n    batch_size: int = 8,\n    learning_rate: float = 5e-5,\n) -&gt; str:\n    \"\"\"\n    Trains the model and saves the fine-tuned model to the specified path.\n\n    Args:\n        train_dataset (Dataset): Training dataset.\n        val_dataset (Dataset): Validation dataset.\n        pretrained_model (str, optional): Pretrained model to fine-tune.\n        model_save_path (str): Path to save the fine-tuned model. Defaults to './model'.\n        output_dir (str, optional): Directory to save training outputs.\n        num_epochs (int, optional): Number of training epochs.\n        batch_size (int, optional): Batch size for training and evaluation.\n        learning_rate (float, optional): Learning rate for training.\n\n    Returns:\n        str: Path to the saved fine-tuned model.\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = SegformerForSemanticSegmentation.from_pretrained(pretrained_model).to(\n        device\n    )\n    data_collator = DefaultDataCollator(return_tensors=\"pt\")\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=num_epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_dir=\"./logs\",\n        learning_rate=learning_rate,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n    )\n\n    trainer.train()\n    model.save_pretrained(model_save_path)\n    print(f\"Model saved to {model_save_path}\")\n    return model_save_path\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.visualize_predictions","title":"<code>visualize_predictions(image_path, segmented_mask, target_size=(256, 256), reference_image_path=None)</code>","text":"<p>Visualizes the original image, segmented mask, and optionally the reference image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the original image.</p> required <code>segmented_mask</code> <code>np.ndarray</code> <p>Predicted segmentation mask.</p> required <code>target_size</code> <code>tuple</code> <p>Target size for resizing images.</p> <code>(256, 256)</code> <code>reference_image_path</code> <code>str</code> <p>Path to the reference image.</p> <code>None</code> Source code in <code>geoai/segmentation.py</code> <pre><code>def visualize_predictions(\n    image_path: str,\n    segmented_mask: np.ndarray,\n    target_size: tuple = (256, 256),\n    reference_image_path: str = None,\n) -&gt; None:\n    \"\"\"\n    Visualizes the original image, segmented mask, and optionally the reference image.\n\n    Args:\n        image_path (str): Path to the original image.\n        segmented_mask (np.ndarray): Predicted segmentation mask.\n        target_size (tuple, optional): Target size for resizing images.\n        reference_image_path (str, optional): Path to the reference image.\n    \"\"\"\n    original_image = Image.open(image_path).convert(\"RGB\")\n    original_image = original_image.resize(target_size)\n    segmented_image = Image.fromarray((segmented_mask * 255).astype(np.uint8))\n\n    if reference_image_path:\n        reference_image = Image.open(reference_image_path).convert(\"RGB\")\n        reference_image = reference_image.resize(target_size)\n        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n        axes[1].imshow(reference_image)\n        axes[1].set_title(\"Reference Image\")\n        axes[1].axis(\"off\")\n    else:\n        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n    axes[0].imshow(original_image)\n    axes[0].set_title(\"Original Image\")\n    axes[0].axis(\"off\")\n\n    if reference_image_path:\n        axes[2].imshow(segmented_image, cmap=\"gray\")\n        axes[2].set_title(\"Segmented Image\")\n        axes[2].axis(\"off\")\n    else:\n        axes[1].imshow(segmented_image, cmap=\"gray\")\n        axes[1].set_title(\"Segmented Image\")\n        axes[1].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use geoai in a project:</p> <pre><code>import geoai\n</code></pre>"},{"location":"examples/download_data/","title":"Download data","text":"In\u00a0[\u00a0]: Copied! <pre>from geoai.download import (\n    download_naip,\n    download_overture_buildings,\n    extract_building_stats,\n)\n</pre> from geoai.download import (     download_naip,     download_overture_buildings,     extract_building_stats, ) In\u00a0[\u00a0]: Copied! <pre>bbox = (-117.6029, 47.65, -117.5936, 47.6563)\n\n# Download NAIP imagery for the specified region\ndownloaded_files = download_naip(\n    bbox=bbox,\n    output_dir=\"naip_data\",\n    max_items=5,\n    # year=2020,\n)\n\nprint(f\"Downloaded {len(downloaded_files)} files.\")\n</pre> bbox = (-117.6029, 47.65, -117.5936, 47.6563)  # Download NAIP imagery for the specified region downloaded_files = download_naip(     bbox=bbox,     output_dir=\"naip_data\",     max_items=5,     # year=2020, )  print(f\"Downloaded {len(downloaded_files)} files.\") In\u00a0[\u00a0]: Copied! <pre># Format: (min_lon, min_lat, max_lon, max_lat)\nbbox = (-117.6029, 47.65, -117.5936, 47.6563)\n\n# Download buildings\noutput_file = download_overture_buildings(\n    bbox=bbox,\n    output_file=\"buildings.geojson\",\n    output_format=\"geojson\",\n    data_type=\"building\",\n    verbose=True,\n)\n\nif output_file:\n    # Extract statistics\n    stats = extract_building_stats(output_file)\n</pre> # Format: (min_lon, min_lat, max_lon, max_lat) bbox = (-117.6029, 47.65, -117.5936, 47.6563)  # Download buildings output_file = download_overture_buildings(     bbox=bbox,     output_file=\"buildings.geojson\",     output_format=\"geojson\",     data_type=\"building\",     verbose=True, )  if output_file:     # Extract statistics     stats = extract_building_stats(output_file)"},{"location":"examples/download_data/#download-data","title":"Download Data\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/","title":"Lidar viz","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"leafmap[lidar]\" open3d\n</pre> # %pip install \"leafmap[lidar]\" open3d In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap <p>Download a sample LiDAR dataset from Google Drive. The zip file is 52.1 MB and the uncompressed LAS file is 109 MB.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://open.gishub.org/data/lidar/madison.zip\"\nfilename = \"madison.las\"\n</pre> url = \"https://open.gishub.org/data/lidar/madison.zip\" filename = \"madison.las\" In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(url, \"madison.zip\", unzip=True)\n</pre> leafmap.download_file(url, \"madison.zip\", unzip=True) <p>Read the LiDAR data</p> In\u00a0[\u00a0]: Copied! <pre>las = leafmap.read_lidar(filename)\n</pre> las = leafmap.read_lidar(filename) <p>The LAS header.</p> In\u00a0[\u00a0]: Copied! <pre>las.header\n</pre> las.header <p>The number of points.</p> In\u00a0[\u00a0]: Copied! <pre>las.header.point_count\n</pre> las.header.point_count <p>The list of features.</p> In\u00a0[\u00a0]: Copied! <pre>list(las.point_format.dimension_names)\n</pre> list(las.point_format.dimension_names) <p>Inspect data.</p> In\u00a0[\u00a0]: Copied! <pre>las.X\n</pre> las.X In\u00a0[\u00a0]: Copied! <pre>las.Y\n</pre> las.Y In\u00a0[\u00a0]: Copied! <pre>las.Z\n</pre> las.Z In\u00a0[\u00a0]: Copied! <pre>las.intensity\n</pre> las.intensity In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"pyvista\")\n</pre> leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"pyvista\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, backend=\"ipygany\", background=\"white\")\n</pre> leafmap.view_lidar(filename, backend=\"ipygany\", background=\"white\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"panel\", background=\"white\")\n</pre> leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"panel\", background=\"white\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, backend=\"open3d\")\n</pre> leafmap.view_lidar(filename, backend=\"open3d\") <p></p>"},{"location":"examples/dataviz/lidar_viz/#visualizing-lidar-data-with-leafmap","title":"Visualizing LiDAR Data with Leafmap\u00b6","text":"<p>This notebook demonstrates how to visualize LiDAR data using leafmap.</p>"},{"location":"examples/dataviz/lidar_viz/#installation","title":"Installation\u00b6","text":"<p>Uncomment and run the following cell to install the required Python packages.</p>"},{"location":"examples/dataviz/lidar_viz/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#download-data","title":"Download data\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#metadata","title":"Metadata\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#read-data","title":"Read data\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#pyvista","title":"PyVista\u00b6","text":"<p>Visualize LiDAR data using the pyvista backend.</p>"},{"location":"examples/dataviz/lidar_viz/#ipygany","title":"ipygany\u00b6","text":"<p>Visualize LiDAR data using the ipygany backend.</p>"},{"location":"examples/dataviz/lidar_viz/#panel","title":"Panel\u00b6","text":"<p>Visualize LiDAR data using the panel backend.</p>"},{"location":"examples/dataviz/lidar_viz/#open3d","title":"Open3D\u00b6","text":"<p>Visualize LiDAR data using the open3d backend.</p>"},{"location":"examples/dataviz/raster_viz/","title":"Raster viz","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"leafmap[raster]\"\n</pre> # %pip install \"leafmap[raster]\" In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nurl = \"https://opendata.digitalglobe.com/events/california-fire-2020/pre-event/2018-02-16/pine-gulch-fire20/1030010076004E00.tif\"\nm.add_cog_layer(url, name=\"Fire (pre-event)\")\nm\n</pre> m = leafmap.Map() url = \"https://opendata.digitalglobe.com/events/california-fire-2020/pre-event/2018-02-16/pine-gulch-fire20/1030010076004E00.tif\" m.add_cog_layer(url, name=\"Fire (pre-event)\") m <p>You can add multiple COGs to the map. Let's add another COG to the map.</p> In\u00a0[\u00a0]: Copied! <pre>url2 = \"https://opendata.digitalglobe.com/events/california-fire-2020/post-event/2020-08-14/pine-gulch-fire20/10300100AAC8DD00.tif\"\nm.add_cog_layer(url2, name=\"Fire (post-event)\")\nm\n</pre> url2 = \"https://opendata.digitalglobe.com/events/california-fire-2020/post-event/2020-08-14/pine-gulch-fire20/10300100AAC8DD00.tif\" m.add_cog_layer(url2, name=\"Fire (post-event)\") m <p></p> <p>Create a split map for comparing two COGs.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.split_map(left_layer=url, right_layer=url2)\nm\n</pre> m = leafmap.Map() m.split_map(left_layer=url, right_layer=url2) m <p></p> In\u00a0[\u00a0]: Copied! <pre>dem_url = \"https://open.gishub.org/data/raster/srtm90.tif\"\nleafmap.download_file(dem_url, unzip=False)\n</pre> dem_url = \"https://open.gishub.org/data/raster/srtm90.tif\" leafmap.download_file(dem_url, unzip=False) <p>Visualize a single-band raster.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"srtm90.tif\", cmap=\"terrain\", layer_name=\"DEM\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"srtm90.tif\", cmap=\"terrain\", layer_name=\"DEM\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>landsat_url = \"https://open.gishub.org/data/raster/cog.tif\"\nleafmap.download_file(landsat_url)\n</pre> landsat_url = \"https://open.gishub.org/data/raster/cog.tif\" leafmap.download_file(landsat_url) <p>Visualize a multi-band raster.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"cog.tif\", bands=[4, 3, 2], layer_name=\"Landsat\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"cog.tif\", bands=[4, 3, 2], layer_name=\"Landsat\") m <p></p> <p>Create an interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://canada-spot-ortho.s3.amazonaws.com/canada_spot_orthoimages/canada_spot5_orthoimages/S5_2007/S5_11055_6057_20070622/S5_11055_6057_20070622.json\"\nleafmap.stac_bands(url)\n</pre> url = \"https://canada-spot-ortho.s3.amazonaws.com/canada_spot_orthoimages/canada_spot5_orthoimages/S5_2007/S5_11055_6057_20070622/S5_11055_6057_20070622.json\" leafmap.stac_bands(url) <p>Add STAC layers to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_stac_layer(url, bands=[\"pan\"], name=\"Panchromatic\")\nm.add_stac_layer(url, bands=[\"B3\", \"B2\", \"B1\"], name=\"False color\")\nm\n</pre> m = leafmap.Map() m.add_stac_layer(url, bands=[\"pan\"], name=\"Panchromatic\") m.add_stac_layer(url, bands=[\"B3\", \"B2\", \"B1\"], name=\"False color\") m <p></p> <p>Provide custom STAC API endpoints as a dictionary in the format of <code>{\"name\": \"url\"}</code>. The name will show up in the dropdown menu, while the url is the STAC API endpoint that will be used to search for items.</p> In\u00a0[\u00a0]: Copied! <pre>catalogs = {\n    \"Element84 Earth Search\": \"https://earth-search.aws.element84.com/v1\",\n    \"Microsoft Planetary Computer\": \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n}\n</pre> catalogs = {     \"Element84 Earth Search\": \"https://earth-search.aws.element84.com/v1\",     \"Microsoft Planetary Computer\": \"https://planetarycomputer.microsoft.com/api/stac/v1\", } In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40, -100], zoom=4)\nm.set_catalog_source(catalogs)\nm.add_stac_gui()\nm\n</pre> m = leafmap.Map(center=[40, -100], zoom=4) m.set_catalog_source(catalogs) m.add_stac_gui() m <p>Once the catalog panel is open, you can search for items from the custom STAC API endpoints. Simply draw a bounding box on the map or zoom to a location of interest. Click on the Collections button to retrieve the collections from the custom STAC API endpoints. Next, select a collection from the dropdown menu. Then, click on the Items button to retrieve the items from the selected collection. Finally, click on the Display button to add the selected item to the map.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>m.stac_gdf  # The GeoDataFrame of the STAC search results\n</pre> m.stac_gdf  # The GeoDataFrame of the STAC search results In\u00a0[\u00a0]: Copied! <pre>m.stac_dict  # The STAC search results as a dictionary\n</pre> m.stac_dict  # The STAC search results as a dictionary In\u00a0[\u00a0]: Copied! <pre>m.stac_item  # The selected STAC item of the search result\n</pre> m.stac_item  # The selected STAC item of the search result <p>To Be able to run this notebook you'll need to have AWS credential available as environment variables. Uncomment the following lines to set the environment variables.</p> In\u00a0[\u00a0]: Copied! <pre># os.environ[\"AWS_ACCESS_KEY_ID\"] = \"YOUR AWS ACCESS ID HERE\"\n# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"YOUR AWS ACCESS KEY HERE\"\n</pre> # os.environ[\"AWS_ACCESS_KEY_ID\"] = \"YOUR AWS ACCESS ID HERE\" # os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"YOUR AWS ACCESS KEY HERE\" <p>In this example, we will use datasets from the Maxar Open Data Program on AWS.</p> In\u00a0[\u00a0]: Copied! <pre>BUCKET = \"maxar-opendata\"\nFOLDER = \"events/Kahramanmaras-turkey-earthquake-23/\"\n</pre> BUCKET = \"maxar-opendata\" FOLDER = \"events/Kahramanmaras-turkey-earthquake-23/\" <p>List all the datasets in the bucket. Specify a file extension to filter the results if needed.</p> In\u00a0[\u00a0]: Copied! <pre>items = leafmap.s3_list_objects(BUCKET, FOLDER, ext=\".tif\")\nitems[:10]\n</pre> items = leafmap.s3_list_objects(BUCKET, FOLDER, ext=\".tif\") items[:10] <p>Visualize raster datasets from the bucket.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_cog_layer(items[2], name=\"Maxar\")\nm\n</pre> m = leafmap.Map() m.add_cog_layer(items[2], name=\"Maxar\") m <p></p>"},{"location":"examples/dataviz/raster_viz/#visualizing-raster-data-with-leafmap","title":"Visualizing Raster Data with Leafmap\u00b6","text":"<p>This notebook demonstrates how to visualize raster data using leafmap. Leafmap can visualize raster data (e.g., Cloud Optimized GeoTIFF) stored in a local file or on the cloud (e.g., AWS S3). It can also visualize raster data stored in a STAC catalog.</p>"},{"location":"examples/dataviz/raster_viz/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install the required packages if needed.</p>"},{"location":"examples/dataviz/raster_viz/#import-packages","title":"Import packages\u00b6","text":""},{"location":"examples/dataviz/raster_viz/#cog","title":"COG\u00b6","text":"<p>A Cloud Optimized GeoTIFF (COG) is a regular GeoTIFF file, aimed at being hosted on a HTTP file server, with an internal organization that enables more efficient workflows on the cloud. It does this by leveraging the ability of clients issuing HTTP GET range requests to ask for just the parts of a file they need. More information about COG can be found at https://www.cogeo.org/in-depth.html</p> <p>For this demo, we will use data from https://www.maxar.com/open-data/california-colorado-fires for mapping California and Colorado fires. Let's create an interactive map and add the COG to the map.</p>"},{"location":"examples/dataviz/raster_viz/#local-raster","title":"Local Raster\u00b6","text":"<p>Leafmap can also visualize local GeoTIFF files. Let's download some sample data</p>"},{"location":"examples/dataviz/raster_viz/#stac","title":"STAC\u00b6","text":"<p>The SpatioTemporal Asset Catalog (STAC) specification provides a common language to describe a range of geospatial information so that it can more easily be indexed and discovered. A SpatioTemporal Asset is any file that represents information about the earth captured in a certain space and time. STAC aims to enable that next generation of geospatial search engines, while also supporting web best practices so geospatial information is more easily surfaced in traditional search engines. More information about STAC can be found at the STAC website. In this example, we will use a STAC item from the SPOT Orthoimages of Canada available through the link below:</p>"},{"location":"examples/dataviz/raster_viz/#custom-stac-catalog","title":"Custom STAC Catalog\u00b6","text":""},{"location":"examples/dataviz/raster_viz/#aws-s3","title":"AWS S3\u00b6","text":""},{"location":"examples/dataviz/vector_viz/","title":"Vector viz","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"leafmap[vector]\"\n</pre> # %pip install \"leafmap[vector]\" In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[0, 0], zoom=2)\ndata = \"https://open.gishub.org/data/vector/cables.geojson\"\nm.add_vector(data, layer_name=\"Cable lines\", info_mode=\"on_hover\")\nm\n</pre> m = leafmap.Map(center=[0, 0], zoom=2) data = \"https://open.gishub.org/data/vector/cables.geojson\" m.add_vector(data, layer_name=\"Cable lines\", info_mode=\"on_hover\") m <p>You can style the vector with custom style callback functions.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[20, 0], zoom=2)\nm.add_basemap(\"CartoDB.DarkMatter\")\ndata = \"https://open.gishub.org/data/vector/cables.geojson\"\ncallback = lambda feat: {\"color\": feat[\"properties\"][\"color\"], \"weight\": 1}\nm.add_vector(data, layer_name=\"Cable lines\", style_callback=callback)\nm\n</pre> m = leafmap.Map(center=[20, 0], zoom=2) m.add_basemap(\"CartoDB.DarkMatter\") data = \"https://open.gishub.org/data/vector/cables.geojson\" callback = lambda feat: {\"color\": feat[\"properties\"][\"color\"], \"weight\": 1} m.add_vector(data, layer_name=\"Cable lines\", style_callback=callback) m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\ndata = \"https://raw.githubusercontent.com/opengeos/leafmap/master/docs/examples/data/countries.geojson\"\nm.add_data(\n    data, column=\"POP_EST\", scheme=\"Quantiles\", cmap=\"Blues\", legend_title=\"Population\"\n)\nm\n</pre> m = leafmap.Map() data = \"https://raw.githubusercontent.com/opengeos/leafmap/master/docs/examples/data/countries.geojson\" m.add_data(     data, column=\"POP_EST\", scheme=\"Quantiles\", cmap=\"Blues\", legend_title=\"Population\" ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_data(\n    data,\n    column=\"POP_EST\",\n    scheme=\"EqualInterval\",\n    cmap=\"Blues\",\n    legend_title=\"Population\",\n)\nm\n</pre> m = leafmap.Map() m.add_data(     data,     column=\"POP_EST\",     scheme=\"EqualInterval\",     cmap=\"Blues\",     legend_title=\"Population\", ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://open.gishub.org/data/duckdb/cities.parquet\"\ngdf = leafmap.read_parquet(url, return_type=\"gdf\", src_crs=\"EPSG:4326\")\ngdf.head()\n</pre> url = \"https://open.gishub.org/data/duckdb/cities.parquet\" gdf = leafmap.read_parquet(url, return_type=\"gdf\", src_crs=\"EPSG:4326\") gdf.head() <p>Visualize point data.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_vector(\n    gdf,\n    get_radius=20000,\n    get_fill_color=\"blue\",\n    zoom_to_layer=False,\n    map_args={\"center\": (40, -100), \"zoom\": 3, \"height\": 500},\n)\n</pre> leafmap.view_vector(     gdf,     get_radius=20000,     get_fill_color=\"blue\",     zoom_to_layer=False,     map_args={\"center\": (40, -100), \"zoom\": 3, \"height\": 500}, ) <p></p> <p>Visualizing polygon data.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://data.source.coop/giswqs/nwi/wetlands/DC_Wetlands.parquet\"\ngdf = leafmap.read_parquet(\n    url, return_type=\"gdf\", src_crs=\"EPSG:5070\", dst_crs=\"EPSG:4326\"\n)\ngdf.head()\n</pre> url = \"https://data.source.coop/giswqs/nwi/wetlands/DC_Wetlands.parquet\" gdf = leafmap.read_parquet(     url, return_type=\"gdf\", src_crs=\"EPSG:5070\", dst_crs=\"EPSG:4326\" ) gdf.head() In\u00a0[\u00a0]: Copied! <pre>leafmap.view_vector(gdf, get_fill_color=[0, 0, 255, 128])\n</pre> leafmap.view_vector(gdf, get_fill_color=[0, 0, 255, 128]) <p></p> <p>Alternatively, you can specify a color map to visualize the data.</p> In\u00a0[\u00a0]: Copied! <pre>color_map = {\n    \"Freshwater Forested/Shrub Wetland\": (0, 136, 55),\n    \"Freshwater Emergent Wetland\": (127, 195, 28),\n    \"Freshwater Pond\": (104, 140, 192),\n    \"Estuarine and Marine Wetland\": (102, 194, 165),\n    \"Riverine\": (1, 144, 191),\n    \"Lake\": (19, 0, 124),\n    \"Estuarine and Marine Deepwater\": (0, 124, 136),\n    \"Other\": (178, 134, 86),\n}\n</pre> color_map = {     \"Freshwater Forested/Shrub Wetland\": (0, 136, 55),     \"Freshwater Emergent Wetland\": (127, 195, 28),     \"Freshwater Pond\": (104, 140, 192),     \"Estuarine and Marine Wetland\": (102, 194, 165),     \"Riverine\": (1, 144, 191),     \"Lake\": (19, 0, 124),     \"Estuarine and Marine Deepwater\": (0, 124, 136),     \"Other\": (178, 134, 86), } In\u00a0[\u00a0]: Copied! <pre>leafmap.view_vector(gdf, color_column=\"WETLAND_TYPE\", color_map=color_map, opacity=0.5)\n</pre> leafmap.view_vector(gdf, color_column=\"WETLAND_TYPE\", color_map=color_map, opacity=0.5) <p></p> <p>Display a legend for the data.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.Legend(title=\"Wetland Type\", legend_dict=color_map)\n</pre> leafmap.Legend(title=\"Wetland Type\", legend_dict=color_map) <p></p> In\u00a0[\u00a0]: Copied! <pre>import leafmap.foliumap as leafmap\n</pre> import leafmap.foliumap as leafmap <p>Check the metadata of the PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://storage.googleapis.com/ahp-research/overture/pmtiles/overture.pmtiles\"\nmetadata = leafmap.pmtiles_metadata(url)\nprint(f\"layer names: {metadata['layer_names']}\")\nprint(f\"bounds: {metadata['bounds']}\")\n</pre> url = \"https://storage.googleapis.com/ahp-research/overture/pmtiles/overture.pmtiles\" metadata = leafmap.pmtiles_metadata(url) print(f\"layer names: {metadata['layer_names']}\") print(f\"bounds: {metadata['bounds']}\") <p>Visualize the PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_basemap(\"CartoDB.DarkMatter\")\n\nstyle = {\n    \"version\": 8,\n    \"sources\": {\n        \"example_source\": {\n            \"type\": \"vector\",\n            \"url\": \"pmtiles://\" + url,\n            \"attribution\": \"PMTiles\",\n        }\n    },\n    \"layers\": [\n        {\n            \"id\": \"admins\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"admins\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#BDD3C7\", \"fill-opacity\": 0.1},\n        },\n        {\n            \"id\": \"buildings\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"buildings\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#FFFFB3\", \"fill-opacity\": 0.5},\n        },\n        {\n            \"id\": \"places\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"places\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#BEBADA\", \"fill-opacity\": 0.5},\n        },\n        {\n            \"id\": \"roads\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"roads\",\n            \"type\": \"line\",\n            \"paint\": {\"line-color\": \"#FB8072\"},\n        },\n    ],\n}\n\nm.add_pmtiles(\n    url, name=\"PMTiles\", style=style, overlay=True, show=True, zoom_to_layer=True\n)\n\nlegend_dict = {\n    \"admins\": \"BDD3C7\",\n    \"buildings\": \"FFFFB3\",\n    \"places\": \"BEBADA\",\n    \"roads\": \"FB8072\",\n}\n\nm.add_legend(legend_dict=legend_dict)\nm\n</pre> m = leafmap.Map() m.add_basemap(\"CartoDB.DarkMatter\")  style = {     \"version\": 8,     \"sources\": {         \"example_source\": {             \"type\": \"vector\",             \"url\": \"pmtiles://\" + url,             \"attribution\": \"PMTiles\",         }     },     \"layers\": [         {             \"id\": \"admins\",             \"source\": \"example_source\",             \"source-layer\": \"admins\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#BDD3C7\", \"fill-opacity\": 0.1},         },         {             \"id\": \"buildings\",             \"source\": \"example_source\",             \"source-layer\": \"buildings\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#FFFFB3\", \"fill-opacity\": 0.5},         },         {             \"id\": \"places\",             \"source\": \"example_source\",             \"source-layer\": \"places\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#BEBADA\", \"fill-opacity\": 0.5},         },         {             \"id\": \"roads\",             \"source\": \"example_source\",             \"source-layer\": \"roads\",             \"type\": \"line\",             \"paint\": {\"line-color\": \"#FB8072\"},         },     ], }  m.add_pmtiles(     url, name=\"PMTiles\", style=style, overlay=True, show=True, zoom_to_layer=True )  legend_dict = {     \"admins\": \"BDD3C7\",     \"buildings\": \"FFFFB3\",     \"places\": \"BEBADA\",     \"roads\": \"FB8072\", }  m.add_legend(legend_dict=legend_dict) m <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://data.source.coop/vida/google-microsoft-open-buildings/pmtiles/go_ms_building_footprints.pmtiles\"\nmetadata = leafmap.pmtiles_metadata(url)\nprint(f\"layer names: {metadata['layer_names']}\")\nprint(f\"bounds: {metadata['bounds']}\")\n</pre> url = \"https://data.source.coop/vida/google-microsoft-open-buildings/pmtiles/go_ms_building_footprints.pmtiles\" metadata = leafmap.pmtiles_metadata(url) print(f\"layer names: {metadata['layer_names']}\") print(f\"bounds: {metadata['bounds']}\") <p>Visualize the PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[20, 0], zoom=2)\nm.add_basemap(\"CartoDB.DarkMatter\")\nm.add_basemap(\"Esri.WorldImagery\", show=False)\n\nstyle = {\n    \"version\": 8,\n    \"sources\": {\n        \"example_source\": {\n            \"type\": \"vector\",\n            \"url\": \"pmtiles://\" + url,\n            \"attribution\": \"PMTiles\",\n        }\n    },\n    \"layers\": [\n        {\n            \"id\": \"buildings\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"building_footprints\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},\n        },\n    ],\n}\n\nm.add_pmtiles(\n    url, name=\"Buildings\", style=style, overlay=True, show=True, zoom_to_layer=False\n)\n\nm\n</pre> m = leafmap.Map(center=[20, 0], zoom=2) m.add_basemap(\"CartoDB.DarkMatter\") m.add_basemap(\"Esri.WorldImagery\", show=False)  style = {     \"version\": 8,     \"sources\": {         \"example_source\": {             \"type\": \"vector\",             \"url\": \"pmtiles://\" + url,             \"attribution\": \"PMTiles\",         }     },     \"layers\": [         {             \"id\": \"buildings\",             \"source\": \"example_source\",             \"source-layer\": \"building_footprints\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},         },     ], }  m.add_pmtiles(     url, name=\"Buildings\", style=style, overlay=True, show=True, zoom_to_layer=False )  m <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://raw.githubusercontent.com/opengeos/open-data/main/datasets/libya/Derna_buildings.geojson\"\nleafmap.download_file(url, \"buildings.geojson\")\n</pre> url = \"https://raw.githubusercontent.com/opengeos/open-data/main/datasets/libya/Derna_buildings.geojson\" leafmap.download_file(url, \"buildings.geojson\") <p>Convert vector to PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>pmtiles = \"buildings.pmtiles\"\nleafmap.geojson_to_pmtiles(\n    \"buildings.geojson\", pmtiles, layer_name=\"buildings\", overwrite=True, quiet=True\n)\n</pre> pmtiles = \"buildings.pmtiles\" leafmap.geojson_to_pmtiles(     \"buildings.geojson\", pmtiles, layer_name=\"buildings\", overwrite=True, quiet=True ) <p>Start a HTTP Sever</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.start_server(port=8000)\n</pre> leafmap.start_server(port=8000) In\u00a0[\u00a0]: Copied! <pre>url = f\"http://127.0.0.1:8000/{pmtiles}\"\nleafmap.pmtiles_metadata(url)\n</pre> url = f\"http://127.0.0.1:8000/{pmtiles}\" leafmap.pmtiles_metadata(url) <p>Display the PMTiles on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\n\nstyle = {\n    \"version\": 8,\n    \"sources\": {\n        \"example_source\": {\n            \"type\": \"vector\",\n            \"url\": \"pmtiles://\" + url,\n            \"attribution\": \"PMTiles\",\n        }\n    },\n    \"layers\": [\n        {\n            \"id\": \"buildings\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"buildings\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},\n        },\n    ],\n}\n\nm.add_pmtiles(url, name=\"Buildings\", show=True, zoom_to_layer=True, style=style)\nm\n</pre> m = leafmap.Map()  style = {     \"version\": 8,     \"sources\": {         \"example_source\": {             \"type\": \"vector\",             \"url\": \"pmtiles://\" + url,             \"attribution\": \"PMTiles\",         }     },     \"layers\": [         {             \"id\": \"buildings\",             \"source\": \"example_source\",             \"source-layer\": \"buildings\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},         },     ], }  m.add_pmtiles(url, name=\"Buildings\", show=True, zoom_to_layer=True, style=style) m <p></p>"},{"location":"examples/dataviz/vector_viz/#visualizing-vector-data-with-leafmap","title":"Visualizing Vector Data with Leafmap\u00b6","text":""},{"location":"examples/dataviz/vector_viz/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install leafmap if needed.</p>"},{"location":"examples/dataviz/vector_viz/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/dataviz/vector_viz/#visualize-vector-data","title":"Visualize vector data\u00b6","text":"<p>You can visualize vector data using the <code>add_vector</code> function. It supports common vector data formats, including GeoJSON, Shapefile, GeoPackage, and any other formats supported by geopandas.</p>"},{"location":"examples/dataviz/vector_viz/#choropleth-map","title":"Choropleth map\u00b6","text":"<p>You can create a choropleth map using the <code>add_data</code> function. It supports GeoJSON, Shapefile, GeoPackage, and any other formats supported by geopandas.</p>"},{"location":"examples/dataviz/vector_viz/#geoparquet","title":"GeoParquet\u00b6","text":"<p>Visualize GeoParquet data with leafmap and lonboard.</p>"},{"location":"examples/dataviz/vector_viz/#pmtiles","title":"PMTiles\u00b6","text":"<p>PMTiles is a single-file archive format for tiled data. A PMTiles archive can be hosted on a commodity storage platform such as S3, and enables low-cost, zero-maintenance map applications that are \"serverless\" - free of a custom tile backend or third party provider.</p>"},{"location":"examples/dataviz/vector_viz/#remote-pmtiles","title":"Remote PMTiles\u00b6","text":"<p>Leafmap can visualize PMTiles hosted locally or remotely.</p>"},{"location":"examples/dataviz/vector_viz/#overture-data","title":"Overture data\u00b6","text":""},{"location":"examples/dataviz/vector_viz/#source-cooperative","title":"Source Cooperative\u00b6","text":"<p>Visualize the Google-Microsoft Open Buildings data hosted on Source Cooperative.</p> <p>Check the metadata of the PMTiles.</p>"},{"location":"examples/dataviz/vector_viz/#local-pmtiles","title":"Local PMTiles\u00b6","text":"<p>tippecanoe is required to convert vector data to pmtiles. Install it with <code>conda install -c conda-forge tippecanoe</code>.</p> <p>Download building footprints of Derna, Libya.</p>"},{"location":"examples/rastervision/semantic_segmentation/","title":"Semantic segmentation","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport torch\nfrom matplotlib import pyplot as plt\n\nfrom rastervision.core.data import ClassConfig, SemanticSegmentationLabels\n\nimport albumentations as A\n\nfrom rastervision.pytorch_learner import (\n    SemanticSegmentationRandomWindowGeoDataset,\n    SemanticSegmentationSlidingWindowGeoDataset,\n    SemanticSegmentationVisualizer,\n    SemanticSegmentationGeoDataConfig,\n    SemanticSegmentationLearnerConfig,\n    SolverConfig,\n    SemanticSegmentationLearner,\n)\n</pre> import os import torch from matplotlib import pyplot as plt  from rastervision.core.data import ClassConfig, SemanticSegmentationLabels  import albumentations as A  from rastervision.pytorch_learner import (     SemanticSegmentationRandomWindowGeoDataset,     SemanticSegmentationSlidingWindowGeoDataset,     SemanticSegmentationVisualizer,     SemanticSegmentationGeoDataConfig,     SemanticSegmentationLearnerConfig,     SolverConfig,     SemanticSegmentationLearner, ) In\u00a0[\u00a0]: Copied! <pre>os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\"\n</pre> os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\" In\u00a0[\u00a0]: Copied! <pre>class_config = ClassConfig(\n    names=[\"background\", \"building\"],\n    colors=[\"lightgray\", \"darkred\"],\n    null_class=\"background\",\n)\n\nviz = SemanticSegmentationVisualizer(\n    class_names=class_config.names, class_colors=class_config.colors\n)\n</pre> class_config = ClassConfig(     names=[\"background\", \"building\"],     colors=[\"lightgray\", \"darkred\"],     null_class=\"background\", )  viz = SemanticSegmentationVisualizer(     class_names=class_config.names, class_colors=class_config.colors ) In\u00a0[\u00a0]: Copied! <pre>train_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/images/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13.tif\"\ntrain_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/labels/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13_Buildings.geojson\"\n\nval_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\"\nval_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\"\n</pre> train_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/images/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13.tif\" train_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/labels/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13_Buildings.geojson\"  val_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\" val_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\" In\u00a0[\u00a0]: Copied! <pre>pred_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\"\npred_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\"\n</pre> pred_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\" pred_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\" In\u00a0[\u00a0]: Copied! <pre>data_augmentation_transform = A.Compose(\n    [\n        A.Flip(),\n        A.ShiftScaleRotate(),\n        A.OneOf(\n            [\n                A.HueSaturationValue(hue_shift_limit=10),\n                A.RGBShift(),\n                A.ToGray(),\n                A.ToSepia(),\n                A.RandomBrightness(),\n                A.RandomGamma(),\n            ]\n        ),\n        A.CoarseDropout(max_height=32, max_width=32, max_holes=5),\n    ]\n)\n</pre> data_augmentation_transform = A.Compose(     [         A.Flip(),         A.ShiftScaleRotate(),         A.OneOf(             [                 A.HueSaturationValue(hue_shift_limit=10),                 A.RGBShift(),                 A.ToGray(),                 A.ToSepia(),                 A.RandomBrightness(),                 A.RandomGamma(),             ]         ),         A.CoarseDropout(max_height=32, max_width=32, max_holes=5),     ] ) In\u00a0[\u00a0]: Copied! <pre>train_ds = SemanticSegmentationRandomWindowGeoDataset.from_uris(\n    class_config=class_config,\n    image_uri=train_image_uri,\n    label_vector_uri=train_label_uri,\n    label_vector_default_class_id=class_config.get_class_id(\"building\"),\n    size_lims=(150, 200),\n    out_size=256,\n    max_windows=400,\n    transform=data_augmentation_transform,\n)\n\nlen(train_ds)\n</pre> train_ds = SemanticSegmentationRandomWindowGeoDataset.from_uris(     class_config=class_config,     image_uri=train_image_uri,     label_vector_uri=train_label_uri,     label_vector_default_class_id=class_config.get_class_id(\"building\"),     size_lims=(150, 200),     out_size=256,     max_windows=400,     transform=data_augmentation_transform, )  len(train_ds) In\u00a0[\u00a0]: Copied! <pre>x, y = viz.get_batch(train_ds, 4)\nviz.plot_batch(x, y, show=True)\n</pre> x, y = viz.get_batch(train_ds, 4) viz.plot_batch(x, y, show=True) In\u00a0[\u00a0]: Copied! <pre>val_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(\n    class_config=class_config,\n    image_uri=val_image_uri,\n    label_vector_uri=val_label_uri,\n    label_vector_default_class_id=class_config.get_class_id(\"building\"),\n    size=200,\n    stride=100,\n    transform=A.Resize(256, 256),\n)\nlen(val_ds)\n</pre> val_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(     class_config=class_config,     image_uri=val_image_uri,     label_vector_uri=val_label_uri,     label_vector_default_class_id=class_config.get_class_id(\"building\"),     size=200,     stride=100,     transform=A.Resize(256, 256), ) len(val_ds) In\u00a0[\u00a0]: Copied! <pre>x, y = viz.get_batch(val_ds, 4)\nviz.plot_batch(x, y, show=True)\n</pre> x, y = viz.get_batch(val_ds, 4) viz.plot_batch(x, y, show=True) In\u00a0[\u00a0]: Copied! <pre>pred_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(\n    class_config=class_config,\n    image_uri=pred_image_uri,\n    size=200,\n    stride=100,\n    transform=A.Resize(256, 256),\n)\nlen(pred_ds)\n</pre> pred_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(     class_config=class_config,     image_uri=pred_image_uri,     size=200,     stride=100,     transform=A.Resize(256, 256), ) len(pred_ds) In\u00a0[\u00a0]: Copied! <pre>model = torch.hub.load(\n    \"AdeelH/pytorch-fpn:0.3\",\n    \"make_fpn_resnet\",\n    name=\"resnet18\",\n    fpn_type=\"panoptic\",\n    num_classes=len(class_config),\n    fpn_channels=128,\n    in_channels=3,\n    out_size=(256, 256),\n    pretrained=True,\n)\n</pre> model = torch.hub.load(     \"AdeelH/pytorch-fpn:0.3\",     \"make_fpn_resnet\",     name=\"resnet18\",     fpn_type=\"panoptic\",     num_classes=len(class_config),     fpn_channels=128,     in_channels=3,     out_size=(256, 256),     pretrained=True, ) In\u00a0[\u00a0]: Copied! <pre>data_cfg = SemanticSegmentationGeoDataConfig(\n    class_names=class_config.names,\n    class_colors=class_config.colors,\n    num_workers=0,  # increase to use multi-processing\n)\n</pre> data_cfg = SemanticSegmentationGeoDataConfig(     class_names=class_config.names,     class_colors=class_config.colors,     num_workers=0,  # increase to use multi-processing ) In\u00a0[\u00a0]: Copied! <pre>solver_cfg = SolverConfig(batch_sz=8, lr=3e-2, class_loss_weights=[1.0, 10.0])\n</pre> solver_cfg = SolverConfig(batch_sz=8, lr=3e-2, class_loss_weights=[1.0, 10.0]) In\u00a0[\u00a0]: Copied! <pre>learner_cfg = SemanticSegmentationLearnerConfig(data=data_cfg, solver=solver_cfg)\n</pre> learner_cfg = SemanticSegmentationLearnerConfig(data=data_cfg, solver=solver_cfg) In\u00a0[\u00a0]: Copied! <pre>learner = SemanticSegmentationLearner(\n    cfg=learner_cfg,\n    output_dir=\"./train-demo/\",\n    model=model,\n    train_ds=train_ds,\n    valid_ds=val_ds,\n)\n</pre> learner = SemanticSegmentationLearner(     cfg=learner_cfg,     output_dir=\"./train-demo/\",     model=model,     train_ds=train_ds,     valid_ds=val_ds, ) In\u00a0[\u00a0]: Copied! <pre>learner.log_data_stats()\n</pre> learner.log_data_stats() In\u00a0[\u00a0]: Copied! <pre>%load_ext tensorboard\n</pre> %load_ext tensorboard In\u00a0[\u00a0]: Copied! <pre>%tensorboard --bind_all --logdir \"./train-demo/tb-logs\" --reload_interval 10\n</pre> %tensorboard --bind_all --logdir \"./train-demo/tb-logs\" --reload_interval 10 In\u00a0[\u00a0]: Copied! <pre>learner.train(epochs=3)\n</pre> learner.train(epochs=3) In\u00a0[\u00a0]: Copied! <pre>learner.train(epochs=1)\n</pre> learner.train(epochs=1) In\u00a0[\u00a0]: Copied! <pre>learner.plot_predictions(split=\"valid\", show=True)\n</pre> learner.plot_predictions(split=\"valid\", show=True) In\u00a0[\u00a0]: Copied! <pre>learner.save_model_bundle()\n</pre> learner.save_model_bundle() In\u00a0[\u00a0]: Copied! <pre>learner = SemanticSegmentationLearner.from_model_bundle(\n    model_bundle_uri=\"./train-demo/model-bundle.zip\",\n    output_dir=\"./train-demo/\",\n    model=model,\n)\n</pre> learner = SemanticSegmentationLearner.from_model_bundle(     model_bundle_uri=\"./train-demo/model-bundle.zip\",     output_dir=\"./train-demo/\",     model=model, ) In\u00a0[\u00a0]: Copied! <pre>learner = SemanticSegmentationLearner.from_model_bundle(\n    model_bundle_uri=\"./train-demo/model-bundle.zip\",\n    output_dir=\"./train-demo/\",\n    model=model,\n    train_ds=train_ds,\n    valid_ds=val_ds,\n    training=True,\n)\n</pre> learner = SemanticSegmentationLearner.from_model_bundle(     model_bundle_uri=\"./train-demo/model-bundle.zip\",     output_dir=\"./train-demo/\",     model=model,     train_ds=train_ds,     valid_ds=val_ds,     training=True, ) In\u00a0[\u00a0]: Copied! <pre>learner.train(epochs=1)\n</pre> learner.train(epochs=1) In\u00a0[\u00a0]: Copied! <pre>learner.plot_predictions(split=\"valid\", show=True)\n</pre> learner.plot_predictions(split=\"valid\", show=True) In\u00a0[\u00a0]: Copied! <pre>predictions = learner.predict_dataset(\n    pred_ds,\n    raw_out=True,\n    numpy_out=True,\n    predict_kw=dict(out_shape=(325, 325)),\n    progress_bar=True,\n)\n</pre> predictions = learner.predict_dataset(     pred_ds,     raw_out=True,     numpy_out=True,     predict_kw=dict(out_shape=(325, 325)),     progress_bar=True, ) In\u00a0[\u00a0]: Copied! <pre>pred_labels = SemanticSegmentationLabels.from_predictions(\n    pred_ds.windows,\n    predictions,\n    smooth=True,\n    extent=pred_ds.scene.extent,\n    num_classes=len(class_config),\n)\n</pre> pred_labels = SemanticSegmentationLabels.from_predictions(     pred_ds.windows,     predictions,     smooth=True,     extent=pred_ds.scene.extent,     num_classes=len(class_config), ) In\u00a0[\u00a0]: Copied! <pre>scores = pred_labels.get_score_arr(pred_labels.extent)\n</pre> scores = pred_labels.get_score_arr(pred_labels.extent) In\u00a0[\u00a0]: Copied! <pre>pred_labels.save(\n    uri=f\"predict\",\n    crs_transformer=pred_ds.scene.raster_source.crs_transformer,\n    class_config=class_config,\n)\n</pre> pred_labels.save(     uri=f\"predict\",     crs_transformer=pred_ds.scene.raster_source.crs_transformer,     class_config=class_config, )"},{"location":"examples/samgeo/arcgis/","title":"Arcgis","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo\n\n%matplotlib inline\n</pre> import os import leafmap from samgeo import SamGeo  %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>workspace = os.path.dirname(arcpy.env.workspace)\nos.chdir(workspace)\narcpy.env.overwriteOutput = True\n</pre> workspace = os.path.dirname(arcpy.env.workspace) os.chdir(workspace) arcpy.env.overwriteOutput = True In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/buildings.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/buildings.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/agriculture.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/agriculture.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/water.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/water.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>image = \"agriculture.tif\"\n</pre> image = \"agriculture.tif\" <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p> In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"ag_masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"ag_masks.tif\", foreground=True, unique=True) <p>If you run into GPU memory errors, uncomment the following code block and run it to empty cuda cache then rerun the code block above.</p> In\u00a0[\u00a0]: Copied! <pre># sam.clear_cuda_cache()\n</pre> # sam.clear_cuda_cache() <p>Show the segmentation result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"ag_annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"ag_annotations.tif\") <p>Add layers to ArcGIS Pro.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.arc_active_map()\n</pre> m = leafmap.arc_active_map() In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"agriculture.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"agriculture.tif\")) In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"ag_annotations.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"ag_annotations.tif\")) <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>in_raster = os.path.join(workspace, \"ag_masks.tif\")\nout_shp = os.path.join(workspace, \"ag_masks.shp\")\n</pre> in_raster = os.path.join(workspace, \"ag_masks.tif\") out_shp = os.path.join(workspace, \"ag_masks.shp\") In\u00a0[\u00a0]: Copied! <pre>arcpy.conversion.RasterToPolygon(in_raster, out_shp)\n</pre> arcpy.conversion.RasterToPolygon(in_raster, out_shp) In\u00a0[\u00a0]: Copied! <pre>image = \"water.tif\"\n</pre> image = \"water.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"water_masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"water_masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre># sam.clear_cuda_cache()\n</pre> # sam.clear_cuda_cache() In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"water_annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"water_annotations.tif\") In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"water.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"water.tif\")) In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"water_annotations.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"water_annotations.tif\")) In\u00a0[\u00a0]: Copied! <pre>in_raster = os.path.join(workspace, \"water_masks.tif\")\nout_shp = os.path.join(workspace, \"water_masks.shp\")\n</pre> in_raster = os.path.join(workspace, \"water_masks.tif\") out_shp = os.path.join(workspace, \"water_masks.shp\") In\u00a0[\u00a0]: Copied! <pre>arcpy.conversion.RasterToPolygon(in_raster, out_shp)\n</pre> arcpy.conversion.RasterToPolygon(in_raster, out_shp) In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(\"agriculture.tif\", output=\"ag_masks2.tif\", foreground=True)\n</pre> sam.generate(\"agriculture.tif\", output=\"ag_masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=0.5, output=\"ag_annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=0.5, output=\"ag_annotations2.tif\")"},{"location":"examples/samgeo/arcgis/#using-the-segment-geospatial-python-package-with-arcgis-pro","title":"Using the Segment-Geospatial Python Package with ArcGIS Pro\u00b6","text":"<p>The notebook shows step-by-step instructions for using the Segment Anything Model (SAM) with ArcGIS Pro. Check out the YouTube tutorial here and the Resources for Unlocking the Power of Deep Learning Applications Using ArcGIS. Credit goes to Esri.</p> <p></p>"},{"location":"examples/samgeo/arcgis/#installation","title":"Installation\u00b6","text":"<ol> <li><p>Open Windows Registry Editor (<code>regedit.exe</code>) and navigate to <code>Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem</code>. Change the value of <code>LongPathsEnabled</code> to <code>1</code>. See this screenshot. This is a known issue with the deep learning libraries for ArcGIS Pro 3.1. A future release might fix this issue.</p> </li> <li><p>Navigate to the Start Menu -&gt; All apps -&gt; ArcGIS folder, then open the Python Command Prompt.</p> </li> <li><p>Create a new conda environment and install mamba and Python 3.9.x from the Esri Anaconda channel. Mamba is a drop-in replacement for conda that is mach faster for installing Python packages and their dependencies.</p> <p><code>conda create conda-forge::mamba esri::python --name samgeo</code></p> </li> <li><p>Activate the new conda environment.</p> <p><code>conda activate samgeo</code></p> </li> <li><p>Install arcpy, deep-learning-essentials, segment-geospatial, and other dependencies (~4GB download).</p> <p><code>mamba install arcpy deep-learning-essentials leafmap localtileserver segment-geospatial -c esri -c conda-forge</code></p> </li> <li><p>Activate the new environment in ArcGIS Pro.</p> <p><code>proswap samgeo</code></p> </li> <li><p>Close the Python Command Prompt and open ArcGIS Pro.</p> </li> <li><p>Download this notebook and run it in ArcGIS Pro.</p> </li> </ol>"},{"location":"examples/samgeo/arcgis/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/samgeo/arcgis/#download-sample-data","title":"Download sample data\u00b6","text":"<p>In this example, we will use the high-resolution aerial imagery from the USDA National Agricultural Imagery Program (NAIP). You can download NAIP imagery using the USDA Data Gateway or the USDA NCRS Box Drive. I have downloaded some NAIP imagery and clipped them to a smaller area, which are available here.</p>"},{"location":"examples/samgeo/arcgis/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/arcgis/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Specify the file path to the image we downloaded earlier.</p>"},{"location":"examples/samgeo/arcgis/#segment-waterbodies","title":"Segment waterbodies\u00b6","text":""},{"location":"examples/samgeo/arcgis/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/samgeo/automatic_mask_generator/","title":"Automatic mask generator","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, show_image, download_file, overlay_images, tms_to_geotiff\n</pre> import os import leafmap from samgeo import SamGeo, show_image, download_file, overlay_images, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.2659, 37.8682, -122.2521, 37.8741]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.2659, 37.8682, -122.2521, 37.8741] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\") m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\")\n</pre> overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\") <p></p>"},{"location":"examples/samgeo/automatic_mask_generator/#automatically-generating-object-masks-with-sam","title":"Automatically generating object masks with SAM\u00b6","text":"<p>This notebook shows how to segment objects from an image using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p> <p>The notebook is adapted from segment-anything/notebooks/automatic_mask_generator_example.ipynb, but I have made it much easier to save the segmentation results and visualize them.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/","title":"Automatic mask generator hq","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo.hq_sam import (\n    SamGeo,\n    show_image,\n    download_file,\n    overlay_images,\n    tms_to_geotiff,\n)\n</pre> import os import leafmap from samgeo.hq_sam import (     SamGeo,     show_image,     download_file,     overlay_images,     tms_to_geotiff, ) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.2659, 37.8682, -122.2521, 37.8741]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.2659, 37.8682, -122.2521, 37.8741] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\") m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\")\n</pre> overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\") <p></p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#automatically-generating-object-masks-with-hq-sam","title":"Automatically generating object masks with HQ-SAM\u00b6","text":"<p>This notebook shows how to segment objects from an image using the High-Quality Segment Anything Model (HQ-SAM) with a few lines of code.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator_hq/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator_hq/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/samgeo/box_prompts/","title":"Box prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo import SamGeo\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo import SamGeo In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) <p>Display the map. Use the drawing tools to draw some rectangles around the features you want to extract, such as trees, buildings.</p> In\u00a0[\u00a0]: Copied! <pre>m\n</pre> m In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-51.2546, -22.1771, -51.2541, -22.1767],\n        [-51.2538, -22.1764, -51.2535, -22.1761],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-51.2546, -22.1771, -51.2541, -22.1767],         [-51.2538, -22.1764, -51.2535, -22.1761],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m In\u00a0[\u00a0]: Copied! <pre>url = \"https://opengeos.github.io/data/sam/tree_boxes.geojson\"\ngeojson = \"tree_boxes.geojson\"\nleafmap.download_file(url, geojson)\n</pre> url = \"https://opengeos.github.io/data/sam/tree_boxes.geojson\" geojson = \"tree_boxes.geojson\" leafmap.download_file(url, geojson) <p>Display the vector data on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"Image.tif\", layer_name=\"image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"Image.tif\", layer_name=\"image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\") m In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=\"mask2.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=\"mask2.tif\", dtype=\"uint8\") <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask2.tif\", cmap=\"Greens\", nodata=0, opacity=0.5, layer_name=\"Tree masks\")\nm\n</pre> m.add_raster(\"mask2.tif\", cmap=\"Greens\", nodata=0, opacity=0.5, layer_name=\"Tree masks\") m <p></p>"},{"location":"examples/samgeo/box_prompts/#segmenting-remote-sensing-imagery-with-box-prompts","title":"Segmenting remote sensing imagery with box prompts\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/box_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/box_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/box_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/box_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p> <p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p> <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p>"},{"location":"examples/samgeo/box_prompts/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"examples/samgeo/box_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"examples/samgeo/box_prompts/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"examples/samgeo/box_prompts/#use-an-existing-vector-file-as-box-prompts","title":"Use an existing vector file as box prompts\u00b6","text":"<p>Alternatively, you can specify a file path to a vector file. Let's download a sample vector file from GitHub.</p>"},{"location":"examples/samgeo/box_prompts/#segment-image-with-box-prompts","title":"Segment image with box prompts\u00b6","text":"<p>Segment the image using the specified file path to the vector mask.</p>"},{"location":"examples/samgeo/fast_sam/","title":"Fast sam","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial segment-anything-fast\n</pre> # %pip install segment-geospatial segment-anything-fast In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.fast_sam import SamGeo\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.fast_sam import SamGeo In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>from samgeo.fast_sam import SamGeo\n\nsam = SamGeo(model=\"FastSAM-x.pt\")\n</pre> from samgeo.fast_sam import SamGeo  sam = SamGeo(model=\"FastSAM-x.pt\") <p>Set the image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(\"Image.tif\")\n</pre> sam.set_image(\"Image.tif\") <p>Segment the image with <code>everything_prompt</code>. You can also try <code>point_prompt</code>, <code>box_prompt</code>, or <code>text_prompt</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam.everything_prompt(output=\"mask.tif\")\n</pre> sam.everything_prompt(output=\"mask.tif\") <p>Show the annotated image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\"mask.png\")\n</pre> sam.show_anns(\"mask.png\") <p></p> <p>Convert the segmentation results from GeoTIFF to vector.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"mask.tif\", \"mask.geojson\")\n</pre> sam.raster_to_vector(\"mask.tif\", \"mask.geojson\") <p>Show the segmentation results on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", opacity=0.5, layer_name=\"Mask\")\nm.add_vector(\"mask.geojson\", layer_name=\"Mask Vector\")\nm\n</pre> m.add_raster(\"mask.tif\", opacity=0.5, layer_name=\"Mask\") m.add_vector(\"mask.geojson\", layer_name=\"Mask Vector\") m <p></p>"},{"location":"examples/samgeo/fast_sam/#segmenting-remote-sensing-imagery-with-fastsam","title":"Segmenting remote sensing imagery with FastSAM\u00b6","text":"<p>FastSAM: https://github.com/CASIA-IVA-Lab/FastSAM</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/fast_sam/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/fast_sam/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/fast_sam/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/fast_sam/#initialize-samgeo-class","title":"Initialize SamGeo class\u00b6","text":"<p>The initialization of the SamGeo class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/input_prompts/","title":"Input prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, tms_to_geotiff\n</pre> import os import leafmap from samgeo import SamGeo, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p>"},{"location":"examples/samgeo/input_prompts/#generating-object-masks-from-input-prompts-with-sam","title":"Generating object masks from input prompts with SAM\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p> <p>The notebook is adapted from segment-anything/notebooks/predictor_example.ipynb, but I have made it much easier to save the segmentation results and visualize them.</p>"},{"location":"examples/samgeo/input_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/input_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/input_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/input_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/input_prompts/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/samgeo/input_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/samgeo/input_prompts_hq/","title":"Input prompts hq","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo.hq_sam import SamGeo, tms_to_geotiff\n</pre> import os import leafmap from samgeo.hq_sam import SamGeo, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p>"},{"location":"examples/samgeo/input_prompts_hq/#generating-object-masks-from-input-prompts-with-hq-sam","title":"Generating object masks from input prompts with HQ-SAM\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the High-Quality Segment Anything Model (HQ-SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/input_prompts_hq/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/input_prompts_hq/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/input_prompts_hq/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/input_prompts_hq/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/input_prompts_hq/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/samgeo/input_prompts_hq/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/samgeo/maxar_open_data/","title":"Maxar open data","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, raster_to_vector, overlay_images\n</pre> import os import leafmap from samgeo import SamGeo, raster_to_vector, overlay_images In\u00a0[\u00a0]: Copied! <pre>url = (\n    \"https://drive.google.com/file/d/1jIIC5hvSPeJEC0fbDhtxVWk2XV9AxsQD/view?usp=sharing\"\n)\n</pre> url = (     \"https://drive.google.com/file/d/1jIIC5hvSPeJEC0fbDhtxVWk2XV9AxsQD/view?usp=sharing\" ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(url, output=\"image.tif\")\n</pre> leafmap.download_file(url, output=\"image.tif\") In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(height=\"600px\")\nm.add_basemap(\"SATELLITE\")\nm.add_raster(\"image.tif\", layer_name=\"Image\")\nm.add_layer_manager()\nm\n</pre> m = leafmap.Map(height=\"600px\") m.add_basemap(\"SATELLITE\") m.add_raster(\"image.tif\", layer_name=\"Image\") m.add_layer_manager() m <p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p> In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 80,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 80, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(\"image.tif\", output=\"mask.tif\", foreground=True)\n</pre> sam.generate(\"image.tif\", output=\"mask.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>raster_to_vector(\"mask.tif\", output=\"mask.shp\")\n</pre> raster_to_vector(\"mask.tif\", output=\"mask.shp\") In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Display the annotations (each mask with a random color).</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotation.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotation.tif\") In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"image.tif\",\n    \"annotation.tif\",\n    label1=\"Image\",\n    label2=\"Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"image.tif\",     \"annotation.tif\",     label1=\"Image\",     label2=\"Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(\"image.tif\", \"annotation.tif\", backend=\"TkAgg\")\n</pre> overlay_images(\"image.tif\", \"annotation.tif\", backend=\"TkAgg\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", layer_name=\"Mask\", nodata=0)\nm.add_raster(\"annotation.tif\", layer_name=\"Annotation\")\nm\n</pre> m.add_raster(\"mask.tif\", layer_name=\"Mask\", nodata=0) m.add_raster(\"annotation.tif\", layer_name=\"Annotation\") m In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"mask.shp\", layer_name=\"Vector\", info_mode=None)\n</pre> m.add_vector(\"mask.shp\", layer_name=\"Vector\", info_mode=None) <p></p>"},{"location":"examples/samgeo/maxar_open_data/#segmenting-satellite-imagery-from-the-maxar-open-data-program","title":"Segmenting satellite imagery from the Maxar Open Data Program\u00b6","text":"<p>This notebook shows how to segment satellite imagery from the Maxar Open Data program for Libya floods.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/maxar_open_data/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/maxar_open_data/#download-sample-data","title":"Download sample data\u00b6","text":"<p>First, let's download a sample image of Derna, Libya from here.</p>"},{"location":"examples/samgeo/maxar_open_data/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#segment-the-image","title":"Segment the image\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#convert-raster-to-vector","title":"Convert raster to vector\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#display-the-segmentation-result","title":"Display the segmentation result\u00b6","text":"<p>First, let's show the result as a binary image.</p>"},{"location":"examples/samgeo/maxar_open_data/#compare-images-with-a-slider","title":"Compare images with a slider\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#display-images-on-an-interactive-map","title":"Display images on an interactive map.\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/","title":"Satellite predictor","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeoPredictor, tms_to_geotiff, get_basemaps\nfrom segment_anything import sam_model_registry\n</pre> import os import leafmap from samgeo import SamGeoPredictor, tms_to_geotiff, get_basemaps from segment_anything import sam_model_registry In\u00a0[\u00a0]: Copied! <pre>zoom = 16\nm = leafmap.Map(center=[45, -123], zoom=zoom)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> zoom = 16 m = leafmap.Map(center=[45, -123], zoom=zoom) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-123.0127, 44.9957, -122.9874, 45.0045]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-123.0127, 44.9957, -122.9874, 45.0045] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\n# image = '/path/to/your/own/image.tif'\n</pre> image = \"satellite.tif\" # image = '/path/to/your/own/image.tif' <p>Besides the <code>satellite</code> basemap, you can use any of the following basemaps returned by the <code>get_basemaps()</code> function:</p> In\u00a0[\u00a0]: Copied! <pre># get_basemaps().keys()\n</pre> # get_basemaps().keys() <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>tms_to_geotiff(\n    output=image, bbox=bbox, zoom=zoom + 1, source=\"Satellite\", overwrite=True\n)\n</pre> tms_to_geotiff(     output=image, bbox=bbox, zoom=zoom + 1, source=\"Satellite\", overwrite=True ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.add_raster(image, layer_name=\"Image\") m <p>Use the draw tools to draw a rectangle from which to subset segmentations on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    clip_box = m.user_roi_bounds()\nelse:\n    clip_box = [-123.0064, 44.9988, -123.0005, 45.0025]\n</pre> if m.user_roi_bounds() is not None:     clip_box = m.user_roi_bounds() else:     clip_box = [-123.0064, 44.9988, -123.0005, 45.0025] In\u00a0[\u00a0]: Copied! <pre>clip_box\n</pre> clip_box In\u00a0[\u00a0]: Copied! <pre>out_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\ncheckpoint = os.path.join(out_dir, \"sam_vit_h_4b8939.pth\")\n</pre> out_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\") checkpoint = os.path.join(out_dir, \"sam_vit_h_4b8939.pth\") In\u00a0[\u00a0]: Copied! <pre>import cv2\n\nimg_arr = cv2.imread(image)\n\nmodel_type = \"vit_h\"\n\nsam = sam_model_registry[model_type](checkpoint=checkpoint)\n\npredictor = SamGeoPredictor(sam)\n\npredictor.set_image(img_arr)\n\nmasks, _, _ = predictor.predict(src_fp=image, geo_box=clip_box)\n</pre> import cv2  img_arr = cv2.imread(image)  model_type = \"vit_h\"  sam = sam_model_registry[model_type](checkpoint=checkpoint)  predictor = SamGeoPredictor(sam)  predictor.set_image(img_arr)  masks, _, _ = predictor.predict(src_fp=image, geo_box=clip_box) In\u00a0[\u00a0]: Copied! <pre>masks_img = \"preds.tif\"\npredictor.masks_to_geotiff(image, masks_img, masks.astype(\"uint8\"))\n</pre> masks_img = \"preds.tif\" predictor.masks_to_geotiff(image, masks_img, masks.astype(\"uint8\")) In\u00a0[\u00a0]: Copied! <pre>vector = \"feats.geojson\"\ngdf = predictor.geotiff_to_geojson(masks_img, vector, bidx=1)\ngdf.plot()\n</pre> vector = \"feats.geojson\" gdf = predictor.geotiff_to_geojson(masks_img, vector, bidx=1) gdf.plot() In\u00a0[\u00a0]: Copied! <pre>style = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(vector, layer_name=\"Vector\", style=style)\nm\n</pre> style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(vector, layer_name=\"Vector\", style=style) m"},{"location":"examples/samgeo/satellite-predictor/#segment-anything-model-for-geospatial-data","title":"Segment Anything Model for Geospatial Data\u00b6","text":"<p>This notebook shows how to use segment satellite imagery using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/satellite-predictor/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/satellite-predictor/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"examples/samgeo/satellite-predictor/#initialize-samgeopredictor-class","title":"Initialize SamGeoPredictor class\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/samgeo/satellite/","title":"Satellite","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, tms_to_geotiff, get_basemaps\n</pre> import os import leafmap from samgeo import SamGeo, tms_to_geotiff, get_basemaps In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[29.676840, -95.369222], zoom=19)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[29.676840, -95.369222], zoom=19) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-95.3704, 29.6762, -95.368, 29.6775]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-95.3704, 29.6762, -95.368, 29.6775] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\n</pre> image = \"satellite.tif\" <p>Besides the <code>satellite</code> basemap, you can use any of the following basemaps returned by the <code>get_basemaps()</code> function:</p> In\u00a0[\u00a0]: Copied! <pre># get_basemaps().keys()\n</pre> # get_basemaps().keys() <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>tms_to_geotiff(output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True)\n</pre> tms_to_geotiff(output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False  # turn off the basemap\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False  # turn off the basemap m.add_raster(image, layer_name=\"Image\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    checkpoint=\"sam_vit_h_4b8939.pth\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     checkpoint=\"sam_vit_h_4b8939.pth\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>mask = \"segment.tif\"\nsam.generate(\n    image, mask, batch=True, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255\n)\n</pre> mask = \"segment.tif\" sam.generate(     image, mask, batch=True, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255 ) In\u00a0[\u00a0]: Copied! <pre>vector = \"segment.gpkg\"\nsam.tiff_to_gpkg(mask, vector, simplify_tolerance=None)\n</pre> vector = \"segment.gpkg\" sam.tiff_to_gpkg(mask, vector, simplify_tolerance=None) <p>You can also save the segmentation results as any vector data format supported by GeoPandas.</p> In\u00a0[\u00a0]: Copied! <pre>shapefile = \"segment.shp\"\nsam.tiff_to_vector(mask, shapefile)\n</pre> shapefile = \"segment.shp\" sam.tiff_to_vector(mask, shapefile) In\u00a0[\u00a0]: Copied! <pre>style = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(vector, layer_name=\"Vector\", style=style)\nm\n</pre> style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(vector, layer_name=\"Vector\", style=style) m <p></p>"},{"location":"examples/samgeo/satellite/#segment-anything-model-for-geospatial-data","title":"Segment Anything Model for Geospatial Data\u00b6","text":"<p>This notebook shows how to use segment satellite imagery using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/satellite/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/satellite/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/samgeo/satellite/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/satellite/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"examples/samgeo/satellite/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/samgeo/satellite/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Set <code>batch=True</code> to segment the image in batches. This is useful for large images that cannot fit in memory.</p>"},{"location":"examples/samgeo/satellite/#polygonize-the-raster-data","title":"Polygonize the raster data\u00b6","text":"<p>Save the segmentation results as a GeoPackage file.</p>"},{"location":"examples/samgeo/satellite/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/samgeo/swimming_pools/","title":"Swimming pools","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[34.040984, -118.491668], zoom=19, height=\"600px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[34.040984, -118.491668], zoom=19, height=\"600px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-118.4932, 34.0404, -118.4903, 34.0417]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-118.4932, 34.0404, -118.4903, 34.0417] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"swimming pool\"\n</pre> text_prompt = \"swimming pool\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Blues\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Swimming Pools\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Blues\",     box_color=\"red\",     title=\"Automatic Segmentation of Swimming Pools\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Blues\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Swimming Pools\",\n)\n</pre> sam.show_anns(     cmap=\"Blues\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Swimming Pools\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Swimming Pools\",\n    blend=False,\n    output=\"pools.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Swimming Pools\",     blend=False,     output=\"pools.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"pools.tif\", \"pools.shp\")\n</pre> sam.raster_to_vector(\"pools.tif\", \"pools.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"pools.tif\", layer_name=\"Pools\", palette=\"Blues\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"pools.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"pools.tif\", layer_name=\"Pools\", palette=\"Blues\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"pools.shp\", layer_name=\"Vector\", style=style) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/samgeo/swimming_pools/#mapping-swimming-pools-with-text-prompts","title":"Mapping swimming pools with text prompts\u00b6","text":"<p>This notebook shows how to map swimming pools with text prompts and the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/swimming_pools/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/swimming_pools/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/swimming_pools/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/swimming_pools/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/swimming_pools/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/samgeo/swimming_pools/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/samgeo/swimming_pools/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/samgeo/swimming_pools/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/samgeo/text_prompts/","title":"Text prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"trees.tif\", \"trees.shp\")\n</pre> sam.raster_to_vector(\"trees.tif\", \"trees.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style) m In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/samgeo/text_prompts/#segmenting-remote-sensing-imagery-with-text-prompts-and-the-segment-anything-model-sam","title":"Segmenting remote sensing imagery with text prompts and the Segment Anything Model (SAM)\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/text_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/text_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/text_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/text_prompts/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/text_prompts/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/samgeo/text_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/samgeo/text_prompts/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/samgeo/text_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/","title":"Text prompts batch","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff, split_raster\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff, split_raster from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.1278, -51.4430], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.1278, -51.4430], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.4494, -22.1307, -51.4371, -22.1244]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.4494, -22.1307, -51.4371, -22.1244] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>split_raster(image, out_dir=\"tiles\", tile_size=(1000, 1000), overlap=0)\n</pre> split_raster(image, out_dir=\"tiles\", tile_size=(1000, 1000), overlap=0) In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict_batch(\n    images=\"tiles\",\n    out_dir=\"masks\",\n    text_prompt=text_prompt,\n    box_threshold=0.24,\n    text_threshold=0.24,\n    mask_multiplier=255,\n    dtype=\"uint8\",\n    merge=True,\n    verbose=True,\n)\n</pre> sam.predict_batch(     images=\"tiles\",     out_dir=\"masks\",     text_prompt=text_prompt,     box_threshold=0.24,     text_threshold=0.24,     mask_multiplier=255,     dtype=\"uint8\",     merge=True,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"masks/merged.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm.add_layer_manager()\nm\n</pre> m.add_raster(\"masks/merged.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m.add_layer_manager() m <p></p>"},{"location":"examples/samgeo/text_prompts_batch/#batch-segmentation-with-text-prompts","title":"Batch segmentation with text prompts\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/text_prompts_batch/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/text_prompts_batch/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/text_prompts_batch/#split-the-image-into-tiles","title":"Split the image into tiles\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/text_prompts_batch/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/#segment-images","title":"Segment images\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/samgeo/text_prompts_batch/#visualize-the-results","title":"Visualize the results\u00b6","text":""}]}